TOPIC BASED LANGUAGE MODELS USING EM Daniel Gildea and Thomas Hofmann University of California Berkeley and International Computer Science Institute 1947 Center Street Berkeley California gildea hofmann icsi berkeley edu history h for notational convenience all parameters are summarized in a vector A graphical model representation that emphasizes the bottleneck principle of the topic variable is depicted in Figure 1 ABSTRACT In this paper we propose a novel statistical language model to capture topic related long range dependencies Topics are modeled in a latent variable framework in which we also derive an EM algorithm to perform a topic factor decomposition based on a segmented training corpus The topic model is combined with a standard language model to be used for on line word prediction Perplexity results indicate an improvement over previously proposed topic models which unfortunately has not been translated into lower word error 1 INTRODUCTION The goal of statistical language models is to assign probabilities to sequences of words and their most prominent application is in speech recognition where language models provide prior probabilities that help in disambiguating acoustically similar utterances By virtue of the chain rule it is sufficient to estimate the probability P wi jhi of a word wi conditioned on the history of i 1 The main challenge in language preceding words hi w1 modeling is to deal with the combinatorial growth in the number of possible histories which implies a data sparseness problem and prevents a straightforward empirical estimation of the required conditional probabilities A simple but commonly applied strategy is to make a n th order Markov approximation i 1 P wi jhi P wi jwi n 1 which yields the class of n gram language models where typically n trigrams While trigram models and variants thereof have proven hard to improve upon they are unable to take advantage of longrange dependencies in natural language Several more recent approaches attempt to overcome this limitation Variable order models 15 adjust the length of the utilized contexts dynamically dependent on the available training data Cache models 13 3 increase the probability for words observed in the history e g by some factor which decays exponentially with distance Trigger models 16 are more general in that they allow to incorporate arbitrary word trigger pairs which are combined in an exponential model Grammar based techniques 12 2 exploit syntactical regularities to model long range dependencies Finally in topic mixture models 11 a number of language models e g n grams are trained on documents of various topics and are then combined at runtime Our approach is closely related to the latter class of topic mixtures in that the proposed model is based on a topic decomposition 9 History 11111 00000 00000 11111 00000 11111 h 00000 11111 00000 11111 Word 00000 11111 t Topic 11111 00000 00000 11111 w 00000 11111 00000 11111 Figure 1 Graphical model representation of the topic factor model The main difference to clustering approaches like the one proposed in 11 is that we do not assume that each document or history belongs to exactly one topic cluster Our approach is based on the less restrictive assumption of a low dimensional approximation in terms of a linear combination of a small number of topic factors A similar approach to language modeling based on a dimension reduction technique known as Latent Semantic Analysis LSA 7 has been proposed in 1 a detailed implementation is provided in 4 Yet compared to the LSA approach that makes use of Singular Value Decomposition techniques our method has the crucial advantage of a strict probabilistic interpretation cf 9 a fact that will be further discussed in Section 4 The model we describe here does not make use of syntax and ignores the order in which words appear In fact implicit in 1 is the simplifying assumption that the influence of different topics on the statistical properties of language is limited to the level of single words unigrams 1 Local regularities can be taken into account at a subsequent stage where we combine the topic model with a standard n gram language model We believe the model might also be profitable combined with a more sophisticated syntactic model such as those mentioned above 2 TOPIC DECOMPOSITION BY EM The topics used by our model are not taken from a predefined hand labeled hierarchy but rather emerge in a data driven manner from the statistics of a corpus of training documents d 2 D Based on the unigram assumption the data is reduced to simple word counts n w d of how often a word w was observed in a particular document d All word counts can be summarized in the term document matrix As a training criterion we utilize the log likelihood i e the log probability of the data under the model 1 3 P wjh X P wjtP tjh t N 1 l N X X nw dlog X P wjtP tjd w d t 2 Here t is a latent class variable that is supposed to refer to different topics P wjt are topic specific word probabilities or topic factors and P tjh are mixing proportions that depend on the 1 This is not a principled limitation of our model yet it offers significant advantages in terms of computational complexity In the training procedure the number of topics i e the number of values the latent variable t can take is predetermined and the parameters P wjt and P tjd are fitted by the Expectation Maximization EM algorithm 8 Starting from randomly initialized values for the parameters this involves the standard procedure of alternating two computational steps the E step to calculate the posterior probability of the latent variables for given parameters and the M step in which the parameters are re estimated The E step amounts to calculating the probability that a particular word w in a document d was generated by the topic factor t For the rth iteration Bayes rule yields history hi and the n gram context are independent conditioned on wi the following approximation formula cen be derived i 1 P wi jhi wi n 1 P r tjw d jtP r tjd PtPPr r ww jt0 P r t0 jd 1 1 0 1 1 3 Of course this assumption is not valid in general as the n word context of the n gram model is part of the history which implies that they are not even marginally independent In our experiments we have also evaluated two alternative interpolation methods of combining the n gram and the topic based model by averaging the respective probabilities i on the linear scale and ii on the log scale Both averaging schemes require an additional interpolation weight Notice that the approximation by 8 as well as log averaging require a re normalization step 4 EXPERIMENTS 4 1 Experimental Results on TDT 1 i 1 P wi jwi n 1 P wi jhi P wi 8 1 The M step adjusts the model parameters given the values for the latent variables calculated in the previous E step P r wjt Pd nw dP r tjw d Pw P nw0 dP r tjw0 d d Pw nw dP r tjw d P r tjd P P r 0 t w nw dP t jw d 0 4 5 0 In our experiments we used a modified annealed E step cf 9 to prevent overfitting This amounts to introducing an exponent to discount the likelihood contribution in 3 0 1 3 USING THE MODEL FOR TESTING During testing the P tjd distributions computed for the training documents can be discarded as they will not apply to new documents used for testing Rather we examine all the words seen so far in the document and calculate an estimate of P tjh for the current history using only the topic factors P wjt The mixing proportions P tjh can be determined during testing by holding the probabilities P wjt constant while estimating P tjh and iterating 3 and 5 only over the words seen previously in the current document Rather than doing the full EM calculation for P tjhi at each step during testing we use an online approximation calculated as follows For our initial experiments we used the TDT 1 corpus of newspaper text and transcribed broadcast news stories The corpus contains 6 797 659 words in 15 862 documents We formed our vocabulary by selecting all words occurring at least twice which gave a vocabulary of 49 225 words The data was augmented with sentence beginning and ending markers but the symbols themselves were not counted in calculating perplexities We used of the data as a training set holding out every 10th article for use in testing In a first series of experiments we investigated the different schemes to combine the topic based model with a conventional n gram built from the same training data The following table describes our test results on 4274 words comprising 10 stories The number of factors in the topic model was 256 a restriction which was made due to complexity considerations Although allowing a larger number of factors could in principle lead to overfitting we found in practice that by using the control parameter the number of topics could be increased with no drop in test set performance 90 0 wi jtP tjhi i P tjhi 6 PtPP wi jt0 P t0 jhi i 1 Pw d nw d P tjd Pw d P tjh P t 7 nw d P tjhi 1 i 1 1 1 1 1 Model Unigram Topic model Trigram Linear interpolation Ptri Ptopic P 1 Log scale interpolation Ptri topic Ptopic Unigram rescaling Ptri Punigram 1 Perplexity 1140 6 829 1 205 2 189 2 180 8 170 1 Table 1 Results on the TDT 1 corpus The rescaling method does not require an additional parameter fit and is nevertheless consistently superior than the interpolation schemes with optimized for linear and for log scale interpolation Using rescaling a reduction of 17 in perplexity was achieved over the trigram model which is relatively close to the 27 reduction from overall unigram to topicbased unigram perplexities To get a more reliable estimate of the model s perplexity we ran the unigram scaling method over a larger test set of 24 850 words in 50 stories Trigram perplexity was 180 8 whereas the combined model s perplexity was 147 2 a reduction of 18 6 Perplexity reductions on individual stories ranged from 8 to 36 Reductions on the five groups of ten stories ranged from 17 1 to 20 3 The improved perplexity results come with an increase in the computational load normalizing over the vocabulary makes the computation of probabilities with the combined model slow This is essentially an online EM algorithm of the type discussed in 14 but here only a single iteration is performed reducing the computational complexity in the test stage to a minimum Experiments using full EM iterations showed negligible improvements with higher computational costs Once the topic mixing proportions P tjh have been determined word probabilities can be calculated according to 1 As mentioned in the introduction the topic model does not take advantage of short range syntactic structure Thus we propose to combine the topic model with a standard language model which contributes a different type of information For simplicity we focus on combining it with a n gram model The combination scheme we favor is based on an intuition from maximum entropy model fitting by Iterated Proportional Scaling 6 We interpret the topic model probabilities as marginal word distributions that should be preserved in the combined model while leaving the higher order structure unaffected Under the assumption that the 09 08 Average log ratio of topic to ngram prob effectively increasing the complexity by a factor of the vocabulary size Running on a 296MHz Ultrasparc roughly 10 words could be processed per minute while the evaluation of a trigram model alone consists primarily of simple table lookups and can process thousands of words per second Experiments with a reduced 20 000 word vocabulary achieved a rate of 2 words per second 4 2 Perplexity Results on the Wall Street Journal Corpus In order to compare the performance of our probabilistic topic model with models based on standard LSA we performed experiments using the same training test data as in 4 The training data consisted of 29 327 337 words in 81 553 articles taken from the Wall Street Journal from 1987 1988 and 1989 The development test data consisted of 159 632 words from the same years and the final test data consisted of 234 120 words from 1995 and 1996 We used the 19 979 word vocabulary provided with the corpus augmented with sentence markers Perplexity results are shown in Table 2 Model Unigram Topic Bigram Bigram Topic Trigram Trigram Topic Dev Test Perpl Change 1046 8 621 1 41 174 3 134 5 20 108 8 89 8 17 Perpl 1107 7 681 9 235 5 187 3 171 0 143 7 Test Change 38 20 16 2 1 5 1 0 5 0 0 5 0 10 10 1 10 10 Position in word frequency list 2 3 10 4 10 5 Figure 2 Relative performance of the topic model by word frequency 0 4 0 3 Average log ratio of topic to ngram prob 0 2 0 1 0 0 1 0 2 Table 2 Results on the Wall Street Journal corpus The 20 improvement over bigram perplexity is significantly higher than the 12 improvement reported by 4 on the same data This stresses the advantage of our probabilistic factor model that has also been verified in other applications 9 10 4 3 Analysis When Does the Model Help It is interesting to consider which words the topic model helps in predicting One might expect that because extremely common function words such as and of and the occur with approximately equal frequency in all documents so that the topic model would be of little use in predicting them In order to test this hypothesis we calculated for each vocabulary item in our test data the average log ratio of the probabilities assigned by the two models 0 3 0 10 10 1 10 Position in story 2 10 3 10 4 Figure 3 Relative performance of the topic model by position in story for the standard method vs 89 4 for the function word method However the function word method has an beneficial side effect in terms of computational complexity because it avoids the costly normalization when evaluating a function word Another interesting way of analyzing the performance of the model is to look at how well it performs as a function of how many words of history are provided to the topic model Such a graph is shown in Figure 3 As expected the longer the history the more reliable the estimate of the article s topic and the better the performance The data also show that the combined model performs worse then the trigram for word 2 through 5 of a story The gain from the topic model plateaus after the 19th word in the story 5 APPLICATION TO SPEECH RECOGNITION RESULTS ON BROADCAST NEWS In order to determine how effective the topic based language model is in a real world application we put it to use in a large vocabulary continuous speech recognition system We used the SPRACH recognition system for broadcast news described in detail in 5 For this experiment we combined the trigram language model with the topic based language model The topic model used for the Broadcast News experiments was trained on 1996 CSR Hub 4 Language Model corpus collected by the Linguistic Data Consortium For efficiency the 100 most frequent words were removed from the training data After removing these words the corpus training set consisted of 60 328 305 words spread over 124 814 documents The trigram used had a vocabulary of 65 432 words however for efficiency the topic model was trained on a vocabulary of only the 20 000 most com N 1 X log Ptopic i ngram wi Pngram wi One simple approximation of the distinction between function and content words is a word s overall frequency We grouped vocabulary items by frequency to examine the correlation between the improvement yielded by the topic model and the word frequency Results are shown in Figure 2 As can be seen the topic based language model actually performs less well than a trigram for roughly the 100 most frequent words in the data Grouping words by their entropy over documents rather than frequency yields similar results These results suggest a simple modification to better handle function words for function words use the n gram probability directly for other words combine the topic and n gram probabilities as before but now normalized only over the non function words The normalization constant is chosen such that the probability assigned to all non function words by the combined model is the same as with the n gram Experiments showed that this approach did not in fact significantly lower the perplexity 89 8 mon words which covers 98 of the data Perplexity results were calculated both on test data from the CSR Hub 4 Language Model corpus and from two episodes of broadcast news for which acoustic data were available These complete episodes were segmented into stories by hand The trigram training data included the broadcast news transcripts used in training the topic model as well as newswire text for a total of roughly 450 million words Perplexity results are shown in Table 3 Sprach 98 Trigram 155 6 228 4 224 0 Topic Trigram 134 3 14 205 0 10 194 7 13 words in test set 76 260 3412 7554 Acknowledgments Daniel Gildea was supported by a National Defense Science and Engineering Graduate Fellowship Thomas Hofmann was supported by a DAAD postdoctoral fellowship 7 REFERENCES 1 J Bellegarda A latent semantic analysis framework for large span language modeling In Eurospeech 97 Rhodes Greece September 1997 2 C Chelba and F Jelinek Exploiting syntactic structure for language modeling In COLING ACL 1998 3 P R Clarkson and A J Robinson Language model adaption using mixtures and an exponentially decaying cache In IEEE ICASSP 97 pages LDC test data CNN episode A CNN episode B Table 3 Perplexity results on Broadcast News The percentage improvement over trigram perplexity is not as high as achieved with the WSJ corpus probably because the trigram used in the WSJ experiments was trained on a relatively small amount of data This result does show however that the topic model can still provide significant improvement over a state of the art trigram model The improvement on the handsegmented episode was smaller no doubt due to the large number of extremely short aritcles in the data Both of these shows contain many short headlines summing up the news of the day something not found in the HUB 4 traiing data Recognition results on CNN episode A actually deteriorated from 35 6 WER with the trigram model to 36 5 with the topic model combined with the trigram One reason this topicbased model may not help as much as perplexity gains would indicate is that the model would tend to improve performance on longer content words which are more easily acoustically distinguishable by the recognizer to begin with In order to test this hypothesis we categorized the recognizer s error according to the word s frequency Frequent Rare Out of Vocab Insertions topic w e r 27 4 27 0 73 8 276 3gram w e r 26 2 27 4 72 1 244 words 1474 1816 122 Table 4 Word error rate by word frequency Table 4 shows results broken down into the 100 most frequent words the words for which trigram probabilities are used the remaining words in the topic model s vocabulary words out of the topic model s vocabulary and insertions This analysis shows that the shorter frequent words are not in fact harder to recognize 