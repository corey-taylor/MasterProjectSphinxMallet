IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING VOL 20 NO 5 JULY 2012 1513 Topic Dependent Class Based n Gram Language Model Welly Naptali Masatoshi Tsuchiya and Seiichi Nakagawa Member IEEE Abstract A topic dependent class TDC based gram language model LM is a topic based LM that employs a semantic extraction method to reveal latent topic information extracted from noun noun relations A topic of a given word sequence is decided on the basis of most frequently occuring weighted noun classes in the context history through voting Our previous work W Naptali M Tsuchiya and S Seiichi Topic dependent language model with voting on noun history ACM Trans Asian Language Information Processing TALIP vol 9 no 2 pp I INTRODUCTION I T has been decades since a statistical gram dominated the usage of language models LMs in automatic speech recognition ASR systems A word based gram uses only Manuscript received June 13 2011 revised September 11 2011 and December 22 2011 accepted December 22 2011 Date of publication January 11 2012 date of current version March 14 2012 This work was supported in part by the Global COE Program Frontiers of Intelligent Sensing from the Ministry of Education Culture Sports Science and Technology Japan The associate editor coordinating the review of this manuscript and approving it for publication was Dr Gokhan Tur W Naptali is with the with the Academic Center for Computing and Media Studies Kyoto University Kyoto 606 8501 Japan e mail naptali ar media kyoto u ac jp M Tsuchiya is with the Department of Information and Multimedia Center Toyohashi University of Technology Toyohashi 441 8580 Japan e mail tsuchiya imc tut ac jp S Nakagawa is with the Department of Computer Science and Engineering Toyohashi University of Technology Toyohashi 441 8580 Japan e mail nakagawa slp cs tut ac jp Digital Object Identifier 10 1109 TASL 2012 2183870 word level information from the context history to predict the next current word Many researchers have tried to alleviate this problem by adding more information Unsupervised class based LMs such as Random Forest LM 35 Model M 18 have been shown to outperform the word based gram However humans try to recognize speech in the context of the topic of the speech Without knowing the topic they may not be able to understand the conversation Incorporating topic information into gram was shown to improve the LM performance either by using the simplest method building several topic specific gram LMs and interpolating it altogether 23 20 19 or by using a more advance method employing a semantic analysis 3 17 4 26 27 33 Previously we have proposed a novel topic based LM named a topic dependent class TDC 29 In terms of perplexity its performance is better than several state of the art baselines such as word based and or class based gram LMs a cache based LM an gram based topic dependent LM and a latent Dirichlet allocation LDA based topic dependent LM The model is based on the belief that noun relations contain latent topic information Hence a semantic extraction method is employed along with a clustering method to reveal and define topics based on nouns only Given a word sequence a fixed size window is used to observe noun occurrences in the context history to decide the topic through voting Finally the topic is integrated as a part of the word sequence in the gram model The process is illustrated by Fig 1 As the number of topics increases the TDC standalone model suffers from a shrinking training corpus 29 Therefore to achieve good resuls the TDC model needs to be interpolated with a word based gram as a general model Although we have solved it by introducing soft voting in the test phase the model can be still improved by performing soft voting in the training phase Soft voting in the test phase converts the TDC model into a TDC mixture model whereas soft voting in the training phase increases the number of word sequences in topics We also introduce soft clustering in the training and test phases Soft clustering makes topic definition more reliable because it permits a noun to belong to multiple topics A common approach to improve an LM is by interpolating it with another LM that captures a different property of the language 6 Chueh and Chien 11 10 constructed an LM that combined information regarding languages from two topics one of which was obtained from the gram history through the Dirichlet distribution 9 and the other from the outer context of gram history topic cache through the multinomial distribution Tam and Schultz 32 also present a topic caching approach via LDA To further improve our TDC model we in 1558 7916 31 1514 IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING VOL 20 NO 5 JULY 2012 Fig 2 LSA illustration on matrix decomposition and dimension reduction Fig 1 Topic dependent class based n gram language model diagram corporate a cache based LM through unigram scaling Unigram scaling dynamic marginal was first introduced by Kneser et al 24 as a special case of the Minimum Discriminant Estimation adaptation with unigram constraints Its aim is to obtain a new LM so that the probability distribution satisfies some constraints and minimizes its relative entropy Kullback Leibler distance against the baseline LM Because the TDC models capture topical words while the cache based gram models consider reoccurring words their combination should improve the performance of the LM To complete our evaluation of the TDC LM for ASR besides reporting its perplexity we also perform automatic speech recognition experiments because perplexity does not always agree with the word recognition accuracy 22 We provide a complete TDC analysis in terms of word error rate WER on rescoring task The evaluation on both perplexity and WER was performed using two language resources they are Wall Street Journal WSJ for English and Mainichi Shimbun newspaper for Japanese In this paper there are three key differences compared to our previous work First we improve TDCs by employing soft clustering and or soft voting techniques which solve data shrinking problems and make TDCs independent of the word based gram in the training and or test phases Second for further improvement we incorporate a cache based LM through unigram scaling because the TDC and cache based LM capture different properties of the language Finally we provide an evaluation in terms of the word error rate WER and an analysis of the automatic speech recognition ASR rescoring task The remainder of this paper is organized as follows In Section II we describe our proposed TDC LM with explanations for soft clustering and soft voting and discuss the incorporation of the cache based gram This is followed in Section III by the evaluation of perplexity in the English and Japanese corpora Section IV provides the evaluation of the WER in the English and Japanese systems Finally the paper ends with conclusions and a discussion of possible future work importance of nouns can also be found in automatic summarization 1 5 18 However it will depend on the application it did not work well for automatic sentence clustering for multi document summarization 16 Unlike words topics are unobservable The relation between nouns is a supporting factor to define topics Latent semantic analysis LSA 3 is employed to reveal these hidden relations to define topics LSA extracts semantic relations from a corpus and maps them into a semantic vector space Discrete indexed words are projected into the LSA space by applying singular value decomposition SVD to a matrix representing the corpus A noun document matrix is used to represent the training rows correspond to nouns and columns corpus in which to documents Let be a representation matrix of dimension The SVD decomposes into three other matrices and 1 Because the dimensionality of the sowhere lution is very large for computing resources and the original matrix is presumed to be noisy dimensions of LSA matrices and are set to be smaller than those of the original 2 and is the best least square fit approximation to where The resulting matrix corresponds with the rows of matrix and matrix corresponds with the columns of matrix see Fig 2 These LSA matrices are used to project words into the dimension LSA vector space We apply a term frequency inverse document frequency tfidf in each document weight to each noun 3 4 5 where is the total number of documents After applying SVD the resulting matrices and contain information regarding words and documents respectively Thus is used to project nouns into the LSA space According to the following equation each noun can be mapped into an dimensional vector space for 6 II TOPIC DEPENDENT CLASS BASED GRAM LM This model is based on the assumption that nouns in a sentence play an important role in the whole discourse and are the core of the underlying LM Chen 7 shows the importance of and is where is a projection matrix of dimension a discrete vector of the noun where the th element of the vector is set to 1 and all other elements are set to 0 For the discrete vector for word is instance if NAPTALI et al TDC BASED GRAM LM 1515 Fig 3 Illustration of hard clustering and k best soft clustering Fig 4 Illustration of hard voting and l best soft voting 0 0 0 1 0 To simplify a continuous vector for word represented by the th row vector of so that each word a continuous vector for is has where serving 7 is the topic class obtained by obwords in outer contexts of the near gram especially nouns Formally can be written as 11 is the voting score for a given window size where defined as follows 12 where if otherwise 13 is a vector representing the noun any familiar Because clustering method can be applied to form semantically similar noun classes which in turn define these clusters as topics A Soft Clustering In the LSA space VQ 14 is applied to cluster these words into topics A VQ algorithm is iterated using the cosine similarity between nouns until the desired number of clusters topics is reached A code centroid word in a VQ codebook corresponds to a topic vector A confidence measure is defined as the distance between a word vector and its class can be calculated using centroid Thus in this case and its topic the same cosine similarity between the noun class for 8 is the word vector mapped into the LSA space of word where and is the centroid vector of the topic class for 9 where is the number of topics The score indicates how confident a noun has to be in the topic class The larger the score is the more typical a word is in the class Previously 29 based on the score we mapped each noun into only one topic class This is known as a hard clustering technique Mapping a word to only one topic is very strict and could be dangerous because in nature a word may belong to multiple topics To make this model more robust soft clustering is performed so that a noun may belong to multiple topics We use a fixed number based on the largest score to map each into classes Fig 3 noun B Soft Voting A TDC with window size probability of a word sequence by leads to an LM in which the is defined Note that and are defined only for nouns otherwise 0 is a assigned If there are no nouns inside the window is defined dummy topic class Equation 11 is known as a hard voting decision In other words we construct the topic dependent gram LM of types Deciding if a given word sequence belongs to a topic or not is also very strict and could hurt the performance of the LM Therefore instead of choosing the best topic we may choose best topics soft voting for a given word sequence Fig 4 Soft voting in the training phase increases the number of word sequences in topics and by extension virtually increases the size of the training corpus Whereas soft voting in the test phase converts the TDC model into the TDC mixture model and 10 becomes TDC 14 where by is the th topic of the best topics obtained 15 and to is a mixture weight for th topic calculated in according TDC 10 16 1516 IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING VOL 20 NO 5 JULY 2012 Fig 6 Backoff of the TDC model The TDC 3 gram backoff to the TDC 2 gram then to the TDC 1 gram and then to the word based 1 gram where the shaded area is a window size of m Fig 5 TDC illustration with soft clustering and soft voting where is a voting score defined by 17 Given a word sequence we look for the nouns inside the window according to the 17 then we map each of these nouns into best classes For number of nouns there are classes in total All of these classes are weighted by The weights for the same class is summed up at most there are unique classes and choose the best topics according to 15 The mixture of these topics was calculated according to the score calculated in the previous step by 16 Finally the TDC is formed by 14 The whole process is illustrated in Fig 5 C TDC Backoff In an gram LM when the model encounters unseen events gram In our it is usually backed off by the shorter model we follow a similar approach in handling unseen events We use the Katz backoff with an absolute discounting method is seen in the training If the sequence where is a count dataset or then TDC 18 close to the predicted word it contains more infor The idea is similar to mation than the distant word hierarchical backoff 39 and decaying cache based model 12 to topic class then incorporate we backed off the word the topic class to the voting for topic decision and discard the A preliminary experiment topic class of distant word was conducted and the result shows that eliminating the distant in the backoff process gives better perplexity word than eliminating the close word probability can be calculated from the The discounted training corpus by using the following equation 20 where is the observed frequency of a particular sequence and is the discounting coefficient factor We use the absolute discounting method to determine 21 is a constant value which usually equals to discount the same amount of probability as the 22 The history otherwise TDC 19 is the discounted probability and is the backoff where weight Note that the backoff method is performed not by elimbut by sliding the window from inating word to Finally if there is no i e then the TDC 1 gram will be backed off to the statistical word based 1 gram See Fig 6 for the TDC backoff illustration This approach was taken because of word on is a shorter history of This is done by releases the last sliding the window so that the window word and adding the word into the window Recording these types of events is comto become putationally expensive therefore because the window size is quite large a topic switch rarely occurs Thus it is assumed that such a word exchange does not affect the topic switch to a great extent or NAPTALI et al TDC BASED GRAM LM 1517 D Interpolation A common way to improve an LM is by combining two or more LMs that capture different properties of the language 6 Here we combine the TDC with a word based gram and a cache based LM 1 Word Based Gram The word based gram LM 21 is the most common LM currently used in ASR systems It is a simple and powerful method based on the assumption that the preceding words Given a current word depends only on word sequence the word based gram predicts the probability according to the following equation is unity We refer to these as the TDC CACHE NGRAM and TDC NGRAM CACHE models respectively The cache window size is equal to the TDC window size with the addition gram history which is not covered by the TDC s of the window as shown in 26 and 27 at the bottom of the page III EVALUATION OF PERPLEXITY To evaluate the proposed TDC LM for ASR we use a simple and widely used approach by calculating its perplexity as defined by 28 NGRAM 23 is a word history where We used a word based gram as the LM for capturing the local constraint through linear interpolation TDC NGRAM 24 where is a weight constant 2 Cache Based LM A cache based LM 25 is based on the notion that words appearing in a document will increase the probability of appearing again in the same document Given a history the unigram cache model is defined by the following equation CACHE 25 Although perplexity calculation does not always agree with the word recognition accuracy 22 it is the first approximation toward a better LM 28 A English Corpus 1 Setup Experimental data were taken from the WSJ corpus between 1987 and 1989 and were divided into training and test datasets The training dataset contains 36 754 891 words in 85 445 documents while the test dataset contains 336 096 words in 809 documents ARPA s official 20o nvp 20k most common WSJ words with non verbalized punctuation is used as the vocabulary and it gives an out of vocabulary OOV rate of 2 47 and 2 57 for the training and test datasets respectively By adding a beginning sentence symbol s an end sentence symbol s and an unknown symbol to map all OOV words the total vocabulary size becomes 19 982 words The TreeTagger toolkit1 is used to filter nouns from the vocabulary2 with the Penn Treebank tagset resulting 12 086 nouns The library used for SVD is SVDLIBC 3 Table I gives eight examples of noun topic classes taken from TDC model with 80 topics The table only shows ten words with the highest confidence measure the closest noun to the centroid 2 Soft Clustering and Soft Voting First we will examine the performance of each soft clustering or soft voting in the training or test phase given by Figs 1http www ims uni stuttgart de projekte corplex TreeTagger 2We used the tagger to label individual word in the vocabulary in the English experiments That is by assuming one word can only have one label However the tagger is applied to the whole corpus in the Japanese experiments It is shown that either method gives good performance 3http tedlab mit edu dr svdlibc denotes how many times occurs in the where history The TDC tries to model topical words and does not model reoccurring words very well Combining these two LMs should improve the performance of the model We ensure that the TDC model is able to increase the probability of reoccurring words by incorporating the cache based LM The cache based model is used to scale the unigram probability of the TDC LM There are two ways of combining LMs i e to scale the TDC before or after it is linearly interpolated with the word based gram Equation 26 shows how to calculate the probability of the before model while 27 shows that of the after model is the normalizawhere is the scaling factor tion factor that guarantees that the summation of probabilities TDC CACHE NGRAM 26 CACHE 27 TDC NGRAM 1518 IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING VOL 20 NO 5 JULY 2012 TABLE I SAMPLE LIST OF TOPIC WORDS OF SEVEN RANDOMLY SELECTED TOPICS FROM TDC MODEL Fig 7 Perplexity of the TDC 3 gram with soft clustering and soft voting Fig 9 Increasing number of parameters of the TDC when soft voting is performed in the training phase Fig 8 Perplexity of the TDC 3 gram with soft clustering and hard voting let an TDC imply that the TDC was performed with best soft clustering in the training phase best soft voting in the training phase best soft clustering in the test phase and best soft voting in the test phase Therefore a 1 1 1 1 TDC implies a TDC with hard clustering and hard voting on the training and test phases Fig 7 shows the result of the TDC standalone model with soft clustering and or soft voting in the training and or test phases If we analyze test phase experiments i e 1 1 1 TDC and 1 1 1 TDC increasing best soft clustering gradually decreases perplexity The improvement is not very significant therefore its perplexity is still worse than that of the word based 3 gram even with 5 best soft voting in the training phase i e 5 5 1 TDC Soft clustering might solve the unreliable topic Fig 10 Perplexity of the TDC 3 gram with soft clustering and or soft voting linear interpolated with the word based 3 gram Note that the word based 3 gram perplexity is 111 6 mapping but the shrinking training data problem remains On the other hand increasing best soft voting in the test phase significantly improves perplexity Because soft voting in the test phase solved the shrinking training data problem by predicting the event by using mixtures of the TDC with 4 best soft voting in the test phase the TDC standalone model outperforms the word based 3 gram By analyzing training phase experiments i e 1 1 1 TDC and 1 1 1 TDC we found out that increasing best soft clustering is also increasing its perplexity Soft clustering in the training or test phase alone does not improve the performance of the TDC LM However if we apply it on both sides training NAPTALI et al TDC BASED GRAM LM 1519 and test phases it improves perplexity slightly as seen in results given by Fig 8 The 1 1 1 TDC experiment suggests that increasing best soft voting in the training phase improves perplexity Unlike soft voting in the test phase soft voting in the training phase increases the number of LM parameters because it maps an event to multiple topics so that the number of parameters increases linearly and becomes at most times larger Fig 9 shows the number of parameters for the TDC with soft voting in the training phase It is increasing the number of word sequences in each topic Thus it solves the shrinking training data problem However similar to the disadvantage of the Random Forest LM performing soft voting in the training phase will increase the memory required for calculations In Fig 8 we can also see that increasing both and of soft clustering and soft voting in the training phase significantly improves perplexity From the 1 1 1 TDC to the 5 5 1 perplexity is reduced by 20 4In our previous paper 29 we have shown that TDC LM performs slightly better than LDA ADAPT 26 27 on perplexity This LDA ADAPT has been shown to perform better than Blei s LDA 4 TABLE II PERPLEXITY OF TDC WITH THE BEST CONFIGURATION TDC model From this table the result suggests that and values do not necessarily show similarity between training and test phases Increasing the value compensates the shrinking data problem in the TDC model where each topic in the TDC model on average is trained only by a division of topic size of and values alleviates a whole corpus While increasing improper topic decision specially for the unseen event Having different optimal values indicates how severe the data sparseness problem is and how reliable the topic decision is 3 Incorporation of the Cache Based LM In this experiment we also used a fixed 80 topics and a window size of 320 Because there are three parameters to tune 26 and 27 namely the linear interpolation weight and a scaling factor we explored the model s behavior by fixing two parameters and varying the third Table III shows the performance of TDC CACHE NGRAM 26 and TDC NGRAM CACHE 27 with baselines of the first four lines Because a cache based LM is trained on the basis of only a limited word history the interpolation weight is usually very small A large scaling factor always gives the best perplexity The best perplexity achieved by TDC CACHE NGRAM is 87 0 That against the gives 22 0 relative improvement word based 3 gram and 9 6 relative improvement against the TDC without a cache based LM combination The TDC NGRAM CACHE model gives even more improvement with the best perplexity 84 9 It implies 23 9 relative imagainst the word based 3 gram and provement 11 8 relative improvement against the TDC without a cache based LM The large improvements in both models support our statement that both LMs capture different aspects of the language A side by side comparison between the TDC CACHE NGRAM and the TDC NGRAM CACHE and It shows that is given by Fig 11 where the TDC NGRAM CACHE is better than TDC CACHE NGRAM Note that the TDC used in Table III is the one with hard clustering and hard voting Perplexity can be further improved by performing soft clustering and soft voting The TDC with soft clustering and soft voting is given in Table IV with the window 1520 IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING VOL 20 NO 5 JULY 2012 Fig 11 Perplexity comparison between TDC3CACHE NGRAM and TDC NGRAM 3 CACHE TABLE III PERPLEXITY OF TDC 3 CACHE NGRAM AND TDC NGRAM 3 CACHE TDC HARD CLUSTERING AND HARD VOTING 1 1 1 1 6 01 for training and test datasets respectively The baseline cache based LM was executed with an increasing window size from 20 to 640 and the best perplexity was achieved with a window size of 160 Because the Mainichi Shimbun corpus contains manually tagged topic information of 17 topics such as sports culture social science economic and entertainment we also conduct a topic dependent language model based on word based gram mixtures denoted as a known topic word based gram All models perplexities are given in Table V Of all the baseline methods it can be seen that our model gives the best perplexity of 53 3 for the cache based LM combination and 57 5 for the TDC with soft voting in the test phase resulting in relative improvements of approximately 25 7 and 19 8 against the word based 3 gram LM respectively We can see that soft clustering and soft voting in training and test phases reduce perplexity makes the TDC standalone model performs as better as the interpolated TDC LM We expect further improvements when we interpolate the soft clustering and soft voting TDC model with the cache based model As the number of grams parameter is increasing the calculation of perplexity will take time because it needs to be normalized However we can see the evaluation on WER in the following section IV EVALUATION OF WORD ERROR RATE IN ASR A English ASR 1 Setup Using the HTK toolkit 37 we trained acoustic models for American English by using 49 190 utterances from the WSJ corpus from 1987 to 1989 The resulting feature vector is 39 dimensional and comprises of 12 MFCCs plus the 0th ceptral and their first and second deviation coefficients are normalized using ceptral mean subtraction We used the CMU pronunciation dictionary 6 containing 39 phonemes without lexical stress HMMs are initialized on the basis of TIMIT phonetic transcriptions Cross word triphones are using tied state triphones based on the decision tree There are 16 Gaussians for the non silent state and 32 for the silent state For more details please refer to 34 A description of the LM and its training data used in this experiment is given in Section III A1 with 20 k vocabulary size Test data taken from the WSJ corpus for November 1992 contain 2729 words 170 utterances in eight documents with each document spoken by a different speaker The OOV rate is 1 83 Note that the size of this test data has been reduced from 330 to 170 utterances to provide a sufficiently long history words at the beginning of each document for the TDC and cache based LMs Additionally we provide development and data to determine TDC parameters which include for the TDC with soft clustering and soft voting on the test set and and for the TDC interpolation Development data were also taken from the WSJ corpus for November 1992 and contain 4010 words 251 utterances in 12 documents with an OOV rate of 3 24 Similar to test data the development data size has been reduced from 491 to 251 utterances The best perplexity values achieved for both datasets are given in Table VI In the table implies linear interpolation 6http www speech cs cmu edu cgi bin cmudict size of 320 For the TDC CACHE NGRAM perplexity is improved from 87 0 to 83 6 for the TDC NGRAM CACHE perplexity is improved from 84 9 to 83 9 These results suggest that the TDC CACHE NGRAM performs comparable to the TDC NGRAM CACHE if the TDC is performed with soft clustering and soft voting Thus in ASR experiments we only employ the TDC NGRAM CACHE model B Japanese Corpus Training data for Japanese taken from the Mainichi Shimbun Japanese newspaper corpus from 1991 to 1998 contain 207 215 663 words in 855 825 documents Test data taken from the Mainichi Shimbun for January 1999 contain 385 863 words in 1119 documents Normally Japanese text does not have spaces between words For this task we used the Mecab toolkit5 Yet Another Part of Speech and Morphological Analyzer and converted the corpus into basic units word part of speech The vocabulary size is 20 k words taken from among most frequent words With a beginning sentence symbol s an end sentence symbol s and an unknown symbol to map all OOV words the total vocabulary size is 20 001 words This gives OOV rates of 4 11 and 5http mecab sourceforge net NAPTALI et al TDC BASED GRAM LM 1521 TABLE IV PERPLEXITY OF TDC AND CACHE BASED COMBINATION WITH SOFT CLUSTERING AND SOFT VOTING TABLE V PERPLEXITY FOR THE MAINICHI SHIMBUN CORPUS TABLE VI TDC BASED PERPLEXITY ON ENGLISH DEVELOPMENT AND TEST CORPORA while implies unigram scaling with the LM in the previous line The TDC referred to in the Test column was executed using parameters optimized for development data whereas that in the Best Test column was optimized with test data as the reference From Table VI we can see that parameters tuned using development data are also stable for test data For the decoder we used the inhouse large vocabulary SPOken continuous speech recognition system SPOJUS Japanese Understanding System 15 By using the two pass decoder 1000 best hypotheses were generated The LM used in the first and second passes is a word based 3 gram yielding a WER 6 3 and 4 6 in the first and second passes respectively 7 In this experiment we observed the behavior of the TDC LM with respect to the WER by using test data First we investigated the improvement on the TDC standalone LM Then we analyzed 7The baseline WER given by the original test data 330 utterances was almost similar to that of reduced test data 170 utterances used for our experiment They were 6 4 and 4 7 for the first and second passes respectively the combination of the TDC a word based gram and a cachebased model with the topic number and window size set to 80 and 320 respectively 2 TDC Stand Alone Model Fig 12 shows the change in the WER as we increased the number of topics or the window size The number of topics varied as 20 40 80 and 160 while the window size varied as 20 40 80 160 and 320 For the TDC we fixed the window size to 80 while increasing the number of topics and similarly while increasing the window size we fixed the number of topics at 20 From Fig 12 we can see that the best WER was achieved with 80 topics and a window size of 320 in subsequent experiments these values were then used as the number of topics and window size respectively Fig 13 shows the changes in the WER as we vary and and in the test phase Increasing sometimes gives a better WER but at other times it makes it worse 4 best soft clustering in the test phase gives an absolute improvement of 0 1 in the WER Meanwhile increasing decreases the WER from 6 8 to 5 1 For the next experiment we performed soft clustering and soft voting in the training phase Results are shown in Table VII Increasing soft clustering and soft voting in the training phase to the 3 best gives a better WER than the word based 3 gram Increasing them further to the 5 best gives a comparable or better WER than the word based 3 gram with cache based unigram scaling Despite a worse perplexity as compared with the word based 3 gram 72 9 versus 62 5 the TDC 3 gram with the best configuration 3 3 4 10 yields a relative improvement in the WER against the baseline of 10 9 1522 IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING VOL 20 NO 5 JULY 2012 Fig 12 Stand alone 1 1 1 1 TDC based WER with increasing number of topics or window size Fig 14 1 1 1 1 TDC NGRAM based WER with increasing the number of topics or window size Fig 13 Stand alone TDC based WER with increasing soft clustering or soft voting in the test phase on the English system TABLE VII TDC BASED WER WITH SOFT CLUSTERING AND SOFT VOTING IN THE TRAINING AND TEST PHASES Fig 15 1 1 1 1 TDC NGRAM CACHE based WER with increasing the number of topics or window size 3 TDC Interpolation Similar to TDC standalone model experiments we first increased the number of topics or window size and calculated the WER Figs 14 and 15 show these results In these figures the TDC 3 gram is denoted as the TDC the word based 3 gram as NGRAM and the cache based 1 gram as CACHE The TDC in this experiment was implemented using hard clustering and hard voting in the training and test phases Both interpolation models give a better WER than the baseline The best WER was achieved by interpolating the three LMs yielding a WER of 4 0 with 20 topics and a window size of 160 Then a combination of the word based 3 gram and the cachebased 1 gram was implemented using the TDC with the best number of topics and window size from the previous section i e 80 topics and a window size of 320 Using development data WERs for the TDC are given in Table VIII Compared with the TDC WER for test data the TDC WER for development data is only slightly better and performs similar to the word based 3 gram unigram scaled by the cache based 1 gram This might be resulted from the higher OOV rate as compared with test data The best WER achieved using parameters optimized on development data is 4 0 i e a 13 0 relative improvement against the baseline with a WER of 4 6 This improvement is statistically significant8 at the 0 88 level 2 tailed Note that there are many combinations of configuration for getting the same WER Table IX gives the ASR results for deletion insertion and substitution errors and also the correctness and accuracy of 8Statistical significance was investigated according to Strik et al 30 31 by using a combination of the Number of Errors per Sentence NES metric and the Wilcoxon Signed Rank WSR test NAPTALI et al TDC BASED GRAM LM 1523 TABLE VIII TDC BASED WER OF ENGLISH DEVELOPMENT AND TEST CORPORA TABLE IX TDC BASED ACCURACY OF THE ENGLISH TEST CORPUS TABLE X EXAMPLES OF IMPROVED UTTERANCES the TDC optimized for test data third column of Table VIII The first two lines give baseline results for the first and second passes respectively followed by rescoring results Rescoring using a word based 3 gram unigram scaled by a cache based 1 gram gives an absolute improvement of 0 2 over the baseline This improvement is worse than that of the TDC Combining a word based 3 gram improves the substitution error slightly Further combining the LM with the cache based unigram through unigram scaling yields a slight absolute improvement of 0 1 in the WER Overall the LM yields a relative improvement of 15 2 in the WER against the baseline Table X shows three examples of improved utterances The Ref shows the transcript of the input utterance The Base is the result given by the word based 3 gram the same result also given by the word based 3 gram unigram scaled by the cache based 1 gram The Prop is the result given by the best TDC 3 gram model For the first example note that focus is a noun and meyerman is an OOV word In the baseline focus was not recognized while the proposed method correctly recognized it For the second and third examples the TDC correctly recognize eroded and gains which are topical words in economics These results show the effectiveness of the TDC LM B Japanese ASR 1 Setup Acoustic models were trained using 27 992 utterances spoken by 175 male speakers from the Japanese News Article Sentence JNAS corpus The feature vector is 38 dimensional and comprises of 12 dimensional MFCCs their first and second deviation coefficients and the first and second deviations TABLE XI TDC BASED PERPLEXITY ON JAPANESE TEST CORPUS of log power The data were used to train 928 Japanese context dependent syllable HMMs CDHHMs Each continuous density HMM has five states of which four have pdfs of output probability Each pdf consists of four Gaussians with full covariance matrices For more details please refer to 38 Training data for the LM were taken from the Mainichi Shimbun corpus from 1991 to 1997 containing 144 804 277 words in 597 539 documents For this experiment we used the morphological analyzer ChaSen9 to convert the corpus into basic units word pronunciation part of speech The vocabulary size is 20 k words taken from words that occur most frequently With a beginning sentence symbol s an to end sentence symbol s and an unknown symbol map all OOV words the total vocabulary size is 20 001 words This gives an OOV rate of 5 04 for training The baseline word based 3 gram was constructed using the 9http chasen legacy sourceforge jp 1524 IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING VOL 20 NO 5 JULY 2012 TABLE XII ACCURACY OF THE JAPANESE ASR SYSTEM Japanese corpus used in the experiment is six times larger than that of English corpus The soft clustering and soft voting technique compensate the data sparseness or shrinking problem by clustering V CONCLUSION AND FUTURE WORKS A TDC is a topic dependent LM with unsupervised topic extraction employing semantic analysis and voting on nouns We demonstrated that a TDC with soft clustering and or soft voting in the training and or test phases improved performances Soft clustering solved the unreliable topic mapping while soft voting solved the shrinking data problem in the TDC Soft clustering in the TDC should be performed in both the training and test phases Soft voting yielded a larger improvement compared with soft clustering Soft voting performed in only one phase either the training or test phase also produced good results We also demonstrated that incorporating a cache based LM improved the TDC further The cache based LM helped the TDC capture an aspect of the language that was not covered such as increasing the probability of co occurring words The evaluation of perplexity showed that the TDC achieved a 25 1 relative reduction in perplexity for the English corpus and a 25 7 relative reduction for the Japanese corpus compared with the baseline Finally we showed that the TDC improved not only perplexity but also the WER The ASR experiment on rescoring showed that the best result for the English system achieves a 15 2 relative improvement and that for the Japanese system achieved a 24 3 relative improvement in the WER as compared with the baseline The only drawback of the TDC LM is that it causes an increase in the number of parameters when performing soft voting in the training phase In this study we used a fixed value of for soft clustering and for soft voting This may cause unimportant word sequences to increase while also increasing the memory requirement In future work we intend to investigate the use of a dynamic threshold based on a confidence measure to reduce the number of parameters in the language model without sacrificing performance The TDC backoff by sliding the window is a simple approach we took for unseen words We will apply more complicated approaches for the TDC backoff REFERENCES 1 R Barzilay and M Elhadad Using lexical chains for text summarization in Proc ACL Workshop Intell Scalable Text Summariz 1997 pp Fig 16 Stand alone TDC based WER with increasing the number of soft clustering or soft voting in the test phase on the Japanese system Test data comprises 100 utterances 1746 words from the Mainichi Shimbun read documents JNAS for the period NAPTALI et al TDC BASED GRAM LM 1525 5 C Bouras and V Tsogkas Improving text summarization using noun retrieval techniques in Knowl Based Intell Inf Eng Syst ser Lecture Notes in Computer Science Berlin Heidelberg Germany Springer 2008 vol 5178 pp 31 H Strik C Cucchiarini and J M Kessens Comparing the performance of two CSRS How to determine the significance level of the differences in Proc Eurospeech Aalborg Denmark 2001 vol 3 pp Masatoshi Tsuchiya received the B E M E and Dr in Informatics degrees from Kyoto University Kyoto Japan in 1998 2000 and 2007 respectively He joined the Computer Center Toyohashi University of Technology Toyohashi Japan which was reconstructed to the Information Media Center in 2005 as an Assistant Professor in 2004 His major interest in research is natural language processing Seiichi Nakagawa M 87 received the Dr Eng degree from Kyoto University Kyoto Japan in 1977 He joined the faculty of Kyoto University in 1976 as a Research Associate in the Department of Information Sciences From 1980 to 1983 he was an Assistant Professor from 1983 to 1990 he was an Associate Professor since 1990 he has been a Professor in the Department of Information and Computer Sciences Toyohashi University of Technology Toyohashi Japan From 1985 to 1986 he was a Visiting Scientist in the Department of Computer Sciences Carnegie Mellon University Pittsburgh PA Dr Nakagawa received the 1997 2001 Paper Award from the IEICE and the 1988 JC Bose Memorial Award from the Institution of Electronics Telecommunications Engineers His major interests in research include automatic speech recognition speech processing natural language processing human interface and artificial intelligence He is a Fellow of IPSJ and IEICE 