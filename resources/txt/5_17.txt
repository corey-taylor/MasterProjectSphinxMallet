Lecture Notes in Artificial Intelligence Edited by R Goebel J Siekmann and W Wahlster 6233 Subseries of Lecture Notes in Computer Science Hrafn Loftsson Advances in Natural Language Processing 7th International Conference on NLP IceTAL 2010 Reykjavik Iceland August 16 18 2010 Proceedings 13 Series Editors Randy Goebel University of Alberta Edmonton Canada Library of Congress Control Number 2010931612 CR Subject Classification 1998 I 2 H 3 H 4 H 5 H 2 J 1 LNCS Sublibrary SL This work is subject to copyright All rights are reserved whether the whole or part of the material is concerned specifically the rights of translation reprinting re use of illustrations recitation broadcasting reproduction on microfilms or in any other way and storage in data banks Duplication of this publication or parts thereof is permitted only under the provisions of the German Copyright Law of September 9 1965 in its current version and permission for use must always be obtained from Springer Violations are liable to prosecution under the German Copyright Law springer Preface The research papers in this volume comprise the proceedings of IceTAL 2010 an international conference on natural language processing NLP IceTAL was the seventh in the series of the TAL conferences following GoTAL 2008 Gothenburg Sweden FinTAL 2006 Turku Finland EsTAL 2004 Alicante Spain PorTAL 2002 Faro Portugal VexTAL 1999 Venice Italy and FracTAL 1997 Organization IceTAL 2010 was organized by the Icelandic Centre for Language Technology ICLT Program Committee Walid El Abed Jan Alexandersson Jorge Baptista Tilman Becker Chris Biemann VIII Organization Caroline Brun Sylviane Cardey Robin Cooper Walter Daelemans Rodolfo Delmonte Markus Dickinson Mikel L Forcada Robert Gaizauskas Filip Ginter Peter Greenfield Philippe de Groote Xerox Corporation France University of Franche External reviewers Krasimir Angelov Organization IX Robert Nesselrath Anna B Gabriel Sekunda Jinsong Su Armando Xinyan Xiao Hao Xiong Conference Sponsors Table of Contents Invited Talks Reliving the History The Beginnings of Statistical Machine Translation and Languages with Rich Morphology Jan Haji c Harmonizing WordNet and FrameNet Christiane D Fellbaum 1 2 Research Papers A Morphosyntactic Brill Tagger for Inflectional Languages Szymon 15 27 39 45 57 67 79 XII Table of Contents Concept Based Representations for Ranking in Geographic Information Retrieval Maya Carrillo 85 97 103 115 121 127 138 150 162 167 179 185 Table of Contents XIII Symbolic Classification Methods for Patient Discharge Summaries Encoding into ICD Laurent Kevers and Julia Medori User Tailored Document 197 209 215 226 238 250 257 263 269 281 293 305 314 320 XIV Table of Contents Semi automatic Endogenous Enrichment of Collaboratively Constructed Lexical Resources Piggybacking onto Wiktionary Franck Sajous Emmanuel Navarro Bruno Gaume Laurent 332 345 357 369 381 393 401 406 418 431 Reliving the History The Beginnings of Statistical Machine Translation and Languages with Rich Morphology Jan Ha ji c Institute of Formal and Applied Linguistics School of Computer Science Charles University Prague Czech Republic hajic ufal mff cuni cz Abstract In this two for one talk first some difficult issues in morphology of inflective languages will be presented Then to lighten up this linguistically and computationally heavy issue a half forgotten history of statistical machine translation will be presented and contrasted with current state of the art in a rather non technical way Computational morphology has been on and off the focus of computational linguistics Only few of us probably remember the times when developing the proper formalisms has been in such a focus a history poll might still find out that some people remember DATR II or other heavyduty formalisms for dealing with the virtually finite world of words and their forms Even unification formalisms have been called to duty and the author himself admits to developing one However it is not the morphology itself not even for inflective or agglutinative languages that is causing the H Loftsson E Harmonizing WordNet and FrameNet Christiane D Fellbaum Department of Computer Science Princeton University Princeton USA fellbaum princeton edu Abstract Lexical semantic resources are a key component of many NLP systems whose performance continues to be limited by the lexical bottleneck Two large hand constructed resources WordNet and FrameNet differ in their theoretical foundations and their approaches to the representation of word meaning A core question that both resources address is how can regularities in the lexicon be discovered and encoded in a way that allows both human annotators and machines to better discriminate and interpret word meanings WordNet organizes the bulk of the English lexicon into a network an acyclic graph of word form meaning pairs that are interconnected via directed arcs that express paradigmatic semantic relations This classification largely disregards syntagmatic properties such as argument selection for verbs However a comparison with a syntax based approach like Levin 1993 reveals some overlap as well as systematic divergences that can be straightforwardly ascribed to the different classification principles FrameNet s units are cognitive schemas Frames each characterized by a set of lexemes from different parts of speech with Frame specific meanings lexial units and roles Frame Elements FrameNet also encodes cross frame relations that parallel the relations among WordNet s synsets Given the somewhat complementary nature of the two resources an alignment would have at least the following potential advantages 1 both sense inventories are checked and corrected where necessary and 2 FrameNet s coverage lexical units per Frame can be increased by taking advantage of WordNet s class based organization A number of automatic alignments have been attempted with variations on a few intuitively plausible algorithms Often the result is limited as implicit assumptions concerning the systematicity of WordNet s encoding or the semantic correspondences across the resources are not fully warranted Thus not all members of a synonym set or a subsumption tree are necessarily Frame mates We carry out a manual alignment of selected word forms against tokens in the American National Corpus that can serve as a basis for semi automatic alignment This work addresses a persistent unresolved question namely to what extent can humans select and agree on the context appropriate meaning of a word with respect to a lexical resource We discuss representative cases their challenges and solutions for alignment as well as initial steps for semi automatic alignment Joint work with Collin Baker and Nancy Ide H Loftsson E A Morphosyntactic Brill Tagger for Inflectional Languages Szymon Acedaski1 2 Institute of Informatics University of Warsaw ul Banacha 2 02 097 Warszawa Poland accek mimuw edu pl Institute of Computer Science Polish Academy of Sciences ul Ordona 21 01 237 Warszawa Poland 1 2 Abstract In this paper we present and evaluate a Brill morphosyntactic transformation based tagger adapted for specifics of highly inflectional languages Multi phase tagging with grammatical category matching transformations and lexical transformations brings significant accuracy improvements comparing to previous work Evaluation shows the accuracy of 92 44 for the Polish language which is higher than the same metric for the other known taggers of Polish stochastic trigram tagger 90 59 and hybrid tagger TaKIPI employing decision tree classifier and automatically extracted rule based tagger used for tagging the IPI PAN Corpus of Polish 91 06 Keywords PoS tagger Brill tagger inflectional language tagger morphosyntactic tagger lexical rules 1 Introduction Morphosyntactic tagging is a classic problem in NLP with applications in many higher level processing solutions namely parsing and then information retrieval speech recognition and machine translation Part of Speech tagging for English is already well explored and many taggers have been built with accuracy exceeding 98 In case of inflectional languages these numbers are much lower reaching 95 78 for Czech 1 and 92 55 for Polish per 2 evaluation by Karwaska and H Loftsson E 4 S Acedaski Table 1 Example tags in Brown s English Tagset and IPI PAN Polish tagset English VBD PPS Polish praet sg m1 perf verb past tense pronoun personal nominative 3rd person singular l participle singular human masculine perfective aspect ppron12 sg nom f pri 1st person pronoun singular nominative feminine the correct tags in some cases Because of this some corpora allow multiple tags to be assigned to a single segment whereas other require fully disambiguated tagging usually providing detailed instructions on how to do this This matter will be further discussed in the Evaluation section Several tagging techniques are commonly known The most frequently used approaches are stochastic e g based on Hidden Markov Models 7 and rulebased1 Brill 4 presents a transformation based Part of Speech tagger for English which automatically chooses good quality transformations given a number of general transformation templates and a training corpus The tagger used for morphosyntactic disambiguation of the current version of the IPI PAN corpus called TaKIPI 8 is a hybrid multiclassifier transformation based tagger Some of the transformations it uses were extracted automatically using machine learning algorithms and then reviewed and adjusted by linguists In this paper we describe and evaluate an implementation of the Brill s algorithm adapted for rich inflectional languages First steps towards this were described by Acedaski and Goluchowski in 2009 9 but that tagger was then rewritten with different approaches used in most parts As in previous work the adaptation involves splitting the process into phases so that at first only the part of speech and a few grammatical categories are disambiguated Remaining categories are determined in the second pass On top of it the new more general approach to transformation templates was developed and additional transformation templates allowing for transformations which look at particular grammatical categories of surrounding segments were added Also lexical transformations were used Finally the tagger was implemented using a new simplified algorithm based on FastTBL 10 and parallelized for better performance 2 The Original Brill Tagger Let us describe the original Brill s algorithm in some detail We assume that we are given three corpora a large high quality tagged training corpus smaller 1 Throughout this paper the term rule based tagger is used to denote systems using hand written rules For the algorithms involving automatic extraction of rules the term transformation based tagger is used A Morphosyntactic Brill Tagger for Inflectional Languages 5 tagged corpus called patch corpus and another one test corpus which we want to tag Brill also assumes that only one correct tag can be assigned to a segment Let s denote the tag assigned to i th segment as ti Tagging is performed in four steps 1 A simple unigram tagger is trained using the large training corpus 2 The unigram tagger is used to tag the patch corpus 3 There are certainly some errors in the tagging of the patch corpus Therefore we want to generate transformations which will correct as many errors as possible a We are given a small list of so called transformation templates Brill uses the following templates in his paper i ti A if ti B oO1 ti o C ii ti A if ti B oO2 ti o Co iii ti A if ti B and i th word is capitalized iv ti A if ti B and i 1 th word is capitalized 3 Adaptation for Inflectional Languages The algorithm described in the previous section was subsequently extended by applying a number of techniques targeted at improving accuracy of tagging of inflectional languages These techniques are 6 S Acedaski 3 1 Multi pass Tagging The first technique is used to reduce the size of the transformation space and to avoid too specific transformations in the first stage It is inspired by 9 where the authors split the tags into two parts sharing only the part of speech In the first run of the Brill tagger the tagset consists of only one of the parts of tags In the second run the tagset comprised of the other parts is used but the previously selected parts of speech are fixed for the second pass Also Tufi 11 proposes using a reduced tagset with easily recoverable grammatical categories not present to improve performance Out goal is different though we try to leave some of the hard to disambiguate categories for later stages so that the tagger already has more information from preceding phases We consider a sequence Ti i 0 k 1 of gradually simplified tagsets T0 is the original tagset and Tj 1 j 0 k 2 are some other tagsets Projections mapping specific tags to more general tags are also needed j Tj Tj 1 For each of the tagsets a separate pass of the Brill algorithm is performed The tag assigned to the i th segment in the p th pass p 1 k is denoted by tp i In the first pass the simplest tagset Tk 1 is used In the p th pass for i th p p 1 segment only tags tp i Tk p are considered such that k p ti ti In our experiments we used only two tagsets T0 being the original and T1 which had information about part of speech case and person only 0 was a natural projection i e the one which strips values of grammatical categories not present in T1 The produced software can be configured for more than two phases with different tagsets 3 2 Generalized Transformation Templates In the original Brill classifier all the transformation templates are of the following form Change ti to A if it is B and In our tagger we generalize the possible transformation templates by allowing other operations than changing the entire tag to be performed Also the current tag of a lexeme need not be fully specified in an instantiation of some transformation template A particular transformation template consists of a predicate template which specifies conditions on the context where the transformation should be applied and an action template describing the operation to be performed if the predicate matches For example in the transformation template change the tag to A if the tag is B the first part change the tag to A is the action template and the A Morphosyntactic Brill Tagger for Inflectional Languages 7 second part the tag is B is the predicate template The same nomenclature is applied to instantiated transformations This generalization was performed in order to allow using more general transformations than allowed by the original algorithm Let s denote by ti case the value of grammatical category case in the tag of the i th segment of the text Now consider the very robust linguistic rule if an adjective is followed by a noun then they should agree in case This rule may be composed p tp i T oO ti o U p ti T oO tp i o Uo p 1 ti T oO1 tp i o U p p ti pos P ti C X p tp i pos P ti C X p oO tp i o pos Q ti o C Y p oO ti o pos Qo tp i o C Yo and action templates 1 tp i V 2 tp i pos R 3 tp i C Z Additionally the actions were implemented in such a way that they were not applied if they were to assign a tag not reported by the morphological analyzer for a particular segment In case of actions 2 and 3 the nearest possible tag was used instead The metric used here is the number of matching values of grammatical categories but only tags with the expected part of speech are considered If no such tags are possible the action is not performed 3 3 Lexical Transformations Another extension which proved very useful are lexical transformation templates proposed by Brill in a later paper 12 Megyesi 13 subsequently explored them for Hungarian which is an agglutinative language with a number of affixes possessing grammatical functions The results were very promising The author used the following predicate templates 8 S Acedaski 1 2 3 4 tp i tp i tp i tp i T T T T orthi contains letter L orthi starts ends with S S 7 orthi with deleted prefix suffix S S 7 is a word orthi1 W orthi 1 W Here orthi simply denotes the orthographic representation of the i th segment Inspired by this work and after some experiments we extended the list of predicate templates by only the prefix suffix matching 1a tp i T orthi ends with S 1b tp i T orthi starts with S tp P tp i C X orthi ends with S 4a i pos p oO tp i o pos Q ti o C Y p tp i pos P ti C X p oO tp 4b i o pos Q ti o C Y orthi o ends with S where S and S are any strings no longer than 3 and 2 characters respectively This resulted in over 1 5 accuracy improvement over the Brill tagger with only generalized transformations as tested for Polish 3 4 Simplified FastTBL Implementation The idea behind the FastTBL algorithm 10 is the minimization of the number of accesses to data structures for storing Finally the tagger was implemented specifically for multiprocessing environment mostly because of the high memory requirements for storing the A Morphosyntactic Brill Tagger for Inflectional Languages 9 Algorithm 1 Pseudocode of the simplified FastTBL algorithm 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 Initializing good and bad data structures for i 0 to len text do for each transformation r which matches at position i do if r corrects the classification of i th segment then increase good r else if r changes the classification of i th segment from correct to wrong then increase bad r end if end for end for Main loop generating transformations loop b arg maxr good r bad r the best transformation if good b bad b threshold then return end if add b to the sequence of generated transformations text text after application of b for each position i in vicinity of the changes performed by b do for each transformation r which matches at position i in text do if r corrected the classification of i th segment in text then decrease good r else if r miscorrected the classification of i th segment in text then decrease bad r end if end for for each transformation r which matches at position i in text do if r corrects the classification of i th segment in text then increase good r else if r miscorrects the classification of i th segment in text then increase bad r end if end for end for text text end loop 10 S Acedaski 4 Evaluation The tagger was evaluated on two corpora of Polish the IPI PAN Corpus of Polish 6 and the new National Corpus of Polish 15 in preparation version dated Table 2 Evaluation results IPI PAN Corpus Sources 3 16 Full tags C WC P R F C WC PoS only P R F Tagger Trigram HMM 17 87 39 90 59 84 51 83 09 83 80 96 79 97 11 96 75 96 78 96 77 TaKIPI 2 Brill 9 2009 Brill this paper 88 68 91 06 90 94 83 78 87 21 96 53 96 54 96 58 96 71 96 65 89 46 83 55 86 40 90 00 92 44 92 44 86 05 89 13 98 17 98 18 98 18 98 16 98 17 Table 3 Evaluation results National Corpus of Polish full tags Thra Time s 2 6 a transformations P1 P2 2748 0 612 2 Acc 92 82 92 68 1450 5175 1 632 1422 6 The minimum good r bad r of the generated transformations Correctness C percent of segments for which the tagger assigned exactly the same set of tags as the golden standard Please note that in the IPI PAN corpus for some segments several tags are marked correct Weak correctness W C percent of segments for which the sets of interpretations determined by the tagger and the golden standard are not disjoint Precision P percent of tags given by the morphological analyzer which were both chosen by the tagger and the golden standard Recall R percent of golden tags which were chosen by the tagger 2P R F measure F F P R A Morphosyntactic Brill Tagger for Inflectional Languages 11 Accuracy Acc any of the above in the case of the National Corpus of Polish which has always one golden tag per segment The times in Table 3 were obtained on a multiprocessor machine with Xeon CPUs clocked at 2 4 GHz with 12MB cache 6 processes were run The tagger was compiled in 64 bit mode which probably negatively impacted performance due to almost doubled memory usage 1 2 GB per process compared to 0 7 GB in similar 32 bit setup but this was not verified It is also worth noting that only TaKIPI does not disambiguate words not known to the morphological analyser even if the input contains a number of possible morphosyntactic interpretations To provide a better insight into the classes of errors generated by the tagger detailed statistics are presented in Tables 4 5 and 6 It can be clearly seen that the most problems the tagger has concern case and gender Slightly fewer errors are reported for number This is similar to previous findings and not unexpected for Polish Nevertheless the introduction of lexical elements in transformation templates gave over 1 5 improvement in accuracy on the National Corpus of Polish Over 60 of all generated transformations do contain lexical matchers The vast majority of them is used for determining the correct case by matching nearby segments suffixes see Table 7 Also they are used for disambiguating rare flexemic classes like qub from conj There are also some categories of errors in the testing corpush which would not be disambiguated by a human looking at the same amount of context Let us present several examples Long nominal phrases especially near sentence or subordinate clause boundaries Table 4 Error rates for parts of speech shown only values 0 01 Expected PoS errs toks subst 3028 0 47 qub 1658 0 26 adj 1596 0 25 ger 1392 0 21 conj 652 0 10 adv 597 0 09 ppas 522 0 08 Expected PoS errs toks prep 471 0 07 pred 363 0 06 num 326 0 05 fin 268 0 04 pact 208 0 03 comp 172 0 03 Table 5 Error rates for grammatical categories shown only values 0 01 Category errs toks case 21259 3 28 gender 16151 2 49 number 4645 0 72 aspect 416 0 06 accommodability 193 0 03 12 S Acedaski Table 6 Specific errors in assignment of grammatical categories top 15 records Expected case nom case acc gender m1 case gen number sg number pl gender m3 gender m3 gender m1 gender f gender m3 gender f case gen case acc gender n Actual errs toks case acc 7188 1 11 case nom 4717 0 73 gender m3 2543 0 39 case acc 2533 0 39 number pl 2460 0 38 number sg 2185 0 34 gender m1 1989 0 31 gender n 1662 0 26 gender f 1375 0 21 gender m3 1243 0 19 gender f 1214 0 19 gender n 1115 0 17 case nom 1105 0 17 case gen 963 0 15 gender m3 907 0 14 Table 7 Sample lexical transformations generated by the tagger in the first pass No r Change case of preposition from acc to loc if it ends 3 with na in practice this asks for the particular preposition na in English on and one of two following segments has case of loc Change case of an adjective from loc to inst if one of 7 the three following segments has case of inst and ends with em good r bad r 2496 113 921 29 tego znaku Zamilowanie do sportu i this sign Passion for sport and Here the underlined word can have either nominal or accusative case Expressions with words like pastwo which may be either a noun country or a pronoun formal plural you o czym pastwo w tej chwili what you the country at the moment This calls for enlarging the lookup context in the future For example predicates like the nearest segment with part of speech P has category C equal X may be good candidates for inclusion This requires extending the vicinity parameter and therefore slows down the computations but may result in better accuracy 5 Conclusions and Future Work The paper presents and evaluates a number of techniques designed to adapt Brill tagger for inflectional languages with large tagsets Especially adding predicates A Morphosyntactic Brill Tagger for Inflectional Languages 13 and actions which allow matching or changing values of single grammatical categories as well as adding lexical transformations were the most valuable modifications of the original algorithm It is worth noting that the tagger does not need any linguistic knowledge provided except the specification of tagsets and the information about the grammatical categories which should be disambiguated in consecutive phases Rule templates are not designed for any specific language Even if some transformation templates are not suitable for considered language they may negatively impact only performance but not accuracy As far as the quality of the new tagger is concerned the reported numbers are at least 1 1 higher than for other existing taggers for Polish although this should be independently verified Also it may be an interesting experiment to use the tagger for other languages like Czech or Hungarian maybe after inclusion of all lexical transformations proposed by Megyesi 13 There are also some other places for improvement not explored yet namely 1 Experimenting with different simplified tagsets and more than 2 passes Tufi 11 proposes using an additional reduced tagset to collapse grammatical categories which are unambiguously recoverable from the lexicon This reduces the transformation space improving performance Others suggest joining some parts of speech or values of grammatical categories which have similar grammatical functions in the first pass to disambiguate them later For example in an intermediate phase one would use the value nom or acc for case 2 Simply enlarging the context of transformation templates may be a good way to go 3 Designing transformation templates which look for the nearest segment with a particular part of speech or value of some grammatical category may improve accuracy The full source code of the tagger is available under the terms of the GNU GPL v3 from its project page http code google com p pantera tagger Acknowledgments I d like to sincerely thank my academic advisor Prof Adam References 1 14 S Acedaski 3 Karwaska D Hybrid Syntactic Semantic Reranking for Parsing Results of ECAs Interactions Using CRFs Enzo Acerbi1 Guillermo 1 Julietta Research Group University of Seville enzoace gperez us es 2 University of Milano Bicocca stella disco unimib it Abstract Reranking modules of conventional parsers make use of either probabilistic weights linked to the production rules or just hand crafted rules to choose the best possible parse Other proposals make use of the topology of the parse trees and lexical features to reorder the parsing results In this work a new reranking approach is presented There are two main novelties introduced in this paper firstly a new discriminative reranking method of parsing results has been applied using Conditional Random Fields CRFs for sequence tagging Secondly a mixture of syntactic and semantic features specifically designed for Embodied Conversational Agents ECAs interactions has been used This approach has been trained with a Corpus of over 4 000 dialogues obtained from real interactions of real users with an online ECA Results show that this approach provides a significant improvement over the parsing results of out of domain sentences that is sentences for which there is no optimal parse among the candidates given by the baseline parse Keywords Embodied conversational agents natural language processing dialogue systems sequence tagging CRFs 1 1 1 Introduction Embodied Conversational Agents Conversational Agents CAs can be defined as communication technologies that integrate computational linguistics techniques with the communication channel of the Web to interpret and respond to statements made by users in ordinary natural language 1 Embo died Conversational Agents ECAs are empowered with a human representation that shows some degree of empathy smiling showing sadness disgust with the user as the dialogue go es on The fact of adding explicit anthropomorphism in Conversational Agents has some effects over the solution designed H Loftsson E 16 E Acerbi G The idea of discriminative reranking of parsing results is not new In 5 6 the authors propose a reranking process over the parsing results using a Maximum Entropy approach Also Collins 7 propose a similar strategy making use of Markov Random Fields and boosting approaches achieving significant improvement on error rate over the baseline system performance The approaches detailed in those papers are based on lexical and syntactic features describing the components of the parse tree and their syntactic relationship The reranking layer is applied over a set of candidates which are obtained with a classical generative parser In 8 an application of the previous proposals for semantic parsing is described In addition to the purely syntactic features the authors include semantic features on the reranking process obtaining partial improvements Reranking Parsing Results Using CRFs 17 In this paper radical a different strategy is proposed all parse tree structure is ignored and only terminal symbols are taken into account To our knowledge there is no previous work on reranking parsing results making use of sequence labeling as reranking metho d 1 3 Generative Parser The approach hereby described relies on a set of candidate parsing results provided by a generative parser The parser used in the experiments was 9 10 a unification grammar based context free parser inspired in the Lexical Functional Grammar formalism 11 The parsing results are therefore provided by means of two different structures the F structure and the C structure The first one is a set of language independent 2 Conditional Random Fields CRFs are probabilistic undirected graphical mo dels Each vertex of the CRF represents a random variable whose distribution is to be inferred and each edge represents a dependency between two random variables X is the sequence of observations and Y is the sequence of unknown state variables that needs to be inferred based on the observations In the application hereby described X is formed by all the words of the sentence while Y is the sequence of their corresponding labels CRFs are especially suitable for sequence labeling problems since independence assumptions are made among Y but not among X Thats is CRFs allow the use of strong interdependent and overlapping features as required by sequence labeling problems Formally CRFs can be defined as follows 12 Let G V E be a graph such that Y Yv vV so that Y indexed by the vertices of G Then X Y is a conditional random field in case when conditioned on X the random variables Yv obey the Markov property with respect to the graph p Yv X Yw w v p Yv X Yw wv where w v means that w and v are neighbors in G The likelihoo d probability for the CRF model is calculated in this way p y x 18 E Acerbi G exp eE k k fk e y e x v V k k gk v y v x 1 Notice that and represent the model s parameters and f and g represent the feature functions that are provided to the model in the training phase 3 3 1 A New Approach Parse Trees as Lexical Sequences A key observation that allows this new approach is the fact that no pair of alternative trees provided by the generative parser share the same sequence of lexical categories This statement is true because the syntactic ambiguity is locally solved by the baseline parser before providing the alternative trees to the reranking mo dule In other words to distinguish one parse tree from another one can just look at the categories assigned to each word in the sentence Therefore the problem of finding the optimal parse boils down to finding the optimal assignment of the lexical category for each word in the sentence among those given by the parser Thus a new parse tree sequence representation is proposed The problem of reranking parsing results is therefore reduced to a word category assignment the new problem is to find the best assignment for the whole sentence which is a typical sequence labeling problem The sequence labeling problem is faced with up to 223 different labels 3 2 New Problem Characteristics The reranking approach described in this paper is conditioned by the following issues Reranking Parsing Results Using CRFs 19 4 4 1 The Proposed Solution Theory The strategy to keep the problem tractable despite the tagset dimension is based on helping the model in two ma jor ways The first one is through the introduction of highly informative features in order to reduce the tagset dimension for every specific word This goal is achieved by exploiting a priori knowledge about a term Secondly the model prediction is driven the mo del is not asked to directly predict the correct label sequence instead the likelihoo d of every sequence is used for optimal selection Additionally the training set size is high enough to ensure the presence of past cases for every label in the tagset Since words in a sentence are strongly interdependent the solution has to be able to model dependencies between entities moreover words can be linked to a big set of features that can help classification but dependencies may exist also between features One of the most well known approaches to sequence labeling is Hidden Markov Models 13 The potential problem using HMMs is that they calculate p x y where x is the word and y is the label The point is that what really needs to be mo deled is p y x A solution can be Maximum Entropy Markov Models MEMM where p y x is calculated using a maximum entropy model But MEMM can suffer the label bias problem CRFs are a suitable mo del for the task at hands since they do not suffer the label bias problem they are not per state normalized like MEMMs instead of being trained to predict each label independently they are trained to get the whole sequence correctly 4 2 Implementation Offline Due to the specificity of the problem the creation of an ad hoc training set has been necessary in order to take into account domain dependent semantic categories The training set is formed by a Corpus of over 4 000 dialogues of Human ECAs interactions and all the alternative parse trees for every sentence During the offline phase the correct alternative has been manually tagged for every sentence The tagging process was done by choosing among a set of sequencelike representation of the parse trees The tagger application graphically shows for each sentence all the possible sequences of lexical categories and allows to select the best sequence or the best combination of sequences Figure 2 shows a screenshot of the application If the sequence selected do es not include all the words in the sentence the excluded words are labelled as Not Used Sometimes the correct parse tree of a sentence is captured by a combination of two or more partial sequences In order to prevent the bad tendency of the mo del to predict too many words as Not Used words between two partial sequences are classified as Link Thus the mo del learns to distinguish between words that can be ignored namely Not Used and words that are functioning as a bridge between partial sequences namely Link Figure 3 shows the merging of the two partial sequences selected in Figure 2 20 E Acerbi G Fig 1 Offline processing of the proposed approach Sentences can be classified in two main categories Besides these two groups extremely bad formed sentences were classified as No Parse and discarded in the training phase 5 of the total amount of analyzed sentences The tagging application allows the user to choose the correct sequence in a time between 5 and 15 seconds approximately depending on the sentence complexity The final average tagging rate was 100 sentences per hour the entire tagging pro cess took over 50 hours Reranking Parsing Results Using CRFs 21 Fig 2 A detail of the application created for corpus tagging Fig 3 Merging sequences in a unique global sequence The MALLET library 14 was used for building the CRF mo del and has been mo dified to obtain the likelihood associated with each candidate The mo del was validated with a 5 fold cross validation details about the dataset are provided in the Experimental Results section Online The online phase refers to the real interactions between the ECA and a user Figure 4 shows the way the CRF model is used at running time the natural language sentence provided by the user is analyzed by the baseline parser and a set of candidate sequences is returned At this point all possible combinations of partial sequences have to be generated and added to the original set of candidates The CRFs mo del trained in the offline phase returns the likelihoo d probability asso ciated with each candidate sequences The highest likelihoo d sequence is identified as the optimal one and the related c structures one or more chosen Features As previously mentioned a set of highly informative features was intro duced in order to limit the number of possible labels for a specific word 22 E Acerbi G Fig 4 Online processing If it is known that word x can be classified only as tag1 tag2 tag3 and tag4 and this information is properly intro duced into the mo del it allows the mo del to focus the prediction on the specific subset ignoring remaining label assignments Three kind of features were used 5 Experimental Results As previously explained only in domain sentences were used to train the mo del but both in domain and out of domain sentences were used to test For the outof domain inputs the mo del is expected to choose a correct syntactic parse tree Reranking Parsing Results Using CRFs 23 which includes all the relevant terms in the sentence The presence of the main concepts in the syntactic tree is a key factor since the ECA will use them to search in the host web page To reduce risk of overfitting a 5 fold cross validation was applied on the indomain dataset The in domain dataset consisted of 4 096 propositions and was divided into 5 subset of approximately 820 sentences each Each time one of the 5 subsets was used to test while the remaining 4 subsets were used to train In this way each sentence in the dataset was used to test exactly once and 4 times to train The k fold technique for performance estimation is computationally very expensive but allows to obtain a more accurate estimate of true accuracy than classical hold out metho ds Out of domain sentences were tested using a mo del trained with the whole in domain dataset The best results have been obtained by setting the CRF s window size to 7 the number of tokens representing context surrounding a word An Intel Quad Core Q9400 2 66 GHz machine with 4 096 MB of RAM was used to train the mo del Training required a very long time to converge before intro duction of phrase based features about 90 hours were necessary to train with the whole in domain dataset After the intro duction of this kind of features traning time decreased to about 40 hours During the experiments a real risk of local minima was detected Performance of both rule based baseline and CRFs based reranking systems were evaluated in terms of accuracy F measure precision and recall The Table 1 shows the baseline rule system performance rules perform well on in domain sentences while on out of domain sentences the performance dramatically drops by losing 13 18 on accuracy and 8 89 on F measure Accuracy was calculated among the whole set of 223 categories while precision recall and F measures were calculated only for those categories that o ccurred more than 20 times in the dataset Table 1 Baseline rule based system performance Accuracy F measure Precision Recall 86 59 92 77 95 32 90 35 73 41 83 88 85 78 82 06 80 00 88 32 90 55 86 21 In domain Out of domain Mixed The CRFs based reranking performance is depicted in Table 2 CRFs perform worse than the baseline system when in domain sentences are considered while they perform better than the baseline system when out of domain sentences are considered In this case CRFs significantly improve the baseline system by obtaining a 5 21 increase on accuracy and a 4 98 increase on F measure Table 3 shows the performance evolution in relation to the training set size Due to considerable computational costs 5 fold cross validation was applied only for the biggest training set the remaining were tested with a simple hold out technique It is worthwhile that each training set in the table isn t a new training 24 E Acerbi G In domain Out of domain Mixed Table 3 Performance evolution on different training set sizes Training set size 800 1 756 2 480 3 600 4 096 Mixed Accuracy Mixed F measure 51 74 59 16 64 43 76 12 72 09 81 70 77 09 87 14 79 63 88 03 Fig 5 Performance evolution on different training set sizes set but only an extended version of the previous one Figure 5 shows how the performance improvements are slowly getting smaller as the training set size increase and is essentially stable around 4 000 sentences 6 Conclusions The performances achieved by the baseline system and the new proposal are quite mirrored rule based performance results are better for the in domain sentences while CRFs are better for the out of domain ones The reason why CRFs outperforms the baseline system on out of domain sentences is mainly because Reranking Parsing Results Using CRFs 25 they learn which terms are relevant for this particular domain even if they are to be parsed within a syntactic tree On the other hand the baseline system has no semantic knowledge when trying to rerank syntactic parse trees The CRFs approach on its own would provide similar results to the baseline system in terms of the overall performance However the baseline approach is still more suitable for this particular ECAs application since as previously explained in domain parsing failures are more harmful than out of domain ones But the work hereby described is not useless Actually the results presented in the previous section clearly suggest that a combination of both approaches rule based for the in domain sentence and CRFs for the out of domain ones would very much increase the overall performance of the system Moreover the relative importance of both approaches depends on the particular domain evaluated In section 4 2 a division in domain versus out of domain sentences of 80 20 was detailed This percentage is very much dependant on the domain and the particular coverage of the ECA application CRFs based approach would be more suitable for applications with higher out of domain input sentences percentage 7 Future Work The best way to make use of this approach is by combining it with the baseline References 1 Lester J Branting K Mott B Conversational Agents The Practical Handbook of Internet Computing Chapman and Hall Boca Raton 2004 2 Robinson S Traum D Ittycheriah M Henderer J What would you ask a Conversational Agent Observations of Human Agent Dialogues in a Museum Setting In Proceedings of the Sixth International Language Resources and Evaluation LREC 2008 Marrakech Morocco 2008 26 E Acerbi G 3 De Angeli A Johnson G Coventry L The Unfriendly User Exploring Social Reactions to Chatterbots In Proceedings of International Conference on Affective Human Factor Design pp Automatic Distractor Generation for Domain Specific Texts Itziar Aldabe and Montse Maritxalar IXA NLP Group University of the Basque Country Spain itziar aldabe montse maritxalar ehu es http ixa si ehu es Ixa Abstract This paper presents a system which uses Natural Language Processing techniques to generate multiple choice questions The system implements different methods to find distractors semantically similar to the correct answer For this task a corpus based approach is applied to measure similarities The target language is Basque and the questions are used for learners assessment in the science domain In this article we present the results of an evaluation carried out with learners to measure the quality of the automatically generated distractors Keywords NLP distractor for educational purposes semantic similarity 1 Introduction The generation of Multiple Choice Questions MCQ one of the measures used for formative assessment is difficult and time consuming The implementation of a system capable of generating MCQs automatically would reduce time and effort and would offer the possibility of generating a great amount of questions easily In our proposal we use Natural Language Pro cessing NLP techniques to construct MCQs integrated in didactic resources There are different NLP based approaches which have proved that the automatic generation of multiple choice questions is viable Some of them focus on testing grammar knowledge for different languages such as English 1 or Basque 2 Others apply semantic features in order to test general English knowledge 3 4 or knowledge of specific domains 5 Our work is fo cused on the automatic generation of MCQ in a specific domain i e science domain The target language is Basque The ob jective is to offer experts a helping tool to create didactic resources Human experts identified the meaningful terms i e words of a text which were to be the blanks of the MCQs Then the system applied semantic similarity measures and used different resources such as corpora and ontologies in the pro cess of generating distractors1 The aim of this work is to study different 1 The incorrect options of the MCQs H Loftsson E 28 I Aldabe and M Maritxalar metho ds to automatically generate distractors of high quality That is to say distractors that correspond to the vocabulary studied by learners as part of the curricula As there must be only one possible answer among the options of each MCQ experts had to discard those distractors that could form a correct answer Our purpose was to evaluate the system itself by means of an evaluation in a real situation with learners The results of a test exercise was used to measure the quality of the automatically generated distractors The evidence provided by the results will be used to improve the metho ds we propose The paper is organised as follows section 2 explains the scenario to generate and analyse the questions The metho ds we have used to generate distractors are explained in section 3 Section 4 presents the experimental settings and section 5 shows the results obtained when evaluating the questions with learners Finally section 6 outlines some conclusions and future work 2 Design of the Scenario We designed an experiment in which most of the external factors which could have an influence on the evaluation process were controlled The multiple choice questions were presented to learners together with the whole text Each MCQ is a stem and a set of options The stem is a sentence with a blank Each blank presents different options being the correct answer the key and the incorrect answers the distractors Example 1 shows an example of MCQs in the context of use Example 1 Espazioan itzalkin erraldoi bat ezartzeak bestalde Lurrari 6 egingo lioke poluitu gabe Siliziozko milioika disko 7 bidaltzea da ikertzaileen ideia Paketetan jaurtiko lirateke eta behin diskoak zabalduta itzalkin itxurako egitura handi bat osatuko lukete Hori bai 8 handiegiak izango lituzke 2 6 a babes b aterki c defentsa d itzala 7 a unibertsora b izarrera c galaxiara d espaziora 8 a kostu b prezio c eragozpen d zailtasun The pro cess of generating and analysing the questions consists of the following steps 2 6 a protection b umbrella c defense d shadow Automatic Distractor Generation for Domain Specific Texts 29 3 Distractor Generation When generating distractors the purpose is to find words which are similar enough to the key but which are incorrect in the context to avoid the generation of more than one correct answer We wanted to generate questions to test the knowledge on a specific domain i e the science domain The implemented metho ds are based on similarity measures For that the system employs the context in which the key appears to obtain distractors which are related to the it 3 1 Word Space Latent Semantic Analysis Similarity measures are usual in different NLP applications such us in generating distractors Two main approaches are used knowledge based metho ds and corpus based metho ds In fact some researches employ WordNet to measure semantic similarity 4 others use distributional information from the corpus 6 and finally there are some studies which exploit both approaches 5 Measuring similarity for minority languages has some limitations The main difficulty when working with such languages is the lack of resources In our case the main knowledge based resource for Basque 7 is not finished yet the Basque WordNet3 is not useful in terms of word coverage as it has 16 000 less synsets for nouns than WordNet 3 0 As a consequence we decided to choose as the starting point a corpus based metho d to carry out the experiments Nonetheless we also used a knowledge based approach to refine the distractor selection task cf Section 3 2 The system uses context words to compute the similarity deploying Latent Semantic Analysis LSA LSA is a theory and metho d for extracting and representing the meaning of words 8 It has shown encouraging results in a number of NLP tasks such as Information Retrieval 9 10 and Word Sense Disambiguation 11 It has also been applied in educational applications 8 and in the evaluation of synonym test questions 12 Our system makes use of Infomap software 13 This software uses a variant of LSA to learn vectors representing the meanings of words in a vector space known as WordSpace In our case it indexes the do cuments in the corpora it pro cesses and performs word to word semantic similarity computations based on the resulting model As a result the system extracts the words that best match a query according to the model Build Word Space and Search As the MCQs we work with are fo cused on the science domain the collected corpus consists of a collection of texts related 3 Nouns are the most developed ones 30 I Aldabe and M Maritxalar to science and technology 14 The corpus is composed of two parts For this work we used the balanced part 3 million words of the specialised corpus In the pro cess of building the model the matrix was created from the lemmatized corpus To distinguish between the different categories of the lemmas the matrix not only took into account the lemma but also its category The matrix contains nouns verbs adjectives and adverbs Once we obtained the mo del based on the specialised corpus we had to set the context to be searched After testing different windows we set the sentence as the context 3 2 Methods for Distractor Generation The words found in the mo del were the starting point to generate the distractors for which different metho ds can be applied The baseline metho d LSA metho d is only based on the output of LSA The rest of the metho ds combine the output of the mo del with additional information to improve the quality of the distractors LSA Method The baseline system provides InfoMap with the whole sentence where the key appears As candidate distractors the system offers the first words of the output which are not part of the sentence and match the same PoS In addition a generation pro cess is applied to supply the distractors with the same inflection form as the key LSA Semantics Morphology One of the constraints here is to avoid the possibility of learners guessing the correct choice by means of semantic and morphology information Let us see as an example a question whose stem is Istripua izan ondoren sendatu ninduen After the accident cured me the key is medikuak the do ctor and a candidate distractor is ospitalak the hospital Both words are related and belong to the same specific domain Learners could discard ospitalak as the answer to the question because they know that the correct option has to be a person in the given sentence The system tries to avoid this kind of guessing by means of semantic information Therefore applying this metho d the system do es not offer ospitalak as a candidate distractor The system uses two semantic resources a Semantic features of common nouns obtained with a semiautomatic metho d 15 The metho d uses semantic relationships between words and it is based on the information extracted from an electronic monolingual dictionary The extracted semantic features are animate human concrete etc and are linked to the entries of the monolingual dictionary b The Multilingual Central Repository MCR which integrates different local WordNets together with different ontologies 16 Thanks to this integration the Basque words acquire more semantic information to work with In this approach the system takes into account the properties of the Top Concept Ontology the WordNet Domains and the Suggested Upper Merged Ontology SUMO Automatic Distractor Generation for Domain Specific Texts 31 In a first step this metho d obtains the same candidate distractors as the LSA method and then it proposes only those which share at least one semantic characteristic with the key To do so the system always tries to find firstly the entries in the monolingual dictionary If they share any semantic feature the candidate distractor is proposed if not the system searches the characteristics in MCR which works with synsets By contrast the output of Infomap are words In this approach we have taken into account all the synsets of the words and checked if they share any characteristic That is if a candidate distractor and the key share any characteristic specified by the Top Concept Ontology the WordNet Domains or SUMO the candidate distractor is suggested One might think that after obtaining distractors which share at least one semantic characteristic with the key the system does not need any extra information to ensure that they are valid distractors However working with all the senses of the words may yield not valid distractors in terms of semantics Moreover there are some cases in which two words share a semantic characteristic induced from MCR but which would not be suitable distractors because of their morphosyntax In the last step the method takes the candidate distractors which share at least one semantic characteristic with the key and it takes into account morphosyntax Basque is an agglutinative language in which suffixes are added to the end of the words Moreover the combination of some morphemes and words is not possible For instance while the lemma ospital hospital and the morpheme ko form the word ospitaleko of the hospital it is not possible to combine the lemma mediku do ctor with the suffix ko since ko is only used to express the locative genitive case with inanimate words As the input text is previously analysed by a morphosyntactic analyser the system distinguishes the lemma and the morphemes of the key It identifies the case marker of the key and it generates the corresponding inflected word of each candidate distractor using the lemma of the distractor and the suffix of the key as basis Once distractors are generated the system searches for any occurrence of the new inflected word in a corpus If there is any occurrence the generated word becomes a candidate distractor The searching is carried out in a Basque newspaper corpus which is previously indexed using swish e4 to ensure a fast search That certain words do not appear in the corpus does not mean that they are incorrect Those distractors that do appear in the corpus will be given preference over distractors of common usage In this step the system tries to avoid candidate distractors which the learners would reject based on their incorrect morphology LSA Specialised Dictionary The third method combines the information offered by the mo del and the entries of an encyclopaedic dictionary of Science and Technology for Basque 17 The dictionary comprises 23 000 basic concepts related to Science and Technology divided into 50 different topics 4 http swish e org 32 I Aldabe and M Maritxalar Based on the candidate distractors generated by the LSA metho d the system searches in the dictionary the lemmas of the key and the distractors If there is an appropriate entry for all of them the candidate distractors which share the topic with the key in the encyclopaedic dictionary are given preference Otherwise the candidate distractors with an entry in the dictionary take preference in the selection pro cess In addition those candidates which share any semantic characteristic cf 3 2 with the key have preference to be suitable distractors LSA Knowledge based Method This method is a combination of corpusbased and knowledge based approaches to measure the similarities Similarity is computed in two rounds First the system selects the candidate distractors based on LSA and then a knowledge base structure is used to refine the selection The knowledge based approach 18 uses a graph based method based on WordNet where the concepts in the Lexical Knowledge Base LKB represent the node in the graph and each relation between concepts is represented by an undirected edge5 Given an input piece of text this approach ranks the concepts of the LKB according to the relationships among all content words To do so Personalized PageRank can be used over the whole LKB graph given an input text e g a sentence the metho d extracts the list of the content nouns which have an entry in the dictionary and relates them to LKB concepts As a result of the PageRank pro cess every LKB concept receives a score Therefore the resulting Personalized PageRank vector can be seen as a measure of the structural relevance of LKB concepts in the presence of the input context In our case we use MCR 1 6 as the LKB and Basque WordNet as the dictionary The metho d is defined as follows Firstly the system obtains a ranked list of candidate distractors based on the LSA mo del Secondly the Personalized PageRank vector is obtained for the stem and the key Thirdly the system applies the graph based method for 20 candidate distractors in the given stem Finally the similarities among vectors computed by the dot pro duct are measured and a reordering of the candidate distractors is obtained 4 Experimental Settings A group of experts chose five articles from a web site6 that provides current and updated information on Science and Technology in Basque As selection criteria they fo cused on the length of the texts as well as the appropriateness to the learners level First of all the experts marked the blanks of the questions and then the distractors were automatically generated To identify the best metho d to generate distractors we designed different experiments where the system applied all the explained metho ds for each blank and text Blanks Experts who work on the generation of learning material were asked to mark between 15 and 20 suitable terms in five texts to create multiple choice 5 6 The algorithm and needed resources are publicly available at http ixa2 si ehu es ukb www zientzia net Automatic Distractor Generation for Domain Specific Texts 33 questions The blanks were manually marked because the aim of the experiment was to evaluate the quality of the distractors in a real situation When pro ceeding the experts did not follow any particular guidelines but followed the usual pro cedure7 The obtained blanks were suitable in terms of the appropriateness of the science domain and the stems In all 94 blanks were obtained As we did not give them any extra information for the marking pro cess experts marked as keys nouns verbs adjectives and adverbs However our study from a computational point of view aimed at generating nouns and verbs 69 14 of the obtained blanks were nouns and 15 95 verbs This shows that the idea of working with nouns and verbs makes sense in a real situation Distractors The distractors were automatically generated for each blank and metho d In the case of the nouns the four mentioned metho ds were applied and in the case of the verbs two methods were applied the LSA metho d and the LSA specialised dictionary metho d8 As the distractor generation task is completely automatic the possibility of generating distractors that are correct in the given context had to be considered That is why before testing them with learners the distractors were manually checked For each metho d we provided experts with the first four candidate distractors We had foreseen to reject the questions which had less than three appropriate distractors However in all the cases three valid distractors were obtained Just 0 95 of the distractors could be as suitable as the key and 3 25 were rejected as dubious For each selected text we obtained four tests corresponding to the four metho ds Moreover a fifth test was manually made by experts who created three different distractors for each blank semantically close to the keys It is important to point out that the experts did not have any information about the distractors obtained from the automatic pro cess Finally the manually built tests were compared to the automatically generated ones Schools and Learners Six different schools took part in the experiment The exercise was presented to the learners as a testing task and the teachers were not familiar with the texts until they handed out the test to their students In all 266 learners of Obligatory Secondary Education second grade participated in the evaluation They had a maximum of 30 minutes to read and complete the test The test was carried out in paper in order to avoid all noise9 249 of the learners completed the test and their results were used to analyse the items questions see section 5 After finishing the testing an external supervisor collected the results of the exercise in situ 7 8 9 In this step the evaluation was blind We did not apply the remaining methods because the verbs in the Basque WordNet need of manual revision We did not want to evaluate the appropriateness of any computer assisted assessment 34 I Aldabe and M Maritxalar 5 Results By means of this evaluation we intended to improve the automatic metho ds explained in section 3 The item analysis metho d was the basis of our evaluation The item analysis method reviews items qualitatively and statistically to identify problematic items The difference between both reviews is that the qualitative metho d is based on experts knowledge and that the statistical analysis is conducted after the items have been given to students This paper is fo cused on the statistical analysis We have used R free software environment10 for statistical computing and graphics of the learners results 5 1 Item Analysis and Distractor Evaluation The analysis of item responses in a quantitative way provides descriptions of item characteristics and test score properties among others There are two main theories to address the problem Classical Test Theory CTT and Item Response Theory IRT Both statistical theories have been already used in the evaluation of the automatic generation of distractors 3 5 In this experiment we explored item difficulty item discrimination and distractors evaluation based on CTT as 5 did However the results obtained by them and our results are not comparable since they test the MCQs separately and we test them within a text Item difficulty The difficulty of an item can be described statistically as the proportion of students who can answer the item correctly The higher the value of difficulty the easier the item Item discrimination a goo d item should be able to discriminate students with high scores from those with low scores That is an item is effective if those with high scores tend to answer it correctly and those with low scores tend to answer it incorrectly The point biserial correlation is the correlation between the right wrong scores that students receive on a given item and the total scores that the students receive when summing up their scores across the remaining items A large pointbiserial value indicates that students with high scores on the overall test are also answering the item correctly and that students with low scores on the overall test are answering the item incorrectly The point biserial correlation is a computationally simplified Pearson s r between the dichotomously scored item and the total score In this approach we use the corrected point biserial correlation That is the item score is excluded from the total score before computing the correlation This is important because the inclusion of the item score in the total score can artificially inflate the point biserial value due to correlation of the item score with itself There is an interaction between item discrimination and item difficulty It is necessary to be aware of two principles very easy or very difficult test items have little discrimination and items of moderate difficulty 60 to 80 answering 10 http www r project org Automatic Distractor Generation for Domain Specific Texts 35 correctly generally are more discriminating Item difficulty and item discrimination measures are useful only to help to identify problematic items Poor item statistics of the results should be put down to ineffective distractors Distractor evaluation to detect poor distractors the option by option responses of high scored and low scored learners groups were examined To this purpose two kind of results will be presented in the next section the number of distractors never chosen by the learners and a graphical explanation of the used distractors 5 2 Interpreting the Results of the Tests Table 1 shows the average of item difficulty and item discrimination results obtained for all the items in a text The table shows the results for the manually and automatically generated tests In the case of item difficulty each row presents the item difficulty index together with the standard deviation as well as the percentage of easy and difficult items In this work we have marked an item to be easy if more than 90 of the students answer it correctly On the other hand an item is defined as difficult when less than 30 of the students choose the correct answer The results shown for the manually generated test are promising near 0 5 and there is not significant differences among the automatic metho ds All of them tend to obtain better results with the second text and tend to create easy items The results of item discrimination take into account the responses of the high scoring and low scoring students The high scoring group is the top 1 3 of the class and the low scoring group comprises students with test scores in the bottom 1 3 of the class Regarding item discrimination the corrected pointbiserial index with its standard deviation as well as the percentage of items with negative values are shown in the table Even though all the results obtain a positive mean a value of at least 0 2 is desirable in 8 out of the 10 cases negative point biserial indexes are obtained These negative values represent the percentage of items correctly responded by a higher number of low scored students than high scored ones To identify the reasons underlying these results we study the option by option responses of high scored and low scored groups Such study led us to evaluate the distractors themselves Figure 1 shows in a graphic way the distribution of the distractors among the low scored and high scored groups The x axe represents the number of lowscored students selecting a distractor and the y axe represents the number of high scored ones In this experiment we have studied the results related to 108 distractors limiting the number of students per test to 20 Regarding the number of different distractors in the case of the manually generated distractors 83 76 85 out of the 108 distractors were chosen In the cases of the automatically generated distractors the results were 64 59 26 for the LSA metho d 54 50 00 for the LSA semantics morphology metho d 67 62 04 for the LSA and encyclopaedic dictionary metho d and 60 55 56 for the LSA knowledge based method 36 I Aldabe and M Maritxalar Table 1 Item Analysis Difficulty Item Difficulty Easy 0 79 Text1 Text2 LSA sem morph Text1 Text2 LSA spec dictionary Text1 Text2 LSA Knowledge based Text1 Text2 Manually generated Text1 Text2 LSA Fig 1 Distractors Evaluation is used when both methods share the point Based only on the selected distractors this last metho d gives the best results in relation to the percentage of distractors that discriminates positively among the low and high scored groups 90 00 54 out of 60 The distractors obtained by the LSA semantics morphology metho d discriminated positively in 87 04 of the cases the LSA dictionary metho d in 79 10 of the cases the LSA metho d in 76 56 of the cases and the manual metho d in 75 90 of the cases In a graphic way the distribution of the low right side of the graphics can be interpreted as the set of goo d distractors The distribution of the high left side of the graphics represents distractors that have to be revised because they confuse high scored students and do not confuse low scored learners The reason could be that low scored learners have not enough knowledge to be confused Looking at the results of the metho ds the LSA metho d tend to confuse more than the other metho ds 14 06 followed by the manual metho d 12 05 the LSA dictionary metho d 11 94 the LSA semantics morphology metho d 7 41 and the LSA knowledge based method 6 67 It seems there is a relation between the number of the selected distractors and the percentage of discrimination the lower the number of distractors the higher the positive discrimination However the LSA metho d do es not follow this assumption Automatic Distractor Generation for Domain Specific Texts 37 In order to improve the methods we are planning to study in more depth the distractors that were never chosen Moreover it is also necessary to analyse on two other aspects the domain nature and the part of speech of the keys We must not forget that experts marked the blanks without being instructed Therefore the blanks did not have to correspond with words related to the specific domain 6 Conclusions and Future Work The article presents a study about automatic distractor generation for domain specific texts The system implements different metho ds to find distractors semantically similar to the key It uses context words to compute the similarity deploying LSA We have used a balanced part of a specialised corpus to build the mo del In the near future we will make use of the whole specialised corpus to mo del it In this approach we have explored item difficulty item discrimination and distractors evaluation based on Classical Test Theory The results shown for the manually generated test were promising and there were not significant differences among the metho ds The item discrimination measure led us to study the option by option responses of high scored and low scored groups and we finished the study with the evaluation of the distractors Such evaluation gave us evidence to improve the metho ds regarding the domain nature and part of speech of the keys and the need to enlarge the context when applying LSA In addition we are planning to test the distractors with more learners Finally the fact that the distractors tend to confuse high scored learners but not low scored learners needs of deeper analysis In our opinion working in a specific domain may improve the quality of the distractors so in the near future we will design new experiments with test exercises independent from the domain to compare the results with the ones obtained in the current study For future work we are also planning to use data mining techniques to identify the blanks of the text Finally reliability measures should also be considered in future research Reliability tells us whether a test is likely to yield the same results if administered to the same group of test takers multiple times Another indication of reliability is that the test items should behave the same way with different populations of test takers Acknowledgments We would like to thank to the institution named Ikastolen Elkartea who has assigned goo d experts in order to work on the tasks of this work This research is being supported by the Basque Government SAIOTEK project S PE09UN36 and ETORTEK project IE09 262 References 1 Hoshino A Nakagawa H Assisting cloze test making with a web application In Proceedings of SITE Society for Information Technology and Teacher Eduation San Antonio U S pp 38 I Aldabe and M Maritxalar 2 Aldabe I Lopez de Lacalle M Maritxalar M Martinez E Uria L ArikIturri An Automatic Question Generator Based on Corpora and NLP Techniques In Ikeda M Ashley K D Chan T W eds ITS 2006 LNCS vol 4053 pp Summarization as Feature Selection for Document Categorization on Small Datasets Emmanuel Anguiano Laboratory of Language Technologies Department of Computational Sciences National Institute of Astrophysics Optics and Electronics INAOE Mexico eanguiano villasen mmontesg inaoep mx 2 Natural Language Engineering Lab ELiRF Department of Information Systems and Computation Polytechnic University of Valencia UPV Spain prosso dsic upv es 1 Abstract Most common feature selection techniques for document categorization are supervised and require lots of training data in order to accurately capture the descriptive and discriminative information from the defined categories Considering that training sets are extremely small in many classification tasks in this paper we explore the use of unsupervised extractive summarization as a feature selection technique for document categorization Our experiments using training sets of different sizes indicate that text summarization is a competitive approach for feature selection and show its appropriateness for situations having small training sets where it could clearly outperform the traditional information gain technique Keywords Text Categorization Text Summarization Feature Selection 1 Introduction Automatic document categorization is the task of assigning free text documents into a set of predefined categories or topics Currently most effective solutions for this task are based on the paradigm of supervised learning where a classification model is learned from a given set of labeled examples called training set 7 Within this paradigm an important process is the identification of the set of features words in the case of text categorization more useful for the classification This process known as feature selection tends to use statistical information from the training set in order to identify the features that better describe the objects of different categories and help discriminating among them Due to the use of that statistical information the larger the training set the better the feature selection Unfortunately due to the high costs associated with data labeling for many applications these datasets are very small Because of this situation it is of great importance to search for alternative feature selection methods specially suited to deal with small training sets In order to tackle the above problem in this paper we propose to apply unsupervised extractive summarization as a feature selection technique in other words we propose reducing the set of features by representing documents by means of a representative subset of their sentences Our proposal is supported on two facts about H Loftsson E 40 E Anguiano extractive summarization First it has demonstrated to capture the essence of texts by selecting their most important sections and consequently a subset of words adequate for their description Second it is an inherently local process where each document is reduced individually bypassing the restrictions imposed by the size of the given training set Through experiments on a collection consisting of three training sets of different sizes we show that text summarization is a competitive approach for feature selection and what is more relevant that it is specially appropriate for situations having small training sets Particularly in this situations the proposed approach could significantly improved the results achieved by the information gain technique The rest of this document is organized as follows Section 2 presents some related works concerning the use of text summarization in the task of document categorization Section 3 describes the experimental platform particularly it details the feature selection process and the used datasets Then Section 4 shows the results achieved by the proposed approach as well as some baseline results corresponding to the application of information gain as feature selection technique Finally Section 5 presents our conclusions and future work ideas 2 Related Work Some previous works have considered the application of text summarization in the task of document categorization Even though these works have studied different aspects of this application most of them have revealed directly or indirectly the potential of text summarization as a feature selection technique Some of these works have used text summarization or its underlying ideas to improve the weighting of terms and thereby the classification performance For instance Ker and Chen 2 weighted the terms by taking into account their frequency and position in the documents whereas Ko et al 3 considered a weighting scheme that rewards the terms from the phrases selected by a summarization method A more ambitious approach consists of applying text summarization with the aim of reducing the representation of documents and enhancing the construction of the classification model Examples from this approach are the works by Mihalcea and Hassan 5 and Shen et al 8 The former work is of special relevance since it showed that significant improvements can be achieved by classifying extractive summaries rather than entire documents Finally the work by Kolcz et al 4 explicitly proposes the use of summarization as a feature selection technique They applied different summarization Summarization as Feature Selection for Document Categorization on Small Datasets 41 selection techniques are comparable for most of the cases the former is a better option for situations restricted by the non availability of large training sets 3 Experimental Platform 3 1 Feature Selection Process Because of our interest to evaluate the effectiveness of text summarization as a feature selection technique we compared its performance against the one of a traditional supervised statistical approach Particularly to summarize the documents we used the well known HITSA directed backward graph based sentence extraction algorithm 6 The choice of this algorithm was motivated by its relevant results in text summarization as well as by its previous usage in the context of document categorization 5 On the other hand we considered the information gain IG measure as exemplar from supervised techniques 9 In a few words the feature selection was carried out as follows 1 Summarize each document from the training set by selecting the k of their most relevant sentences in line with the selected summarization method 2 Define the features as the set of words extracted from the summaries eliminating the stop words In contrast to this approach the common statistical feature selection process defines the features as the set of words having positive information gain IG 0 within the entire dataset That is it selects the words whose presence or absence gives the larger information for category prediction 3 2 Evaluation Datasets For the experiments we used the R8 collection 1 This collection is formed by the eight largest categories from the Reuters 21578 corpus which documents belong to only one class It contains 5189 training documents and 2075 test documents With the aim of demonstrating the appropriateness of the proposed approach for situations having small training sets we constructed two smaller collections from the original R8 corpus R8 41 and R8 10 consisting of 41 and 10 training documents per class respectively These collections contain 328 and 80 training documents and the original 2075 test documents Details can be found in Table 1 Table 1 Documents distribution in different data sets Class R8 Training Set R8 41 Training Set R8 10 Training Set Test Set earn acq trade crude money fx interest ship grain 2701 1515 241 231 191 171 98 41 41 41 41 41 41 41 41 41 10 10 10 10 10 10 10 10 1040 661 72 112 76 73 32 9 42 E Anguiano 4 Results In the experiments we evaluated the effectiveness of feature selection by means of the classification performance Our assumption is that given a fixed test set and classification algorithm the better the feature selection the higher the classification performance In particular in all experiments we used a Support Vector Machine as classification algorithm term frequency as weighting scheme and the classification accuracy and micro averaged F1 as evaluation measures Table 2 shows two baseline results The first one corresponds to the usage of all words as features i e without applying any feature selection method except by the elimination of stop words whereas the second concerns the usage of only those words having positive information gain From these results it is clear that the IGbased approach is pertinent for situations having enough training data where it could improve the accuracy in 1 5 However it is also evident that it has severe limitations to deal with small training datasets For instance it only could define 20 relevant features for the R8 10 collection which represented just 1 of the whole set of words causing a decrement in the classification accuracy of around 50 Table 2 Baseline results without feature selection and using the information gain criterion R8 Features All features IG 0 Accuracy F1 Features R8 41 Accuracy F1 Features R8 10 Accuracy F1 17 336 1 691 85 25 86 51 842 857 5 404 54 78 75 42 89 782 539 2 305 20 71 71 35 57 702 0402 Table 3 and table 4 show results from the proposed method for different summary sizes ranging from 10 to 90 of the original sentences of the training documents The achieved results are encouraging they show that text summarization is a competitive approach for feature selection and what is more relevant that it is especially appropriate for situations having small training sets In particular for the reduced collections R8 41 and R8 10 very small summaries from 10 to 50 could outperform with statistical significance the results obtained by the application of the IG based approach IG 0 as well as those obtained using all words as features We evaluated statistical significance of the results using the z test with a confidence of the 95 Table 3 Results accuracy of proposed method using summaries of different sizes R8 Sum Size Number features Our method Top IG Number features R8 41 Our method Top IG Number features R8 10 Our method Top IG 10 20 30 40 50 60 70 80 90 8 289 9 701 11 268 12 486 13 326 14 560 15 626 16 339 17 063 87 13 88 53 89 20 87 90 87 42 86 89 86 75 86 70 86 27 85 45 85 54 85 78 85 78 85 88 85 64 85 54 85 69 85 35 1 943 2 445 3 089 3 569 3 919 4 348 4 671 5 004 5 263 83 47 82 27 82 89 83 52 81 40 79 66 80 10 80 43 78 89 80 43 78 02 78 31 78 60 79 13 78 89 78 94 78 31 78 60 706 902 1 178 1 392 1 523 1 722 1 890 2 082 2 230 76 77 70 17 64 67 75 23 75 08 69 40 69 73 71 23 72 58 52 24 56 87 52 34 54 07 64 10 67 52 69 69 69 83 71 66 Summarization as Feature Selection for Document Categorization on Small Datasets 43 In order to have a deep understanding of the capacity of the proposed method we compared its results against those from a classifier trained with the same number of features but corresponding to the top IG values indicated in Table 4 as Top IG As can be noticed our method always obtain better results indicating that the information gain cannot be properly evaluated from small training sets Regarding this fact it is interesting to notice that for the R8 10 collection our method allowed a 7 of accuracy improvement from 71 71 to 76 77 by means of a 70 feature reduction from 2 305 to 706 whereas for the same compression ratio the features selected by their IG value caused a 28 drop in the accuracy from 71 71 to 52 24 Table 4 F1 measure of the proposed method using summaries of different sizes R8 Sum Size Our method Top IG R8 41 Our method Top IG R8 10 Our method Top IG 10 20 30 40 50 60 70 80 90 876 886 891 877 870 864 862 861 856 846 846 848 848 848 846 845 847 843 842 834 836 842 819 798 800 803 784 817 790 789 789 791 787 786 780 781 776 709 654 766 763 683 685 693 712 572 659 618 631 700 717 716 698 703 5 Conclusions and Future Work This paper studied the application of automatic text summarization as a feature selection technique in the task of document categorization Experimental results in a collection having three training sets of different sizes indicated that summarization and information gain a statistical feature selection approach are comparable when there are enough training data such as in the original R8 collection whereas the former is a better option for situations restricted by the non availability of large training sets as in the cases of R8 41 and R8 10 collections This behavior is because summarization is a local process where each document is reduced individually without considering the rest of the documents while statistical techniques such as IG require lots of training data in order to accurately capture the discriminative information from the defined categories Due to this characteristic as future work we plan to examine the pertinence of summarization based feature selection into a semi supervised text classification approach It is important to mention that the success of summarization depends on the nature of the documents In this paper we evaluated the proposed method in a collection of news reports demonstrating its usefulness As future work we plan to determine its appropriateness for other kinds of documents such as web pages and emails Acknowledgments This work was done under partial support of CONACYT Project grants 83459 82050 106013 and scholarship 271106 and the MICINN TIN200913391 C04 03 Plan I D i TEXT ENTERPRISE 2 0 research project 44 E Anguiano References 1 Cardoso Cachopo A Improving Methods for Single Label Text Categorization PhD Thesis Technical University of Lisboa Portugal October 2007 2 Ker S J Chen J N A Text Categorization Based on a Summarization Technique In ACL 2000 Workshop on Recent Advances in Natural Language Processing Honk Kong 2000 3 Ko Y Park J Seo J Improving Text Categorization Using the Importance of Sentences Information Processing and Management 40 2004 4 Kolcz A Prabakarmurthi V Jugal K Summarization as a Feature Selection for Text Categorization In Tenth International Conference on Information and Knowledge Management CIKM 2001 Atlanta GA USA 2001 5 Mihalcea R Hassan S Using the Essence of Texts to Improve Document Classification In RANLP 2005 Borovetz Bulgaria 2005 6 Mihalcea R Graph based Ranking Algorithms for Sentence Extraction Applied to Text Summarization In ACL 2004 Barcelona Spain 2004 7 Sebastiani F Machine Learning in Automated Text Categorization ACM Computing Survey 34 1 A Formal Ontology for a Computational Approach of Time and Aspect LaLIC Langues Logiques Informatique et Cognition University of La Sorbonne Maison de la recherche 28 rue Serpente Paris 75006 France Abstract This paper provides a linguistic semantic analysis of time and aspect in natural languages On the basis of topological concepts notions are introduced like the basic aspectual opposition between event state and process or that of time of utterance for the treatment of deictic categories that are used to analyse the semantics of grammatical tenses or more general situations This linguistic model yields a conceptualisation reused for the definition of a formal ontology of time and aspect This ontology is provided with a formal framework based on type theory that enables the processing of temporal and aspectual semantic entities and relations Keywords aspect time semantics formal ontology type theory knowledge representation 1 Introduction This paper addresses the problem of minimizing the distance between 1 time and aspect conceptualized from natural languages and 2 a computational mo del enabling a formal treatment of the semantics of texts Section 1 gives general information about time and aspect and intro duces a specific linguistic mo del resting on topological concepts and taking into account notions like temporal deixis or primitive aspectual values such as state pro cess or event and derived ones like resultative state see 1 Section 2 provides an original formal framework using an applicative language with Church functional types to express this linguistic ontology and then defines concepts and relations of this formal ontology of time and aspect This work intends to reach a greater expressivity compared to time notions investigated for instance within mo dal logic like with tense logic LTL or ITL if natural language semantics description is the goal to reach because it develops an interval based semantics integreting aspectual properties more suitable for natural languages analysis On the other hand the reasoning aspect is not considered here This paper is a knowledge representation investigation leading to a finer expressivity for applications like ontologies population or formal definition of grammatical operators H Loftsson E 46 A Arena and J P 2 An Analysis of Aspect and Temporality Time notions conceptualized from natural languages are often classified by linguistics into two main concepts that of time and that of aspect The former deals with locating situations in time with respect to the time of utterance e g deictic references like yesterday or other situations in time not related to it e g after the war The latter can be defined following 2 as the parameter that studies the different ways of viewing the internal temporal constituency of a situation Those definitions as it is going to be explained in the next sections can be given a more precise characterization adopting a theoretical point of view For the present purpose building an ontology of time and aspect first a list of fundamental entities that can be found in the semantics of aspect and time is drawn up and then relations over them are identified according to a specific theory that is developed here 2 1 Linguistic Concepts for a Model of Aspect and Time Discussing the different linguistic theories of aspect is not the main purpose of this paper simply a brief overview of a few typologies of aspectual semantic values will be provided then some comments are given on how aspectual values in languages can be conveyed by linguistic elements and finally the theoretical linguistic concepts adopted here are laid out Before intro ducing theoretical elements some linguistic examples of aspectual1 oppositions are considered without committing to any particular classification 1 a b c d John John John John walked was walking has walked walked to the station Dealing with aspect the classification of verbs made by the philosopher Vendler has often been taken up discussed and also refined He provided a set of four classes of verbs based on their distinct aspectual properties along with basic linguistic tests to determine if a verb belongs to a given class The four classes are the following activity e g run walk achievement e g see reach the top accomplishment e g run a mile build a house and state e g believe to be in the room For more details about this classification see 3 Later this aspectual typology has been refined on several points notably the semantic definitions of the aspectual classes Indeed according to different semantic features other classifications are proposed for instance in 4 5 6 Mourelatos intro duces a more fundamental aspectual distinction between event state and process from which Vendler s classes can be expressed in a hierarchical way Another ma jor refinement about the Vendler classification concerns the elements it applies to As it has been argued by Dowty Verkuyl or Mourelatos an aspectual semantic classification should not be restricted to verbs but rather 1 Regardless of temporal information for the moment A Formal Ontology for a Computational Approach of Time and Aspect 47 to situations described by whole sentences namely verbs along with their arguments and other aspectual mo difiers like adverbials2 Consider the following examples 2 a b c d Paul Paul Paul Paul drinks drinks read a read a beer a beer book in one hour book for one hour The linguistic theory of time and aspect that is worked out here is now introduced with its formal framework The overall theory is described in 9 10 Definitions of aspectual semantic values rest on topological concepts and more precisely on topological intervals of instants open closed half open for which topological properties have a linguistic meaning This interval based semantics is in line with a model theoretic approach In other words linguistic expressions to which a logical representation will be given are given a truth value with respect to topological intervals3 of the mo del We give now information about entities or linguistic concepts being taken into account in the mo del Dealing with time in natural languages the notion of temporal reference system see 1 is useful to understand its semantics Different types of temporal reference systems are defined first that created by the utterance pro cess and from which other situations can be located the enunciative reference system it is said to be actualized e g now yesterday secondly that being not actualized e g the first of March or that of possible situations e g if I were rich In the mo del by hypothesis the linguistic time is defined as an organized set of temporal reference systems and each temporal reference system being a continuous and ordered set of instants They are structured by three relations the relation of identification that of differentiation for before and after relations and the breaking relation meaning that a temporal reference system is not located with respect to the enunciative reference system like with for instance the marker once upon a time Regarding aspect the mo del of this theory is based on three primitive semantic values those of process event and state and to each is given a conceptual content analyzed from linguistic data and cognitive considerations A situation has the value of a pro cess when it expresses an ongoing action without last 2 3 Linguistic markers being variable from one language to another For a theoretical introduction on aspect with examples taken from different languages see 2 Dependencies between aspectual values of a sentence and its constituents is known as the aspectual composition phenomena For instance see 7 for a treatment of aspect in relation with the different semantic types of nominal arguments as in example 2 a and 2 b or 8 which notably takes into account adverbials as in 2 c and 2 d Many linguists have argued about the necessity of intervals for the semantics of time Bennet Culioli Partee 48 A Arena and J P instant It is formally represented by a right open and left closed interval there is an initial but no final instant A situation is a state when it expresses an absence of mo difications and its semantics is represented by an open interval On the other hand a situation is an event when it refers to a mo dification between two static situations and it is represented by a closed interval with an initial and a final instant For instance a situation having the aspectual value of state is said to be realized onto an open interval true for every instant belonging to the given interval in the mo del Situations having aspectual values are given an underlying logical expression namely an applicative expression ASP where ASP is a class of aspectual operators PROC STATE EVENT applying to being a timeless4 predicative relation those operator yielding aspectualised predicative relation realized on topological intervals Up to now entities state event process temporal referential of the model have been defined now relations that can be predicated over them are going to be investigated 2 2 Linguistic Relations for a Model of Aspect and Time Before intro ducing temporal relations a preliminary remark is made on the notion of utterance time that is defined by the utterance pro cess and realized on a right open interval not a single instant but a process because uttering takes time and is unaccomplished see again 1 The right boundary of this pro cess is written T0 and is not the last instant of the utterance pro cess but the first instant of the non realized instants In terms of the temporal structure the bound T0 of the utterance process separates the past from the future in an asymmetric way the future having a non linear structure This is one of the reasons why linguistic time can t be conceptualized as a linear order Thus temporal relations are expressed by precedence or identification5 relation holding between a situation with an aspectual value and the utterance pro cess Consider some linguistic examples and their corresponding intervals 4 5 Without temporal or aspectual value More precisely refers to the notion of lexis or dictum denoting what is said Relations between topological intervals are defined formally in the next section A Formal Ontology for a Computational Approach of Time and Aspect 49 Aspectual values such as defined above are said to be primitive to the extent that they can be combined to express derived aspectual semantic values like the perfect A sentence like expresses a situation where an event and a state are in a specific configuration The bound between the event and the state is defined as a continuous cut as defined by Dedekind6 and the adjacent to the event resulting state refers to a causal relation holding between both intervals which correspond to the semantics of the perfect As defined in the mo del aspectual semantic values are not indeed independent from each other Consider some simple examples like 3 a He was washing the car b He washed the car c He has washed the car Those situations all refer to a common telic predicative relation that has to reach a final term to be true here the right bound of the event The clause a refers to an underlying unaccomplished process right bound open Once this pro cess is achieved right bound reached it is turned into an event the clause b From this event can be related some causal situations expressed by a perfect value as in the clause c Those aspectual properties lead to a general network of dependencies intro duced in 1 6 A continuous cut written tc For a set of instants E linearly oriented and such that for A1 and A2 two subsets of E the following conditions are verified 1 A1 A2 E 2 A1 A2 and 3 A1 A2 tc is a continuous cut of E when either tc A1 A2 or tc A1 and tc A2 exclusively and tc 50 A Arena and J P Fig 1 The core information of the ontology consists of a set of constraints between aspectual values Arrows in this network can either mean is a sort of like in the proposition an activity state is a sort of state or implies like in an event implies a resulting state or contains like in a progressive process is contained in an activity state e g the plane is flying vs the plane is in fly See 10 for further details and a linguistic account for those relations These theoretical linguistic developments are in line with an ontological investigation to the extent that it answers to some questions about the time nature e g analysis of intervals bounds constraints and relation between them Now are intro duced some tools to formally represent this linguistic ontology 3 A Formal Ontology of Time and Aspect First a specific framework is laid out to make possible the expression of aspectual semantics introduced in the previous section 3 1 Formal Framework According to 12 there exists in the literature of knowledge representation several meanings for the term ontology One meaning identifies an ontology to a specific conceptualization of a knowledge base here the formal semantics of time and aspect intro duced in section 1 Another meaning concerns the formal account7 given to such conceptualization which is the topic of this section The formalism that is used here is that of applicative systems with types Applicative formalisms have been developed along with combinatory logics by Curry who intro duced the notion of operator and the basic operation of application The notion of type here is that of functional types introduced by Church 7 For instance description logics first order logic or applicative systems like combinatory logic or lambda calculus A Formal Ontology for a Computational Approach of Time and Aspect 51 Thus the basic structure for such system rests on the fundamental distinction between operator and operand The former being applied to the later to return a result Each of those three entities having a specific functional type This notion of type is used to characterize classes of ob jects operators can be applied to The construction rule for functional types is the following 1 Primitive types are Types 2 If and are Types then F is a Type 1 F is the functional type constructor and F is the type of an operator that can be applied only to operands having the type and is the type of the result The application rule is the following X F Y XY 2 An ontology being often defined as a set of concepts with relations over them it is necessary to formally define the notion of concept that is used here Following Frege a concept is defined as a function f D where D is a domain of ob jects and truth values Concepts are asso ciated to unsaturated entities and ob jects to saturated entities e g the concept ishuman can be applied to the saturated ob ject John to return true Entities of the ontology of time and aspect intervals are referred to as types8 and concepts and types are related by the following equivalence9 a x iff is x a 3 The left clause of this equivalence can also be expressed by the proposition a is an instance of the type x The definition of a concept is given by writing e g for a type x is x a a has the properties inherited from the concept isX Whence if an ob ject has a given type it inherits all properties asso ciated to the concept e g John human is human John has bo dy John and has mind John A relation here binary has a functional type built recursively using the rule 1 In the following diagram R is the name of an arbitrary binary relation holding between two typed ob jects 8 9 The closest notion of type used in this sense can be found in the literature on ontologies in computer science under the term universal taken from philosophy See for instance 13 Types are used to express general relations that hold between classes of objects like types in categorial grammars or the TBox level in description logics Types are written with bold lower case letters and name of concepts are written with upper case 52 A Arena and J P is x a is x b R F xF yH a x Ra F yH b y Ra b H a x R FxFyH b y Fig 2 The same binary relation R is represented graphically on the left and by its applicative tree on the right This tree rests on the application rule 2 The type H corresponds to truth values Double lines in graphical representation refer to the equivalence 3 3 2 Definitions of Concepts and Relations in the Ontology of Time and Aspect As it has been mentioned linguistic entities identified in section 2 1 namely the different topological intervals of instants and temporal reference systems are identify to primitive types in the formal ontology Other types are intro duced for technical reasons that are explained below Table 1 This table establishes the typology of entities required in the formal ontology of time and aspect Types H ref inst intv intv topo intv topo B intv topo B intv topo B cl intv topo B op intv topo B ho Entity description Truth values Temporal reference system see section 2 1 Instant Interval Interval with topological properties Unbounded interval Bounded interval Closed bounded interval see section 2 1 Open bounded interval see section 2 1 Half open bounded interval see section 2 1 Remark 1 Having established three different types respectively for closed open or half open intervals it is possible to express polymorphic relations between specific intervals For instance a relation R holding between a closed and a half open interval will have the functional type R F intv topo B cl F intvtopo B op H All arguments with other types than those in the signature of the relation10 will lead to a type error 10 This means is used to express semantic constraints between aspectual values For instance two events respectively true on closed intervals cant be adjacent or meet in A Formal Ontology for a Computational Approach of Time and Aspect 53 Fig 3 Each relation in this ontology is typed For instance the relation can have the type F inst F inst H or F ref F ref H The seven relations co ad ov pr coi cot in are defined in table 2 The relation is defined from the determination operator see 14 and shares some properties with the subsumption relation An ontology being not a simple typology unstructured set of entities the next figure graphically represents specific articulations of types taken from table 1 and then a conceptual content is given definitions following the figure 3 to types following the equivalence 3 and to relations Definition 1 A temporal reference system type ref is a strict total ordered set T where T is a non empty set of instants is called the precedence relation and verifies the additional properties of density and continuous cut see footnote 6 is ref R R T T 4 Definition 2 An interval type intv is a non empty convex subset of a temporal reference system is intv I R is ref R I a b I a b t R a t t I 5 Definition 3 A topological interval type intv topo is an interval to which operators of the point set topology like interior boundary or closure can be applied11 11 Indeed open intervals of any totally ordered set can define a topology on this set Here there exists a topological space T O where T is the non empty set of instants and O is a topology on T consisting of all open subsets of T verifying the specific topological axioms 54 A Arena and J P Here are recalled the basic topological notions from which more specific topological intervals will be defined like the closed or half open intervals Given an interval I and a temporal reference system R such that I R is intv topo b ho I is intv topo b I I Int I BdL I 10 Concepts being defined the next paragraph will fo cus on relations over topological intervals defined in defintion 5 closed open and half open They can be defined from F inst F inst H and F intv F intv H being respectively the precedence relation between instants and the classical set theoretic inclusion between intervals The difference with Allen relations 15 or Van Benthem12 definitions is that the semantics of bounds is taken into consideration and related to a logicolinguistic analysis Each relation is provided with a set of admissible types for its arguments called its signature and as mentioned in remark 1 this signature is used to avoid certain undesired configurations between topological intervals semantic constraints of the ontology see figure 1 For instance given the relation ad with the signature F intv topo B cl F intv topo B op H and the following configurations 12 Here definitions are based on and as in period structures from 16 but periods are defined differently A Formal Ontology for a Computational Approach of Time and Aspect 55 Table 2 This table provides definitions for relations holding between topological intervals Symb Name Definition Rep co coincidence A co B A B B A ad adjacence A ad B BdR A BdL B ov overlap A ov B i i A i B BdL A BdL B pr precedence A pr B BdR A BdL B coi initial coincidence A coi B A B BdL a BdL B cot terminal coincidence A cot B A B BdR a BdR B in interiority A in B BdL B BdL A BdR B BdR A Configuration 1 will lead to a semantic type error13 because the type F intv topo B cl F intv topo B cl H is not included in the signature of the relation ad whereas configuration 2 is well typed e g value of resulting state see figure 1 This ontology with specific semantic constraints being developed it enables the definition of a specific mo del I R V where 1 I is the set of all open closed or half open intervals defined by I i is intv topo b ho i is intv topo b op i is intv topo b cl i 2 R co ad ov pr coi cot in the set of typed binary relations over I 3 V P R I a valuation function assigning to each predicative relation in the set P R a subset of I where it is realized 4 Conclusion The main contribution of the article lies in the establishment of the formal ontology of time and aspect section 3 as a means or a toolkit to express formally some specific semantic constraints analyzed from aspectual situations in texts References 1 13 The meeting point being contained in both intervals a proposition could have contradictory truth value at this point e g to be standing to be sitting down 56 A Arena and J P 3 Vendler Z Verbs and times The philosophical review 66 2 A Non linear Semantic Mapping Technique for Cross Language Sentence Matching Rafael E Banchs and Marta R Costa Barcelona Media Innovation Centre Barcelona Spain rafael banchs marta ruiz barcelonamedia org Abstract A non linear semantic mapping procedure is implemented for crosslanguage text matching at the sentence level The method relies on a non linear space reduction technique which is used for constructing semantic embeddings of multilingual sentence collections In the proposed method an independent embedding is constructed for each language in the multilingual collection and the similarities among the resulting semantic representations are used for crosslanguage matching It is shown that the proposed method outperforms other conventional cross language information retrieval methods Keywords Cross language Information Retrieval Semantic Mapping Multidimensional Scaling 1 Introduction Cross language information retrieval CLIR which is a subfield of the traditional information retrieval IR provides users with access to information that is in a different language from their queries CLIR is gaining more and more attention as the availability of information in languages different from English increases in the Internet It has become one popular research area in information retrieval during the last 10 years 1 Research in CLIR has been significantly encouraged by three wellknown evaluation campaigns a cross language information retrieval track at TREC the Cross Language Evaluation Forum CLEF and the NTCIR Asian Language Evaluation Recently some CLIR real applications have appeared such as the crosslanguage search by Google on 2007 and the user driven multilingual news aggregation Europe Media Monitor Given a query in a given source language the aim of CLIR is retrieving relevant documents in a target language In 2 four different strategies for matching a query with a set of documents in the context of CLIR were identified cognate matching document translation query translation and interlinguas Nowadays one of the most popular approaches is query translation However this approach is bilingual in nature and in a highly multilingual environment with for instance N languages it may be impractical as N N 1 translation directions must be accounted for On the contrary an interlingua based approach would only require N mappings or translations to be accounted for In this sense this latter strategy seems to be more suitable in those applications involving a large number of languages H Loftsson E 58 R E Banchs and M R Costa In this work we focus on the specific problem of cross language text matching at the sentence level In this problem a segment of text in a given source language is used as query for recovering a similar or equivalent segment of text in a different target language This task is essential to some specific applications such as parallel corpora compilation 3 and cross language plagiarism detection 4 We address the problem under consideration by means of an interlingua based CLIR system that follows a non linear semantic mapping approach similar to the one presented in 5 Semantic mapping techniques have been successfully used for concept association and related term identification tasks 6 7 We illustrate here that this kind of non linear mapping can constitute a very effective and valuable strategy for the problem under consideration Some other recent approaches have achieved interesting results in CLIR applications by using regression canonical correlation analysis an extension of canonical correlation analysis 8 The rest of the paper is structured as follows In section 2 the implemented interlingua based CLIR method is described In section 3 the proposed methodology is illustrated by performing cross language text matching at the sentence level on a penta lingual document collection Also within this section the performance quality of the implemented system is evaluated and compared against two conventional CLIR systems showing that the proposed approach outperforms the other two Finally in section 4 the most relevant conclusions derived from the experimental results are presented 2 Cross Language Semantic Mapping The fundamental issue behind the proposed CLIR method is the idea of semantic mapping As illustrated in 5 starting from the term document matrix representation of a given document collection it is possible to build a semantic representation for the collection by using the non linear projection technique known as multidimensional scaling 9 Moreover if a multilingual parallel document collection is available a semantic map can be computed independently for each language s document subset and the resulting maps will exhibit a high degree of similarity among them The observed similarities among the maps are mainly because such maps are able to capture the most prominent semantic relationships among the documents within the collection which are indeed language independent The structural similarities observed among the different semantic maps provide the possibility of placing documents from different languages into any other language generated map In this way these maps can be actually interpreted as an interlingua type of representation The general procedure for CLIR by means of semantic mapping can be summarized as follows A Non linear Semantic Mapping Technique for Cross Language Sentence Matching 59 Fig 1 Schematic representation of interlingua based CLIR by means of semantic mapping A linear transformation operator T for projecting documents or queries from the original high dimensional space into the designated low dimensional semantic map can be inferred from the multilingual set of anchor documents as follows M T D T M D 1 1 where D is a square matrix of size NxN being N the number of available anchor documents which contains the distances among the anchor documents in the original high dimensional space the document similarity matrix and M is the KxN matrix containing the coordinates of the anchor documents in the reduced k dimensional semantic map The matrix M which represents the coordinates for anchor documents in the computed semantic map is obtained by applying MDS to similarity matrix D More specifically the algorithmic setting for the proposed methodology considers using the cosine distance for constructing the similarity matrix D and Sammon s projection criterion for computing the semantic map M 10 Different from the procedure described in 5 where a monolingual projection operator was computed here we compute a cross language projection operator for which M is computed on the retrieval language and D is computed on the source language This cross language variant of the method has been proven to provide better results than the original monolingual projection operator 11 Once the projection operator has been computed any new probe document or query can be placed into the designated retrieval map by using m Td 2 60 R E Banchs and M R Costa where d represents a vector containing the distances between the probe document or query and the anchor documents in the original high dimensional space T is the projection operator defined in 1 and m is a vector containing the coordinates for the probe document or query in the low dimensional map Additionally as many maps can be generated as there are different languages in the multilingual collection we propose a multiple map combination approach based on a majority voting strategy According to this a retrieval map is constructed for each language in the multilingual collection Then all probe documents and queries are projected into all maps where similarities are computed and individual rankings are performed Finally a global ranking is obtained by majority voting of all individual rankings This procedure is illustrated in Figure 2 Fig 2 Majority voting strategy implemented for combining individual map rankings 3 Cross Language Sentence Matching Experiments As already mentioned in the introduction in this work we focus on the problem of cross language text matching at the sentence level In this particular task a segment of text in a given source language is used as query for recovering an equivalent segment of text in a different target language In this section the methodology described above is evaluated and compared with other two CLIR approaches another interlingua based approach which is based on latent semantic indexing 12 and a more conventional query translation approach 13 which considers a cascade combination of a machine translation system and a monolingual IR system 3 1 Multilingual Sentence Dataset The dataset considered for the experiments is a multilingual sentence collection that was extracted from the Spanish Constitution which is available for downloading at A Non linear Semantic Mapping Technique for Cross Language Sentence Matching 61 the Spanish government s main web portal www la moncloa es In this website all constitutional texts are available in five different languages including the four official languages of Spain Spanish Table 1 Main statistics for the overall multilingual dataset and the selected test set Overall Dataset English Number of sentences 611 Number of words 15285 Vocabulary size 2080 Average sentence length 25 01 Selected Test Set English Number of sentences 200 Number of words 4667 Vocabulary size 1136 Average sentence length 23 34 Spanish 611 14807 2516 24 23 Spanish 200 4492 1256 22 46 Finally and for illustrative purposes one sample sentence from the multilingual collection is presented in Table 2 Table 2 A sample sentence from Spanish Constitution s multilingual sentence collection Language English Spanish 3 2 Experimental Evaluation of the Proposed Technique In this subsection the proposed methodology is illustrated by performing crosslanguage sentence matching on the Spanish Constitution s multilingual collection and its performance quality is evaluated by means of the top 1 and top 5 accuracies measured over the test subset that was described in Table 1 The specific task to be considered consists of recovering a sentence from the English version of the Spanish Constitution using as a query the same sentence in any of the four Spanish languages Spanish 62 R E Banchs and M R Costa of four hundred sentences constituted the anchor document collection that was used for constructing the maps One map was constructed for each of the five languages available in the collection by using multidimensional scaling The space dimension of the constructed maps was set to 350 As already reported in 5 and 11 where experiments considering a full range of reductions were presented space reductions down to dimensionalities above 75 the size of the anchor document collection provide appropriate embeddings for MDS and LSI based methods to be comparable Notice also that reducing the dimensionality down to 350 implied overall space reductions ranging from 83 in the case of English to 90 in the case of Euskera Finally following 1 and 2 transformation matrices were constructed for all constructed semantic maps and all test sentences from each language were placed into them Sentence matching was performed at each individual map by using the cosine distance as a similarity metric Table 3 summarizes results for all sentence matching exercises conducted over the five constructed maps as well as the implemented majority voting strategy and the four considered query languages Spanish Table 3 Top 1 and Top 5 accuracies for all conducted experiments on cross language sentence matching based on semantic maps Spanish Several interesting observations can be derived from results reported in Table 3 First of all it can be seen that regardless the semantic map used for sentence matching the best scores are always obtained when Spanish is the query language This might be explained by the fact that the constitutional texts used are originally Spanish texts which have been further translated into the other four languages According to this it could be expected for Spanish sentence projections to be more coherent than other language projections across all maps and in consequence it would be reasonable to assume that best scores should be obtained in those cases where Spanish is either the query or the retrieval language On the other hand the worst results are consistently obtained for all cases in which Euskera is the query language This is explained by the morphological complexity of the language which is evidenced in Table 1 as it exhibits the largest vocabulary and the smallest number of running words Nevertheless surprisingly when the retrieval semantic map is derived from the Euskera anchor documents resulting scores are as A Non linear Semantic Mapping Technique for Cross Language Sentence Matching 63 good as the ones obtained from any other of the maps This verifies the high degree of language independence the generated semantic maps can provide Another interesting observation that can be derived from Table 3 is the fact that with the exception of Euskera queries the English map is the best single semantic map for sentence matching when considering top 1 matches At a first glance one may think that this must be related to English being the target language of the task under consideration Nevertheless if top 5 matches are considered best results are achieved with semantic maps constructed from Galego and Spanish anchor document collections which does not support the previous finding as well as does not seem to have any logical justification In this sense further research will be needed to come up with a clear understanding on these issues Finally it can be also concluded from Table 3 that the majority voting strategy implemented for combining all semantic maps is not actually providing a significant improvement on the sentence matching task under consideration Indeed it is only in two cases Spanish top 1 and 64 R E Banchs and M R Costa four Spanish languages plus English Portuguese and French However it must be advised that as the Euskera to English translation direction is not provided by this service the considered task could not be evaluated with this contrastive system for this specific language pair On the other hand the monolingual information retrieval step was implemented by using Solr which is an XML based open source search server based on the Apache Lucene search library 15 Table 4 summarizes the results obtained from the comparative evaluation between the proposed semantic map based methodology and the two contrastive systems Only those results corresponding to the majority voting strategy are reported for semantic maps for comparing these results to individual semantic mapping results the reader can refer to Table 3 Table 4 Comparative evaluation of the proposed method majority voting of semantic maps vs LSI based and query translation CLIR techniques Spanish As seen from the table the proposed methodology clearly outperforms the two contrastive systems in all the cases Notice however that the observed differences are small as all systems are providing high accuracy values in most of the cases Some additional experimentation has been conducted with several different subsets of the same 200 sentence test dataset and similar results have been consistently obtained This suggests that the observed differences among the methods in Table 4 although small are significant The results reported in Table 4 show that non linear semantic maps described here seem to be more suitable for cross language sentence matching than both the linear projections provided by the LSI based method and the language conversions provided by state of the art machine translation Also it can be verified that both contrastive systems perform the same for top 1 accuracies but query translation outperforms the LSI based approach for top 5 accuracies 4 Conclusions and Future Work A non linear semantic mapping procedure has been implemented for cross language text matching at the sentence level The proposed method relies on a non linear space reduction technique multidimensional scaling for constructing semantic embeddings of a given multilingual document collection These semantic representations can be exploited for implementing an interlingua based CLIR system In the considered CLIR task a segment of text in a given source language is used as query for recovering a similar or equivalent segment of text in a different target language The proposed method is evaluated and compared against two conventional cross language information retrieval methods over a penta lingual sentence collection A Non linear Semantic Mapping Technique for Cross Language Sentence Matching 65 extracted from the Spanish Constitution Results presented in this work show that the proposed methodology outperforms the other two methods on the specific task under consideration Despite the positive results the majority voting strategy that was implemented for combining the individual rankings obtained from different semantic maps does not seem to provide any significant improvement with respect to the independent use of the individual semantic maps In this sense further research will be needed to determine the best combination strategy which might include for example some optimization procedure over a linear combination of the sentence similarities computed on the different maps Additionally some other interesting problems that must be addressed in future research have been also identified References 1 Kishida K Technical issues of cross language information retrieval a review Information Processing and Management 41 3 66 R E Banchs and M R Costa 4 Potthast M Stein B Eiselt A Comparison of Paraphrase Acquisition Techniques on Sentential Paraphrases Houda Bouamor LIMSI CNRS Univ Paris Sud F 91403 Orsay France Abstract In this article the task of acquisition of subsentential paraphrases is discussed and several automatic techniques are presented We describe an evaluation methodology to compare these techniques and some of their combinations This methodology is applied on two corpora of sentential paraphrases obtained by multiple translations The conclusions that are drawn can be used to guide future work for improving existing techniques Keywords Paraphrase Monolingual bi phrase patterns 1 Introduction Deciding whether two text units convey the same meaning is one of the most important needs in Natural Language Processing As natural language offers many possible alternatives for expression the ability to determine that two words or phrases have equivalent meaning in context is required for analyzing text In question answering for instance this can be used to extract correct answers expressed with words that differ from those in the question Large scale acquisition of sets of equivalent text units is an active field of research Applications can be in text analysis for example to allow different wordings in Machine Translation evaluation 9 or in text generation for example to help writers to find more appropriate wordings 14 A number of techniques have been proposed for acquiring text units in a paraphrasing relationship defined by a reciprocal textual entailment between the two units These techniques are designed to acquire paraphrases from specific types of resources Monolingual corpora have been extensively used to test the distributional hypothesis which states that text units o ccurring in similar contexts may be paraphrases The limitation in themes or genres in a comparable corpus increases the probability of extracting accurate paraphrase pairs in context Bilingual parallel corpora have also been used to test the translation equivalence hypothesis which states that text units sharing translations in at least one other language may be paraphrases In contrast few works have addressed using monolingual parallel corpora made up of paraphrases at the sentential level This can be explained by the facts that very few such corpora are available and that their construction can be H Loftsson E 68 H Bouamor A Max and A Vilnat costly and difficult to design However because they asso ciate sentences which express the same meaning such corpora allows for the most reliable acquisition of paraphrase pairs Contrary to what is the case with comparable monolingual corpora or parallel bilingual corpora paraphrases can be observed directly and examples of contexts in which one may substitute one with the other can be extracted straightforwardly This work fo cusses on the acquisition of accurate paraphrases in context from monolingual parallel corpora and on combining the results obtained from different techniques The remainder of the paper is organised as follows In section 2 we will review the main approaches for acquiring subsentential paraphrases and then describe three particular existing techniques that can be applied on sentential paraphrases in section 3 a technique based on statistical learning of word alignments one based on the symbolic representation of linguistic variation and another based on the syntactic fusion of sentences An experimental framework for comparing and combining the outputs of these techniques will be described in section 4 Our methodology for building a suitable corpus by multiple translation will be explained as well as an existing metho dology for evaluating the performance of the various techniques Lastly we will conclude and describe our future work in section 5 2 Previous Work on Subsentential Paraphrase Acquisition The hypothesis that if two words or by extension two phrases occur in similar contexts then they may be interchangeable has been extensively tested This distributional hypothesis attributed to Zellig Harris was for example applied to syntactic dependency paths in the work of Lin and Pantel 13 Their results take the form of equivalence patterns with two arguments such as X asks for Y X requests Y X s request for Y X wants Y Y is requested by X Using comparable corpora where the same information probably exists under various linguistic forms increases the likelihood of finding very close contexts for subsentential units Barzilay and Lee 2 propose a multi sequence alignment algorithm that take structurally similar sentences and build a compact lattice representation that encode local variations The work by Bhagat and Ravichandran 4 describes an application of a similar technique on a very large scale The hypothesis that two words or phrases are interchangeable if they share a common translation into one or more other languages has also been extensively followed in works on subsentential paraphrase acquisition Bannard and CallisonBurch 1 describe a pivoting approach that can exploit bilingual parallel corpora in several languages The same technique has been applied to the acquisition of local paraphrasing patterns in Zhao et al 17 The works of Callison Burch 5 and Max 14 have shown how the monolingual context of a sentence to paraphrase can be used to improve the quality of the acquired paraphrases Another approach consists in modelling local paraphrasing identification rules The work of Jacquemin on the identification of term variants 8 which exploits Comparison of Paraphrase Acquisition Techniques on Sentential Paraphrases 69 rewriting morphosyntactic rules and descriptions of morphological and semantic lexical families can be extended to extract the various forms corresponding to input patterns from large monolingual corpora All the previous approaches can pro duce inappropriate pairs that do not correspond to paraphrastic variants in context This is largely due to the fact that the corpora that they use never explicitly encode paraphrasing relationships between text units For instance a paraphrase obtained by pivoting through another language may not have been observed in the context of the original phrase the phrase it is too early to could be automatically extracted for the original phrase this is not the time to by pivoting through the French phrase il est trop t ot pour an acceptable translation for some o ccurrences of the two English phrases but it would clearly not be appropriate with a context such as this is not the time to be negative Likewise extracting a phrase that appears in contexts very similar to that of an original phrase is limited by the effectiveness of the mo deling of context used for instance several occurrences of Spain defeated France and Spain lost to France should not be used as evidence for establishing a paraphrasing relationship between the two verbs 1 In contrast whenever parallel monolingual corpora aligned at the sentence level are available the task of subsentential paraphrase acquisition can be cast as one of word alignment between two aligned sentences 2 Previous works have exploited multiple translations which o ccur very infrequently naturally But such translations are sometimes pro duced albeit in small quantities for example as multiple reference translations for evaluating Machine Translation outputs automatically Barzilay and McKeown 3 applied the distributionality hypothesis on such parallel sentences and Pang et al 16 proposed an algorithm to align sentences by recursive fusion of their common syntactic constituants Callison Burch et al 6 describe an automatic metric that can be used to compare techniques extracting subsentential paraphrases from pairs of sentential paraphrases 3 Acquisition of Subsentential Paraphrases from Sentential Paraphrases As discussed in section 2 acquiring subsentential paraphrases is a challenging task In this work we consider the most simple scenario where sentential paraphrases are available and words phrases from one sentence can be aligned to words phrases from the other sentence In this section we describe several techniques and their implementation that perform the task of subsentential unit alignment and how their results can be combined in a simple way We will also describe an existing evaluation metric that will allow us to compare the performance of these techniques 1 2 As previously said the use of comparable corpora provides a promising way to alleviate this issue as limiting the corpus to for example press coverage for the same piece of news strongly increases the probability of finding very close contents We do not address here the discourse and implication issues which make aligning the full contents of two such sentences not possible in all cases 70 H Bouamor A Max and A Vilnat 3 1 Statistical Word Alignment Method Word In phrase based Statistical Machine Translation bilingual phrases are extracted from parallel corpora as the basic units for translating These biphrases are often extracted in two steps 15 first word alignments are found in both directions and some heuristics is then applied to symmetrize these alignments and to extract biphrases from the resulting alignment Word alignment mo dels are learnt from the full training set of parallel sentences in the training corpus more data typically resulting in improved performance Furthermore because the underlying training algorithms make the assumption that sentences in a pair form a strict correspondance sentences that are complete and somehow literal translations will make word alignment and biphrase extraction more precise 3 Note that alignment mo dels typically support alignment to Null tokens for words which could not be aligned thus representing word insertions deletions However these words can still be included in the extracted biphrases depending on the heuristics used Using such alignment mo dels for the monolingual case has already been tested but to our knowledge no work has reported using parallel monolingual corpora due to their lack of availabibility 4 For this work we collected sentential paraphrases to build a monolingual parallel corpus In order to increase the number of examples at the sentential level we take all pairings for paraphrases belonging to the same group if they exist to build our training corpus 5 We used the Moses system 12 for word alignments using Giza 15 and default symmetrisation Once an alignment matrix is available we extract biphrases corresponding to possible subsentential paraphrases using the following criterion 6 two phrases from each sentence constitute a paraphrase pair if all words from one phrase are aligned to at least one word from the other phrase and not to words outside it 3 2 Term Variant Identification Method Term Sentential paraphrases can use common words but also synonyms and more generally phrasal paraphrases For such pairings and under certain conditions that are met with the types of corpus that we use rules can be expressed to mo del acceptable variations Research on term variant extraction thus offers a direct solution to the problem of subsentential alignment for some units We have 3 4 5 This partly explains why some language pairs are harder to align than others Works based on translational equivalence such as 1 alleviate this issue by using more readily available bilingual parallel corpora However one of the main limitation of this approach which motivated works on context modeling for validating the extracted paraphrases 5 14 is that the extracted biphrases are only indirectly aligned The strong limitation of our work for using parallel monolingual corpora finds here one of its main justifications Note that this will give a clear advantage to the statistical word alignment technique over the other techniques that we will discuss which do not currently support exploiting information from other sentences or other sentence pairings Comparison of Paraphrase Acquisition Techniques on Sentential Paraphrases 71 used the Fastr system 8 which takes as input a corpus and a list of terms and outputs the indexed corpus in which terms and variants are recognized using sets of meta rules applying to term rules to define acceptable variations Meta rules allow us to define morphosyntactic rewriting patterns as well as morphologic and semantic lexical relationships Fastr offers a large ruleset mostly for nominal and verbal terms and large lists of morphological variants and synonyms The controlled indexing program of Fastr extracts all variants from a list of terms from a corpus We used it to find phrase alignments in both directions Given a pair of sentential paraphrases all phrases up to a given length were extracted from one of the paraphrases Those were then used by Fastr as input terms to perform controlled indexing of the corpus consisting solely of the other paraphrase Only biphrases that are found in of both directions are kept We mo dified the program configuration so that it accepts one word terms useful for synonym detection but otherwise used its default resources This consequently performs a biphrase extraction fo cussed on nominal and verbal terms 3 3 Alignment by Syntactic Fusion Method Synt The exploitation of the parallelism of two sentential paraphrases can be pushed further if two such paraphrases share their high level syntactic structure then it is possible to guide the alignment of their words by recursively aligning their syntactic subconstituents Pang et al 16 proposed an implementation of this idea which is illustrated on Figure 1 The two sentences of a paraphrase pair are first syntactically parsed Fusion then takes place in the following way two syntactic subtrees are recursively merged if their root category and the categories of the ordered list of their daughter subtrees match Otherwise when the categories of the daughter subtrees do not match a list of alternative derivations is created at that node In order to avoid mistakenly merging some subtree configurations the authors intro duced a lexical blocking mechanism which prevents merging two subtrees if a content that belongs to the accessible vocabulary of one daughter subtree of the first subtree also belongs to the accessible vocabulary of a different daughter subtree of the second subtree This prevents merging in cases such as active and passive voice sentential paraphrases where in spite of matching high level syntactic structures the agent and patient have been swapped In a last step the obtained parse forest is linearized to yield a word lattice This lattice can finally be reduced by merging edges with the same words that have common prefixes or suffixes All subpaths originating from the same start and end nodes thus represent subsentential units in an equivalence relation 6 The set of all pairs of subsentential paraphrases encoded in the lattice can be effectively extracted by a simple traversal of the lattice Note that the presented algorithm can work with any number of input sentential paraphrases as illustrated in the original article 16 6 Note that in the extreme case of two sentences whose root nodes cannot be merged the smallest pair of equivalent units is made up the two full sentences 72 H Bouamor A Max and A Vilnat Fig 1 Illustration of Pang et al s 16 syntactic fusion algorithm Our implementation of this technique revealed several limitations First this algorithm stops all merging whenever lexical blocking is fired which was probably motivated by the fact that the ob jective of the authors was to generate new sentential paraphrases from a set of existing sentences However because we want to extract as many accurate pairings as possible we allow merging to continue on subconstituents which should not be affected by lexical blocking We also addressed the strong dependency of the algorithm on the precision of the automatic syntactic parses used Indeed we observed many cases in our development data where legitimate merging did not take place because of incompatible parse trees resulting from locally wrong parse trees However wrong syntactic parses can sometimes produce goo d alignments if both parses can be merged in the same way as valid parses would allow Using the Berkeley probabilistic parser 10 the k best parses for both sentences are used and the ith parse of the first sentence and the j th parse of the second one kept are those for which the correponding lattice is the most compact before reduction The intuitive motivation for this choice is that the more compact a lattice is before reduction the more two sentences have been aligned which is a sought property given the parallelism of the input sentences and that the reduction operation should reduce as few as possible nodes that would not have been previously merged due to compatible subparses of the two sentences We chose Comparison of Paraphrase Acquisition Techniques on Sentential Paraphrases 73 a value of 5 for k thus limiting the number of merged configurations to 52 25 per sentence pair 3 4 Combination of Paraphrase Pairs from Different Techniques All the presented techniques for extracting sub sentential alignments were run independently to pro duce candidates for phrases up to 7 tokens pairs of identical phrases excluded Each technique makes use of its own hypotheses and resources and it can be expected that they could have different performances depending on the configurations of the merged sentences Working on efficiently combining these techniques is therefore an interesting issue In this work we started by considering the simple case of output combination by performing set unions and intersections Taking the union of the candidate pairs of two or more techniques is expected to increase the recall of found pairs relative to a reference alignment while taking the intersection is expected to increase the precision of such pairs It is of particular interest to measure how a combination of both measures would behave for such cases 4 4 1 Experimental Framework Corpus Collection In order to build development and test corpora we set up a web based data acquisition experiment Volunteer contributors were asked to translate sets of sentences into French7 The same sets of sentences were translated by several contributors from any of 10 languages from the Europarl corpus 11 of European parliamentary debate The web interface provided the contributors who were not for the most part professional translators with several convenience tools to help them in their task One of them allowed checking a reference translation from Europarl which was not used to build our corpus to make local improvements and corrections once an initial candidate translation was submitted This technique proved quite successful in ensuring an acceptable quality for most submitted translations All translations used in these experiments were manually checked by a unique annotator who followed the rule to remove any translation which showed too strong a bias towards the reference translation between the two versions of a translation In order to measure the similarity degree between lexical paraphrases obtained from different languages we computed the overlap coefficient which represents the lexical overlap percentage between the vocabularies of two sentences S1 and S2 S1 S2 1 CO min S1 S2 7 We considered important to use for our experiments a language for which all our contributors and human annotators had a native or near native command 74 H Bouamor A Max and A Vilnat Table 1 Similarity degree between paraphrases obtained from different languages All tokens Lemmas of content words en es de it pt en es de it pt en 0 90 172 0 64 69 0 59 89 0 63 84 0 62 58 0 90 172 0 65 69 0 61 89 0 66 84 0 64 58 es 0 62 57 0 63 57 0 64 51 0 57 57 0 68 57 0 68 51 de 0 58 67 0 61 53 0 59 67 0 62 53 it 0 65 50 0 66 50 pt The minimum number of pairs for a group was set to 20 in this experiment The left side of Table 1 shows the average coefficient of lexical overlap for all tokens on the selected groups of paraphrases and for different source languages Numbers shown as indices represent the number of sentential paraphrases common translations obtained from two languages For instance the 172 paraphrases obtained from English have an average of 90 common tokens In contrast we find that those from two different languages contain on average between 36 and 42 different tokens These values show as we expected that we obtain more lexical variation using different source languages for semantically equivalent sentences We repeated this experiment considering this time only lemmatised forms of content words Results shown on the right side of Table 1 show a similarity degree which is slightly higher than in the previous experiment varying from 57 to 68 for different languages This still shows a significant level of lexical variation in the translation pro cess at the level of the content words used In order to simulate various degrees of parallelism between two sentences in a pair we built two sub corpora from our full corpus we took a set of 50 sentences which were selected on the basis that 4 independent valid translations from English and one valid translation from German Spanish Italian and Portuguese were available 8 We therefore obtained two corpora of 50 groups of 4 paraphrases each In each group one paraphrase is randomly selected as a reference paraphrase to which the three others will be aligned Three human annotators were then asked to manually align at the token level each of the 300 sentence pairs 2 corpora x 50 groups x 3 alignments The Yawat 7 manual word alignment tool was used to allow aligning sentences by visual selection of phrases and optional checking on word alignment matrices Each sentence pair was annotated by a single annotator as the work of Callison Burch and al 6 reports an acceptable inter annotator agreement rate on such a task 9 We nevertheless asked one of the judges to check all alignments and make the necessary mo difications to make them more uniform Reference biphrases were finally automatically extracted from the token alignment matrix by following the rule previously described at the end of section 3 1 8 9 Note that the version of the Europarl corpus that we used did not contain information about the original language for sentences It should be noted that their work was on news text in English and that their annotators had been provided with a detailed annotation guide Comparison of Paraphrase Acquisition Techniques on Sentential Paraphrases Source English English English 75 Contributed translation Plusieurs orateurs ont An example of a paraphrase group is shown on Figure 2 As can be seen the source language often implies an important bias for the pro duction of the contributed translation which results in part from the fact that our contributors were not professional translators One can also notice that some original translations may express slightly different content resulting from the choices of the sequence of translators involved to obtain these translations 4 2 Experimental Results In order to evaluate and compare the results of the implemented techniques for subsentential paraphrase extraction we followed the Parametric approach described in 6 the set of candidate biphrases extracted from a sentence pair is compared with a set of reference biphrases obtained through human annotation as described in section 4 1 by computing precision and recall values The former value corresponds to the proportion of candidates produced by a technique which are correct relative to the reference biphrases while the latter value corresponds to the proportion of reference biphrases that are extracted by a technique As we are also interested in the combination of both these values when combining candidate sets we also computed an F measure value F1 which considers recall and value as equally important We run the three techniques described in section 3 on our two subcorpora denoted en2fr and xx2fr and computed evaluation scores on their result sets and on result sets obtained for simple combinations Results are given on Table 2 It is first quite apparent that the performance of all techniques both in terms of precision and recall is highly dependent on the nature of the sentential paraphrase pairs which can be interpreted as a higher complexity for aligning sentence pairs pro duced from different languages If Synt is unsurprisingly very sensitive to this Word also seem to be significantly impacted when attempting alignment on less literal sentences which can be compared to the higher difficulty in aligning unrelated languages during training in Statistical Machine Translation 76 H Bouamor A Max and A Vilnat Table 2 Results obtained for each technique and some combinations of their outputs in terms of precision P and recall R of their candidate biphrases relative to reference biphrases The F1 values gives as much importance to precision and recall Word Term Synt TW TW TS TS WS WS TWS Paraphrases obtained by translating from English en2fr P 41 94 41 19 50 16 41 54 55 97 46 48 80 76 40 83 71 21 40 46 R 3578 67 07 3 07 8 77 67 66 2 48 11 26 0 58 67 83 8 02 68 41 F1 51 61 5 87 14 93 51 47 4 76 18 13 1 16 50 98 14 41 50 58 Paraphrases obtained by translating from 4 languages xx2fr P 27 05 35 98 40 46 27 08 42 98 39 46 28 57 26 91 50 15 26 90 R 2517 51 80 3 05 8 26 52 92 1 94 11 08 0 23 53 43 6 63 54 39 F1 35 54 6 07 13 72 35 83 3 72 17 30 0 47 35 79 11 72 36 00 Looking at recall we can note a strong difference between Word on the one side and Term and Synt on the other side with the latter two proposing much fewer paraphrase pairs from the reference alignments The proportion of aligned words is unsurprisingly higher for Word as this statistical word alignment technique attempts aligning as many words as possible although aligning to a NULL token is possible under certain circumstances Nonetheless Word still achieves a reasonable precision score Note however that this technique benefited from a positive bias as it was able to exploit all sentential paraphrase pairings to build its alignment mo del and therefore could effectively make use of redundancy while the two other techniques could not take such information into account as implemented Term seems to be specialized in extracting very fo cused biphrases Synt achieves the best precision overall with a 10 point advantage for the paraphrases obtained from one language over paraphrases obtained from 4 different languages Para 1 German En ce qui concerne les relations internationales la Fig 3 Examples of bi phrases extracted by different techniques from a pair of paraphrases produced from German and Italian sentences biphrases in bold belong to the reference set Comparison of Paraphrase Acquisition Techniques on Sentential Paraphrases 77 The various tested combinations reveal expected gains in recall for union and in precision for intersection Accordingly on the en2fr corpus maximum precision is obtained by computing intersection sets with the results of Synt and maximum recall is obtained by computing union sets with the results of Word Results are roughly similar on the xx2fr corpus with the notable exception of the TS combination which obtained a comparatively much worse performance than on the other corpus Figure 3 illustrates an example of alignment results between two paraphrases obtained from German and Italian whose alignment is difficult as confirmed by the low number of biphrases in the reference set Word was unable to reliably align the words and produced many incorrect biphrases Term and Synt have instead proposed only few candidates which reflects again the difficulties of matching encountered by these two techniques 5 Conclusion and Future Work In this article we have described the task of subsentential paraphrase extraction from sentential paraphrases a resource which is rare but which allows us as we argued to concentrate on the most natural scenario for observing such local paraphrases Furthermore such sentential paraphrases allow us to trivially extract contextual information e g words linked by a grammatical dependency to words in the paraphrase pair associated to paraphrase pairs that can be used to bootstrap context profiles for which the paraphrase pair is valid We have described three techniques initially developed for different purposes which operate at various levels and use different resources and compared them on two subcorpora representing two levels of parallelism for sentences Acceptable levels of precision and recall relative to a reference alignment were achieved and simple combinations of results yielded gains for one of the two metrics Our future work will be organized along three different lines First we want to be able to generalize the obtained subsentential paraphrases to learn paraphrasing patterns which integrate contextual information Then we plan to first independently improve each of the presented techniques and then work on efficient hybrid implementations at extraction time Finally we want to study techniques for validating paraphrases acquired on monolingual parallel corpora on much more readily available monolingual comparable corpora Acknowledgements This work was supported by a grant from LIMSI The authors wish to thank the volunteer contributors who took part in the data collection References 1 Bannard C Callison Burch C Paraphrasing with bilingual parallel corpora In Proceedings of ACL Ann Arbor USA 2005 2 Barzilay R Lee L Learning to paraphrase an unsupervised approach using multiple sequence alignment In Proceedings of NAACL HLT Edmonton Canada 2003 78 H Bouamor A Max and A Vilnat 3 Barzilay R McKeown K Extracting paraphrases from a parallel corpus In Proceedings of ACL Toulouse France 2001 4 Bhagat R Ravichandran D Large scale acquisition of paraphrases for learning surface patterns In Proceedings of ACL HLT Columbus USA 2008 5 Callison Burch C Syntactic constraints on paraphrases extracted from parallel corpora In Proceedings of EMNLP Hawai USA 2008 6 Callison Burch C Cohn T Lapata M Parametric An automatic evaluation metric for paraphrasing In Proceedings of COLING Manchester UK 2008 7 Germann U Yawat Yet Another Word Alignment Tool In Proceedings of the ACL 2008 HLT Demo Session Columbus Ohio pp Digital Learning for Summarizing Arabic Documents Mohamed Mahdi Boudabous1 Mohamed ANLP Research Group MIRACL Laboratory Faculty of Economic Sciences and Management of Sfax FSEGS B P 1088 3018 Sfax Tunisia 2 LPL Laboratory CNRS 1 Abstract We present in this paper an automatic summarization method of Arabic documents This method is based on a numerical approach which uses a semi supervised learning technique The proposed method consists of two phases The first one is the learning phase and the second is the use phase The learning phase is based on the Support Vector Machine SVM algorithm In order to evaluate our method we conducted a comparative study that involves the results generated by our system AIS Arabic Intelligent Summarizer with that realized by a human expert The obtained results are very encouraging and we plan to extend our evaluation on a larger corpus to ensure the performance of our system Keywords Automatic summarization Arabic documents Machine Learning Numerical approaches 1 Introduction In the current context we have to deal with a huge mass of electronic textual documents available through the net We need tools offering fast visualization of the documents so that the user can evaluate its relevance Automatic summarization provides a solution which makes it possible to extract interesting information for an advantageous reuse Indeed the summary helps the reader to decide whether the original document contains the required information or not Moreover in some cases the reader does not need to read the totality of the original document simply because the required information is in the summary 1 Automatic summarization approaches are inspired by various orientations Some approaches rely on symbolic techniques based on the analysis of the discourse and its discursive structure some others are based on numerical treatments based on statistical or even on learning 2 In addition the majority of automatic summarization systems mainly treat documents in Indo European languages such as English and French To our knowledge there are only few implementations of these methods on Arabic language such as LAKHAS 3 and Al Lakas El eli 4 Thus there is an increasing need to develop H Loftsson E 80 M M Boudabous M H Maaloul and L H Belguith automatic summarization systems dedicated to Arabic to handle the increasing amount of electronic documents written in Arabic 1 Thus the achievements in the field of automatic summarization are generally set out again according to the used approaches Mainly three approaches are distinguished numerical symbolic and hybrid Our contribution is in the context of numerical approach and we propose a system for the automatic summarization of Arabic documents which is based on a purely Machine learning ML technique ML technique within the framework classification is shown to be a promising way to combine automatically sentence features 5 In our method a classifier is trained to distinguish between two classes of sentences summary and non summary ones Statistical features that we consider in this work are partly from the state of art and they include cue sentencess and positional indicators 6 title keyword similarity 7 and other features This paper is structured around four sections Section 1 presents most related works to ours Section 2 exposes the proposed method and the summarizing workflow and Section 3 describes the implementation of our approach and the primary results Section 4 presents the conclusion and the future works 2 Related Work Three approaches have been proposed to the summarizing of documents Linguistic approaches based on a formal representation of knowledge contained in documents or on a reformulation technique Indeed these approaches are usually a formal representation of knowledge contained in documents or on reformulation techniques Numerical approaches are based on calculating a score associated for each sentence to estimate its importance relative to other sentences of the document This score is calculated by using various statistical methods probabilistic and learning Hybrid approaches combine the previous approaches to improve the quality of the summary In this paper we explore a numerical approach and present some examples Numerical approaches are essentially based on calculating a score associated for each sentence to estimate its importance The final summary will only keep the sentences that have the highest scores There are two main techniques statistical and learning techniques Recently various authors have explored Machine Learning techniques to summarize documents 7 This is thanks to the best performance of these techniques The learning techniques are classified into three classes The first class is the supervised learning this class is based on two phases the learning phase that use a training corpus of a very large size and the validation phase that use another corpus called validation corpus 8 The second class is the semi supervised learning that has only a learning phase this phase requires a training corpus of small size 9 10 The third one is the non supervised learning which does not require either a training corpus or a validation corpus The numerical approaches can be applied to all types of corpus and can operate in a big number The most important systems which are based on the numerical approaches are LAKHAS system 3 which summarizes Arabic documents in XML format CBSEAS Clustering Based Sentence Extractor for Automatic Summarization system Digital Learning for Summarizing Arabic Documents 81 11 treats the case of multi document summary Its principle is that the more redundant information are the more important they will be Our method treats the numerical approaches that have proven their effectiveness in other languages More precisely we use Machine learning techniques based on semisupervised learning this choice is justified by the fact that it allows involving a system with only a small number of labeled sentences and a large number of not labeled ones 3 Proposed Method In this section we present an overview of the proposed method and the summarizing workflows for the HTML documents 3 1 An Overview of Our Method We propose a new method for the automatic summarization of the newspaper articles in Arabic language It is based on a Machine learning technique More precisely it is based on the semi supervised learning technique which is composed of two phases the first one is the learning phase which allows the system to learn how to extract summary sentences We use Support Vector Machines algorithm SVM for this phase The second phase is the use phase which allows users to summarize a new document Fig 1 presents the details of the proposed method and the two phases 3 2 Summarization Workflow 3 2 1 The Learning Phase In this phase the system designer should provide the training corpus and the extraction features to perform the learning The training corpus is composed of the source documents and their summaries All the documents are initially pretreated to prepare their segmentation in titles sections paragraphs and sentences This segmentation is based on the criteria of punctuation and HTML tags After the segmentation step each sentence of the segmented document will be notified according to some features This step leads to the construction of a set of the vectors V corresponding to the values of the specific features to the sentence These vectors are called extraction vectors or score vectors Each vector is associated with a Boolean criterion which indicates the sentence class summary or non summary The extraction vector has the following structure V1 S1 S2 S3 Sn where Si is the score of the criterion i and n is the number of the criteria In the learning phase extraction vectors are combined to associate a score with each feature and generate rules 3 2 2 The Use Phase In this phase the user provides a HTML document as an input for the system This document is segmented and notified in order to generate a set of extraction vectors The system uses the generated rules to classify each sentence 82 M M Boudabous M H Maaloul and L H Belguith Fig 1 The principle of the proposed method 4 The AIS System The method that we proposed for automatic summarization of Arabic documents has been implemented through the AIS Arabic Intelligent Summarizer system In this section we present the implementation details and the preliminary results 4 1 Implementation Details Our corpus is composed of 500 Arabic documents collected from the web These documents represent newspaper articles selected according to various orientations sport economy education etc The newspaper articles are of HTML type with a UTF 8 coding The summaries of these documents are produced by three human experts Then we use the index of kappa1 to calculate the similarity between human experts and generate one summary for each document 1 http kappa chez alice fr Digital Learning for Summarizing Arabic Documents 83 After the segmentation step we use 15 features to classify each sentence Some of these features are detailed in Table 1 Table 1 Features details Features Position in the text First sentence in the section First sentence in the paragraph Range of the paragraph Tf_idf score Tf score Title keywords Indicative expressions Details Indicates the position of the sentence in the text Indicates if the sentence is the first in the section or not Indicates if the sentence is the first in the paragraph or not Indicates the range of paragraph that contains the sentence Calculates the tf idf of the score Calculates the Tf of the score Presents the number of title keywords in the sentence Presents the number of indicative expressions in the sentence Finally we obtain a file that contains the set of extraction vectors which constitute the input of the learning phase In the learning phase we use the SVM algorithm to learn how to classify the summary and non summary sentences At the end of the learning phase a score is associated with each feature Some features can have a score of zero The SVM algorithm generates a rule by summing scores associated with each feature The system uses the generated rules to calculate the score of each sentence If the score is positive the sentence will be considered as a summary sentence otherwise the sentence is considered as a non summary sentence Finally the system combines summary sentences to obtain the summary 4 2 Preliminary Results We used 60 documents of our corpus to experiment our system i e 50 documents for the learning phase and 10 documents for the evaluation phase The obtained summaries are compared to the human summaries The average measures for Precision Recall and F measure are respectively 0 992 0 991and 0 991 see Table 2 Table 2 Evaluation results Precision 0 992 Recall 0 991 F measure 0 991 Weighted Avg 5 Conclusion and Future Work In this paper we have proposed a method for automatic summarization of Arabic documents Our method is implemented by AIS system and is based on the Machine learning technique Our work focuses on a particular type of documents i e the newspaper articles in HTML format We believe that the preliminary results are very encouraging Indeed the F measure is equal to 0 991 We note that we used a small 84 M M Boudabous M H Maaloul and L H Belguith corpus for the evaluation but as perspectives we plan to extend the evaluation on a larger corpus We also intend to apply the proposed method for other types of documents such as XML and TXT References 1 Concept Based Representations for Ranking in Geographic Information Retrieval Maya Carrillo1 2 1 Abstract Geographic Information Retrieval GIR is a specialized Information Retrieval IR branch that deals with information related to geographical locations Traditional IR engines are perfectly able to retrieve the majority of the relevant documents for most geographical queries but they have severe difficulties generating a pertinent ranking of the retrieved results which leads to poor performance A key reason for this ranking problem has been a lack of information Therefore previous GIR research has tried to fill this gap using robust geographical resources i e a geographical ontology while other research with the same aim has used relevant feedback techniques instead This paper explores the use of Bag of Concepts BoC a representation where documents are considered as the union of the meanings of its terms and Holographic Reduced Representation HRR a novel representation for textual structure as re ranking mechanisms for GIR Our results reveal an improvement in mean average precision MAP when compared to the traditional vector space model even if Pseudo Relevance Feedback is employed Keywords Geographic Information Retrieval Vector Model Random Indexing Context Vectors Holographic Reduced Representation 1 Introduction Geographic Information Retrieval GIR deals with information related to geographic locations such as the names of rivers cities lakes or countries 18 The first and second authors were supported by Conacyt scholarships 208265 and 165545 respectively while the third fifth and sixth authors were partially supported by SNI Mexico This work has been also supported by Conacyt Project Grant 61335 H Loftsson E 86 M Carrillo et al Information that is related to a geographic space is called geo referenced information which is often linked to locations expressed as place names or phrases that suggest a geographic location For instance consider the query ETA in France Traditional IR techniques will not be able to pro duce an effective response to this query since the user information need is very general Therefore GIR systems have to interpret implicit information contained in do cuments and queries to provide an appropriate response to a query This additional information would be needed in the example to match the word France with other French cities as Paris Marseille Lyon etc Recent developments in GIR systems have demonstrated that the GIR problem is partially solved through traditional or minor variations of common IR techniques It is possible to observe that traditional IR engines are able to retrieve the ma jority of relevant documents for most geographical queries but they have severe difficulties generating a pertinent ranking of the retrieved results which leads to poor performance An important source of the ranking problem has been the lack of information Therefore previous research in GIR has tried to fill this gap using robust geographical resources i e a geographical ontology whilst other research has used relevance feedback techniques instead As an alternative our method suggests representing additional information incorporating concept based representations We think that concept based schemes provide important information and that they can be used as a complement to the Bag of Words representations Our goal is therefore to investigate whether combining word based and concept based representations can be used to improve GIR In particular we consider the use of two do cument representations a Bag of Concepts BoC as proposed by Sahlgren and 2 GIR Related Work Geographical Information Retrieval GIR considers the search for do cuments based not only on conceptual keywords but also on spatial information i e Concept Based Representations for Ranking in GIR 87 geographical references 18 Formally a geographic query geo query is defined by a tuple what relation where 19 The what part represents generic terms non geographical terms employed by the user to specify its information need which is also known as the thematic part The where term is used to specify the geographical areas of interest Finally the relation term specifies the spatial relation which connects what and where For example in query Child labor in Asia the what part would be Child labor the relation term would be in and the where part Asia GIR was evaluated at the CLEF forum 14 from 2005 to 2008 under the name of the GeoCLEF task 15 Several approaches were focused on solving the ranking problem during these years Common employed strategies are a query expansion through feedback relevance 6 9 10 b re ranking retrieved elements through adapted similarity measures 7 and c re ranking through information fusion techniques 9 10 11 These strategies have been implemented following two main paths first techniques that have paid attention to constructing and including robust geographical resources in the pro cess of retrieving and or ranking do cuments And second techniques that ensure that geographical queries can be treated and answered by employing very little geographical knowledge As an example of those in the first category previous research employed geographical resources in the pro cess of query expansion Here they first recognize the geographical named entities geo terms in the given geo query by employing a GeoNER1 system Afterwards they then employ a geographical ontology to search for these geo terms and retrieve some other related geographical terms The retrieved terms are then used as feedback elements to the GIR engine However a ma jor drawback with these approaches is the huge amount of work needed in order to create such ontologies for instance Wang et al in 6 employ two different geographical taxonomies Geonames2 and WorldGazetter3 to construct a geographical ontology with only two spatial relations part of and equal This leads to the fact that the amount of geographical information included in a general ontology is usually very small which limits it as an effective geographical resource Some other approaches that fo cus on the re ranking problem propose algorithms that consider the existence of Geo tags4 therefore the ranking function measures levels of topological space proximity or geographical closeness among the geo tags of retrieved do cuments and geo queries 7 In order to achieve this geographical resources are needed Although these strategies work well for certain type of queries in real world applications neither geo tags nor robust geographical resources are always available In contrast approaches that do not depend on any geographical resource have proposed and applied variations of the query expansion pro cess via relevance 1 2 3 4 Geographical Named Entity Recognizer Geonames geo coding web service http www geonames org WorldGazetteer http www world gazetteer com A Geo tags is a label that indicates the geographical focus of certain document or geographical query 88 M Carrillo et al feedback without special consideration for geographic elements 8 9 Despite this they have achieved acceptable performance results sometimes even better than those obtained employing resource based strategies There is also work fo cusing on the re ranking problem it considers the existence of several lists of retrieved documents from one or more IR engines For instance one IR engine can be configured to manage a thematic index i e non geographical terms while another IR engine is configured to manage only geographical indexes 8 9 10 11 18 Therefore the ranking problem is seen as an information fusion problem where simple strategies only apply logical operators to the lists e g AND in order to generate one final re ranked list 10 while others apply techniques based on information redundancy e g CombMNZ Round Robin or Fuzzy Borda 8 10 11 18 Recent evaluation results indicate that there is not a notable advantage of resource based strategies over metho ds that do not depend on any geographical resource 11 Motivated by these results our metho d do es not depend on the availability of geographical resources but we contemplate the use of different lists of ranked retrieved do cuments VSM BoC and HRR looking for improvement of the base ranker efficiency by the combination This work differs from previous efforts in that we consider in the re ranking pro cess the context information and syntactic structure contained in geo queries and retrieved do cuments This additional information is captured by BoC and HRR representations which need special vectors built by Random Indexing RI 3 Random Indexing The vector space model VSM 16 is probably the most widely known IR mo del mainly because of its conceptual simplicity and acceptable results The mo del creates a space in which both documents and queries are represented by vectors This vector space is represented by V a n x m matrix known as term do cument matrix where n is the number of different terms and m is the number of do cuments in the collection The VSM assumes that term vectors are pair wise orthogonal This assumption is very restrictive because the similarity between each document query pair is only determined by the terms they have in common not by the terms that are semantically similar in both There have been various extensions to the VSM One example is Latent Semantic Analysis LSA 17 a method of word co o ccurrence analysis to compute semantic vectors context vectors for words LSA applies singular value decomposition SVD to V the term document matrix in order to construct context vectors As a result the dimension of the pro duced vector space will be significantly smaller by grouping together words that mean similar things consequently the vectors that represent terms cannot be orthogonal However dimension reduction techniques such as SVD are expensive in terms of memory and processing time As an alternative there is a vector space metho dology called Random Indexing RI 3 which represents an efficient scalable and incremental method for building context vectors which express the distributional profile of linguistic terms Concept Based Representations for Ranking in GIR 89 RI overcomes the efficiency problems by incrementally accumulating k dimensional index vectors into a context matrix R of order n x k where k m but usually on the order of thousands This is done in two steps 1 A unique random representation known as index vector is assigned to each context either document or word consisting of a vector with a small number of non zero elements which are either 1 or 1 with equal amounts of both For example if index vectors have twenty non zero elements in a 1024 dimensional vector space they have ten 1s and ten 1s Index vectors serve as indices or labels for words or do cuments 2 Index vectors are used to pro duce context vectors by scanning through the text Every time a target word t occurs in a context c the index vector of the context ic is added to the context vector of t tc Thus the context vector of t is updated as tc ic In this way R is a matrix of k dimensional context vectors that are the sum of the terms contexts Notice that these steps will pro duce a standard termdo cument matrix V of order n x m if we use unary index vectors of the same dimensionality as the number of contexts Such m dimensional unary vectors would be orthogonal whereas the k dimensional random index vectors are only nearly orthogonal However Hecht Nielsen 21 stated that there are many more nearly orthogonal directions in a high dimensional space than truly orthogonal directions which means that context matrix R n x k will be an approximation of the term do cument matrix F n x m The approximation is based on the Johnson Lindenstrauss lemma 21 which states that if we project points in a vector space into a randomly selected subspace of sufficiently high dimensionality the distances between the points are approximately preserved Then the dimensionality of a given matrix V can be reduced by projecting it through a matrix P Rnxk Vnxm Pmxk 1 Random Indexing has several advantages 1 It is incremental which means that the context vectors can be used for similarity computations even after just a few do cuments have been pro cessed 2 It uses fixed dimensionality which means that new data do not increase the dimensionality of the vectors 3 It uses implicit dimensionality reduction since dimensionality is much lower than the number of contexts in the data k m There are works that have validated the use of RI in text pro cessing tasks for example Sahlgren Karlgren 12 demonstrated that Random Indexing can be applied to parallel texts for automatic bilingual lexicon acquisition Sahlgren 4 BoC Document Representation BoC is a recent representation scheme intro duced by Sahlgren 90 M Carrillo et al the union of the meanings of its terms This is accomplished by generating term context vectors for each term within the do cument and generating a do cument vector as the weighted sum of the term context vectors contained within that document Thus the m do cuments in a collection D are represented as s di j 1 hji gj i 1 m 2 where s is the number of terms in do cument di gj is the context vector of term j and hji is the weight assigned to term j in the do cument i according to the weighting scheme considered The context vectors used in BoC are generated using RI and Document Occurrence Representation DOR DOR is based on the work of Lavelli et al 13 and considers the meaning of a term as the bag of do cuments in which it o ccurs When RI is used together with DOR the term t is represented as a context vector u t k 1 bk 3 where u is the number of do cuments containing t and bk is the index vector of do cument k then the contribution of do cument k to the specification of the semantics of term t For instance the context vector for a term t which appears in the do cuments d1 1 0 1 0 and d2 1 0 0 1 would be 2 0 1 1 If the term t is encountered again in document d1 the existing index vector of d1 would be added one more time to the existing context vector to pro duce a new context vector for t of 3 0 2 1 Context vectors generated through this pro cess are used to build do cument vectors as BoC Thus a do cument vector is the sum of the context vectors of its terms 5 HRR Document Representation In addition to BoC we explore the use of syntactic structures prepositional phrases such as in Asia to represent spatial relations and re rank the retrieved do cuments The traditional IR metho ds that include compound terms extract and include them as new VSM terms 4 5 We explore a different representation of such structures which uses a special kind of vector binding called holographic reduced representations HRRs 2 to reflect text structure and distribute syntactic information across the document representation Fishbein and Eliasmith have used the HRRs together with Random Indexing for text classification where they have shown improvement under certain circumstances having BoC as the baseline 1 It is important to mention that up to now we are not aware of other work that uses RI together with HRRs The Holographic Reduced Representation HRR was intro duced by Plate 2 as a metho d for representing compositional structure in distributed representations HRRs are vectors whose entries follow a normal distribution N 0 1 n Concept Based Representations for Ranking in GIR 91 They allow to express structure using a circular convolution operator to bind terms This circular convolution operator binds two vectors x x0 x1 xn 1 and y y0 y1 yn 1 to pro duce z z0 z1 zn 1 where z x y is defined as n 1 zi k 0 xk yi k i 0 to n 1 subscripts are mo dulo n 4 Circular convolution is an operator which do es not increase vector dimensionality making it excellent for representing hierarchical structures We adopt HRRs to build a text representation scheme in which spatial relations SR could be captured Therefore to define an HRR do cument representation the following steps are done a Determine the index vectors for the vocabulary by adopting the random indexing metho d as described earlier b Tag text of do cuments using a Name Entity Recognition System c Bind the tf idf weighted index vector of each location entity to its location role This location role is an HRR which represents a preposition i e in near around across etc extracted from the text considering the preposition preceding the lo cation entity d Add the resulting HRRs where the spatial relations are enco ded to obtain a single HRR vector e Multiply the resulting HRR by an attenuating factor f Normalize the HRR obtained so far to get the vector which represents the do cument For example when given a spatial relation R in Asia R will be represented using the index vectors r1 for Asia where r1 will be joined to its location role an HRR role1 which represents the relation in Then the in Asia vector will be R role1 r1 5 Thus given a do cument D with spatial relations in tx1 ty1 its normalized vector will be built as D role1 tx1 role1 ty1 6 where is a factor less than one intended to lower the impact of the co ded relations Queries are pro cessed and represented in a similar way 6 Experimental Setup We used in our experiments Lemur5 The results pro duced by the VSM configured in Lemur were taken as our baseline Our experiments were conducted using the English do cument collection for the GeoCLEF track This collection is composed of news articles taking 56 472 from the Glasgow Herald British 1995 and 113 005 from the LA Times American 1994 to total 169 477 news articles We worked with the queries of GeoCLEF 2007 and GeoCLEF 2008 a set of 50 queries from number 51 to 100 These queries are described in three parts 5 http www lemurproject org 92 M Carrillo et al a the main query or title b a brief description and c a narrative We took the title and description for all our experiments except for the query representations with HRR where we also considered the narrative statement in order to have improved relations for representation It is worth mentioning that Lemur results worsen when the narrative is included To investigate whether combining word based and concept based representations can be used to improve the GIR we considered two phases The aim of the first was to retrieve as many relevant do cuments as possible for a given query whereas the purpose of the second was to improve the final ranking of the retrieved do cuments by applying BoC and HRR representations Lemur was used to pro cess the 169 477 do cuments first with the queries for 2007 and then with the queries for 2008 Thereafter only the top 1000 documents ranked by the VSM were selected for each query These sub collections were pro cessed to generate the BoC representations of its do cuments and queries BoC representations were generated by first stemming all words in the sub collections using the Porter stemmer We then used Random Indexing to pro duce context vectors for the given sub collection The dimensionality of the context vectors was fixed at 4096 The index vectors were generated with 10 1s and 10 1s distributed over the 4096 dimensions This vector dimension and density were empirically determined These context vectors were then tf idf weighted and added up for each document and query as described earlier to pro duce BoC representations On the other hand HRRs were generated by firstly tagging all sub collections with the Named Entity Recognition System of Stanford University6 Afterwards the single word locations preceded by the preposition in were extracted This restriction was taken after analyzing the queries for each year and realizing that only about 12 of them had a different spatial relation HRRs for documents and queries were then pro duced by generating a 4096 HRR to represent the in relation The in HRR vector was then bound to the index vector of the identified locations by a Fast Fourier Transform implementation of circular convolution tf idf weighted added and multiplied by 1 6 to represent each do cument as described earlier to generate spatial relations representations Finally the evaluation of the results after re ranking the do cuments was carried out with the Mean Average Precision MAP 7 Results We consider two experiments a The aim of the first was to prove that incorporating context information and syntactic structure for re ranking do cuments in GIR could improve precision i e to explore the use of BoC and HRR representations b The ob jective of the second was to compare our strategies against a traditional re ranking mechanism known as Pseudo Relevance Feedback PRF First Experiment Table 1 compares Lemur results with the results pro duced by adding the Lemur similarity values with its corresponding values from BoC to 6 http nlp stanford edu software CRF NER shtml Concept Based Representations for Ranking in GIR Table 1 MAP results for Geo CLEF collection 2007 2008 Lemur Lemur BoC Diff Lemur BoC HRR Diff 2007 0 1832 0 2079 13 48 0 2085 13 81 2008 0 2445 0 2619 7 12 0 2628 7 48 93 pro duce Lemur BoC which is a new list re ranked according to the new values Then the same pro cess as described above was followed but now adding LemurBoC values to HRR values to pro duce Lemur BoC HRR We only considered the set of supported queries that is the queries that have at least one relevant do cument 22 queries in 2007 and 24 in 2008 Notice how MAP is incremented in a constant way always at above 7 From the queries considered in 2007 1 query kept the same MAP pro duced by Lemur after adding BoC The MAP of 5 queries decreased Positively there are 16 queries improved by BoC The favorable percentages of improvement for 10 queries are observed in Table 2 above the 14 When HRRs were added to Lemur BoC only the query 64 that was not improved by BoC and in consequence not in Table 2 was affected This query had a percentage of change equal to 4 35 which was raised to 30 43 by the representation of its 5 spatial relations From the queries shown in Table 2 the unaffected queries have none or one spatial relation while the queries enhanced by adding the HRRs have on average 4 We found that HRRs improve precision when there are distinctive and specific spatial relations for example in Finland instead of in northern Europe Therefore when geographical information given is more precise HRRs help to achieve improved effectiveness However when the number of retrieved relevant documents Table 2 MAP for query improvement by BoC in 2007 and 2008 and their spatial relations Qry ID 52 57 58 60 61 67 69 70 72 75 Results 2008 76 80 82 84 85 86 91 93 95 96 Results 2007 Lemur Lemur BoC 0 0022 0 0038 0 204 0 2473 0 0197 0 0268 0 0022 0 0397 0 0959 0 1321 0 2569 0 2950 0 0701 0 0964 0 043 0 0509 0 4859 0 6179 0 3522 0 4580 0 44 0 4857 0 2518 0 2555 0 0005 0 0015 0 1385 0 2183 0 4554 0 4767 0 0592 0 1101 0 0625 0 1667 0 7375 0 8340 0 491 0 5320 0 2232 0 2418 Diff 72 73 21 23 36 04 1704 55 37 75 14 83 37 52 18 37 27 17 30 04 10 39 1 47 200 00 57 62 4 68 85 98 166 72 13 08 8 41 8 33 SR Lemur BoC HRR Diff additional 0 0 0038 0 00 6 0 2577 4 21 0 0 0268 0 00 1 0 0397 0 00 1 0 1318 0 23 0 0 2950 0 00 1 0 0963 0 14 0 0 0509 0 00 1 0 6179 0 00 2 0 4612 0 70 12 0 5000 2 94 1 0 2555 0 00 3 0 0018 20 00 0 0 2183 0 00 0 0 4767 0 00 2 0 1130 2 63 1 0 1667 0 00 1 0 8340 0 00 6 0 5337 0 26 11 0 2454 1 49 94 M Carrillo et al is low with few relations to compare it is difficult to affect the ranking with the HRRs In 2008 3 queries kept the same MAP pro duced by Lemur after adding BoC The MAP of 9 queries decreased and 12 queries improved Table 2 shows 10 queries improved by BoC where favorable percentages of improvement are depicted From these 10 queries those that were improved after adding the HRRs have at least 2 spatial relations Our conclusion is that the relative small contribution to improve precision demonstrated by HRR is due to the limited amount of spatial relations appearing in the set of queries used We believe that the higher the number of spatial relations to be represented the greater the contribution of this representation We perform a paired t student test to measure the statistical significance of our MAP results The MAP differences for GeoCLEF 2007 resulted significant in a confidence interval of 95 for both Lemur BoC and Lemur BoC HRR however the results are below the median of the year 0 2097 by 0 57 In this year the top system at CLEF reached a MAP of 0 2859 9 However it used a very complex configuration and several external resources four Geographical Gazetteers a Feature Type Thesaurus to categorize geo terms and a Shape Toolbox a database which contains a shape file available for each country The MAP improvement for 2008 is not statistically significant Even so the MAP median of the participants in Geo CLEF 2008 was of 0 2370 15 which is 6 45 lower than that generated by our proposal This year the team at the top obtained a MAP of 0 3040 6 They used two ontologies constructed manually employing information from narratives In addition they used Wikipedia in the retrieval pro cess In contrast we do not use any complex external resource Second Experiment Finally we compare the Lemur BoC HRR results with a traditional re ranking metho d known as Pseudo Relevance Feedback PRF In order to apply this approach we used the VSM representing queries and documents as tf idf vectors and computing similarity with the cosine function PRF treats the n top ranked do cuments as true relevant do cuments for a given query then queries are expanded by adding the k words selected from the n top do cuments and then a second IR pro cess is done with the expanded query Table 3 presents results also for queries with relevant do cuments when the top 2 and 5 do cuments are taken to extract 5 10 and 15 words Query texts are built from title and description fields The values that improve Lemur MAP are Table 3 Difference between PRF MAP and Lemur BoC HRR MAP Lemur BoC PRF with 2 documents HRR 5 terms 10 terms 15 terms GeoCLEF 2007 0 2085 0 1925 0 1617 0 1533 Difference 8 31 28 94 36 01 GeoCLEF 2008 0 2628 0 2539 0 2596 0 2505 Difference 3 51 1 23 4 91 PRF with 5 documents 5 terms 10 terms 15 terms 0 1963 0 1703 0 1593 6 21 22 43 30 89 0 2306 0 2242 0 2101 13 96 17 22 25 08 Concept Based Representations for Ranking in GIR 95 depicted in bold and those obtained with our proposal in italics The difference in MAP between PRF technique and our Lemur BoC HRR proposal is about 6 21 or higher in favor of our metho d in 2007 and 1 23 or higher in 2008 8 Conclusion and Future Work In this paper we have presented two do cument representations for re ranking do cuments and improving precision for GIR RI was used to build context vectors to create BoC representations which capture context information It also defines index vectors used in the HRR representations When working with RI the appropriate selection of the values for vector length and vector density is an open research topic Our results have been compared with the VSM in its Lemur implementation They have showed that i BoC can improve the initial ranker ii HRR representation improved the ranking of queries However its utility could not be totally verified because of the lack of spatial relations to be represented ii we foresee that when more relations are added to the HRRs a better ranking is achieved It should be noted that in the experiments conducted only one type of spatial relation in was considered we think if more types of relations near around across far etc are added as long as they are present in the queries it could lead to improved results iii comparing our metho d against PRF pro duces higher scores for this new method Therefore the overall results demonstrate that our approach is appropriate for re ranking do cuments in GIR We will continue working with other collections where queries have not only spatial relations but other syntactic relations i e compound nouns verb sub ject which could be represented and together with the context information allow us to explore in depth the usefulness of the proposed representations as a mechanism for re ranking do cuments to improve precision References 1 Fishbein J M Eliasmith C Integrating structure and meaning A new method for encoding structure for text classification In Macdonald C Ounis I Plachouras V Ruthven I White R W eds ECIR 2008 LNCS vol 4956 pp 96 M Carrillo et al 7 Martinis B Cardoso N Chavez M S Andrade L Silva M J The University of Lisbon at Geoclef 2006 In Working notes for the CLEF Workshop Spain 2006 8 Larson R R Cheshire at Geoclef 2008 Text and fusion approaches for GIR In Working notes for the CLEF 2008 Workshop Aarhus Denmark 2008 9 Using Machine Translation Systems to Expand a Corpus in Textual Entailment Julio J Castillo National University of Cordoba FaMAF Cordoba Argentina National Technological University FRC Cordoba Argentina Abstract This paper explores how to increase the size of Textual Entailment Corpus by using Machine Translation systems to generate additional t h pairs We also analyze the theoretical upper bound of a Corpus expanded by machine translation systems and propose how it computes the confidence of a classification translator based RTE system At the end we show an algorithm to expand the corpus size using Translator engines and we provide some results over a real RTE system Keywords textual entailment machine translation system double translation process 1 Introduction The Recognizing Textual Entailment RTE task is defined as a directional relationship between a pair of text fragments or sentences called the text T and the hypothesis H Thus we say that T entails H if a human reads T would infer that H is most likely true Machine learning algorithms were widely used for the task of recognizing textual entailment 1 2 3 in the past RTE Challenges Some authors 4 showed how the accuracy increases when we add more training examples and other authors holds the necessity of larger corpus 5 In any case a larger corpus enables a more detailed analysis of the problem domain and will let us build more accurate classifiers In this paper we show how a machine translation system could increase the size of a RTE Corpus and we also suggest how a translator can be used as a tool to help us with classification of new unknowns t h pairs The remainder of the paper is organized as follows Section 2 describes one approach driven by Machine Translation Systems and provides an analysis about the possible increasing size of the Corpus with us proposing a confidence measure for such approach whereas Section 3 shows experimental evaluation and discussion of the results Finally Section 4 summarizes the conclusions and lines for future work 2 Machine Translation Approach In this section we propose to use Machine Translation System to expand the current RTE Corpus sizes H Loftsson E 98 J J Castillo First we define double translation process as the process of starting with a String in English translating it to another language for example Spanish and backing it forward to the English language source Thus our motivation is based on the fact that we could use a double translation process to produce equivalents Texts and Hypothesis and so these new pairs can be taken as training set Also we suggest how a translator can be used as a tool to help us with classification of new t h pairs 2 1 Double Translation Process Double translation process can be defined as the process of starting with an S String in English translating it to a foreign language F S for example Spanish and backing it forward to the English source language F 1 S Thus the observation of that double translation process can increase the Corpus size and also can be generalized using N Translators engine It is important to note that the quality of the translation is given by the Machine Translation System and we will suppose that the sense of the sentence should not be modified by the Translator This indeed is the situation almost for the majority of the cases in our first experiments see Section 2 2 Bellow we provide a theoretical justification of the increment of the corpus size with n pairs using k translators which is O n k 2 t h pairs C q is the increased Notation C is a RTE Corpus which consists of size of the Corpus C using t h t h Tr String String t Tr t Tr t DoubleTranslationOfTheTrTranslator where t and Tr t are in English Lemma Given Tr1 Tr2 Trk translators and C a RTE Corpus with nIf t h pairs Tri t p Tr j h p i j i j i 1 k p 1 n then C k k 1 2 n Proof By structural induction on K As a practical result by using one translator over only one RTE dataset of 800 pairs we could obtain up to 3200 pairs in total and using two translators we could obtain a new dataset with 7200 pairs upper bound Using Machine Translation Systems to Expand a Corpus in Textual Entailment 99 2 2 Other Uses of Machine Translation in RTE Systems Machine Translation can be used as a feature in a machine learning algorithm 6 Indeed by using a MT system it is possible to reduce the complexity of some of the sentences and by this way RTE task will be easier We addressed several simple experiments using Machine Translation engines and we provide some examples below One original t h pair of RTE3 development set is T A leading human rights group on Wednesday identified Poland and Romania as the likely locations in eastern Europe of secret prisons where al Qaeda suspects are interrogated by the Central Intelligence Agency H CIA secret prisons were located in Eastern Europe Translated pair using Microsoft Bing Translator T A group of human rights on Wednesday had identified Poland and Romania as likely locations in Eastern Europe of secret prisons where suspects of Al Qaida are interrogated by the Central Intelligence Agency H Secret CIA prisons were in Eastern Europe Translated pair using Google Translator T A prominent human rights group on Wednesday identified Poland and Romania as the likely locations in eastern Europe of secret prisons where al Qaeda suspects are interrogated by the Central Intelligence Agency H the secret CIA prisons located in Eastern Europe Translated pair using Yahoo Babel Fish Translator T A main group of human rights Wednesday identified Poland and Rumania like the probable locations in Eastern Europe of secret prisons where the company the suspects of al Qaeda interrogate H The secret prisons of the company were located in Eastern Europe In order to move the RTE task towards more realistic application scenarios this year in the TAC RTE5 Challenge the texts come from a variety of sources and include typographical errors and ungrammatical sentences In this context the translation can help with this objective In the previous example eastern is a grammatical error on the original corpus but using Bing Translator this error was fixed Also we can produce some interesting variation such as al Qaeda and Al Qaida In the last pair by using BabelFish the CIA was changed for company which is an error of the translator in the context of this pair It seems an error of Babel Fish resolving acronyms Another use of Translator is to provide synonyms and expression with the same meaning An additional example is given bellow Again the following pair belongs to the RTE3 development set 100 J J Castillo The source pair 170 is entailment Yes T The man known as just the Piano Man has left the hospital and has returned home to his native Germany According to British tabloids the man after losing his job in Paris travelled to the UK through the Channel Tunnel H The Piano Man came from Germany Translated pair using Microsoft Bing Translator T The man known an as the Piano Man has left the hospital and has returned to his native Germany According to British tabloids man after losing her job in Paris traveled to the United Kingdom on the channel tunnel H Piano man came from Germany In this example we see how UK was translated to United Kingdom acronyms resolution and also we see interesting variations as traveled and travelled Thus it seems that by using a MT engine it is possible to improve the semantic resources of RTE Systems 2 3 Confidence of Double Translation Process The double translation process can be used in the production step when addressing machine learning algorithms or otherwise in testing stage in other systems By this way we can define for a test set ti hi pair 1 si T H RTE T H 0 otherwise In this case RTE is a result classification of a system for Recognizing Textual Entailment Then we can define Confidence as Confidence T H Additionally it is possible to prove that n i 1 RTE Tri T Tri H n Thus we can choose a threshold and only accept as a valid entailment a pair that outperforms this threshold Ck k 1 2 n is the upper bound for a double translation process and this bound only could be reached when the predicate Tri t p Trj hp i j i j i 1 k p 1 n holds Generally H is very simple For this reason a double translation process could not affect the Hypothesis On the other hand since T Text is complex we expect that every translator engine will return a different result 3 Experimental Evaluation and Discussion of the Results We show the following algorithm in order to obtain additional given Corpus t h pairs from a Using Machine Translation Systems to Expand a Corpus in Textual Entailment 101 1 Start with a RTE x Corpus with C n 2 For each t i hi i 1 n 3 For each Translator Tr1 Tr2 Trk If Tr j ti ti Tr j hi hi j 1 k Add Tr j ti Tr j hi to Cnew Where C new is the new Corpus obtained as the union between C and the new outputs pairs of the algorithm In order to test our claim over a real RTE system we performed some experiments using our RTE system 3 but without using the NER filter This RTE system is based on a machine learning approach that produces feature vectors for RTE3 RTE4 and RTE5 The chosen features quantify lexical syntactic and semantic level by matching between texts and hypothesis sentences Thus we generated a feature vector with the following components for both Text and Hypothesis Levenshtein distance a lexical distance based on Levenshtein a semantic similarity measure based on Wordnet and LCS longest common substring metric We use the following two classifiers to learn every development set Support Vector Machine and Multilayer Perceptron MLP and we choose Spanish as intermediate language First we started with RTE3 datasets applying the algorithm proposed using only one Machine Translation System Microsoft Bing Translator in order to generate additional pairs which was named as RTE3 Bing Secondly we split this new dataset in 200 400 600 and 800 pairs respectively Finally we tested these training sets over RTE5 development set in two way task We summarized the results in the following table Table 1 Results of two way classification task SVM Classifier Training set RTE3 55 5 RTE3 Bing 56 5 RTE3 200pairs RTE3 Bing 55 67 RTE3 400pairs RTE3 Bing 55 83 RTE3 600pairs RTE3 Bing 56 5 RTE3 RTE3 Bing 56 MLP Classifier 56 58 56 5 58 83 57 67 59 33 Interestingly a not statistical significant different was obtained between RTE3 and RTE3 Bing using SVM or MLP as learning algorithm One important point in these experiments is that adding more training set obtained with our algorithm does not decrease the performance with neither combination of learning algorithm and training sets Finally the best performance of our system was achieved with Machine Learning Algorithm with RTE3 RTE3 Bing dataset and it was obtained an interesting increase but not statistically significant of 3 33 accuracy However additional evidence is needed in order to support this claims but it seems promising Also it is important to note that translation web services as Google 102 J J Castillo Translator or Microsoft Bing is frequently updated Therefore the result of the translation could not be the same at different times and so we would expect better results 4 Conclusions and Future Work In this work we propose the use of Machine Translation systems as a way to increase the corpus sizes We also show the maximum size which can be yield and we present an algorithm to increase training sets We concluded that for our algorithm Microsoft and Google MT are more useful that Yahoo Babelfish MT and also we note that these results are strong dependent of the RTE system architecture However further analysis is required to determine the impact of Machine Translation in Textual Entailment Systems by using others RTE systems Future work will be oriented to explore more deeply how Machine Translation could improve the accuracy of the RTE Systems and to test over different datasets and RTE systems available Finally we will test the double translation process but passing through Spanish Portuguese Dutch and Russian as intermediate language and assessing the improvement that they can yield References 1 Marneffe M MacCartney B Grenager T Cer D Rafferty A Manning C Learning to distinguish valid textual entailments In RTE2 Challenge Italy 2006 2 Zanzotto F Pennacchiotti M Moschitti A Shallow Semantics in Fast Textual Entailment Rule Learners In RTE3 Prague 2007 3 Castillo J Recognizing Textual Entailment Experiments with Machine Learning Algorithms and RTE Corpora In Cicling 2010 Iai Romania 2009 4 Inkpen D Kipp D Nastase V Machine Learning Experiments for Textual Entailment In RTE2 Challenge Venice Italy 2006 5 Newman E Stokes N Dunnion J Carthy J UCD IIRG Approach to the Textual Entailment Challenge In PASCAL Proc of the First Challenge Workshop Recognizing Textual Entailment 2005 6 Agichtein E Askew W Liu Y Combining Lexical Syntactic and Semantic Evidence for Textual Entailment Classification In TAC 2008 Gaithersburg Maryland USA 2008 7 Bentivogli L Dagan I Dang H Giampiccolo D Magnini B The Fifth PASCAL Recognizing Textual Entailment Challenge In Proceedings of Textual Analysis Conference NIST Maryland USA 2009 8 Dolan B Quirk C Brockett C Unsupervised construction of large paraphrase corpora exploiting massively parallel news sources In COLING 2004 Proceedings of the 20th International Conference on Computational Linguistics Association for Computational Linguistics Morristown NJ USA p 350 2004 9 Castillo J A Machine Learning Approach for Recognizing Textual Entailment of the Spanish In North American Chapter of ACL 2010 10 Vanderwende L Dolan W B What syntax can contribute in entailment task Springer Heidelberg 2006 11 Dagan I Dolan B Magnini B Roth D Recognizing textual entailment Rational evaluation and approaches Natural Language Engineering 15 4 Frames in Formal Semantics Robin Cooper Department of Philosophy Linguistics and Theory of Science University of Gothenburg Box 200 S 405 30 1 Introduction In his classic paper on frame semantics Fillmore 12 says Frame semantics comes out of traditions of empirical semantics rather than formal semantics It is most akin to ethnographic semantics the work of the anthropologist who moves into an alien culture and asks such questions as What categories of experience are enco ded by the members of this speech community through the linguistic choices that they make when they talk A frame semantics outlook is not or is not necessarily incompatible with work and results in formal semantics but it differs importantly from formal semantics in emphasizing the continuities rather than the discontinuities between language and experience The ideas I will be presenting in this paper represent not so much a genuine theory of empirical semantics as a set of warnings about the kinds of problems such a theory will have to deal with If we wish we can think of the remarks I make as pre formal rather than non formalist I claim to be listing and as well as I can to be describing phenomena H Loftsson E 104 R Cooper which must be well understoo d and carefully described before serious formal theorizing about them can become possible In this paper we will make a connection between formal semantics and frame semantics by importing into our semantic analysis ob jects which are related to the frames of FrameNet 1 Our way of doing this will be different from for example 1 An important part of our proposal will be that we intro duce semantic objects corresponding to frames and that these ob jects can serve as the arguments to predicates We will use record types as defined in TTR type theory with records 2 3 5 13 to characterize our frames The advantage of records is that they are ob jects with a structure like attribute value matrices as used in linguistics Labels corresponding to attributes in records allow us to access and keep track of parameters defined within semantic ob jects This is in marked contrast to classical model theoretic semantics where semantic ob jects are either atoms or unstructured sets and functions We will first give a brief intuitive introduction to TTR and show how it can be used to represent frames Sect 2 We will then show how we propose to represent the contents of verbs in a compositional semantics Sect 3 The use of frames here leads us naturally from the Priorean tense operators used by Montague to the Reichenbachian account of tense 22 preferred by most linguists working on tense and aspect which involves what we will think of as parameters for speech time event time and reference time The use of frames also leads us to a particular view of Partee s puzzle about temperature and price first discussed in 16 PTQ reprinted as Chap 8 of 17 We will discuss this in Sect 4 Our solution to this puzzle relates to Fernando s 9 11 theory of events as strings of frames which we discuss in Sect 5 Finally Sect 6 we will consider how our proposal can be used to talk about how agents can mo dify word meaning by adjusting the parameters of word contents This relates to a view of word meaning as being in a constant state of flux as we adapt words to describe new situations and concepts In Sect 7 we draw some conclusions 2 Using TTR to Represent Frames Consider the frame Ambient temperature defined in the Berkeley FrameNet2 by The Temperature in a certain environment determined by Time and Place is specified Its core frame elements are given in 1 1 Attribute The temperature feature of the weather Degree A mo difier expressing the deviation of the Temperature from the norm Place The Place where it is a certain Temperature Temperature A quantity or other characterization of the Temperature of the environment Time The Time during which an ambient environment has a particular Temperature 1 2 http framenet icsi berkeley edu Accessed 25th Oct 2009 Frames in Formal Semantics 105 To make things of a manageable size we will not include all the frame elements in our representation of this frame We have also changed the names of the frame elements to suit our own purposes We will say that an ambient temperature frame is a record of type 2 x e time 2 e lo cation ctemp at in Ind Time Loc temp at in e time e lo cation x We will call this type AmbTemp It is a set of four fields each consisting of a label to the left of the colon and a type to the right of the colon A record of type AmbTemp will meet the following two conditions 3 A TTR Approach to Verbs in Compositional Semantics Consider an intransitive verb such as run The simplest way to think of this is as corresponding to a predicate of individuals Thus 3 would represent the type of events or situations where the individual a runs 3 run a However as anybody who has thought about tense and aspect knows we need to get time into the picture somewhere If you look up run on FrameNet3 you will find that on one of its readings it is asso ciated with the frame Self motion Like many other frames in FrameNet this has a frame element Time which in 3 Accessed 1st April 2010 106 R Cooper this frame is explained as The time when the motion occurs This is what Reichenbach 22 called more generally event time and we will use the label etime We will add an additional argument for a time to the predicate and create a frame type 4 4 4 e time TimeInt crun run a e time For the type 4 to be non empty it is required that there be some time interval at which a runs We use TimeInt as an abbreviation for the type of time intervals 5 start Time 5 end Time c start end No constraints are placed on when that time interval in 4 should be Thus this frame type corresponds to a tenseless proposition something that is not available in the Priorean setup 18 19 that Montague employs where logical formulae without a tense operator correspond to a present tense interpretation In order to be able to add tense to this we need to relate the event time to another time interval normally the time which Reichenbach calls the speech time 5 A past tense type anchored to a time interval is represented in 6 6 e time TimeInt ctns e time end start This requires that the end of the event time interval has to precede that start of the speech time interval In order for a past tense sentence a ran to be true we would need to find an ob ject of both types 4 and 6 This is equivalent to requiring that there is an ob ject in the result of merging the two types given in 7 e time TimeInt 7 ctns e time end start crun run a e time Suppose that we have an utterance u that is a speech event of type 8 phon a ran 8 s time TimeInt cutt uttered phon s time 4 5 Of course we are ignoring many other frame elements which occur in FrameNet s Self motion which could be added to obtain a more detailed semantic analysis Uses of historic present tense provide examples where the tense is anchored to a time other than the speech time Frames in Formal Semantics 107 where a ran is the type of strings of an utterance of a concatenated with an utterance of ran Then we can say that the speech time interval in 7 is u s time That is the past tense constraint requires that the event happened before the start of the speech event In a complete treatment both the type of the speech event 8 and the content 7 would be packeted together in a single sign type together with more information about syntax HPSG style see 4 for a preliminary indication of how this would look 7 is a type which is the content of an utterance of the sentence a ran In order to obtain the content of the verb ran we need to create a function which abstracts over the individual a Because frames will play an important role as arguments to predicates below we will not abstract over individuals but rather over frames containing individuals The content of the verb ran will be 9 e time TimeInt 9 r x Ind ctns e time end start crun run r x e time 4 The Puzzle about Temperature and Prices Montague 16 intro duces a puzzle presented to him by Barbara Partee From the premises the temperature is ninety and the temperature rises the conclusion ninety rises would appear to follow by normal principles of logic yet there are o ccasions on which both premises are true but none on which the conclusion is Exactly similar remarks can be made substituting price for temperature Montague s solution to this puzzle in 16 was to analyze temperature price and rise not as predicates of individuals as one might expect but as predicates of individual concepts For Montague individual concepts were mo delled as functions from possible worlds and times to individuals To say that rise holds of an individual concept do es not entail that rise holds of the individual that the concepts finds at a given world and time Our strategy is closely related to Montague s However instead of using individual concepts we will use frames By interpreting rises as a predicate of frames for example of type AmbTemp as given in 2 we obtain a solution to this puzzle e time TimeInt 10 r x Ind ctns e time crun rise r e time Note that a crucial difference between 9 and 10 is that the first argument to the predicate rise is the complete frame r rather than the value of the x field which is used for run Thus it will not follow that the value of the x field i e 90 in Montague s example is rising While there is a difference in the type of 108 R Cooper the argument to the predicates a record as opposed to an individual the type of the complete verb content is the same x Ind RecType that is a function from records of type x Ind to record types This ability to use different types internally but still have the same overall type for the content of the word is convenient for compositional semantics But now the question arises what can it mean for a frame to rise 5 Fernando s String Theory of Events In an important series of papers including 8 9 10 11 Fernando intro duces a finite state approach to event analysis where events can be seen as strings of punctual observations corresponding to the kind of sampling we are familiar with from audio technology and digitization pro cessing in speech recognition When talking about the intuition behind this analysis Fernando sometimes refers to strings of frames in a movie e g in 10 But in many cases what he is calling a movie frame can also be seen as a frame in the sense of this paper as well Thus an event of a rise in temperature could be seen as a concatenation of two temperature frames that is an ob ject of type AmbTemp AmbTemp We have seen a concatenation type previously in our characterization of a phonology type in 8 That is because phonological events are also to be seen as event strings in Fernando s sense 11 shows a type of event for a rise in temperature using the temperature frame AmbTemp in 2 e time TimeInt x Ind start e time e time start Time e location Loc c temp at in start e time start e location start x temp at in x Ind end e time e time end Time e location start e location Loc c temp at in end e time end e location end x temp at in event start end AmbTemp AmbTemp cincr start x end x 11 Here we make use of manifest fields 7 such as 12 e time e time start Time which restrict the type in the field to be a singleton type of the unique ob ject represented after the equality sign Thus 12 is syntactic sugar for 13 e time Time e time start This uses a singleton type represented by Time e time start If some ob ject a is of type T a T then Ta is a type such that b Ta iff b a That is we Frames in Formal Semantics 109 restrict the type to be the type of a unique particular ob ject It should also be noted that path names such as start e time always begin at the root of the record type rather than the most local record type in which they occur 11 is then the type of events where there is a rise in ambient temperature An event e of this type will be of type rise e start e e time In fact we will make the stronger requirement that if r AmbTemp and i TimeInt then e rise r i iff e 11 e start r and e e time i 6 Word Meaning in Flux For all 11 is based on a very much simplified version of FrameNet s Ambient temperature it represents a quite detailed account of the lexical meaning of rise in respect of ambient temperature detailed enough in fact to make it inappropriate for rise with other kinds of sub ject arguments Consider price The type of a price rising event could be represented by 14 e time TimeInt x Ind e time e time start Time e location Loc start commodity Ind cprice of at in price of at in start commodity start e time start e location start x 14 x Ind e time e time end Time e location start e location Loc end commodity start commodity Ind cprice of at in price of at in end commodity end e time end e location end x event start end Price Price cincr start x end x 14 is similar to 11 but crucially different A price rising event is not surprisingly a string of price frames rather than ambient temperature frames The type of price frames Price is given in 15 x e time 15 e lo cation commo dity cprice of at in Ind Time Loc Ind price of at in commo dity e time e lo cation x If you look up the noun price in FrameNet6 you find that it belongs to the frame Commerce scenario which includes frame elements for goods corresponding to our commo dity and money corresponding to our x field If you compare the 6 Accessed 8th April 2010 110 R Cooper FrameNet frames Ambient temperature and Commerce scenario they may not initially appear to have very much in common However extracting out just those frame elements or roles that are relevant for the analysis of the lexical meaning of rise shows a degree of correspondence They are nevertheless not the same Apart from the obvious difference that the predicate in the constraint field that relates the various roles involves temperature in the one and price in the other price crucially involves the role for commo dity since this has to be held constant across the start and end frames We cannot claim that a price is rising if we check the price of tomato es in the start frame and the price of oranges in the end frame This corresponds to a situation which is familiar to us from work on the Generative Lexicon 20 21 where the arguments to words representing functions influence the precise meaning of those words For example fast means something different in fast car and fast road although of course the two meanings are related There are two important questions that arise when we study this kind of data 7 http en wikipedia org wiki Risen _ video _game accessed 4th February 2010 Frames in Formal Semantics 111 The type of the rising event described here could be something like 17 e time TimeInt x Ind e time e time start Time start e lo cation Loc ime cat at start x start e lo cation start e t x start x Ind 17 e time e time end Time end e lo cation Loc cat at end x end e lo cation end e time event start end Position Position cincr height start e lo cation height end e lo cation This relies on a frame type Position given in 18 x Ind e time Time 18 e lo cation Loc cat at x e lo cation e time 18 is perhaps most closely related to FrameNet s Locative relation 17 is structurally different from the examples we have seen previously Here the content of the x field the fo cus of the frame which in the case of the verb rise will correspond to the sub ject of the sentence is held constant in the string of frames in the event whereas in the case of rising temperatures and prices it was the fo cus that changed value Here it is the height of the location which increases whereas in the previous examples it was important to hold the location constant 8 This makes it difficult to see how we could give a single type which is general enough to include both varieties and still be specific enough to characterize the meaning of rise It appears more intuitive and informative to show how the variants relate to each other in the way that we have done The second question we had concerned whether there is a fixed set of possible meanings available to speakers of a language or whether speakers create appropriate meanings on the fly based on their previous experience Consider the examples in 19 19 a Mastercard rises b China rises 8 We have used height start end e location in 17 to represent the height of the location since we have chosen to treat Loc the type of spatial location as a basic type However in a more detailed treatment Loc should itself be treated as a frame type with fields for three coordinates one of them being height so we would be able to refer to the height of a location l as l height 112 R Cooper While speakers of English can get an idea of the content of the examples in 19 when stripped from their context they can only guess at what the exact content might be It feels like a pretty creative pro cess Seeing the examples in context as in 20 reveals a lot 9 20 a Visa Up on Q1 Beat Forecast Mastercard Rises in Sympathy By Tiernan Ray Shares of Visa V and Mastercard MA are both climbing in the aftermarket reversing declines during the regular session after Visa this afternoon reported fiscal Q1 sales and profit ahead of estimates and forecast 2010 sales growth ahead of estimates raising enthusiasm for its cousin Mastercard b The rise of China will undoubtedly be one of the great dramas of the twenty first century China s extraordinary economic growth and active diplomacy are already transforming East Asia and future decades will see even greater increases in Chinese power and influence But exactly how this drama will play out is an open question Will China overthrow the existing order or become a part of it And what if anything can the United States do to maintain its position as China rises It seems like the precise nature of the frames relevant for the interpretation of rises in these examples is being extracted from the surrounding text by a technique related to automated techniques of relation extraction in natural language pro cessing 7 Conclusion We have suggested that a notion of frame can be of use in an approach to formal semantics dealing with hard empirical questions of lexical semantics and linguistic processing The important aspect of our analysis is that we have semantic ob jects corresponding to frames and allow these to be arguments to predicates We have illustrated this with an old puzzle from formal semantics the Partee puzzle concerning the rising of temperature Our solution is very similar in strategy to that originally proposed by Montague It differs in that we use frames where Montague used individual concepts The additional detail of the lexical semantic analysis obtained by using frames comes at a cost however It has as a consequence that there is not obviously a single meaning or even a small set of meanings asso ciated with rise Rather rise means something slightly different for temperatures and prices ob jects rising in 9 http blogs barrons com stockstowatchtoday 2010 02 03 visa up on q1beat forecast mastercard moves in sympathy mod rss_BOLBlog accessed 4th February 2010 http www foreignaffairs com articles 63042 g john ikenberry the rise of china and the future of the west accessed 4th February 2010 Frames in Formal Semantics 113 location not to mention countries as in China rises This spread of meanings seems to be important if we are to draw the kinds of detailed inferences that speakers of a language are able to draw from these examples We have argued that there is no fixed set of meanings but rather that speakers of a language create meanings on the fly for the purposes of interpretation in connection with a given speech or reading event This idea is related to the notion of meaning potential discussed for example in 15 and a great deal of other literature While we have made no precise proposal for how speakers go about creating new situation specific meanings in this paper we believe that the kinds of structured semantic ob jects such as frames that we are proposing in this paper will facilitate an account of this Our record types comprise a collection of fields which can be used to correspond to frame elements New meanings can be constructed from old ones by adding subtracting or modifying such fields thus providing possibilities for change that are not so obviously available in traditional possible world semantics based on functions from possible worlds and times to denotations Acknowledgements This research was supported in part by VR project 20091569 Semantic analysis of interaction and coordination in dialogue SAICD and Swedish Tercentenary Foundation Project P2007 0717 Semantic Coordination in Dialogue SemCoord I am grateful to Jonathan Ginzburg and Staffan Larsson for discussion Previous versions of this material have been presented at the seminar of the Centre for Language Technology in Gothenburg the linguistics seminar of the Department of Philosophy Linguistics and Theory of Science at the University of Gothenburg and the Grammar Festival organized by the Department of Swedish at the University of Gothenburg I am grateful to the audiences on all these o ccasions for useful discussions and improvements References 1 Bos J Nissim M Combining Discourse Representation Theory with FrameNet In Favretti R R ed Frames Corpora and Knowledge Representation pp 114 R Cooper 7 Coquand T Pollack R Takeyama M A logical framework with dependently typed records Fundamenta Informaticae XX Clustering E Mails for the Swedish Social Insurance Hercules Dalianis1 Magnus Rosell1 2 and Eriks Sneiders1 Department of Computer and Systems Science DSV Stockholm University Forum 100 164 40 Kista Sweden 2 KTH CSC 100 44 Stockholm Sweden hercules dsv su se rosell csc kth se eriks dsv su se 1 Abstract We need to analyse a large number of e mails sent by the citizens to the customer services department of a governmental organisation based in Sweden To carry out this analysis we clustered a large number of e mails with the aim of automatic e mail answering One issue that came up was whether we should use the whole e mail including the thread or just the original query for the clustering In this paper we describe this investigation Our results show that only the query and the answering part should be used but not necessarily the whole e mail thread The results clearly show that the original question contains more useful information than only the answer although a combination is even better Using the full e mail thread does not downgrade the result Keywords E government query answering e mail threads Swedish clustering 1 Introduction In Sweden the public authorities have been in the lead to implement E government This includes communication with the citizens through various electronic channels One such channel is to put important information on their web sites Citizens often do not find the information they are seeking however and initiate communication in one of several ways such as telephone calls e mails chat lines etc The Swedish Social Insurance Agency1 SSIA receives more than 10 000 emails from citizens each week These are answered manually by handling officers Many of the e mails from the public are very similar Therefore a lot would be gained if these re occurring questions could be answered automatically or semiautomatically To accomplish this first the common questions must be identified The e mails are either sent directly to an available address or via a web form on the agency s web site When a citizen uses the web form he she also has to assign 1 www forsakringskassan se H Loftsson E 116 H Dalianis M Rosell and E Sneiders a category to it such as parental benefit 2 Previous Research An e mail consists of a header including sender and receiver addresses subject matter etc and body text The body text may also be divided into several zones of different kinds of content such as sender zones author greeting signoff quoted conversation zones reply forward and boilerplate zones signatures advertising disclaimer attachment 2 Previous work on clustering of e mails has discussed the inclusion of different parts of the e mails but has not tried different parts of the body In 3 using a combination of the header and body gives better results than using only the body In 4 the authors let the user weight the importance of the parts to cc from subject date body Whereas previous research was aimed at personal inboxes we study e mails sent to a whole organisation 3 Text Sets and Preprocessing We received about 9 000 e mails from the SSIA Around 4 000 of these were either sent directly without the use of the web form or assigned a miscellaneous category other questions Clustering E Mails for the Swedish Social Insurance Agency 117 3 1 Extracting Parts of the E Mail Thread The e mails we obtained were actually complete e mail threads as they had developed up until the moment they were extracted at the SSIA The number of items in a thread varied from one to 40 although 96 2 percent of all threads where up to four components long The principle of separating thread components was empirically obtained by working on a large number of e mails The system iteratively cuts off the top message It first looks for several successive lines that start with If these are found then everything above these lines is the top message Otherwise the system looks for a typical message separator line such as abc doc com wrote Original message etc in several languages Swedish English Norwegian with a certain level of wording freedom If this does not help it looks for an array of lines that start with From To Date Subject in different languages This method is based on heuristics but works comparably well For our clustering experiments we created four sets of texts Using a few simple rules we removed the e mail headers and characters indicating quotation citation of previous messages in the thread We were not allowed to use the headers due to the sensitive nature of these e mails The results for each of the different sets were tokenised and lemmatised using the Swedish grammar checking program Granska 5 The resulting texts still contained a lot of non word character sequences coming from signatures advertisements disclaimers etc To try to remove them we have used several simple methods We removed words shorter than three characters and longer than 20 since this only removes a few interesting words and captures some of the nonwords Further we removed all words only appearing in only one e mail see appendix in 6 since they did not contribute to the similarity between e mails We also used a common stoplist of Swedish words 3 3 Statistics for the Preprocessed Text Sets Table 1 gives some statistics for the extracted and preprocessed text sets the number of texts and lemmas as well as the average number of different lemmas per text and the average number of texts in which each lemma occurs 4 Clustering For each text set we constructed an ordinary term document matrix with tf idfweights We defined similarity between texts as the cosine measure 118 H Dalianis M Rosell and E Sneiders We have used the K Means algorithm see for instance 7 as it is simple fast and therefore suitable for interactive exploration In the end we want the handling officers to use clustering as a tool to obtain an overview of the trends in the questions and to indentify common questions this in an interactive manner as described in 8 5 Evaluation Since internal clustering quality measures are based on the representation we can not use them to compare results based on different representations i e our text sets External quality measures compare the clustering with a categorisation We have the categorisation made by the citizens It may not be ideal but at least it groups questions with similar content We want clusterings to compare well with this categorisation although we do not expect them to be very similar We prefer a clustering to be more similar rather than less similar however There are many external quality measures We prefer information theory based measures as these take the whole distribution of texts over categories and clusters into account For this reason we use the Normalised Mutual Information NMI between the clustering and the categorisation see 9 Table 1 Clustering results for four different text sets based on the original question only the first answer only both first question and answer and the full e mail thread The first four measures describe the text sets after preprocessing The last measure is the average clustering result in NMI Normalised Mutual Information of 20 K Means clusterings to nine clusters compared with the categorisation Standard deviations are shown in parenthesis Text Set Measure Question Answer Question and Answer Thread Number of Texts 4 652 4 681 4 839 4 841 Number of Lemmas 2 929 2 055 3 956 4 398 Lemmas Text 12 2 9 2 19 5 23 2 Texts Lemma 19 3 21 0 23 9 25 5 NMI 0 28 0 03 0 14 0 02 0 40 0 03 0 38 0 04 6 Experiments and Discussion In Table 1 we report average results in NMI for nine to 20 clusterings of the different text sets with the standard deviation shown in parenthesis In order for two results to be considered different they as a rule of thumb they need not overlap with their standard deviations We choose nine clusters as the categorisation has nine categories The tendencies we describe are similar for other numbers of clusters The result clearly shows that the textual information in the question Question is better than what is in the answer Answer The result gets even better Clustering E Mails for the Swedish Social Insurance Agency 119 however if we also include the answer Question and Answer The result for the entire e mail thread Thread is the same as for Question and Answer As shown in Table 1 the Normalised Mutual Information NMI for the query and answering part is 0 12 units higher than for only the query alone It is not surprising that the result is better for the set of questions than for the set of answers as the categories are chosen by the citizens who also formulated the questions The answers are often shorter than the questions see the statistics in Table 1 use a more formal language and do not necessarily include the same terms as their corresponding question This makes the answers harder to group Combined with the question however the answer does give more information than using only the question for the clustering algorithm to work with as similar questions tend to be answered in similar ways By the same reasoning the result when using the entire thread is used should be even better The questions that require more responses follow up questions with answers however are probably more complicated and therefore harder to group The categorisation of the first question might not even be suitable for the entire thread as it may well include new questions regarding other matters As the full thread Thread contains most information and it performs equally well with the questions and answers Question and Answer we will use it in our further work 7 Conclusions and Future Work We have compared clusterings of e mails sent to the SSIA based on different parts of the e mail thread texts The results clearly show that the original question contains more useful information than only the answer although a combination is even better Using the full e mail thread does not downgrade the result We plan to involve the handling officers in our next investigation We will let them explore clusterings of the e mails and interview them to learn whether an approach like this is actually useful and if it can provide insights help to find common questions and formulate standard answers Acknowledgements We would like to thank Anne Lie Karlsson at References 1 Knutsson O Pargman T Dalianis H Rosell M Sneiders E Increasing the efficiency and quality of e mail communication in e Governmnent using language technology In Proc of IFIP e Government Conference 2010 EGOV 2010 Lausanne Switzerland August 29 September 2 2010 to be published 2 Lampert A Dale R Paris C Segmenting email message text into zones In Proc of the 2009 Conference on Empirical Methods in Natural Language Processing EMNLP 2009 2009 120 H Dalianis M Rosell and E Sneiders 3 Huang Y Govindaraju D Mitchell T M de Carvalho V R Cohen W W Inferring ongoing activities of workstation users by clustering email In OpenMaTrEx A Free Open Source Marker Driven Example Based Machine Translation System Sandipan Dandapat1 Mikel L Forcada1 2 Declan Groves1 3 Sergio Penkale1 John Tinsley1 and Andy Way1 Centre for Next Generation Localisation School of Computing Dublin City University Glasnevin Dublin 9 Ireland 2 Departament de Llenguatges i Sistemes Inform atics Universitat d Alacant E 03071 Alacant Spain 3 1 Abstract We describe OpenMaTrEx a free open source examplebased machine translation EBMT system based on the marker hypothesis comprising a marker driven chunker a collection of chunk aligners and two engines one based on a simple proof of concept monotone EBMT recombinator and a Moses based statistical decoder OpenMaTrEx is a free open source release of the basic components of MaTrEx the Dublin City University machine translation system Keywords example based machine translation corpus based machine translation free open source software 1 Introduction We describe OpenMaTrEx a free open source FOS example based machine translation EBMT system based on the marker hypothesis 1 It comprises a marker driven chunker a collection of chunk aligners and two engines one based on the simple proof of concept monotone recombinator previously released as Marclator 1 and a Moses based deco der 2 OpenMaTrEx is a FOS version of the basic components of MaTrEx the Dublin City University machine translation MT system 3 4 Most of the co de in OpenMaTrEx is written in Java although there are many important tasks that are performed in a variety of scripting languages A preliminary version 0 71 has been released for download from http www openmatrex org on 2nd June 2010 under a FOS licence 2 The architecture of OpenMaTrEx is the same as that of a baseline MaTrEx system 3 4 as MaTrEx it can wrap around the Moses statistical MT 1 2 http www openmatrex org marclator GNU GPL version 3 http www gnu org licenses gpl html H Loftsson E 122 S Dandapat et al deco der using a hybrid translation table containing marker based chunks as well as statistically extracted phrase 3 pairs OpenMaTrEx has been released as a FOS package so that MaTrEx components which have successfully been used 5 6 7 may be combined with components from other FOS machine translation FOSMT toolkits such as Cunei4 8 Apertium5 9 etc 6 Indeed using components released in OpenMaTrEx researchers have previously used statistical mo dels to rerank the results of recombination 10 used aligned marker based chunks in an alternative deco der which uses a memory based classifier 11 combined the marker based chunkers with rule based components 12 and used the chunker to filter out Moses phrases for linguistic motivations 13 The rest of the paper is organized as follows Section 2 describes the principles of training and translation in OpenMaTrEx section 3 describes the EBMT specific components in OpenMaTrEx section 4 describes its software requirements and briefly explains how to install and run the available components A sample experiment performed on a standard task with OpenMaTrEx is described in section 5 and results are compared to those obtained with a a standard statistical machine translation SMT system Concluding remarks are made in section 6 2 OpenMaTrEx Training and Translation Training with OpenMaTrEx may be performed in two different mo des In MaTrEx mode 1 Each example sentence in the sentence aligned source text and its counterpart in the target training text are divided in subsentential segments using a marker based chunker Chunks may optionally be tagged according to their initial marker word to further guide the alignment pro cess 2 A complete 3 4 5 6 7 In statistical MT the term phrase is stretched to refer to any contiguous sequence of words http www cunei org http www apertium org For a longer list of FOSMT systems visit http fosmt info http www fjoch com GIZA html OpenMaTrEx Free Open Source EBMT System 123 Translation may be performed as training in two ways 3 EBMT Specific Components Chunker The main chunker in OpenMaTrEx is based on the marker hypothesis 1 which states that the syntax of a language is marked at the surface level by a set of marker closed category words or morphemes The chunker in OpenMaTrEx deals with left marking languages a chunk starts at a marker word and must contain at least one non marker word Punctuation is also used to delimit chunks Version 0 71 provides marker files for Catalan Czech English Portuguese Spanish Irish French and Italian Marker files specify one marker word or punctuation in each line its surface form its category and optionally its subcategory A typical marker word file contains a few hundred entries Chunk aligners There are a number of different chunk aligners available in OpenMaTrEx The default aligner aligns chunks using a regular Levenshtein edit distance with a combination of costs specified in a configuration file optionally allowing jumps or block movements 3 The default combination uses two costs a probability cost based on word translation probabilities as calculated by using GIZA and Moses see training step 2 in section 2 and a cognate cost based on a combination of the Levenshtein distance the longest common subsequence ratio and the Dice co efficient As in 3 equal weights are used as a default for all component costs specified Translation table merging To run the system in MaTrEx mode markerbased chunk pairs are merged with phrase pairs from alternative resources here Moses phrases Firstly each chunk pair is assigned a word alignment based on the refined GIZA alignments for example please show me por favor 124 S Dandapat et al then carried out from step 6 scoring which calculates the required scores for all feature functions including the reordering mo del based on the combined counts A binary feature distinguishing EBMT chunks from SMT chunks may be added for subsequent MERT optimization as was done in 16 4 Technical Details Required software OpenMaTrEx requires the installation of the following software GIZA Moses IRSTLM 17 and a set of auxiliary scripts for corpus prepro cessing8 and evaluation mteval 9 Refer to the INSTALL file that comes with the distribution for details Installing OpenMaTrEx itself OpenMaTrEx may easily be built simply by invoking ant or an equivalent tool on the build xml provided The resulting OpenMaTrEx jar contains all the relevant classes some of which will be invoked using a shell OpenMaTrEx see below Running A shell OpenMaTrEx has options to initialise the training development and testing sets to call the chunker and the aligner to train a target language mo del with IRSTLM to run GIZA and Moses training jobs to merge marker based chunk pairs with Moses phrase pairs to run MERT optimization jobs and to execute the deco ders Future versions will contain higherlevel ready made options for the most common training and translation jobs For detailed instructions on how to perform complete training and translation jobs in both MaTrEx and Marclator mo de see the README file Test files will be provided in the examples directory of the OpenMaTrEx package 5 A Sample Experiment To show how OpenMaTrEx can be used to improve baseline SMT results we report on a simple experiment using 200 000 randomly selected sentences from the 8 9 http homepages inf ed ac uk jschroe1 how to scripts tgz We currently use version 11b from ftp jaguar ncsl nist gov mt resources OpenMaTrEx Free Open Source EBMT System 125 Table 1 A sample experiment using 200 000 randomly selected sentences from the Spanish English fraction of Europarl as provided for the Third Workshop on SMT WMT08 Testing was performed on the 2 000 sentence test set provided by WMT08 System Baseline Moses Simple merging Feature based merging BLEU 30 59 30 42 30 75 NIST EBMT pairs 7 5171 27 60 7 5156 29 53 7 5269 33 55 nicely with the number of marker based chunks actually used during translation It would be interesting to pursue a more detailed study of the actual differences in the translations produced when using more linguistically motivated chunk pairs 6 Concluding Remarks and Future Work We have presented OpenMaTrEx a FOS EBMT system including a markerdriven chunker with marker word files for a few languages chunk aligners a simple monotone recombinator and a wrapper around Moses so that it can be used as a deco der for a merged translation table containing Moses phrases and marker based chunk pairs OpenMaTrEx releases the basic components of MaTrEx the Dublin City University machine translation system under a FOS license to make them available to researchers and developers of MT systems As for future work version 1 0 will contain among other improvements a better set of marker files improved installing and running pro cedures with extensive training and testing options and improved do cumentation further versions are expected to free open source additional MaTrEx components Acknowledgements The original MaTrEx co de on which OpenMaTrEx is based was developed among others by S Armstrong Y Graham N Gough D Groves H Hassan Y Ma B Mellebeek N Stroppa J Tinsley and A Way We specially thank Y Graham and Y Ma for their advice P Pecina helped with Czech markers and Jim O Regan with Irish markers M L Forcada s sabbatical stay at Dublin City University is supported by Science Foundation Ireland SFI through ETS Walton Award 07 W 1 I1802 and by the Universitat d Alacant Spain Support from SFI through grant 07 CE I1142 is acknowledged References 1 Green T The necessity of syntax markers two experiments with artificial languages Journal of Verbal Learning and Behavior 18 126 S Dandapat et al 3 Stroppa N Way A MaTrEx DCU machine translation system for IWSLT 2006 In Proceedings of IWSLT 2006 pp Head Finders Inspection An Unsupervised Optimization Approach Grupo de Procesamiento de Lenguaje Natural Universidad Nacional de 1 2 Abstract Head finder algorithms are used by supervised parsers during their training phase to transform phrase structure trees into dependency ones For the same phrase structure tree different head finders produce different dependency trees Head finders usually have been inspired on linguistic bases and they have been used by parsers as such In this paper we present an optimization set up that tries to produce a head finder algorithm that is optimal for parsing We also present a series of experiments with random head finders We conclude that although we obtain some statistically significant improvements using the optimal head finder the experiments with random head finders show that random changes in head finder algorithms do not impact dramatically the performance of parsers Keywords Syntactic Parsing Head Finders Genetic Algorithms Bilexical Grammar 1 Introduction Head finder algorithms are used by supervised syntactic parsers to transform phrase structure trees into dependency ones The transformation is carried out by selecting a word as the head in every constituent Head finder algorithms are based on a set of head finder rules which provides instructions on how to find the head for every type of constituent For every internal node of a tree the head finder rules specify which children of the node contains the head word The first set of head rules based on linguistic principles was introduced in 1 and it is used by many state of the art statistical parsers like 2 3 4 5 with only minimal changes The standard set of head finder rules was handcrafted and consequently not optimized for parsing therefore there might exist different sets of head finder rules that can improve parsing performance In this paper we investigate their role in parsing and we experiment with two different state of the art parsers We present an optimization algorithm that improves the standard set of head finders one rule at the time with the goal of finding an optimal set of rules Even though our optimization algorithm produces statistically significant improvements they hardly obtain a better performance In order to better understand why our optimization algorithm cannot produce bigger improvements we test the stability of the search space We test this by generating different head finders we generate head finders that always select the right most and left H Loftsson E 128 M A most subtrees as the trees containing the headword We also generate 137 random sets of rules and we test head finders that are not consistent that is head finders whose set of rules change during the same training session Our optimization procedure aims at finding the best possible set of rules that improves parsing performance Our procedure is defined as an optimization problem and as such it defines the quality measure that it has to optimize its search space and the strategy it should follow to find the optimal set of rules among all possible solutions The search space is the possible sets of rules our procedure optimizes one rule in the set at a time A new set of rules is then created by replacing an original rule in the standard set with its optimized rule The quality measure for a rule set is computed in a serie of steps First the training material is transformed from phrase structure trees into dependency ones using the rule to be evaluated second a bilexical grammar 6 is induced from the dependency tree bank and finally the quality of the bilexical grammar is evaluated The quality of the grammar is given by the perplexity PP and missed samples MS found in the automata of the grammar as explained in Section 3 3 Finally the strategy for traversing the search space is implemented by means of Genetic Algorithms Once we obtain an optimized set of rules we proceed to evaluate its impact in two parsers Collins s parser 5 by means of Bikel s implementation 4 and the Stanford parser 7 These two parsers have their source code available and their head finder algorithms are rather easy to modify We considered experimenting also with the Maltparser 8 but its performance is hard to evaluate when its head finder is modified Our experiments show that the parsing performance of the two parsers is insensitive to variations in head finders They also show that among all possible head finders our optimization procedure is capable of finding improvements Our experiments also show that in the presence of inconsistent head finder rules parsers performance drops 1 6 and 0 9 for Bikel s and for Stanford respectively Our experimental results with random head finders show that modifications in the rule for VP produced the biggest impact in the performance of the two parsers More interestingly our experiments show that inconsistent head finders are more stable than random deterministic head finder We argue that this is the case because the variance on the structures the later produce is considerably bigger with respect to the former Our experiments also show that Stanford parser performance is more stable with respect to variations in the head finder rules than Bikel s All in all our experiments show that even though it is possible to find some new set of rules that improves parsers performance head finding algorithms do not have a decisive impact on the performance of these two state of the art syntactic parsers this also indicates that the reason for their performance lies beyond the procedure that is used to obtain dependencies The rest of the paper is organized as follows Section 2 explains head finding algorithms Section 3 presents the quality measure used in our optimization algorithm while Section 4 discusses the search space and the strategy to traverse it Section 5 presents how random rules are generated Section 6 presents the results of our experiments and Section 7 introduces related work Finally Section 8 concludes the paper Head Finders Inspection An Unsupervised Optimization Approach 129 2 Head Finding Algorithms For each internal node of a phrase structure tree the head finder HF determines which of its subtrees contains the head word of the constituent The procedure of transforming a phrase structure into a dependency one starts in the root of the tree and moves downwards up to the tree preterminals The HF has as a parameter a set of head finder rules R R contains one rule for each possible grammatical category Formally let R be rgc1 rgck where rgci is the head finder rule associated with the grammatical category gci the set gc1 gck is the set of all grammatical category tags like S VP ADJP ADVP SBAR for the Peen Tree Bank 9 PTB S NP VP NP g Tl1 Tlk Tr1 Trs Fig 2 A simple phrase structure tree NNP NNP VBZ Ms Haag plays NNP Elianti Fig 1 Sentence 2 from Section 2 of the PTB The nodes where the head of each constituent is searched for is marked in boldface A head finding rule rgc is a vector of pairs d1 t1 dkn tkn where ti is a non terminal and dm lef t right is a search direction We also use the notation of direction vector to refer to the vector d1 dki which is the projection of the first component of the head finding rule vector Similarly the tags vector t1 tki is the projection of the second component We refer to a head finder rule as one vector of pairs or as a pair of vectors For example l TO l VBD l VBN l MD l VBZ l VB l VBG l VBP l VP l ADJP l NN l NNS l NP is the rule that is associated with tag VP in the standard set of head finder rules It is important to highlight that our definition of head finder rule is a simplification of the standard head finder rule In the standard definition of rules for tags NP and NX sets of non terminals are used instead of simple non terminals Our definition excludes this situation because otherwise the size of the search space makes the optimization procedure unfeasible In order to show how the head finder algorithm works we introduce a few auxiliary functions root T returns the root node of the phrase structure tree T children T g returns the list of children of node g in T and subtreeList T returns the list of subtrees of T ordered from left to right For example if T is the tree in Figure 2 then root T g children T g root Ti1 root Tlk root Tr1 root Trs and subtreeList T Tl1 Tlk Tr1 Trs Using this definition we formally define in Figure 3 the algorithm HFR that transforms a phrase structure tree into a dependency one where R is a set of head finder rules 130 1 2 3 4 5 6 7 8 9 10 11 12 13 M A Consider the tree in Figure 1 Suppose that the head finder rule for tag VP is l TO l VBD l VBN l MD l VBZ l VB l VBG l VBP l VP l ADJP l NN l NNS l NP When the head finder algorithm reaches the node VP it looks from left to right a tag TO since it cannot find such tag it looks from left to right a tag VBD it keeps changing what it is looking for until it looks from left to right for a tag VBZ Once it has found it it marks that subtree as head and it recursively inspects all subtrees 3 A Quality Measure Based in Bilexical Grammars This section introduces the quality measure used in our optimization procedure Our procedure is based on the optimization of a quality measure q defined over a set of head finder rules In order to compute q for a given set of rules we proceed as follows We transform Sections 01 22 of PTB into dependency structures Using the resulting dependency tree bank we build a bilexical grammar and finally we compute a quality measure on this grammar The measure over the bilexical grammar is a formula that takes into account PP and MS of the set of automata that define the bilexical grammar 3 1 Bilexical Grammars Bilexical grammars are a formalism in which lexical items such as verbs and their arguments can have idiosyncratic selective influences on each other We define a bilexical grammar B as a 3 tuple Ro rc wC lc cC where Head Finders Inspection An Unsupervised Optimization Approach 131 0 Researchers NN 1 can MD 2 apply VB 3 for IN 4 permission NN 5 to 6 use TO VB 7 the DT 8 probes NN 9 for IN 10 brain NN 11 studies NN 12 dot DOT Fig 4 Tree extracted from the PTB file wsj 0297 mrg and transformed to a dependency tree node are ordered in a sequence with respect to each other and the node itself so that each node may have both left children that precede it and right children that follow it A dependency tree T is grammatical if for every tag token c that appears in the tree lc accepts the possibly empty sequence of c s left children from right to left and rc accepts the sequence of c s right children from left to right 3 2 Induction of Bilexical Grammars Bilexical grammars can be induced from a dependency tree bank by inducing two automata for each tag in C The induction of Bilexical Grammars is carried out in a supervised fashion Our training material comes from transforming Sections Table 1 Bags of left and right dependents extracted from dependency tree in Figure 4 Left dependents are to be read from right to left All displayed sets are singletons Word 0 1 2 3 4 i i i Tlef Tright t NN NN NN MD MD NN MD VB DOTSYB VB VB VB IN IN IN IN NN NN NN NN TO 132 M A 3 3 Quality Measure for Grammars The measure q of a set of head finder rules is defined as a measure of the grammar that is built from using the head finder rule to transform the PTB into dependencies The measure is then defined over the automata that defined the grammars The measure over bilexical grammars contains two components The first one called test sample perplexity PP is the per symbol log likelihood of strings belonging to a test sample according to the distribution defined by the automaton The minimal perplexity PP 1 is reached when the next symbol is always predicted with probability 1 while PP corresponds to uniformly guessing from an alphabet of size The second component is given by the number of missed samples MS A missed sample is a string in the test sample that the automaton fails to accept One of such instance suffices to have PP undefined Since an undefined value of PP only witnesses the presence of at least one MS we count the number of MS separately and compute PP without considering MS The test sample that is used to compute PP and MS comes from all trees in sections 00 01 of the PTB These trees are transformed to dependency ones by using HFRc where Rc is the candidate set of rules Better values of M S and P P for a grammar mean that its automata capture better the regular language of dependents by producing most strings in the automata target languages with fewer levels of perplexity The quality measure of grammar is then the mean of PP s and MS s for all automata in the grammar 4 Building and Traversing the Search Space This section introduces the search space and the strategy to traverse it in our optimization procedure The search space consists of different sets of head rules The standard set of rules contains 26 rules The longest is the one associated with ADJP it contains 18 entries Finding a new set of rules means that we should find a new set of 26 vectors For each candidate rule we have to transform the PTB build the bilexical grammar and compute PP and MS for all automata It takes us 1 2 minutes to evaluate one candidate set of rules In principle all possible head rules can be candidate rules but then the search space would be huge and it would be computationally unfeasible to traverse it In order to avoid such search space we run a series of experiments where we optimize one rule at a time For example one of our experiments is to optimize the rule associated with VB Our search space contains all possible set of rules where all rules exept the one associated to VB are as in the standard set of head finder rules To optimize one rule we traverse the search space with Genetic Algorithms Genetic Algorithms need for their implementation 1 Definition of individuals each individual codifies one candidate of the head finder rule that is being optimize 2 A fitness function defined over individuals the quality measure is computed by constructing a set of head finder rules by adding the candidate rule to the standard set of rules building a bilexical grammatical and evaluating it as described in Section 3 3 Finally 3 A strategy for evolution we apply two different operations to individuals namely crossover and mutation crossover gets 0 95 probability of being applied while mutation gets 0 05 We select individuals using the roulette wheel strategy 11 In our experiments Head Finders Inspection An Unsupervised Optimization Approach 133 in each generation there is a population of 50 individuals we let the population evolve for 100 generations The mutation function is easily defined by computing a random permutation of the rule tags vector and a random sample of its direction vector The crossover operation is defined as follows Let g1 gi gn and h1 hi hn be the tag vectors of two different individuals and let i be random number between 1 and n The crossover produces two new individuals The tag vector of one of the individuals is defined as follows sub g1 gn hi 1 hn 5 Stability of Head Finders As it is shown in Section 6 our optimization method only improves the performance of Bikel s parser The reason for the lack of improvement of Stanford parser can be either because our optimization method is ill defined or because the parser is indifferent to the set of head finders rules However using no head finder that is non dependency grammars performance never reaches beyond 75 So heads and dependencies based on heads are an important element in parsing performance In this section we present experiments that try to shed light on this issue We tested 1 randomly generated head finder rules 2 head finders whose rules were reversed 3 head finders that always choose right or left and 4 inconsistent head finders Random Head Finders We experiment generating several sets of head finder rules A random set of head finder rules is created by replacing one rule in the standard set of head finder rules by one random rule A random rule is created by randomly permuting the elements of both the tags vector and the direction vector Experimental results for this head finder are shown in Table 3 A In Figure 5 the first row shows the head rule defined in the standard set for category S The second row shows a random permutation of this rule The last row shows a reverse permutation of the original one The reverse permutation of a rule is obtained by reversing the order of its tag vector and leaving its direction vector unchanged In this example the rule presented in the second row is calculated using the permutation 7 1 4 8 2 3 6 5 for the tags vector and its random direction vector was l r r l l l l l We generate 119 rules and consequently 119 different head finder rule sets Each of these sets differs in one rule from the standard set In this way we show the impact of each rule in the overall parsing performance We also experiment with a set of rules were all of its rules were randomly generated We test 7 of such random sets for each parser Experimental results for this head finder are shown in Table 4 134 M A Original S l TO l IN l VP l S l SBAR l ADJP l UCP l NP sampled rule S l UCP r TO r S l NP l IN l VP l ADJP l SBAR reverse rule S l NP l UCP l ADJP l SBAR l S l VP l IN l TO Fig 5 The first row shows the original Collins s head rule for S The second row shows a random permutation of the original rule The last row is the reverse of the original rule Reverse Rules There are 26 rules in the standard set of head finder rules We generate 26 new sets by changing one rule at a time by its reverse The reverse of a rule is constructed by reading it from left to right The impact of reverse rules in parsing performance is shown in Table 3 B Left Most and Right Most Head Finders We define two special algorithms for finding heads The always leftmost and always rightmost algorithm chooses for each internal node the leftmost and rightmost subtree respectively These are special cases of head finder algorithms that cannot be expressed with a set of rules In order to implement these algorithms we modified both parser implementations The results are shown in Table 2 B Non deterministic Head Finders All previous experiments were based on deterministic head finders every time they are used to transform a given phrase structure tree they transform into the same dependency tree We implemented a non deterministic head finder algorithm this algorithm flips a coin every time it has to decided where the head is When this head finder is used to transform a phrase structure into a dependency tree it produces different dependency trees for every time it is called We report results for 7 of these experiments for each parser they can be seen in Table 4 6 Experimental Results In this section we show the results of all our experiments In all experiments we used Sections 02 21 of the PTB for training and Section 23 for testing The optimization algorithm use Sections 00 01 for computing the quality measure defined on automata Our experiments aim to analyze the variation in performance by changing one or more head rules in the standard set of head rules The rules that are modified by our experiments correspond to tags WHADJP CONJP WHNP SINV QP RRC S ADVP NAC SBAR VP SQ ADJP WHPP SBARQ PP WHADVP Table 2 A shows the performance of the set of rules produced by the optimization procedure Each row displays labeled precision labeled recall significance level pval and harmonic mean F1 The baseline row reports the performance of both parsers using the standard set of rules pval was computed against the baseline We consider a result as statistically significant if its significance level pval is below 0 05 Performance value were computed using the evalb script significance values were measured using Bikel s Randomized Parsing Evaluation Comparator script The table shows that the performance in the Stanford parser using our optimized head finder set of rules is below the baseline however this decrease in performance is not statistically significant Head Finders Inspection An Unsupervised Optimization Approach 135 Table 2 A The result of the experiments corresponding to the optimized head finder The upper part shows evaluation in Bikel s parser while the bottom with Stanford parser B First column shows the F1 when all worst performing rules reported in Table 4 A are put together Second and third columns show average F1 for the always right most and always left most head finders Num Bikel Baseline optimal head L R L P pval R pval P F1 88 53 88 63 88 583 88 72 88 85 0 006 0 002 88 785 85 742 0 131 85 727 Parser F1 worst choice F1 Right Most F1 Left Most Bikel 82 486 83 024 85 102 Stanford 84 092 84 206 85 566 Stanford Baseline 85 26 86 23 optimal head 85 24 86 22 0 098 A B The best set of head rules was obtained by combining all rules that our optimization method produced Table 4 shows the results of the random head rule generation The Table contains one row per each rule that was permuted We consider 17 different rules for each we build 7 17 new random rules Each row shows the maximal the minimal and the average F1 measure we obtained Table 3 B shows F1 measure for the 17 rules that were obtained by reversing one of the rules in the standard set at a time From Tables 3 A and B we can see that the rule defined for tag VP has the greatest impact on the performance of both parsers The first column of Table 2 B shows the results for the head finder that is built by using the 17 rules with the worst performance in experiments in Table 4 The second and the third columns show the results for the head finder algorithms that choose always the leftmost and always the rightmost respectively Table 3 A Parsing results obtained by replacing one rule in the standard set by a random rule Each row shows the average maximal and minimal impact in the F1 measure for each parser B Experiments result for each Head finder built with the reverse of head rule One column for each parser rule tag WHADJP CONJP WHNP SINV QP RRC S ADVP NAC SBAR VP SQ ADJP WHPP SBARQ PP WHADVP avg 88 589 88 586 88 586 88 485 88 538 88 588 88 092 88 586 88 594 88 195 87 330 88 571 88 616 88 583 88 583 88 617 88 607 Bikel max 88 596 88 601 88 596 88 608 88 604 88 595 88 569 88 615 88 607 88 653 88 471 88 592 88 698 88 583 88 583 88 668 88 706 min 88 583 88 582 88 577 88 009 88 474 88 583 87 458 88 564 88 581 88 013 85 870 88 562 88 566 88 583 88 583 88 583 88 583 avg 85 739 85 739 85 739 85 749 85 747 85 739 85 689 85 740 85 743 85 734 85 247 85 739 85 727 85 739 85 739 85 740 85 739 Stanford max min 85 739 85 739 85 739 85 739 85 739 85 739 85 772 85 730 85 759 85 732 85 739 85 739 85 726 85 652 85 743 85 739 85 743 85 743 85 739 85 733 85 612 84 918 85 739 85 739 85 739 85 718 85 739 85 739 85 739 85 739 85 740 85 739 85 739 85 739 Gram tag WHADJP SBAR CONJP VP WHNP SQ SINV ADJP QP WHPP RRC SBARQ S PP ADVP WHADVP NAC B F1 88 596 87 990 88 600 85 820 88 596 88 562 88 465 88 626 88 490 88 583 88 595 88 583 88 178 88 583 88 613 88 593 88 603 S F1 85 739 85 733 85 739 84 249 85 739 85 739 85 758 85 726 85 748 85 739 85 739 85 739 85 670 85 739 85 739 85 739 85 743 A B 136 M A Table 4 Experiments result of random choice of rules for each experiment we show the impact in the F1 measure for the average maximal and minimal Rand No Det Random Det Parser avg max min avg max min Bikel 86 976 87 166 86 754 86 001 87 974 83 857 Stanford 84 810 84 997 84 691 84 805 85 625 84 360 Table 4 shows results for the non deterministic and deterministic head finders The set of rules are obtained by changing rules for the 17 tags considered in our work We run 7 tests for deterministic and 7 tests for non deterministic head finders In both cases we calculate the average the maximum and the minimum obtained for the measure F1 The results show that the non deterministic head finder is more stable because the variation between the minimum and the maximum results is lower A priori this is a surprising result because the dependency trees used to induce the grammar during the training phase have percolated inconsistent heads We think that the non deterministic head finders are more stable because in average they make more correct choices In contrast if deterministic head finders contain an erroneous rule all the resulting dependency trees are wrong This fact is also supported by the results reported in Table 2 B It shows that using the head finder built out of the worst performing rules is the one with the worst performance in both parsers The performance drops nearly 6 and 1 9 for Bikel and Stanford respectively The right most head finder decline is the next considering the performance downfall 7 Related Work Similar work has been published in 12 and an improved version can be found in the Bikel s thesis 4 In this work the authors tried to induce head rules by means of defining a generative model that starts with an initial set of rules and uses an EM like algorithm to produce a new set of rules that maximize the likelihood They used different sets of rules as seeds for the EM but the approach only shows improvement when the standard set of rules is used In contrast to our approach none of their improvements were statistically significant They also show that when the seed is a set of random rules the overall performance decreases In a different approach 13 the authors present different unsupervised algorithms for head assignments used in their Lexicalized Tree Substitution Grammars They study different types of algorithms based on entropy minimization familiarity maximization and several variants of these algorithms Their results shows that using the head finder they induced they obtain an improvement of 4 over a PCFG parser using an standards head assignments In our work we don t use lexicalized grammars Our approach is based on improvements to a given rule set as opposed to theirs where they use unsupervised methods to find assigments for heads 8 Conclusions In our approach we aim at generating dependency trees that improve the performance of the statistical parser To do so we vary the head rules that are used while transforming Head Finders Inspection An Unsupervised Optimization Approach 137 constituent trees into dependency ones Besides finding some new rules which hardly improve parsers performance we found that variations in head finding algorithms do not have a decisive impact on the syntactic parsers performance to the extent that an aleatory translation of the phrase structure trees into dependency ones can be used without damaging considerably the parsing performance However removing head finding altogether produces a 10 decrease in performance considerably higher than the 1 9 and 6 decreases in performance produced by the worst possible head finders Therefore head finders are crucial for the performance of dependency parsers but their variations are not References 1 Magerman D M Natural language parsing as statistical pattern recognition Ph D thesis Stanford University 1994 2 Charniak E A maximum entropy inspired parser In NAACL 2000 2000 3 Klein D Manning C Accurate unlexicalized parsing In Proc 41st ACL 2003 4 Bikel D On the Parameter Space of Generative Lexicalized Statistical Parsing Models PhD thesis University of Pennsylvania 2004 5 Collins M Three generative lexicalized models for statistical parsing In ACL 1997 1997 6 Eisner J Bilexical grammars and a cubictime probabilistic parser In Proceedings of IWPT04 1994 7 Klein D Manning C Distributional phrase structure induction In CoNLL 2001 2001 8 Nivre J A A Maltparser A language independent system for data driven dependency parsing In Natural Language Engineering pp Estimating the Birth and Death Years of Authors of Undated Documents Using Undated Citations Yaakov HaCohen Kerner1 and Dror Mughaz2 1 1 Dept of Computer Science Jerusalem College of Technology 91160 Jerusalem Israel 2 Dept of Computer Science Bar Ilan University 52900 Ramat Gan Israel kerner jct ac il myghaz cs biu ac il Abstract Precious historical treasures might be hidden between the lines of a text There are many implicit details which can be extracted from a text particularly if one has access to an entire corpus of texts pertaining to the given subject One of these details is the identification of the era in which the author of the given document s lived For rabbinic documents written in Hebrew and Aramaic which are almost without exception undated and do not contain any bibliographic section this problem is extremely important The aim of this novel research is to find in which years an author was born and died based on his documents and the documents of other authors whose birth and death years are known who refer to the author under discussion or are mentioned by him Such estimates can help determine the time frame in which certain documents were written and in some cases identify an anonymous author In the framework of this research we formulate various kinds of iron clad heuristic and greedy constraints defining the birth and death years of an author based on citations referring to him or mentioned by him Experiments applied on a corpus containing texts composed by rabbinic authors show reasonable results Keywords Citation analysis Hebrew Hebrew Aramaic documents knowledge discovery time analysis undated citations undated documents 1 Introduction Citations are a defining feature of many kinds of documents e g academic legal and religious Authors cite previous works which are related in some way to their own work or to their discussion Citations included in documents are important information resources of interest to researchers Therefore automatic extraction and analysis of citations from documents are of great importance Recent developments e g computerized corpora and search engines enable accurate extraction of citations As a result citation analysis has an increased importance A citation is a brief reference in the body of the text to a source of published information A reference includes bibliographic details about a source that is mentioned in a citation The reference is found at end of a document in a reference list Citations are presented in agreed typographical formats Different disciplines have different conventions citation in footnotes citations with numbers e g 1 or mixed symbols such as Cohen98 or Cohen 1998 Harvard style citations H Loftsson E Estimating the Birth and Death Years of Authors of Undated Documents 139 Garfield 2 was the first to propose automatic production of citation indexes extraction and analysis of citations from corpora of academic papers Powley and Dale 5 develop techniques to extract from a given academic paper a list of citations and for each citation the corresponding reference in the reference list They find each instance of a citation in the body of the paper parse it into a set of author names and years and find the segment of text from the references which contains the corresponding reference Teufel et al 8 use extracted citations and their context for automatic classification of citations to their citation function the author s reason for citing a given paper Some research has been done concerning the improvement of retrieval performance using terms Ritchie et al 6 show that document indexing based on combinations of terms used by citing documents and terms from the document itself give better retrieval performance than standard indexing of the document terms alone In 7 Ritchie et al investigate how to select text from around the citations in order to extract good index terms in order to improve retrieval effectiveness Citations are a defining feature not just of academic papers but also and even more of rabbinic responsa answers written in response to Jewish legal questions authored by rabbinic scholars Citations included in rabbinic literature are more complex to define and to extract than citations in academic papers written in English because 1 In contrast to academic papers there is no reference list that appears at the end of a responsa 2 There is an interaction with the complex morphology of Hebrew and Aramaic For example citations can be presented with different types of prefixes e g and when and when in and in and when in included in the citation word s 3 Natural language processing in Hebrew and Aramaic has been relatively little studied 4 Many citations in Hebrew Aramaic documents are ambiguous For instance a a book titled magen avot was composed by four different Jewish authors and b The abbreviation m b relates to two different Jewish authors and has also other meanings which are not authors names and 5 At least 30 different syntactic styles are used to present citations This number is higher than the number of citation patterns used in academic papers written in English e g see 5 Each specific document written by a specific author can be referred to in at least 30 general possible citation syntactic styles Furthermore each citation pattern can be expanded to many other specific citations by replacing the name of the author and or his book responsa by each one of their other names e g different spellings full names short names first names surnames and nicknames with without title and abbreviations The citation recognition in this research is done by comparing each word to a list of 298 known authors and many of their books responsa This list contains 19 506 specific citations that relate to names nick names and abbreviations of these authors and their writings Basic known citations were collected and all other citations were produced from them based on an automatic extension process using regular expressions 140 Y HaCohen Kerner and D Mughaz Hebrew Aramaic documents in general and Hebrew Aramaic responsa in principle present various interesting text mining problems Firstly Hebrew is richer in its morphology forms than English According to linguistic estimates Hebrew has 70 000 000 valid inflected forms while English has only 1 000 000 1 In Hebrew there are up to seven thousand declensions for one stem while in English there are only a few declensions Secondly these kinds of documents include a high rate of abbreviations about 20 while more than one third of them about 8 are ambiguous 4 A previous research that works on corpora which contain responsa referring to Jewish law written in Hebrew Aramaic dealt with text classification 3 In this research HaCohen Kerner et al investigate whether the use of stylistic feature sets and or name based feature sets is appropriate for classification of documents to the ethnic group of their authors and or periods of time when the documents were written and or places where the documents were written In addition HaCohen Kerner et al 4 have experience with the processing of such texts from the viewpoint of disambiguation of ambiguous abbreviations The current research is a continuation of this long term research interest In this research we present a novel model that estimates the birth and death years of a given author using undated citations of other authors whose birth and death years are known who refer to him or mentioned by him The documents are undated nontime stamped and mentions of years or historical events in the documents are very rare The estimations are based on various constraints of different degree of certainty iron clad heuristic and greedy constraints The constraints are based on general citations without cue words and citations with cue words such as father son rabbi teacher student friend and late of blessed memory This paper is organized as follows Section 2 presents various constraints of different degree of certainty iron clad heuristic and greedy constraints that are used to estimate the birth and death years of responsa authors Section 3 describes the model Section 4 introduces the tested dataset the results of the experiments and their analysis Section 5 summarizes concludes and proposes future directions 2 Citation Based Constraints This section presents the citation based constraints formulated for the estimation of the birth and death years of an author X based on his documents and on other authors Yi documents who mention X or one of his documents We assume that the death years for those who died and birth years of all authors are known excluding those of the investigated author Below are given some notions and constants that are used Estimating the Birth and Death Years of Authors of Undated Documents 141 Various types of citations exist general citations without cue words and citations with cue words such as father son rabbi teacher student friend and late of blessed memory Another classification of the discussed citations is to those referring to living authors and those referring to dead authors In contrast to academic papers responsa include much more citations to dead authors than to living authors We will introduce citation based constraints of different degrees of certainty ironclad I heuristic H and greedy G Iron clad constraints are absolutely true without any exception Heuristic constraints are almost always true Exceptions can occur when the heuristic estimates for MIN MAX and MIN_FATHER are incorrect Greedy constraints are rather reasonable constraints for responsa authors However sometimes wrong estimates can be drawn while using these constraints Each constraint will be numbered and its degree of certainty will be presented in brackets 2 1 Iron clad and Heuristic Constraints First of all we present two general heuristic constraints based on authors that cite X which are based on regular citations i e without mentioning special cue words e g friend son father and rabbi General constraint based on authors that were cited by X D X MAX B Yi MIN 1 H X must be alive when he cited Yi so we can use the earliest possible age of publishing of the latest born author Y as a lower estimate for X s death year General constraint based on authors that cite X B X MIN D Yi MIN 2 H All Yi must have been alive when they cited X and X must have been old enough to publish Therefore we can use the earliest death year amongst such authors Yi as an upper estimate of X s earliest possible publication age and thus his birth year Posthumous citation constraints Posthumous constraints estimate the birth and death years of an author X based on citations of authors who refer to X as late of blessed memory or on citations of X who mentions other authors as late Figure 1 describes possible situations where various kinds of authors Yi i 1 2 3 refer to X as late The lines depict authors life spans where the left edges represent the birth years and the right edges represent death years In this case as all Yi refer to X as late we know that all Yi died after X and some of the Yi might be still alive but we do not know when they were born in relation to X s birth Y1 was born before X s birth Y2 was born after X s birth but before X s death and Y3 was born after X s death X Y1 Y2 Y3 time axix Fig 1 Citations mentioning X as late 142 Y HaCohen Kerner and D Mughaz D X MIN D Yi 3 I However we know that X must have been dead when Yi cited him as late so we can use the earliest born such Y s death year as an upper estimate for X s death year Like all authors dead authors of course have to comply to constraint 2 as well Let us now look at the cases where the author X we are studying refers to other authors Yi as late Figure 2 describes possible situations where X refers to various kinds of authors Yi i 1 2 3 as late All Yi died before X s death or maybe X is still alive Y1 died before X s birth Y2 was born before X s birth and died when X was still alive and Y3 was born after X s birth and died when X was still alive Y1 Y2 Y3 time axix Fig 2 Citations by X who mentions others as late X D X MAX D Yi 4 I X must be alive after the death of all Yi who were cited as late by him Therefore we can use the death year of the latest born such Y as a lower estimate for X s death year B X MAX D Yi MAX 5 H X was probably born after the death year of the latest dying person who X wrote about Therefore we can use the death year of the latest born such Y minus his maximal life period as a lower estimate for X s born year Contemporary citation constraints Contemporary citation constraints calculate the upper and lower bounds of the birth year of an author X based only on citations of known authors who refer to X as their friend student rabbi This means there must have been at least some period in time when both were alive and intellectually active Figure 3 describes possible situations where various kinds of authors Yi refer to X as their friend student rabbi Y1 was born before X s birth and died before X s death Y2 was born before X s birth and died after X s death Y3 was born after X s birth and died before X s death and Y4 was born after X s birth and died after X s death Like all authors contemporary authors of course have to comply to constraints 1 and 2 as well Y1 Y2 Y3 Y4 time axix X Fig 3 Citations by authors who refer to X as their Friend Student Rabbi Estimating the Birth and Death Years of Authors of Undated Documents 143 B X MIN B Yi MAX MIN 6 H All Yi must have been alive when X was alive and all of them must have been old enough to publish Therefore X could not be born MAX MIN years before the earliest birth year amongst all authors Yi D X MAX D Yi MAX MIN 7 H Again all Yi must have been alive when X was alive and all of them must have been old enough to publish Thus X could not be alive MAX MIN years after the latest death year amongst all authors Yi Intellectual son father based constraints Son based constraints calculate the upper and lower bounds of the birth and death years of an author X based only on citations of only one known author who refers to X as his son According to rabbinic conventions X can be either a truly son i e a biological son or an intellectual son i e a student Figure 4 describes five possible situations Yi i 1 2 3 refer to X as their truly son In all these cases Yi were born before X s birth Y1 died before X s birth maximum 9 months before X s birth Y2 died before X s death and Y3 died after X s death Y1 is not a possible father in the discussed context since in this case Y1 cannot refer to his son who was born only after Y1 s death However in Jewish rabbinic documents it is possible that an author Yi e g Y4 or Y5 will call his student X a son meaning an intellectual son although X is not his truly son In such a case Yi the father can be born even after X s birth X Y1 Y2 Y3 Y4 Y5 time axix Fig 4 Citations by authors who refer to X as their son When taking into account situations such as an intellectual son X towards Y4 or Y5 all son based constraints are expressed by the friend student rabbi based constraints 6 7 If a biological bond i e a truly son can be absolutely identified than a unique constraint can be formulated Father based constraints calculate the upper and lower bounds of the birth and death years of an author X based only on citations of known authors who refer to X as their father Also here according to rabbinic conventions X can be either a truly father i e a biological father or an intellectual father i e a rabbi or a teacher Therefore all father based constraints are expressed by the friend student rabbi based constraints 6 7 144 Y HaCohen Kerner and D Mughaz 2 2 Greedy Constraints We also formulate and apply greedy constraints These bounds are sensible in many cases but which can nevertheless sometimes lead to wrong estimates It is important to mention that the greedy constraints are applied in combination with the iron clad and heuristic constraints This is because in many cases some of the greedy constraints are not applied because lack of explicit citations citations with cue words In such cases we use the estimations that are products of the iron clad and heuristic constraints Greedy constraint based on authors who are mentioned by X B X MAX B Yi 8 G Most of the citations in our research domain relate to dead authors Thus most of the citations mentioned by X relate to dead authors That is most of Yi were born before X s birth and died before X s death Therefore a greedy assumption will be that X was born no earlier than the birth of latest author mentioned by X Greedy constraint based on authors who refer to X D X MIN D Yi 9 G As mentioned above most of the citations mentioned by Yi relate to X as dead Therefore most of Yi die after X s death Therefore a greedy assumption will be that X died no later than the death of the earliest author who refers to X Refinement of constraints 8 9 are presented by constraints 10 13 Constraints 10 11 are due to X citing Yi and Constraints 12 13 are due to Yi citing X Greedy constraint for defining the birth year based only on authors who were cited by X B X MAX D Yi 10 G When taking into account only citations that are cited by X most of the citations relate to dead authors That is most of Yi died before X s birth Therefore a greedy assumption will be that X was born no earlier than the death of the latest author mentioned by X Greedy constraint for defining the birth year based only on authors who are mentioned by X as a friend B X MIN B Yi 11 G When taking into account only citations that are mentioned by X which relate to contemporary authors a greedy constraint can be that X was born no later than the birth of the earliest author mentioned by X as a friend Greedy constraint for defining the death year of X based only on authors who cited X as late D X MIN B Yi 12 G Estimating the Birth and Death Years of Authors of Undated Documents 145 When taking into account only citations that are mentioned by Yi who relate to X as late a greedy assumption can be that X died no later than the birth of the earliest author who cited X as late Greedy constraint for defining the death year of X based only on authors who cited X as a friend D X MAX D Yi 13 G When taking into account only citations that are mentioned by Yi who cited X as a friend all Yi must have been alive when X was alive and all of them must have been old enough to publish Therefore a greedy assumption will be that X died no earlier than the death of the latest author who cited X as a friend We do not present greedy constraints regarding son and father because they can be intellectual son and father and not truly relatives 3 The Model The main steps of the model are presented below Most of these steps were processed automatically except for steps 2 and 3 that were processed semi automatically 1 Cleaning the texts Since the responsa may have undergone some editing we must make sure to ignore possible effects of differences in the texts resulting from variant editing practices Therefore we eliminate all orthographic variations 2 Normalizing the citations in the texts For each author we normalize all kinds of citations that refer to him e g various variants and spellings of his name books documents and their nicknames and abbreviations For each author we collect all citation syntactic styles referred to him and then replace them to a unique string 3 Building indexes e g authors citations to late friend student rabbis son father and calculating the frequencies of each item 4 Citation identification into various categories of citations including self citations 5 Performing various combinations of iron clad and heuristic constraints on the one hand and greedy constraints on the other hand to estimate the birth and death years for each tested author 6 Calculating averages and std deviations for the best iron clad and heuristic version and the best greedy version 4 Experimental Results The examined dataset includes 3 488 responsa1 authored by 12 Jewish rabbinic scholars two of whom are still alive All these authors lived in the last 130 years and were very productive regarding the number of documents and citations that were written by them On average there are about 291 documents for each scholar These responsa were written in the last 100 years The total number of words is about 6 887 351 words average per documents is 1 975 words This corpus includes citations to 298 1 Contained in the Global Jewish Database The Responsa Project at Bar Ilan University Http www biu ac il ICJI Responsa 146 Y HaCohen Kerner and D Mughaz authors including the 12 investigated authors The dataset before the normalization step step 2 in section 4 includes 106 923 citations i e mentions of other works which are about 8 910 citations in average for each author and about 31 citations for each document 19 506 of these citations are different Since this dataset represents a special corpus containing responsa authored by 12 authors who lived in the last 130 years the incoming posthumous citations count is always 0 This special situation enables us to correct death ages which are higher than the current year That is if the upper bound of D X is greater than the current year then we change it to the current year If the investigated authors died a few hundreds years ago then the upper bounds would probably been much worse The situation with these authors also means that we did not apply average posthumous constraints greedy rules 8 10 for the birth year and greedy rules 9 12 for the death year In a different corpus situation where all authors are roughly from the same period these greedy rules help but not here where many ancient authors are cited i e some of the lower bounds can be hundreds years ago and if we use them than the estimation for B X will be too low and therefore very bad Several characteristics of this dataset are presented below On average each author cites 8 910 citations while only about 10 of them are posthumous citations and about 6 of them are contemporary citations About 99 8 of the citations are implicit i e they are not accompanied with cue words that identify whether the citations are posthumous or contemporary The average number of citations to each author is 88 including self citations and 33 excluding self citations That is most of the citations 62 5 are self citations Among the explicit citations those with cue words the average number of posthumous citations 10 25 is about twice greater than the average number of contemporary citations 5 67 That is about two thirds of the explicit citations are posthumous On average for each author there are much more outgoing citations 8 910 than incoming citations 88 in general and more outgoing contemporary citations 6 than incoming contemporary citations 4 Table 1 compares the ground truth about the birth and death years on the one hand to the best iron clad and heuristic version and on the other hand to the best greedy version Since this is a novel problem it is difficult to evaluate the results in the sense that although we can compare how close the system guess is to the actual birth death years what we cannot do is assess how close is close i e there is no real notion of what a good result is Currently we use the notion difference which is defined as the estimated value minus the ground truth value Some of the estimates for birth and death years are not integer values This finding is due to the use of average functions in certain versions e g two last sub rows in tables 2 and 3 Table 1 shows that the best experimental results have been achieved by the best greedy version which was better than the best iron clad and heuristic version as follows 1 Its average birth year and death year differences 13 04 and 15 54 respectively are better than those of the best iron clad and heuristic version 22 and 22 67 respectively 2 The absolute differences of 12 out of 24 estimates were less or equal to 6 5 years versus only 5 such estimates of the best iron clad and heuristic version and 3 The standard deviation of the birth year s greedy estimate is less than its comparable Estimating the Birth and Death Years of Authors of Undated Documents 147 iron clad and heuristic standard deviation This indicates that the results of the best greedy version are steadier Indeed the best greedy version was better than the best iron clad and heuristic version only in 14 out of 24 estimates of birth and death years Therefore these results are still not enough significant Table 2 presents the experimental results using the various iron clad and heuristic constraints only section 2 1 The minimal average birth year and death year differences 22 and 22 67 respectively have been achieved by the version of the average late based constraints constraints 3 6 This result was obtained using the average Table 1 Experimental results using various groups of constraints Author X 1 2 3 4 5 6 7 8 9 10 11 12 Name of X in Hebrew Ground truth Best iron clad heuristic version Birth year 1879 1885 1888 5 1862 5 1888 5 1887 1857 5 1913 5 1833 5 1915 5 1916 1906 5 Death year 1971 5 1959 5 1981 1952 1981 1958 5 1950 1988 1923 1980 5 1981 1981 Ave Std dev Differences for best iron clad heuristic version Birth Death year year 38 34 5 26 29 5 31 5 29 17 5 1 22 0 5 15 30 5 40 5 13 18 5 2 56 5 46 5 14 5 21 5 29 2 14 3 5 22 17 15 22 67 13 28 Best greedy version Birth year 1899 5 1910 1894 1884 1874 5 1880 5 1885 1889 1889 1874 5 1920 1890 5 Death year 1953 1989 1953 1959 5 1958 1995 1980 5 1959 1971 5 1950 2009 1989 Ave Std dev Differences for best greedy version Birth Death year year 17 5 53 1 0 26 57 4 6 5 14 5 1 21 5 6 13 17 5 27 6 1 2 5 26 5 9 6 1 19 5 6 13 04 9 32 15 54 20 00 Birth year 1917 1911 1920 1880 1889 1902 1898 1895 1890 1901 1914 1910 Death year 2006 1989 Alive 1953 1959 1989 1963 1986 1969 1959 Alive 1995 Table 2 Experimental results using different groups of constraints Group of cons Upper and lower bounds Average of absolute differences in years Birth year Death year 35 83 38 67 43 42 26 33 43 42 55 83 43 42 26 33 75 75 55 83 22 00 22 67 37 58 33 67 37 58 38 25 87 58 33 67 87 58 38 25 45 08 29 79 Cons 1 2 Posthumous citation cons cons 2 3 cons 2 5 cons 3 4 Contemporary cons 1 2 4 5 cons B X D X B X D X B X D X B X D X B X D X B X ave B X B X D X ave D X D X B X D X B X D X B X D X B X D X B X ave B X B X D X ave D X D X 148 Y HaCohen Kerner and D Mughaz of the upper and the lower bounds of the birth year as estimate for the birth year and the average of the upper and the lower bounds of the death year as estimate for the death year This version is better than the version that contains the two most simple constraints 1 2 which do not take into consideration any cue words This finding indicates that the posthumous and contemporary constraints do contribute to the estimates The result achieved by the best iron clad version was successful also because an important correction that was done by us concerning the iron clad constraints dealing with the estimation of D X That is if the upper bound of D X is greater than the current year then we change it to the current year If the investigated authors died a few hundreds years ago then the upper bounds would probably been much worse In general the results achieved by the contemporary friend constraints were worse than those achieved by the late constraints That might be due to the fact that there more posthumous citations than contemporary citations Table 3 Experimental results using different versions of the greedy constraints Group of cons Average of absolute differences in years Birth year Death year 13 42 17 30 43 42 26 30 37 58 33 67 13 04 15 54 Cons 8 9 Posthumous cons 10 12 Contemporary cons 1 2 4 5 Ave friend cons B X ave 10 11 D X ave 12 13 Table 3 presents the results achieved by the different versions of the greedy constraints section 2 2 The minimal averages of absolute differences in years for the birth and death years 13 04 and 15 54 respectively have been achieved by the greedy version of the average friend based constraints constraints 10 13 5 Summary Conclusions and Future Work To the best of our knowledge we are the first to investigate the estimation of the birth and death years of the authors using undated citations referring to them or written by them This investigation was performed on a special case of documents i e responsa where special writing rules are applied The estimation was based on the author s documents and documents of other authors whose birth and death years are known who refer to the discussed author or are mentioned by him To do so we formulate various kinds of iron clad heuristic and greedy constraints The best estimates have been achieved using the version of the average contemporary greedy constraints Regarding the estimation of the birth and death years of an author X it is important to point that citations mentioned by X or referring to X are more suitable to estimate the birth and death writing years of X rather than his real birth and death years This model might be applied with suitable changes to similar research problems that might be relevant for some historical legal or religious document collections Usually such documents include citations to previous documents of the same kind Estimating the Birth and Death Years of Authors of Undated Documents 149 We plan to improve the estimation of the birth and death years of authors by 1 Combining and testing new combinations of iron clad heuristic and greedy constraints 2 Improving existing constraints and or formulating new constraints e g statistical based constraints 3 Defining and applying heuristic constraints that take into account various details included in the responsa e g dates in case that they appear events names of people concepts special words and collocations that can be dated 4 Conducting additional experiments using many more responsa written by more authors is supposed to improve the estimates 5 Checking why the iron clad heuristic and greedy constraints tend to produce more positive differences and 6 Testing how much of an improvement we got from a correction of the upper bound of D x and how much we will at some point use it for a corpus with long dead authors Definition and application of additional kinds of constraints is planned 1 Constraints that are based on historical events mentioned in the documents and 2 Threegeneration constraints i e constraints that relate to biological or preceding relations e g grand son and grand student Another interesting future research is the disambiguation of ambiguous citations Acknowledgements The authors thank Simone Teufel for reviewing drafts of this article and offering many helpful comments and three anonymous reviewers for their reviews References 1 Choueka Y Conley E S Dagan I A Comprehensive Bilingual Word Alignment System Application to Disparate Languages Hebrew English In Veronis J ed Parallel Text Processing pp Using Temporal Cues for Segmenting Texts into Events Ludovic Jean Louis Romaric CEA LIST Vision and Content Engineering Laboratory Fontenay aux Roses F 92265 France ludovic jean louis romaric besancon olivier ferret cea fr Abstract One of the early application of Information Extraction motivated by the needs for intelligence tools is the detection of events in news articles But this detection may be difficult when news articles mention several occurrences of events of the same kind which is often done for comparison purposes We propose in this article new approaches to segment the text of news articles in units relative to only one event in order to help the identification of relevant information associated with the main event of the news We present two approaches that use statistical machine learning models HMM and CRF exploiting temporal information extracted from the texts as a basis for this segmentation The evaluation of these approaches in the domain of seismic events show that with a robust and generic approach we can achieve results at least as good as results obtained with a specialized heuristic approach Keywords Information extraction text segmentation temporal cues 1 Introduction The detection of events has always been a major issue of Information Extraction It was already addressed in the Message Understanding Conferences MUC 6 and is still a subject of interest in more recent evaluations such as ACE Automatic Content Extraction 4 This detection is the trigger of the extraction process which aims at filling the slots of a template defining the typical information associated with a certain type of events In the domain of terrorist attacks for instance such process identifies the type of an attack bombing etc and extracts slot information such as its date its location or its target The content of these slots are typically named entities whereas events appear as a direct relation between named entities for example the Date September 1989 Date Hurricane Hugo Hurricane Hurricane as a verb or a verbal noun Hurricane Hurricane Hugo Hurricane struck Location South Carolina Location in Date 1989 Date or can extend beyond the scope of a sentence Most of the work about slot filling in Information Extraction focuses on the first two cases mainly because the identification of a relation between the mention of an event and a named entity often use lexico syntactic patterns or syntactic relations However as it was already underlined in 11 and later analyzed more H Loftsson E Using Temporal Cues for Segmenting Texts into Events 151 precisely in 22 a significant number of such relations can be identified only at the discourse level This fact was taken into account in some existing works mainly through coreference resolution 23 or by acquiring and using domainknowledge for guiding the slot filling process 21 8 In this article we tackle this issue through the means of discourse segmentation More specifically we propose to segment texts according to the events they refer to in order to narrow the span of text to explore for linking a named entity to an event mention As time is an important feature for discriminating events as illustrated by 18 we chose to perform this segmentation by relying on temporal cues The extraction of temporal information from texts has been widely studied in different fields of Natural Language Processing since this kind of information is useful for many applications In the field of Information Extraction temporal information is for instance use to find the ordering of events 5 16 or to identify their overlapping 18 In our work the dates associated with event mentions are useful for segmenting texts into events However each mention of an event doesn t necessarily appear with a date in the same sentence as it is illustrated by the following example 1 Hurricane Hurricane Hugo Hurricane in Date 1989 Date was a Level class 4 Level storm 2 Hurricane Hurricane Hugo Hurricane caused Damages 7 billion damage Damages in the Caribbean Sea and South Carolina In sentence 1 the date 1989 is linked to the event mention through syntactic dependencies whereas without named entity coreference resolution linking a date to the event mention in sentence 2 is not possible The problem of associating a date to an event is addressed for instance in 5 which proposes a set of heuristics for assigning time stamps to the events of a text by relying on three temporal cues in each sentence the presence of a date and the tenses of verbs more globally the date of the document In 7 each sentence is not assigned a date but a date associated with an event is propagated to an undated event according to the relations between them e g a cause consequence relation In relation to this problem 15 proposes a segmentation model that views texts as sequences of situations These situations are defined through three types of entities temporal entities locations and persons Moreover 15 distinguishes between texts with a simple structure and texts with a complex structure The first ones are centered on one event only that is considered from one viewpoint All entities in this case tend to contribute to the definition of this event The second type of texts refer to several events among which a main event can be distinguished together with subordinate events In this article we focus more particularly on the segmentation into events of texts with a complex structure We present the general principles of our work in Section 2 while Section 3 is dedicated to the way we apply these principles with machine learning techniques Finally we present in Section 4 the results of the evaluation of the method on French news articles in the seismic domain 152 L Jean Louis R 2 Principles and Objectives Event extraction as presented in this article takes place in a wider context of technology watch in which users are mainly interested in the most recent events In this context our goal is to synthesize from news articles the information about such recent events into a kind of dashboard1 However news articles often refer to several comparable events generally for pointing out the similarities and differences between a recent event and older ones In our case we are not interested in these older events and we consider them as a source of noise for extracting information about the main event of a news article We adopted a two step strategy to tackle this problem Fig 1 Segmentation of a text into events The work of this article mainly focuses on the segmentation of texts into events Following this perspective we adopted a representation of texts turned towards events a text is viewed as a sequence of sentences in which each sentence is characterized by the presence or the absence of an event2 In addition we also focus in this work on the identification of the named entities associated with the main event of a news article We have therefore decided not to differentiate one secondary event from another Thus we propose to classify sentences according to the following three categories 1 2 This approach is a little bit different from most works in the information extraction field in which information is searched for all the events of a certain type The hypothesis one sentence one event is a simplification but it is globally not too simplistic in our application domains Using Temporal Cues for Segmenting Texts into Events 153 3 Approaches for Segmenting Text into Events In this work we treat the problem of segmenting texts into events as a classification problem each sentence of the text must be associated with an event type As we also consider that the sequence of sentences contain valuable information for this classification and we want classify all sentences a graphical model of sequence annotation is particularly suited We describe in this section two standard sequence classification models for this task Hidden Markov Model HMM and Conditional Random Fields CRF 3 1 Text Preprocessing The segmentation itself uses a sentence representation composed of linguistic cues extracted from the text We therefore perform a linguistic analysis of the text consisting in tokenization sentence boundary detection Part Of Speech tagging verb tense analysis and named entity recognition This linguistic processing pipeline is implemented using the linguistic analyzer LIMA 1 3 2 HMM Model Hidden Markov Model 19 is a sequence classifier widely used in NLP for example named entity recognition POS tagging etc and has also been applied on text segmentation in particular for topic segmentation 24 HMM are stochastic models used to find an underlying sequence of hidden states from the sequence 154 L Jean Louis R of observable data In our case we want to find the sequence of events associated with a given text considered as a sentence sequence We assume that the segmentation is a Markov process which means that the state associated with the current sentence only depends on previous state we propose to use the temporal information verb tenses as observation and the event categories as hidden states The transition matrices are obtained from a corpus of manually annotated texts supervised learning An illustration of the HMM model used in our approach is presented in Figure 2 Fig 2 Illustration of text segmentation using HMM A constraint of the HMM is that for a given observation the sequence of states does not originally take into account dependencies between the current state and previous observations and does not allow to easily integrate several criteria without multiplying the number of observation and the amount of training data To avoid this constraint we also tested a CRF model presented in the following section 3 3 CRF Model Since they were introduced in 2001 Conditional Random Fields 13 have been widely and successfully used in several NLP fields In text segmentation and classification 9 have obtained good results applying CRF models to classify sentences contained in scientific abstracts into four categories objective methods results conclusions The main difference between HMM and CRF is that HMM aim at maximizing the joint probability P x y between a sequence of observations x and a sequence of state y whereas CRF compute conditional probability P y x in order to associate a sequence of states with the observations The advantage of the conditional approach is that it allows to represent the sequence of observation as a vector whose components are related to features attached to the observation These features allow to integrate more linguistic expert knowledge in the models A more formal definition of CRF is P y x 1 Z x exp F y x Using Temporal Cues for Segmenting Texts into Events 155 F y x i f y x i Z x y exp F y x where F y x is a vector whose components are the values of the features for the input sequence and is a vector of weights associated with the features and Z x is a normalizing factor which depends on all possible sequences of states To apply CRF to text segmentation we use the following features 4 Evaluation We present in this section the results obtained by applying statistical learning algorithms for segmenting texts into events The evaluation process is composed of two stages first an intrinsic evaluation of the segmentation approach are the identified segments correct and second a final evaluation on the intended application i e the impact of text segmentation on the extraction of the entities associated with the main event We compare the statistical based approaches to a domain specific heuristic HeurSeg and a paragraph based heuristic ParaSeg HeurSeg is used in an existing application dedicated to information extraction for seismic events It is mainly based on the presence and the value of dates different date values correspond to different text segments the main event segment being the one containing the most recent date and between two distinct dates the boundaries of the segment are set according to the presence of sentence and paragraph breaks and the presence of domain specific entities between the two dates ParaSeg determines event categories at a paragraph level a sentence is associated with the main event category if it belongs to the first two paragraphs otherwise the sub event category is chosen We notice that HMM and CRF use different features for their classification decisions HMM decision is only based on the sequence of verb tenses whereas CRF decision uses a richer set of features described in section 3 3 Therefore we also use a Maximum Entropy model MaxEnt 20 as comparative model for the CRF with the same set of features in order to confirm the interest of the information given by the sequence for the segmentation 156 L Jean Louis R From an implementation point of view we used a set of Python scripts together with several reference toolkit NLTK3 CRF 4 and MaxEnt5 respectively for HMM CRF and MaxEnt models 4 1 Corpus In order to evaluate our text segmentation approach we used a corpus of 501 French news articles concerning earthquake events The articles were collected between February 2008 and early September 2008 partly from the French Agence France Presse newswire 1 3 of the corpus and partly from Google News 2 3 of the corpus These articles deal with 142 distinct major earthquake events The corpus contains both articles with a simple structure only one event and articles with a complex structure several events 252 articles 50 report at least one secondary event The corpus has been manually annotated for named entities by domain analysts only for entities associated with the article main event6 Table 1 reports the categories of named entities associated with an earthquake event together with their distribution in the corpus The table shows that the distribution of named entities is not homogeneous many location names are found 947 whereas only few geographical coordinates Geo are present in the articles 30 As a consequence the overall performance of the latter category will be strongly influenced by a matched missed entity while the former entity category will not In order to evaluate our text segmentation approach we annotated a subset of the corpus composed of 140 articles with event segmentation information Most of the selected articles in the subset contain a main and a secondary event some short articles might not contain secondary event information Table 2 shows the distribution of event categories in the annotated subset The most represented event category is Main Event 70 which is consistent with the factual aspect of news articles The Secondary Event includes without distinction all earthquake events that are comparable to the Main Event notice that among the selected news the real number of distinct secondary events evoked in the article can go up to 4 Finally the annotated subset has on average 1 7 distinct secondary events mentioned per article 4 2 Intrinsic Evaluation of the Segmentation of Texts into Events This section presents the results in terms of precision and recall percentage denoted P and R for different text segmentation approaches These results where obtained through 5 fold cross validation on the annotated subset 140 news articles using 4 5 for training and the remaining 1 5 for testing We 3 4 5 6 http www nltk org http crfpp sourceforge net http webdocs cs ualberta ca lindek downloads htm Annotators could annotate several entities of the same type if they thought the entities were equally satisfactory as pieces of information related to the article main event for instance for location entities annotators could annotate both a city and a country name Using Temporal Cues for Segmenting Texts into Events 157 Table 1 Distribution of named entities in the annotated corpus 3306 named entities for 501 articles Entity type Event type Location Date Time Magnitude Damages Geo Number 499 947 470 345 484 531 30 Nature type of event earthquake tsunami place where the event occurred date when the event occurred time when the event occurred magnitude damages caused by the event geographical coordinates of the event Table 2 Distribution of the event categories in the annotated corpus 1659 annotated segments in 140 news articles Event type Number Percentage of annotated segments Main Event 1168 70 Secondary Event 287 17 Background 213 13 report in Table 3 the segmentation results for statistical and heuristic based approaches We first notice that both ParaSeg and HeurSeg are outperformed for the precision on Main Event by assigning all sentences to the most frequent category see Table 2 Results for ParaSeg and HeurSeg are comparable in terms of precision but ParaSeg achieves a poor score on recall which is explained by the fact that most articles contain short paragraphs and few sentences are associated with main event in this case Concerning the statistical segmentation models the HMM results prove that the unique verb tense criterion is not sufficient to discriminate all event categories while the main event category is correctly identified 83 0 recall and 93 6 precision secondary event and background categories are poorly identified and they can be useful for a general purpose Information Extraction task Results also demonstrate that performances for secondary event and background categories are improved with CRF and MaxEnt models better results with CRF which confirms the importance of considering the successive sentence categories while segmenting the text into events 4 3 Evaluation of Text Segmentation for Information Extraction The goal of the text segmentation presented here is to delimit text segments that refer to a single event category in order to link the relevant entities to the main event linking is made within a text segment In order to evaluate the impact of the segmentation on this linking task we used a simple heuristic for linking the entities to the main event based on the hypothesis that pieces of information contained in the articles are organized according to their importance in the newscast the most relevant pieces of information which are generally associated 158 L Jean Louis R Event type Main Event Secondary Event Background Table 4 Results for the entity to main event linking Entity type Damages Date Event type Geo Location Magnitude Time All NoSeg R P 83 5 77 9 38 4 35 9 82 1 81 6 86 7 96 3 41 0 40 9 93 5 93 0 61 0 51 2 66 6 63 5 ParaSeg R P 62 1 61 0 34 3 32 6 78 7 78 2 44 8 50 0 39 2 39 1 72 7 73 2 25 3 22 5 53 1 51 8 HeurSeg R P 76 3 74 4 69 3 65 0 79 3 78 8 66 7 74 1 56 0 55 9 86 2 85 9 56 4 49 2 71 0 68 6 HMM R P 69 8 65 1 48 9 45 6 59 1 58 8 86 7 96 3 61 2 61 1 66 7 66 2 78 8 71 5 63 4 61 1 CRF Gold Std R P R P 80 2 75 3 76 7 73 5 64 4 60 1 89 5 86 9 76 7 76 2 85 6 85 6 83 3 92 6 100 0 100 0 57 4 57 3 86 4 86 4 86 7 86 1 93 4 93 4 63 4 55 5 92 2 91 3 71 7 68 8 87 5 86 3 with the main event generally appear before the subordinate pieces of information associated with either a secondary event or the background Following this idea we use the following linking heuristic for each entity category select as most relevant entity the first entity found in the segment Table 4 summarizes the results of the entity linking task while using heuristicbased and learning based approaches for the segmentation into events We also report as baseline the results of the entity linking without event segmentation NoSeg and with the reference segmentation on the 140 annotated articles Gold Std NoSeg achieves fairly good results for the entity linking process even higher than the HMM 2 7 F1 Measure These results are significantly improved by HeurSeg 6 2 F1 Measure compared to NoSeg which demonstrates the interest of segmentation for the entity linking task Even if there are variations depending on the type of entities the CRF model gives results equivalent to those obtained with the heuristic approach and even slightly better which is quite satisfactory since the CRF model is a generic method while the heuristic is explicitly dependent on the domain Finally the best results obtained by Gold Std also show that there is still room for improvement using a smarter entity linking strategy 5 Related Work The work we have presented in this article focuses on text segmentation with two main characteristics this segmentation is performed according to the event Using Temporal Cues for Segmenting Texts into Events 159 type of sentences and relies on temporal cues The use of temporal cues for discourse segmentation was mainly explored by linguistic and psycho linguistic works through the study of the role of clause initial temporal adverbials as segmentation markers From the psycholinguistic viewpoint 2 showed that clauseinitial temporal adverbials are correlated with topic shifts whereas from a more linguistic viewpoint 10 uncovered a more complex situation where the role of clause initial temporal adverbials is text type dependent Our use of temporal cues for text segmentation is both more crude and more extensive we mainly rely on the sequence of grammatical tenses and we use other temporal cues such as the presence of dates or temporal adverbials as features for identifying more accurately secondary event and background sentences Segmenting texts according to their events was also addressed by some few works In 12 this segmentation was mainly based the identification in texts of the components of a type of events defined by a priori domain knowledge However two typical discourse structures of the news articles they considered were also taken into account for this segmentation one is made of a sequence of different events while the other one is structured as in our case round a main event with several mentions with references to minor events 3 also heavily based its segmentation of texts on the identification the components of a priori template but tested several discourse inspired heuristics for assigning a clause to an event as for instance favoring the event of the most recently assigned clause Finally the closest work to ours from this viewpoint is 17 which tagged sentences with four event labels new event continuing event back reference to an earlier event no reference to an event These labels are close to our three event types except that we don t distinguish the introduction of a new event from its continuation The model used in 17 is a probabilistic Finite State Automaton relying on the MDI algorithm for its learning part While this work aims at modelling the discourse structure of texts from the viewpoint of events it differs from ours in that it directly models sequences of event labels and don t exploit temporal information 17 showed that its segmentation had a positive impact on its final task i e grouping sentences in news articles that refer to the same event but didn t report results for only segmentation 6 Conclusion and Future Work The aim of our work is to segment texts into events in order to make easier the linking of relevant entities to the main event of a news article In our approach we addressed this segmentation problem as a sequence classification task where the goal is to determine the type of event associated with each sentence We proposed and evaluated three models a HMM model which uses as single decision criterion the succession of tenses in a text a MaxEnt model which integrates additional temporal cues temporal expressions dates into its decision and a CRF model which integrates the same features as MaxEnt in addition to the dependencies between the successive event types While conducting comparative experiments of the models on a corpus of news articles concerning earthquake 160 L Jean Louis R events we have shown that the CRF model provides the best results for the segmentation of texts into events In addition we tested the impact of segmenting the texts into events on the identification of the entities related to the main event and we showed that CRF which is a learning based model achieves equivalent results and even slightly better than those obtained with a heuristic based approach by using a much more generic approach Regarding the generalization of the approach we have obtained encouraging results for initial tests by applying the models on an English corpus of news articles using the models learned from French Concerning the application domain we used a corpus of news articles related to the earthquake field in which information is quite structured Nevertheless we believe our approach can provide reasonable results in other areas which we are planning to experiment in the near future Finally a detailed error analysis on the test corpus showed that the process of linking the entities to the main event is a major source of error as we are currently using a simplistic heuristic We will therefore focus our future research on this issue by using both entity density criteria and linguistic criteria e g explicit syntactic dependencies between entities References 1 Using Temporal Cues for Segmenting Texts into Events 161 10 Ho Dac L M Enriching the Adjective Domain in the Japanese WordNet Kyoko Kanzaki1 Francis Bond2 Takayuki Kuribayashi1 and Hitoshi Isahara1 1 National Institute of Information and Communications Technology 3 5 Hikaridai Seikacho Sorakugun Kyoto 619 0289 Japan kanzaki kuribayashi isahara nict go jp 2 Nanyang Technology University 14 Nanyang Drive Singapore 637332 bond ieee org Abstract We released Japanese WordNet Version 1 0 in March 2010 and are continuing to enrich the Japanese WordNet in several directions The current version of the Japanese WordNet is a kind of translation of Princeton WordNet 3 0 and we used WordNets of multiple languages in order to disambiguate Japanese translations Although the structure is based on Princeton WordNet 3 0 some information is still insufficient in our Japanese WordNet For example the adjective domain lacks semantic relations such as antonyms attributes and so on As part of our ongoing work to enrich the Japanese WordNet we are investigating attribute nouns for which adjectives express values Keywords WordNet Adjective Japanese 1 Japanese WordNet We are building a Japanese version of WordNet and released version 1 0 in March 2010 The WordNet project at Princeton has been a resounding success creating a resource that is widely used in research and has been emulated in many languages 7 In order for a lexical resource to be widely adopted it must be both accessible and usable The Princeton WordNet is accessible because it is released under a nonrestrictive license and it is usable because it has not just precise information but also reasonable coverage especially of common words There have been several initiatives to create a Japanese WordNet but none of them has yet produced something that is both accessible and usable The large amount of previous work shows the great interest and value of producing a Japanese WordNet We therefore decided to construct one as follows 1 First automatically translate the Princeton WordNet into Japanese All relations e g hypernyms and meronyms come from Princeton WordNet 3 0 Second the most frequent 20 000 synsets are manually checked Third the synsets are linked to a corpus Fourth the data are released under an open license The Japanese WordNet is based on the structure of the English WordNet Japanese near synonyms are added to the existing English synsets For example one of the English synsets consisting of seal has the explanation any of numerous marine H Loftsson E Enriching the Adjective Domain in the Japanese WordNet 163 mammals that come on shore to breed chiefly of cold regions and has the following azarashi and azarashi Japanese words associated with it A statistical summary of the contents of WordNet 3 0 and Japanese WordNet 1 0 is shown in Table 1 Table 1 Statistics for WordNet 3 0 and Japanese WordNet 1 0 from websites 9 and 10 Unique strings Synsets Senses word sense pair in English synset word pair in Japanese WordNet 3 0 155 287 117 659 206 941 Japanese WordNet 1 0 92 241 56 741 157 398 2 Creating the Japanese WordNet Our approach to building the Japanese WordNet is the standard expansion approach translate WordNet synsets to another language and take over the structure 8 We did this both to keep a compatible structure with WordNet and because we had access to a variety of resources that would make the task easier Our main innovation is that we are using WordNets in multiple languages to disambiguate the Japanese translations thus providing more reliable estimates The English Japanese lexicon has two translations for bat i e koumori mammal and batto club However because there is no way of distinguishing between them we get a mixture of meanings with koumori bat n 1 and batto bat n 5 Chiropteran bat n 1 is not in any of the English Japanese lexicons we used and bat n 5 has no synonyms Therefore using only the English WordNet as source and English Japanese lexicons there is no way to disambiguate them However both synsets are also in the French WordNet bat n 1 is chauve souris and bat n 5 is batte gourdin These are not ambiguous in the same way chauvesouris goes only to koumori and batte only to batto Thus if we can match through two languages the mapping is much more likely to be the correct sense The actual algorithm we used was as follows For each synset in WordNet 3 0 164 K Kanzaki et al being linked in multiple languages Thus we built and released Japanese WordNet 1 0 but many improvements remain to be made 3 Adjective Domain in WordNet 3 0 and Japanese WordNet As for the adjective domain basic adjectives were translated in our Japanese WordNet Table 2 shows a statistical comparison of the adjective domain between WordNet 3 0 and Japanese WordNet However the adjective domain in the Japanese WordNet is still insufficient compared to WordNet 3 0 Table 2 A statistical comparison of the adjective domain WordNet 3 0 Japanese WordNet Unique strings 21 479 4 494 Synsets 18 156 8 915 Word sense pairs 30 002 17 679 According to WordNet 3 0 adjectives are arranged in clusters containing head synsets and satellite synsets Each cluster is organized around antonymous pairs and occasionally antonymous triplets The antonymous pairs or triplets are indicated in the head synsets of a cluster Most head synsets have one or more satellite synsets each of which represents a concept that is similar in meaning to the concept represented by the head synset In the Japanese WordNet head synsets and satellite synsets are not clearly distinguished As shown in the following examples adjectives in WordNet 3 0 have several semantic pointers such as antonym attribute similar to derivationally related form also see and pertainym by which adjectives are related with other synsets i e not only other adjectives but also other parts of speech like nouns and verbs In the Japanese WordNet they have not been created yet These useful relations between synsets or words across a part of speech should be introduced in the Japanese WordNet Examples 1 refers to a pointer and refers to a synset Pointer antonym 01661289 a good right ripe 4 Enriching the Adjective Domain in the Japanese In WordNet 3 0 through the pointer attribute an adjective is linked to a noun for which an adjective expresses its value Such nouns called attributive nouns are linked to their hypernyms via a pointer inherited hypernym As an example hypernyms for abundant are shown below The hypernyms quantity amount Enriching the Adjective Domain in the Japanese WordNet 165 magnitude property attribute abstraction and entity can be identified via the pointer attribute and inherited hypernym Example 2 An example of inherited hypernym for abundant teeburu ga yasui nedan ni natta table cheap price became The price of a table became a cheap price b teeburu ga yasuku natta table cheap in adverbial form became A table became cheap In the above examples the meaning of yasuku naru become cheap is equal to yasui nedan ni naru become a cheap low price that is nedan price can be omitted when the adnominal usage of yasui 166 K Kanzaki et al cheap in adnominal form changes to the adverbial form yasuku cheap in adverbial form In this case nedan price is a meaning implied by yasuku in adverbial usage cheap 5 Future Direction As shown in the above sections both the English and Japanese WordNet are not sufficient in the adjective domain e g an attribute concept of cheap does not exist There are several methods for finding categories of words 2 3 4 5 6 These methods showed good results for generating hypernym concepts mainly from nouns and verbs We have to develop a method useful for finding relations between attribute concepts and adjectives As a future direction we will contribute to the development of the English and Japanese WordNet by considering what are attribute concepts for adjectives Acknowledgment We are grateful for discussion and resources from the Kyoto Project which is co funded by References 1 Bond F Isahara H Kanzaki K Uchimoto K Boot strapping a WordNet using multiple existing WordNets In 6th International Conference on Language Resources and Evaluation LREC 2008 pp Comparing SMT Methods for Automatic Generation of Pronunciation Variants Panagiota Karanasou and Lori Lamel Spoken Language Processing Group LIMSI CNRS 91403 Orsay France pkaran lamel limsi fr Abstract Multiple pronunciation dictionaries are often used by automatic speech recognition systems in order to account for different speaking styles In this paper two methods based on statistical machine translation SMT are used to generate multiple pronunciations from the canonical pronunciation of a word In the first method a machine translation tool is used to perform phoneme to phoneme p2p conversion and derive variants from a given canonical pronunciation The second method is based on a pivot method proposed for the paraphrase extraction task The two methods are compared under different training conditions which allow single or multiple pronunciations in the training set and their performance is evaluated in terms of recall and precision measures Keywords P2P conversion pronunciation lexicon SMT Moses pivot paraphrasing 1 Introduction Pronunciation variation is one of the factors that influences the performance of an automatic speech recognition ASR system especially for spontaneous speech Predicting pronunciation variations that is alternative pronunciations observed for a linguistically identical word is a complicated problem and depends on a number of factors such as the linguistic origin of the speaker the speaker s education and so cio economic level the speaking style and conversational context and the relationship between interlocutors The construction of a goo d pronunciation dictionary is thus critical to ensure acceptable ASR performance 8 Moreover the number of pronunciation variants that need to be included in a dictionary depends on the system configuration 1 A variety of methods have been proposed in order to obtain pronunciation variants from the canonical pronunciations baseforms of words and can be broadly grouped into data based and knowledge based metho ds Knowledgebased metho ds using phonological rules 3 17 require specific linguistic skills are not language independent and do not always capture the irregularities in natural languages By contrast the data driven approaches are based on the idea that given enough examples it should be possible to predict the pronunciation of unseen words in the grapheme to phoneme task or generate multiple H Loftsson E 168 P Karanasou and L Lamel pronunciations for improved speech recognition In this paper the latter task of generation of pronunciation variations is addressed using data based metho ds Other data based metho ds proposed in the literature for the mo deling of pronunciation variations include the use of neural networks 4 confusion tables 14 and decision trees 16 or automatic generation of rules for phoneme to phoneme conversion 6 All these methods predict a phoneme for each input symbol using the input symbol and its context as features but ignore any structure in the output The metho ds proposed in this paper take advantage of both the input and the output context and can predict variable length phoneme sequences The two methods presented in this paper aim to automatically generate pronunciation variants of words for which a canonical pronunciation is available The first metho d is based on the simple use of Moses 7 a publicly available phrasebased statistical machine translation tool as a phoneme to phoneme converter to generate an n best list of pronunciation variants The second metho d is based on a paraphrase metho d that uses bilingual parallel corpora and is founded on the idea that paraphrases in one language can be identified using a phrase in another language as a pivot In the case of multiple pronunciation generation sequences of modified phonemes found in pronunciation variants are identified using a sequence of graphemes in the corresponding word as a pivot The paper is organized as follows Section 2 describes the two metho ds used in this study Section 3 describes the experimental framework and details about the corpora used and the training conditions applied Section 4 presents the evaluation results of the automatic generation of multiple pronunciations in terms of precision and recall Conclusions and some discussions for future work are reported in Section 5 2 Phoneme to Phoneme Conversion This section presents the two proposed methods in detail and compares their strengths and weaknesses pointing out their utility in different situations Both metho ds aim to produce pronunciation variants of the initial canonical phonemic transcription Since the canonical pronunciations are not explicitly indicated in the master lexicon see Section 3 1 the longest one is taken as the canonical form since the reduced forms often correspond to variants found in conversational speech The first method generates pronunciation variants using only the phonemic transcriptions of words while the second metho d makes use of both the orthographic and phonemic transcriptions and thereby permits to the system to also benefit from the information provided by the orthographic transcription of a word As we will see later in the results analysis this last characteristic of the second metho d is particularly useful under certain training conditions 2 1 Moses as a Phoneme to Phoneme Converter Moses has already been proposed for the grapheme to phoneme g2p conversion 5 9 task A pronunciation dictionary is used in the place of an aligned bilingual text corpora The orthographic transcription is considered as the source Comparing SMT Methods for Automatic Generation 169 language and the pronunciation as the target language In the case that the pronunciation dictionary has a reasonable coverage of the language of interest this metho d can be successfully used for g2p conversion because it has all the desired properties of a g2p system To predict a phoneme from a grapheme it takes into account the local context of the input word from a phrase based mo del and allows sub strings of graphemes to generate phonemes The phoneme sequence information is mo deled by a phoneme n gram language mo del that corresponds to the target language model in machine translation More technical details on the Moses components will be given in the Section 3 2 These properties are also desired for a p2p converter which has moreover higher potential for capturing pronunciation variation phenomena in languages like English where orthography and pronunciation generally have a looser relationship than in other languages A second direction explored in this work is based on the idea of seeing the use of SMT tools with a monolingual corpora for paraphrase generation 11 as being analogous to generating pronunciation variants These similar approaches to two distinct problems led us to the idea of trying to use Moses for p2p conversion In this case the source language and the target language are aligned phonemic transcriptions As the source language we define the canonical pronunciation the longest one1 and as target language itself and or its variants depending on their existence or not in the different versions of the training set as presented in the Section 3 1 2 2 Pivot Paraphrasing Approach This metho d is based on the one presented in 2 Paraphrases are alternative ways of conveying the same information We can easily see the analogy with multiple pronunciations of the same word The multiple pronunciations are alternative phonemic expressions of the same orthographic information In 2 a bilingual parallel corpus is used to show how paraphrases in one language can be identified using a phrase in another language as a pivot In the problem of automatic generation of pronunciation variants a corpus of wordpronunciation pairs is used as the analogy of the aligned bilingual corpus instead of the pronunciation pronunciation aligned corpus in the previous metho d The idea is to define a paraphrase pronunciation variant probability that allows paraphrases pronunciation variant sequences extracted from a bilingual parallel corpus to be ranked using translation probabilities and then rerank the generated pronunciation variants taking the contextual information into account The translation table that is used is extracted by Moses In 2 the authors look at the English translation of foreign language phrases find all occurrences of those foreign phrases and then look back to determine to what other English phrases they correspond The other English phrases are seen as potential paraphrases In the pronunciation generation case we look at all the entries in the translation table find the sequences of graphemes to which a sequence of phonemes is translated and then look back to what other sequences of phonemes the particular sequence of graphemes is translated 1 Most of the variants reflect reduced pronunciations found in casual speech 170 P Karanasou and L Lamel Phrase alignments in a parallel corpus are used as pivots between English pronunciation paraphrases These two way alignments are found using recent phrase based approaches to statistical machine translation In the following definitions f is a graphemic sequence and e1 and e2 are phonemic sequences The paraphrase probability p e2 e1 is assigned in terms of the translation mo del probabilities p f e1 and p e2 f Since e1 can be translated as multiple foreign language phrases graphemic sequences we sum over f e 2 arg max p e2 e1 e2 e1 e2 e1 1 2 arg max p f e1 p e2 f f This returns the single best paraphrase e 2 irrespective of the context in which e1 appears Since the best paraphrase may depend on information about the sentence that e1 appears in the paraphrase probability can be extended to include the sentence S e 2 arg max p e2 e1 S 3 e2 e1 This allows the candidate paraphrases to be ranked based on additional contextual information in the sentence S A simple language mo del probability is included which can additionally rank e2 based on the probability of the sentence formed by substituting e2 for e1 in S The language mo del is trained on the correct pronunciations of the training set For the reranking based on the language model we use the SRI toolkit 13 Finally some more pruning is done on the reranked list keeping the maximum of ten five or one pronunciation variants per canonical pronunciation without changing the order of the elements of the reranked list An example of a paraphrase pattern in the pronunciation dictionary is2 discounted dIskWntxd discounted dIskWnxd discountenance dIskWnNxns discountenance dIskWntNxns The alternative pronunciations differ only in the part that can be realized as either nt or n while the rest remains the same 3 3 1 Experiments Corpus The LIMSI Master American English dictionary serves as basis of this work It is a pronunciation dictionary with 187975 word entries excluding words starting with numbers with on average 1 2 pronunciations per word The pronunciations are represented using a set of 45 phones 8 each phone corresponding to a 2 The phone set used is given in Table 1 of 8 Comparing SMT Methods for Automatic Generation 171 single character The dictionary has been created with extensive manual supervision Each dictionary entry has the orthographic transcription of a word and its pronunciations one or more 18 of the words are asso ciated with multiple pronunciations The ma jority of words have only one pronunciation leaving it to the acoustic mo del to represent the observed variants in the training set that are due to allophonic differences Moreover since the dictionary is mostly manually constructed it is certainly incomplete with respect to coverage of pronunciation variants particularly for uncommon words The pronunciations of words of foreign origin mostly proper names may also be incomplete since their pronunciation depends highly on the speaker s knowledge of the language of origin This means that some of the automatically generated variants are likely to be correct or plausible even if they are not in the current version of the Master dictionary Case distinction is eliminated since in general it do es not influence the word s pronunciation the main exceptions being the few acronyms which have a spoken and spelled form Some symbols in the graphemic form are not pronounced such as the hyphen in compound words The dictionary contains a mix of common words acronyms and proper names It should be noted that these last categories are difficult cases for g2p or p2p converters and particular effort has been made to pronounce proper names in text to speech synthesis technology 12 The corpus is randomly split into a training a development dev and a test set The dev set is necessary for the optimisation of the weights of Moses mo del as will be later explained tuning and the test set is used for the evaluation of the system This division is based on dictionary entries so that all the pronunciations of a given word will be in the same set If not we would have the paradox of training the system with certain pronunciations and asking it to generate only the different pronunciations of the same word found in the test set The dev set has 9000 entries and the test set 16000 entries The original dictionary entries of training dev and test sets were transformed to have one graphemic transcription pronunciation pair per entry as opposed to one entry corresponding to the graphemic transcription of a word with all its pronunciation variants This is to have a format that resembles the aligned parallel texts used for training machine translation mo dels After transformation the dev and test sets have 11196 and 19782 distinct entries All the results are calculated for the same test set so that their comparison is legitimate Three different training conditions are compared for the two p2p systems 1 Train on the entire dictionary Words may have one or more pronunciations tr set 2 Train only on words with two or more pronunciation variants All words have multiple pronunciations tr set m 3 Train on the entire dictionary using only the longest canonical pronunciation to have one pronunciation per word tr set l At this point a further preparation of the training set for each metho d is required For the metho d where Moses is used as a p2p converter a monolingual 172 P Karanasou and L Lamel parallel corpus is needed meaning that both the source language and the target language will have phonemes as elements The source language is always formed by the canonical pronunciation segmented into phonemes The target language is formed of the corresponding pronunciations depending on the training condition For the pivot metho d the training set is used as a parallel corpus with one graphemic transcription pronunciation pair per line with spaces separating characters in order to use Moses as in a g2p task to generate a translation table that will be used to extract paraphrased sequences Each word is a source sentence with each grapheme being an element of the source sentence and each pronunciation is a target sentence with each phoneme forming an element of the target sentence Table 1 gives an overview of the data sets used with the number of entries distinct pairs and the average number of pronunciations word in the three training conditions after prepro cessing Table 1 Training conditions Training set Number of entries Average number prons word tr set 201423 1 2 tr set m 67769 2 3 tr set l 162974 1 0 It can be seen in Table 1 that there are large differences for the three training conditions For tr set m the number of entries diminishes to one third of the original dictionary However the number of pronunciations per word almost doubles In this case the extra information given by the canonical pronunciations of words with only one pronunciation is lost but we allow the systems to change the frequency relationship between the phrases of the canonical pronunciations and the phrases found in pronunciation variants and see how this influences the generation of pronunciation variants which is our main interest in this work In the third training condition only the canonical pronunciation of each word is kept in the training data This allows us to see if pronunciation variants can be generated even under limited training conditions For example this condition corresponds to generating variants from the output of a rule based g2p system which if originally developed for speech synthesis may not mo del pronunciation variants or to enriching a dictionary with limited pronunciation variants 3 2 System The system that is used to train the mo dels in both metho ds is based on Moses In the first proposed metho d Moses is used as a p2p converter in an one stage pro cedure Besides the phrase translation table a phoneme based 5 gram language mo del is built on the pronunciations in the training set using the SRI toolkit 13 Moses also calculates a distortion mo del but our dictionary do es not include a sufficient number of metathesis cases so the monotonic deco ding Comparing SMT Methods for Automatic Generation 173 does not change the final results Finally the combination of all components is fully optimized with a minimum error training step tuning on the dev set The tuning strategy we used was the standard Moses training framework based on the maximization of the BLEU score 10 The optimized weights generated by tuning are added to the configuration file Moses can also provide an n best translation list This list gives the n best translations of a source string with the distortion the translation and the language mo del weights as well as an overall score for each translation As stated earlier we keep only the 1 5 or 10 best translations i e pronunciation variants per canonical pronunciation Some pronunciations have fewer possible variants in which case all variants are taken In the pivot metho d generating pronunciation variants is a four stage pro cedure Moses is used in the first stage for g2p conversion and extraction of the translation table In the second stage the paraphrased pairs with their probabilities are extracted from the canonical pronunciations of the test set as previously described The 10 best paraphrases for each input phonemic sequence are extracted with a maximum length of 3 for the extracted paraphrases In the third stage the paraphrases are substituted in the canonical pronunciations of the test set for all their o ccurrences with all the possible combinations only in the first o ccurrence only in the second o ccurrence in the first and in the second o ccurrence etc limiting to 3 the maximum number of o ccurrences of the same paraphrase in a canonical pronunciation In the fourth and final stage the generated list of pronunciation variants is reranked based on the context The context is expressed by the same phoneme based 5 gram language mo del used in the first method The SRI toolkit is used to rerank the multiple pronunciation n best list modifying its probabilities As for the first metho d the 1 5 or 10 best variants are kept for each canonical pronunciation in the ordered list 4 Evaluation Different measures have been proposed to evaluate the predictions of pronunciation variants derived from the original canonical form The most frequently used are precision and recall first introduced in information retrieval 15 The canonical pronunciations xi of the test set can have one or more variants yi yi is a set Moreover our systems can generate one or more variants f xi f xi is also a set Thus the recall that corresponds to a couple yi f xi is the number of correct generated variants for a canonical pronunciation in the test set divided by the number of correct variants given in the test set for this canonical pronunciation f xi yi 4 ri yi The precision is the number of correct generated variants divided by the number of generated variants f xi yi 5 pi f xi 174 P Karanasou and L Lamel The total recall is the mean value of the recall of each example r 1 n n ri i 1 6 Analogously the total precision is the mean value of the precision of each example We refer to the previous definitions as micro recall and micro precision respectively If the examples are normalized by the number of expected variants correct variants in the reference the total recall becomes r n i 1 ri yi n i 1 yi 7 In this last case the macro recall is defined Macro precision is defined analogously The macro measures give more weight to the examples with multiple variants while the micro measures consider all the examples equally weighted It is important to do the evaluation on a pair level and not just consider the error rate of generated pronunciations to avoid counting as correct a generated pronunciation that do es not correspond to the canonical pronunciation it is associated with in the reference There is always the possibility that our system will generate a pronunciation out of a baseform i e canonical pronunciation that is not a variant of this baseform but however is a correct variant of another baseform This is counted as a false generation Another thing that should be noted is that we prune the canonical pronunciation pronunciation variant pairs that do not include a new variant This is important in order to improve the precision because while the pivot metho d generates only pronunciation variants when Moses is used as p2p converter it often outputs the canonical pronunciation that was used as input because it learns from the training data that the most probable pronunciation corresponding to a given canonical pronunciation is usually itself This depends a lot on the training conditions and makes this metho d inappropriate in certain cases To control the precision of our systems an upper limit is put to the number of n best variants that are kept in the hypotheses The 1 5 and 10 best variants per canonical pronunciation are generated consecutively The n best list is limited to 10 because preliminary studies showed that larger n only slightly improves recall while severely degrades precision There is quite a bit of overgeneration since in the 19k pronunciation pronunciation pairs of the test set there are only 4k pairs with pronunciation variants This could not be avoided with a random selection of the test set from the original dictionary where only 18 of words have variants as already stated However there is the possibility that some of the generated variants which are not in the reference and therefore counted as errors could be considered acceptable by a human judge Evaluating the system by an automatic measure cannot take into account the potential lack of coverage of the reference dictionary The two systems Moses as phoneme to phoneme converter m p2p and the pivot paraphrasing method p p2p were tested for the 3 training conditions presented earlier The results using the two proposed evaluation metrics are Comparing SMT Methods for Automatic Generation 175 Table 2 Results using Moses as phoneme to phoneme converter for the 3 training conditions Training set Measure tr set Micro recall Macro recall tr set m Micro recall Macro recall tr set l Micro recall Macro recall 1 best 0 20 0 19 0 21 0 Table 3 Results using the pivot paraphrasing method for the 3 training conditions Training set Measure tr set Micro recall Macro recall tr set m Micro recall Macro recall tr set l Micro recall Macro recall 1 best 0 29 0 26 0 25 0 22 0 09 0 09 5 best 10 best 0 60 0 70 0 56 0 66 0 56 0 70 0 53 0 66 0 26 0 38 0 24 0 35 shown in Tables 2 and 3 respectively We only present recall measures in the tables because this is what is of most interest in the particular task It is more important to cover possible pronunciations than to have too many since other metho ds can be applied to reduce the overgeneration alignment with audio manual selection use of pronunciation probabilities etc The best value that both precision and recall can obtain is 1 However the best value of precision is often further limited depending upon the number of elements of the n best list and the overgeneration that cannot be avoided As can be expected for both methods the number of correctly generated variants increases with the size of the n best list This is normal not only because the number of hypothesis increases with the size of the n best list but also because there are canonical pronunciations in the test set that have more than one variant approximately 1 6 of the part of the test set with multiple pronunciations which cannot be captured when an insufficient number of pronunciations is generated It is also interesting to compare the results of the first and the second training conditions whole dictionary vs keeping only entries with multiple variants for the two metho ds In the second case the amount of training data is only one third of the original training set However the results are more or less the same for both training conditions This may be because the information that the mo del is using to learn how to generate variants is mostly captured by the multiple pronunciations in the training set and less by the fewer variations observed in the canonical pronunciations of one pronunciation words What the mo del is learning in this case is fo cused on the relationship between the canonical pronunciation and other variants and therefore has effectively more relevant information and it does not get watered down by the self pro duction This may compensate for the reduced amount of data 176 P Karanasou and L Lamel A comparative analysis of the two methods can also be made In the first whole training set and in the second only entries with variants training conditions using Moses as a p2p converter gives better results in terms of the generation of pronunciation variants for both micro and macro measures when the 5 best and the 10 best variants are kept However when only the 1 best generated pronunciation is kept the pivot metho d gives better results This is due to the generation of canonical pronunciations by Moses when used as p2p converter which are subsequently removed from the results because they already exist in the input The number of variants generated by Moses p2p when only the 1 best is kept are quite limited This is why while the recall is lower than that of the pivot method the precision is higher It can be seen that the results change when the training set is limited to the canonical pronunciations only the third condition In this case the pivot metho d manages to pro duce some results while for Moses the mo del fails to generate any variants this is why the corresponding columns are left empty in Table 2 and all the variants that are in the 5 best or the 10 best lists are false These results warrant a bit more discussion The results are promising for the pivot metho d because even when the training dictionary has few or no pronunciation variants the pivot metho d can still be used to generate some alternative pronunciations This can be explained by the fact that the pivot metho d uses also the graphemic information Even if no variants are included in the training set it can still find graphemic sequences of words that correspond to different phonemic sequences and consider these phonemic sequences as possible mo difications of pronunciations For example in the training set the word autoroute is pronounced ctorut and the word shouting is pronounced sWtIG These words have the graphemic sequence out in common which can be used as a pivot between the phonemic sequences ut and Wt These phonemic sequences become a paraphrased pair that generates correctly the variants rWts and ruts of the word routes found in the test set This illustrates the difficulty of generating pronunciations in English because the correspondence between orthographic forms and canonical pronunciations do es not follow strict rules which would prevent the pivot metho d from finding mo dified phonemic sequences corresponding to the same graphemic sequence This is not the case when Moses is used as phoneme to phoneme converter When no variants are given to the system it do es not have any additional information in order to be trained for the task of generating multiple pronunciations It is like trying to train an SMT system without a target language It can just learn to align the phonemic sequences with themselves which is fine for the g2p task but is not applicable to the generation of variants In this case is it wrong to use Moses for this task as it is obvious that it has nothing to learn from the training data 5 Conclusions This paper has reported on applying two data based approaches inspired from research in statistical machine translation to the problem of generating pronunciation variants One of the ob jectives of this work was to compare these Comparing SMT Methods for Automatic Generation 177 two approaches to mo deling pronunciation variations The approaches differ in the way that information about pronunciation variation is obtained The approach using Moses as phoneme to phoneme converter takes into account only the information provided by the phonemic transcriptions The pivot metho d uses information from both the phonemic and the orthographic transcriptions When the full dictionary that contains words with one or more pronunciations is used for training the Moses based metho d gives better results than the pivot based one This is also the case when training is carried out on only entries with multiple pronunciations However when the training dictionary do es not contain any pronunciation variants the Moses based metho d cannot be used while pivot can still learn to generate variants This is an advantage of the pivot metho d and could be useful for languages without well developed multiplepronunciation dictionaries This arises from the use of information provided by the orthographic transcription by the pivot metho d An interesting follow up study is to use the pivot metho d to propose variants as a post pro cessing step to a g2p system Another case to study is the influence of the p2p converter on the results of a g2p converter if their output n best lists are combined We have started some preliminary experiments in these directions with promising results In future work we will evaluate the proposed metho ds at generating pronunciations and variants for proper names which are the most difficult cases to handle These also account for the ma jority of words that need to be added to a dictionary once a reasonably sized one is available for the given language Another important outstanding issue concerns the proper way to evaluate the ability of a system to generate pronunciation variants In this work recall and precision have been used however other measures such as phoneme accuracy can also be applied In this case it may be appropriate to have phone class dependent penalties with certain confusions being more important than others In order to improve the precision the n best lists need to be more heavily pruned One direction to explore is using audio data to remove pronunciations however this can only apply to words found in the audio data The ultimate test of course is how the variants affect the accuracy of a speechto text transcription system Acknowledgments This work is partly realized as part of the Quaero Programme funded by OSEO French State agency for innovation and by the ANR EdyLex project References 1 Adda Decker M Lamel L Pronunciation variants across system configuration language and speaking style Speech Communication 29 178 P Karanasou and L Lamel 4 Fukada T Yoshimura T Sagisaka Y Automatic generation of multiple pronunciations based on neural networks Speech Communication 27 1 Automatic Learning of Discourse Relations in Swedish Using Cue Phrases Stefan Karlsson and Pierre Nugues Lund University Lund Institute of Technology Department of Computer Science Box 118 S 221 00 Lund Sweden stefan karlsson 342 student lth se Pierre Nugues cs lth se Abstract This paper describes experiments to extract discourse relations holding between two text spans in Swedish We considered three relation types cause explanation evidence CEV contrast and elaboration and we extracted word pairs eliciting these relations We determined a list of Swedish cue phrases marking explicitly the relations and we learned the word pairs automatically from a corpus of 60 million words We evaluated the method by building two way classifiers and we obtained the results Contrast vs Other 67 9 CEV vs Other 57 7 and Elaboration vs Other 52 2 The conclusion is that this technique possibly with improvements or modifications seems usable to capture discourse relations in Swedish Keywords rhetorical relations discourse relations cue phrases 1 Introduction Rhetorical relations and the Rhetorical structure theory 1 form a framework to describe and interpret the organization of a text In this theory relations consist of annotated links tying two text spans as for example the clauses in the sentence Malaria H Loftsson E 180 S Karlsson and P Nugues Initially the newspaper was published once a week but in Dec 1850 it was transformed into a daily 2 Uggleupplagan vol 3 1157 Rhetorical relations can be associated with certain cue words or phrases such as 2 A Statistical Model Some word pairs are frequent in contrasts hypothetically for example week and daily as in the example above and other pairs in explanations i e exists and develops Instead of extracting relations with manual rules we can try to derive automatically sets of words involved in specific relations from corpora Marcu and Echihabi proposed an unsupervised method 5 to train Using the Automatic Learning of Discourse Relations in Swedish Using Cue Phrases 181 3 3 1 Experimental Setup Extraction of Text Spans We considered three discourse relations cause explanation evidence CEV contrast and elaboration We compiled a Swedish corpus using texts from the Runeberg project 45 million words and the European Parliament proceedings 6 16 million words a multilingual corpus where we used the Swedish source parts We then inspected the corpus manually and incrementally built the extraction patterns shown in Table 1 Table 1 Swedish extraction patterns used in the experiments BOS indicates the beginning of the sentence and EOS the end of the sentence Contrast BOS BOS BOS BOS men EOS ehuru EOS Cause Explanation Evidence BOS BOS BOS BOS BOS BOS BOS BOS BOS BOS BOS Elaboration BOS vilket EOS BOS hvilket EOS The Nordisk Familjebok encyclopedia from the end of the 19th century and the beginning of the 20th century represents a large part of the corpus This explains why we had to use words like ty because and ehuru in spite of as markers that do not belong to present day Swedish The corpus was randomly divided into a training set 90 and a test set 10 To improve training we used only verbs and nouns 5 We tagged the corpus words with their part of speech using the Granska tagger 7 We kept the nouns and the verbs and discarded the rest of the words including the markers from the patterns 182 S Karlsson and P Nugues Finally we compiled the training examples 130 796 contrasts 37 319 CEV and 43 387 elaborations and a test set of 14 643 contrasts 4 107 CEV and 4 976 elaborations all extracted using the patterns in Table 1 3 2 Evaluation Methods For the evaluation we built binary classifiers to distinguish where cardinal is the number of entries in the table We found that a lambda of 0 05 seemed to maximize the accuracy of the classifiers In a similar experiment 9 used the value of 0 25 3 3 Results Table 2 shows the accuracy of the classifiers A result of 67 9 in the Contrast vs Other condition is in the same range as the results obtained for English 5 which reported between 60 and 70 for most relations The results for Elaboration vs Other that reached 52 2 were significantly lower however Table 2 The accuracy of each classifier In each case the baseline is 50 Relation Accuracy Contrast vs Other 67 9 CEV vs Other 57 7 Elaboration vs Other 52 2 Automatic Learning of Discourse Relations in Swedish Using Cue Phrases 183 4 Conclusions Results around 60 clearly indicates that the classifier is better than a random assignment of text spans to each class The result with elaboration is not completely satisfying though which first of all can be accounted to the fact that we only used 43 387 training examples As perspectives some simple improvements could be made Since there is no intrinsic order in contrast relations the table could be made commutative However we did not consider it a critical point since there were more than 130 000 training examples of contrasts The most critical point though is to find the best set of cues phrases for each discourse relation The corpora used in this experiment was quite small for the task and we had to use many cue phrases at one time With a larger training set we could determine which phrases contribute most to the model without introducing noise for example by comparing results obtained by including or excluding training examples from a particular extraction pattern Not only the size of the corpora limits the performance of this technique The example words that indicate a contrast i e week and daily in the example in Sect 1 can possibly stand in other types of discourse relations Such overlapping word pairs will dim the statistical accuracy of the model no matter the size of the corpora This is a major limitation of the general approach taken and can only be dealt with by introducing other types of classification information to distinguish between the rhetorical relations In English possibly WordNet 10 11 or FrameNet 12 could be used to figure out which word pairs indicate a particular relation To sum up we presented evidence of a feasible technique for the automatic extraction of discourse relations in Swedish Marcu and Echihabi showed 5 that using this technique as a complement to extracting cue phrase marked sentences can increase the number of correctly classified contrasts from 26 to 77 Further investigations are however necessary to evaluate more accurately the applicability of this algorithm in Swedish Acknowledgments Lars Aronsson provided most of the corpora used from the Runeberg project Jonas References 1 Mann W C Thompson S A Rhetorical structure theory A theory of text organization Technical Report RS 87 190 Information Sciences Institute 1987 2 Meijer B ed Nordisk familjebok Uggleupplagan edn Nordisk familjeboks 184 S Karlsson and P Nugues 4 Corston Oliver S Computing Representations of the Structure of Written Discourse PhD thesis University of California Santa Barbara 1998 5 Marcu D Echihabi A An unsupervised approach to recognizing discourse relations In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics ACL 2002 Philadelphia pp The Representation of Diatheses in the Valency Lexicon of Czech Verbs Charles University in Prague Institute of Formal and Applied Linguistics kettnerova lopatkova ufal mff cuni cz Abstract In the present paper we deal with diatheses in Czech from a lexicographic point of view We propose a method of their description in the valency lexicon of Czech verbs VALLEX We distinguish grammatical and semantic diatheses as two typologically different changes in verbal valency structure In case of grammatical diatheses these changes are regular enough to be described by formal syntactic rules In contrast the changes in valency structure of verbs associated with semantic diatheses vary even within one type of diathesis Thus for the latter type we propose to set separate valency frames corresponding to their members and to capture the changes in verbal valency structure by lexical rules based on an adequate lexical semantic representation of verb meaning Keywords valency lexicon grammatical and semantic diatheses changes in valency structure of verbs lexical semantic representation of verbs 1 Introduction Valency behavior of verbs is so heterogenous that it cannot be described by general syntactic rules Instead it must be captured in the form of lexical entries separately for each verb Prototypically a single meaning of a verb corresponds to a single valency frame However in many cases semantically close uses of verbs can be syntactically structured in different ways See the following examples 1 a Peter smeared butter on bread The research reported in this paper was carried out under the project of MSMT CR No MSM002162083 It was supported by the grant No LC536 and partially by the grant No GA P406 2010 0875 H Loftsson E 186 V to be contraintuitive Thus we propose to describe the changes in the valency structure of semantically related uses of verbs by means of formal syntactic and lexical rules First let us fo cus on the pairs in 1a 2a and 1b 2b respectively The changes in the valency structure of the verb to smear are expressed by grammatical means we refer to the relation between these uses of the verb as a grammatical diathesis In contrast the changes in the valency structure of the verb to smear in pairs 1a 1b and 2a 2b respectively are expressed by lexical semantic means We refer to the relation between such uses of verbs as a semantic diathesis The representation of grammatical and semantic diatheses is proposed here for the valency lexicon VALLEX which aims at the explicit description of valency behavior of Czech verbs 1 This lexicon takes the Functional Generative Description henceforth FGD as its theoretical background 7 In FGD valency is related primarily to the tectogrammatical layer i e the layer of linguistically structured meaning The valency characteristics are enco ded in a form of a valency frame which is mo deled as a sequence of frame slots corresponding to valency complementations of a verb labeled by rather coarse grained tectogrammatical roles as Actor Patient Effect Direction etc 5 In addition possible morphemic forms are specified for each valency complementation For our purposes we enhance FGD i with the concept of lexical conceptual structures 6 representing lexical semantic properties of verbs and ii with the open set of labels for situational participants as Agent Recipient Filler Surface etc The paper is structured as follows In Section 2 we define the notions situation and perspective which play a crucial role in the characteristics of diatheses Then on the basis of the correspondence between situational participants valency complementations and surface syntactic positions we distinguish two types of diatheses grammatical diatheses Section 3 and semantic diatheses Section 4 In Section 3 1 and 4 1 the representation of grammatical and semantic diatheses in the valency lexicon is proposed respectively Conclusion and an outlook for future work is presented in Section 5 2 Situation vs Perspective The members of both types of diatheses are usually characterized as constructions denoting the same situation though each time from a different perspective Thus the concepts situation and perspective play a key role in the characteristics of diatheses First let us fo cus on the concept of a situation The term do es not refer to a real life situation it is rather a situation mo deled by language i e a linguistic situation The linguistic situation related to an event represents a set of facts and entities i e participants linked in a unified structure Thus an analysis of a particular situation denoted by the verb must involve not only the specification 1 http ufal mff cuni cz vallex 2 5 The Representation of Diatheses in the Valency Lexicon of Czech Verbs 187 of the relevant number of its participants but also the description of the relations between them see e g 3 For example the situation portrayed by the uses of the verb to smear in examples 1 and 2 consists of three participants labeled as Agent Cover and Surface and it may be informally described as an Agent covers a Surface of an ob ject with a Cover We refer to this part of the verbal meaning as a situational meaning and to its components as situational participants Situational meaning represents an abstract mo del of situation which has not yet been linguistically structured Sentences expressing the same situational meaning can be usually structured in several ways i e different situational participants can occupy the syntactically prominent positions of sub ject and direct ob ject This results in different perspectives from which the situation is viewed see e g 2 In case of the verb to smear the situation can be viewed from the perspective of Agent i e Peter as in 1a and 1b from the perspective of Cover i e butter as in 2a or from the perspective of Surface i e bread as in 2b The different perspectives in these sentences are manifested by grammatical and lexical semantic means As a result we distinguish two typologically different changes in the verbal valency structure Grammatical diatheses refer to the relation between the uses of verbs characterized by the differences in the mapping of valency complementations and surface syntactic positions as in 1a 2a and 1b 2b These differences are based on grammatical means Section 3 In contrast semantic diatheses refer to the relation between the uses of verbs characteristic of the different correspondence between situational participants and valency complementations as in 1a 1b and 2a 2b These differences are expressed by lexical semantic means i e by the change of lexical unit Section 4 3 Grammatical Diatheses Sentences related by a grammatical diathesis express the same situational meaning however they are characterized by different perspective which results from the changes in the mapping between valency complementations and surface syntactic positions Further within this type the linking of situational participants and valency complementations remains unchanged This can be illustrated by the pairs of examples 1a 2a and 1b 2b the first one repeated here as 3 3 a Peter smeared butter on bread 188 V 1 The use of the marked member of grammatical diatheses is conditioned by the grammatical meaning of the verb represented by a specific verbal grammateme in FGD 4 In Czech verbal forms of these marked members typically consist either of auxiliaries and non finite form of lexical verb or they have a reflexive form 2 The marked member of grammatical diatheses is prototypically connected with the shift of some of situational participants from the prominent surface syntactic position of sub ject to a less prominent syntactic position 3 The correspondence between situational participants and valency complementations remains unchanged thus the set of situational participants is directly enco ded in the valency frame by a sequence of valency complementations It implies that the number of valency complementations and their type are preserved and the changes in the valency frame are limited only to the changes in morphemic forms of the valency complementations The asymmetry between the mapping of the situational participants and the surface syntactic positions corresponding to the verb to smear in example 3 is illustrated in Figure 1 Fig 1 The changes in the mapping of the valency complementations and the syntactic positions of the verb to smear associated with the passive grammatical diathesis 3 1 Representation of Grammatical Diatheses In this section we propose the representation of grammatical diatheses in the valency lexicon VALLEX The changes in valency structure asso ciated with grammatical diatheses are regular enough to be described by general syntactic rules these rules are stored in the grammar component of the lexicon Then it is sufficient to indicate the applicability of a certain rule in a special attribute attached to each relevant lexical unit of a verb in the data component of the lexicon Let us demonstrate our approach on the example of the recipient diathesis The marked construction as in 5 is characterized by the verbal form consisting of the auxiliary dostat to get and the past participle of a lexical verb The structural condition on the recipient diathesis is the presence of a valency complementation expressed by the dative case in the valency frame corresponding The Representation of Diatheses in the Valency Lexicon of Czech Verbs 189 to the situational participant Recipient The following verbs satisfy this condition doporu cit to recommend nahradit to recompense na Table 1 Recip r rule for the recipient diathesis Recip r verbal grammateme valency frame Unmarked Recip 0 ACTnom ADDRdat Marked Recip 1 ACTod gen ADDRnom Note 1 2 3 190 V 2 The shift of the valency complementation ACT from the sub ject position into the adverbial position is manifested by the change of its morphemic form from the nominative into the prepositional group od genitive 3 The shift of the valency complementation ADDR from the indirect ob ject position into the prominent sub ject position is expressed by the change of its morphemic form from the dative into the nominative 4 Every valency complementation that is not listed in the rule is preserved For example if we apply the rule Recip r to the valency frame describing the unmarked use of the verb p rid elit to allocate example 4 we derive the valency frame corresponding to the verb in the marked construction of the recipient diathesis example 5 as follows ACTnom ADDRdat PATacc Recip r ACTod gen ADDRnom PATacc Other grammatical diatheses may be described in the same way Passive diathesis Zam estnanci informovali 4 Semantic Diatheses Sentences related by a semantic diathesis express the same situational meaning similarly as grammatical diatheses However in case of semantic diatheses the different perspective is reflected by the changes in the mapping of situational participants and valency complementations This can be illustrated by examples 1a 1b and 2a 2b the first one repeated here as 6 6 a Peter smeared butter on bread The Representation of Diatheses in the Valency Lexicon of Czech Verbs 191 In contrast to grammatical diatheses semantic diatheses are not connected with changes of grammatical categories of verbs They are rather related to a small number of well delimited semantic classes of verbs which share certain facets of meaning The members of semantic diatheses satisfy the following criteria 1 Semantic diatheses are expressed by lexical semantic means i e by different lexical units The members of semantic diatheses do not differ from each other in a specific grammatical meaning of a verb instead they differ in structuring situational participants into a valency frame 2 Semantic diatheses are characterized by shifts of some of situational participants from the prominent surface syntactic position of ob ject or sub ject to a less prominent syntactic position 3 The changes in a verbal valency structure arisen from the changes in the correspondence between situational participants and valency complementations may affect the number of valency complementations their type as well as their morphemic form s The asymmetry between the correspondence of the situational participants and the valency complementations of the verb to smear in example 6 is illustrated in Figure 2 Fig 2 The changes in the mapping of the situational participants and the valency complementations of the verb to smear associated with the semantic diathesis 4 1 Representation of Semantic Diatheses In this section we propose the representation of semantic diatheses in the valency lexicon VALLEX As the members of semantic diatheses differ in the correspondence between situational participants and valency complementations an appropriate lexical semantic representation of the situational meaning of the verb is necessary for their adequate description For this purpose we adopt the lexical conceptual structures proposed in 6 Furthermore contrary to grammatical diatheses the changes in the verbal valency structure associated with semantic diatheses vary even within a single type of diathesis It follows that they cannot be described by general syntactic 192 V rules For these reasons we propose to specify separate lexical units corresponding to the individual members of semantic diatheses in the data component of the lexicon these lexical units are interlinked by a relevant type of semantic diathesis In the grammar component the changes in the verbal valency structure are represented by lexical rules Our approach can be explained on the example of the locative semantic diathesis see below Let us mention some other types of Czech semantic diatheses that may be described in the same way Material Product diathesis Na rezal The Representation of Diatheses in the Valency Lexicon of Czech Verbs 193 i In the data component the members of semantic diathesis are represented by separate lexical units which are interlinked by a relevant type of semantic diathesis ii In the grammar component the changes in valency structure of verbs are captured by a lexical rule determining the changes in the mapping of situational participants and valency complementations Locative Diathesis of Verbs Indicating Creating Co occurrence Let us demonstrate our approach on the example of the verb nalo zit to load With respect to its semantic properties we determine the following three situational participants Agent Filler and Container The situational meaning of the verb may be informally described as an Agent fills a Container with a Filler This meaning is syntactically structured in two ways as in 7 and 8 above Both sentences 7 and 8 express the change of location of the situational participant Filler caused by the participant Agent In comparison with the variant 7 variant 8 is semantically more 194 V the LCS b the same correspondence between the variables and the labels is preserved as in LCS a With respect to the complexity we consider the variant corresponding to the LCS a as unmarked and the variant characterized by the LCS b as the marked one Table 2 The possible mapping of the situational participants and the valency complementations of the verbs expressing creating co occurrence specifying the relation inside x Agent ACT ACT ACT y Filler z Container examples PAT DIR nalo zit seno na Considering the mapping between the valency complementations and the situational participants represented by the variables in the LCS a and LCS b Table 2 we formulate the lexical rule Lo c r1 for the locative diathesis The rule Loc r1 can be applied also to other verbs expressing creating co o ccurrence specifying the relation inside e g nato cit to draw nasypat to pour doplnit to add as well as to those verbs specifying the relation outwards see below inside Filler Container outwards Cover Surface LCS a LCS c PAT DIR Loc r1 Loc r1 LCS b LCS d EFF PAT Commentary on the Loc r1 On the left side of the rule the valency complementations of the unmarked member of the diathesis are given i e the situational participant mapped onto PAT in the valency frame of the verb represented by the LCS a and LCS c below is changed into EFF in the frame corresponding to the LCS b and LCS d If EFF is not present in the valency frame then this participant is not linguistically structured see in Table 2 The situational participant mapped onto the valency complementation DIR in the valency frame of the unmarked member of the diathesis corresponds to the valency complementation PAT in the frame of the marked member The rule Loc r1 holds also for the changes in the mapping of situational participants and valency complementations of the verbs indicating creating coo ccurrence with the relation outwards e g nat The Representation of Diatheses in the Valency Lexicon of Czech Verbs 195 PAT Surface barvou EFF Cover 10 Petr ACT Agent nat rel zed Eng Peter ACT smeared the wall PAT Surface with paint EFF Cover c x ACT SMEAR CAUSE BECOME y ON z d x CAUSE BECOME z SMEARED x ACT SMEAR CAUSE BECOME y ON z BY MEANS OF However in case of the verbs expressing creating co o ccurrence with the relation outwards another set of labels for the situational participants is asso ciated with the variables in the LCS c and LCS d x Agent y Cover and z Surface Despite the different set of labels the changes in the mapping of the situational participants and the valency complementations are described by the same rule Loc r1 as the changes of the verbs indicating the relation inside Locative Diathesis of Verbs Indicating Destroying Co occurrence For the description of the changes in the correspondence between situational participants and valency complementations of the verbs expressing the event destroying co o ccurrence we formulate the lexical rule Loc r2 We demonstrate cistit to clean see 11 and 12 this rule on the example of the verb o 11 Jana ACT Agent o cistila outwards Cover Surface inside Filler Container LCS e PAT DIR Loc r2 Loc r2 LCS f ORIG PAT 196 V Commentary on the Loc r2 On the left side of the rule the set of valency complementations representing the unmarked member of the diathesis is given The situational participant Cover or Filler mapped onto PAT in the valency frame represented by the LCS e is mapped onto ORIG in the frame represented by the LCS f If ORIG is not present in the marked frame then this participant is not expressed The situational participant Surface or Container corresponding to the valency complementation DIR in the valency frame of the unmarked member of the diathesis described by the LCS e is mapped onto the valency complementation PAT in the frame of the marked use represented by the LCS f 5 Conclusion and Future Work We have distinguished two types of relations between semantically close uses of verbs which are syntactically structured in different ways grammatical and semantic diatheses We have proposed their representation in the valency lexicon VALLEX The changes in a verbal valency structure asso ciated with grammatical diatheses are described by formal syntactic rules which determine regular changes in morphemic form s of complementations Thus both verbal uses may be represented by a single lexical unit with ascribed information on applicability of individual formal syntactic rule for a relevant grammatical diathesis In contrast the changes typical of semantic diatheses are represented by lexical rules which formally describe the changes in the mapping of situational participants and valency complementations It implies that both verbal uses are represented by separate lexical units interlinked by a relevant type of semantic diathesis In the future we intend to represent typologically different changes in valency structure of verbs in the lexicon in a similar way References 1 Anderson S R On the Role of Deep Structure in Semantic Interpretation Foundations of Language 7 Symbolic Classification Methods for Patient Discharge Summaries Encoding into ICD Laurent Kevers and Julia Medori CENTAL Abstract This paper addresses the issue of semi automatic patient discharge summaries encoding into medical classifications such as ICD 9CM The methods detailed in this paper focus on symbolic approaches which allow the processing of unannotated corpora without any machine learning The first method is based on the morphological analysis MA of medical terms extracted with hand crafted linguistic resources The second one ELP relies on the automatic extraction of variants of ICD 9CM code labels Each method was evaluated on a set of 19 692 discharge summaries in French from a General Internal Medicine unit Depending on the number of suggested classes the MA method resulted in a maximal F measure of 28 00 and a highest recall of 46 13 The best Fmeasure for the second method was 29 43 while the maximal recall was 52 74 Both methods were then combined The best recall increased to 60 21 and the maximal F measure reached 31 64 Keywords Patient discharge summaries ICD 9 CM classification symbolic approach 1 Introduction This paper presents work done in collaboration with one of the ma jor hospitals in Brussels les Cliniques universitaires Saint Luc The Belgian government requires hospitals to report their activity through the enco ding of patient s stays into ICD 9 CM1 Co des from this classification symbolise diagnoses procedures aggravating factors such as allergies smoking but also elements in the patient s past history that may influence his her current health status The encoding task is generally done by professional co ders Their work consists in going through the patient s medical record and translating into ICD 9 CM co des every activity that o ccurred during the patient s stay The main source of information co ders use is patient discharge summaries PDS that physicians write after each patient s stay These do cuments are written in free text in the form of a letter addressed to the patient s GP or external care professionals The enco ding task is a tedious and demanding task that requires very specialized 1 International Classification of Diseases Ninth revision Clinical Modification H Loftsson E 198 L Kevers and J Medori skills Consequently many hospitals try to reduce the amount of work involved by trying to partly automate this pro cess Our goal is to build a tool that would help co ders by providing them with a set of most likely co des Indeed the full automation of the task is difficult to achieve as the PDS seldom contains all the necessary information2 The automation of the enco ding process can be considered as analogous to a classification problem It involves classifying PDS into a nomenclature Here ICD 9 CM codes are considered as classes There are two main approaches to this classification problem symbolic and statistical metho ds Statistical approaches are based on machine learning metho ds and require a large amount of annotated data for training which makes it difficult for this type of approach to face a change of nomenclature Saint Luc will adopt ICD 10 in the near future and to classify certain very specific co des for which we have only sparse data available In this paper the aim is therefore to develop a symbolic approach to the encoding task Symbolic approaches involve linguistic knowledge are therefore usually more language dependent and require more time for development However in one of our metho ds metho d 2 we will see that it is possible to partly generate the linguistic resources automatically Two metho ds are described metho d 1 is based on the morphological analysis of medical language metho d 2 relies on the automatic extraction of variants of ICD 9 CM co de labels Metho d 2 is thus limited to the vocabulary used in the resource whereas metho d 1 uses a wider range of vocabulary They should therefore complement each other After a brief description of related work section 2 and evaluation data section 3 sections 4 and 5 will detail the two metho ds and their respective results Then their combination will be discussed in section 6 before considering ways to improve the performance of the system in section 7 2 Related Work Since the early 1990s many scientists have looked into the possible automation of the enco ding pro cess 1 2 3 As mentioned above there are two main approaches to the enco ding task knowledge based e g MedLEE 4 and machine learning e g Auto co der 5 Both approaches scored highly in the Computational Medicine Challenge in 2007 6 among the best three systems two combined a statistic and a symbolic approach 7 and only one relied only on a symbolic approach 8 All these studies were developed on English language Pereira et al 9 built a fully symbolic system for French relying on a linguisticbased indexing system into the French version of the MeSH classification and then mapping it to ICD 10 What is noteworthy is that most systems even when choosing a statistical approach still rely on a linguistic component The results of most of these studies are promising Autocoder achieved very high precision for two thirds of the assigned co des but the do cuments are often more structured 2 Additional information can be found in the full medical record but to avoid having to deal with the variety of formats in the record we decided to focus our work on PDS as sources of information Symbolic Classification Methods for PDS Encoding into ICD 199 than our PDS e g diagnoses clearly marked And they are also often limited with regard to the number of co des involved or to the types of do cuments 3 Data The ICD 9 CM is a hierarchical nomenclature comprising 15 688 codes which are 4 or 5 characters long The first three characters represent a general category of diagnoses and the next one or two digits specify the exact diagnosis Fig 1 The ICD 9 CM is divided into 1 135 general categories In the perspective of a coding help our study will classify according to these categories letting the co der choose the right co de into the hierarchy within each suggested category Code 001 0010 0011 0019 Label Cholera Cholera due to Vibrio cholerae Cholera due to Vibrio cholerae el tor Cholera unspecified Fig 1 Hierarchical structure of ICD9 CM Our evaluation data consists in 19 692 PDS in French taken from the General Internal Medicine unit Patients in this unit suffer from very diverse diseases A wider range of co des is therefore used 6 029 different co des dispatched into 895 categories The PDS in our corpus were assigned 150 116 co des 137 336 categories which makes an average of 7 6 co des 7 categories per document Note that 27 241 out of 895 of the categories were used less than 6 times These manually assigned co des are used as a gold standard for our evaluation In order to broaden the scope of our linguistic resources we used the UMLS3 as a source of variants for the ICD 9 CM co de labels The UMLS metathesaurus unifies and integrates into one unique resource many medical nomenclatures from different languages giving to each concept a unique concept identifier CUI Using this CUI we were able to extract from this metathesaurus different wordings for the ICD 9 CM co de labels These synonyms were then added to the original co de labels as illustrated in Fig 2 Class French label 061 Dengue Dengues Fi evre dengue Infection par le virus de la dengue English label Dengue Dengues Dengue fever Infection by the dengue virus Source ICD 9 CM UMLS UMLS UMLS Fig 2 Definition of class 061 with terms from ICD 9 CM and UMLS 3 Unified Medical Language System http www nlm nih gov research umls 200 L Kevers and J Medori 4 Classification Method 1 Morphological Analysis MA This metho d follows a two part structure to re create the work of human co ders who first read the PDS while highlighting information that needs to be enco ded and then assign codes to these expressions Therefore the first mo dule extraction module section 4 1 already presented in 10 aims at restricting the information to be processed by selecting informative sequences of text The second mo dule encoding module section 4 2 is a new one which translates the extracted phrases into co des 4 1 Extraction Module This mo dule selects in PDS sequences of text that convey information that needs to be encoded diagnoses pro cedures allergies etc This mo dule is based on specialized dictionaries and transducers built with Unitex4 11 The specialized dictionaries were built partly automatically and completed manually The main dictionaries are the dictionaries of diagnoses and pro cedures These two dictionaries were automatically constructed using the French nomenclatures comprised in the UMLS as described in section 3 As cataloguing all the different ways a physician may mention a diagnosis is difficult these dictionaries still needed to be completed manually All the dictionaries were then automatically inflected To compensate the lack of exhaustivity of the dictionaries we needed to use more flexible linguistic resources to detect diagnoses and informative data We therefore used finite state transducers5 Transducers are represented as graphs see Fig 3 Each path of the graph represents a recognized sequence These hand crafted transducers aim at marking up the phrases that mention a diagnosis or a pro cedure by adding XML tags to the original text In Fig 3 the graph recognizes different ways of mentioning fractures fracture and sprains foulures as well as the bo dy part affected with a link to the anatomical dictionary in subgraph d localisation It then outputs the XML tags MALINDET indicating that the phrase is a diagnosis Fig 3 Transducer for fractures 4 5 A corpus processing tool http www igm univ mlv fr unitex A transducer is not only able to recognize a sequence of elements but also to produce a related output Symbolic Classification Methods for PDS Encoding into ICD 201 When diseases or pro cedures are not in the dictionaries graphs allow us to locate these terms thanks to clues in the text the context in which the term appears and its morphology Contexts like The patient presents with shows signs of indicate that what follows may be a disease or a symptom and will therefore be extracted The morphological clues consist mainly of word endings like pathy indicating a disease or therapy for a pro cedure Detecting contexts is also very important as it influences the way the pathology will be encoded a negated diagnosis will not be enco ded or past history illnesses are co ded differently This extraction mo dule was evaluated in 10 on a corpus of 220 PDS 66 6 of phrases highlighted manually by the professional co ders were also extracted by the system and 85 5 of the extracted phrases were diagnoses More work has been done on the system since then but it has not yet been re evaluated 4 2 Encoding Module Once the terms containing information to be enco ded are extracted they need to be matched to an ICD 9 CM co de This classification metho d aims at taking advantage of the fact that medical language has a rich morphology When comparing the original terms used in the PDS and the corresponding co de label we observed that many terms looked morphologically close We therefore developed a methodology very similar to a bag of words approach where the extracted terms and the ICD co de labels were matched according to the morphemes composing them The breaking down into morphemes was done using 6 7 202 L Kevers and J Medori Fibroscopie bronchique PDS Bronchoscopie par fibre optique ICD 9 CM fibrbronch scopie scopie bronchfibre ique optique Fig 4 Example of bronchoscopy label Cj S Ti Cj NTi Cj NTi NCj where NTi Cj is the number of common words between Ti and Cj NTi and NCj are the number of words on each side The resulting bag of word of each extracted phrase is then compared to all the co de labels and their synonyms To assign co des to the entire PDS the maximum similarity value for each co de is kept Score Cj max S Ti Cj All the co des are ordered according to their similarity value The first co des in this list are then considered as the most likely codes to be assigned to the document The score for each parent category is the maximum score of its children co des The output list is then ranked according to this parent category score This list can be returned as it is or shortened using a thresholding function 14 4 3 Results The enco ding evaluation was conducted on 19 692 do cuments The measures Recall Precision and F measure F1 were macro averaged8 For a do cument R gives the proportion of manually assigned classes retrieved in the suggested list and P the proportion of goo d classes as defined by the gold standard into this list The best results are reported on table 1 Depending on the will to promote recall or on the contrary precision we can tune the thresholding function acceptance level and then compute intermediate results Of course a higher recall level always comes with a higher number of suggested categories Table 1 Evaluation of MA method Recall R Precision P F measure F1 Nb classes Threshold Best Recall 46 13 14 70 21 10 20 No Best F measure 34 52 27 34 28 00 8 6 Yes 5 Method 2 Extended Lexical Patterns ELP This method previously described in 14 and used in 15 on parliament documents is based on the insight that well described classes9 can be sufficient to find a significant intersection with the text vocabulary Class labels are extracted from 8 9 Computed for each document and then averaged Class defined by a descriptive set of words and or compound expressions Symbolic Classification Methods for PDS Encoding into ICD 203 existing terminological resources e g thesauri nomenclatures For this paper the ICD 9 CM enhanced with the UMLS synonyms was used section 3 Our approach attaches value to compound expressions because of their high descriptive power They are often used to refer to complex concepts or ob jects and as a result are goo d items to contribute to class definition The extended lexical patterns ELP method consists in two steps First the automatic transformation of a class definition resource into a term extraction resource Section 5 1 It detects in each text a list of expressions considered as interesting for class inference Then the class assignment Section 5 2 step uses this result for classification The first step is performed once while the second must be repeated for each new do cument 5 1 Extraction Resource Building The original nomenclature is automatically converted to finite state transducers compatible with Unitex They are made of a lexical elements from the original class label b other more generic items like grammatical co des10 or meta labels11 The aim is to increase the coverage with generic elements while preserving the goo d precision induced by the lexical units The transducer output is the class related to the recognized sequence First for each category all IDC 9 CM labels and their UMLS synonyms are gathered The complex expressions are then automatically cleaned and split Some recurrent and non informative parts like sans autre 10 11 Example N for nouns A for adjectives V for verbs Example TOKEN recognizes any token 204 L Kevers and J Medori The second step is stopwords pro cessing They are not removed but replaced with meta labels like TOKEN 12 This substitution improves the coverage to similar expressions where small vocabulary variations o ccur Original form Atteinte d un nerf Injury to nerves Modified pattern Atteinte TOKEN nerf Example of recognized forms Atteinte du de d un des nerf Stemming is also used to broaden the recognition Snowball13 16 provided stems used to build regular expressions14 The patterns are then able to recognize expressions with words beginning with these stems Grammatical variations are therefore covered gender number agreement noun form adjectival form swap etc Note that we also remove accents and decapitalize letters Original form Oed eme de membre The next step aims at allowing more important variations additional words may appear e g adjectives but also other signs comas parentheses etc To do this we allow any token to be inserted between two elements of the transducer This is achieved with a special insert subgraph which contains a TOKEN tag Figure 6 shows the final transducer automatically generated for class 061 Fig 6 Final transducer for class 061 All transducers are gathered into a main transducer which also contains some additional elements to locate negative contexts and to avoid the assignment of classes to expressions such as absence d infection no infection 5 2 Class Inference Once the extraction resource is available it is applied to each new do cument The result of this operation is a list of expressions15 see Fig 7 Each item of the list comes with a class identifier and in the case of a negative context the minus tag is added 12 13 14 15 Note that a TOKEN tag is not allowed on its own or at the begin or at the end of a pattern An implementation of the Porter algorithm http snowball tartarus org The signs and determine the regular expression and specify the anchor at the beginning of the string An expression can of course occur more than once and can also be linked to more than one class In this last case the expression participate to the score of each class Symbolic Classification Methods for PDS Encoding into ICD zona 053 oesophagite moderee aspecifique 947 extremement douloureux 729 infection a mycobacterie 031 gastroscopie Z44 fond de oeil Z16 acide E96 pas de atteinte du nerf 957 anemie normochrome normocytaire 285 zona 053 sequellaires apicales droite tuberculose 137 hyperthyroidie 242 intestin grele Z45 goitre 706 tuberculose V12 goitre 240 205 Fig 7 List of recognized expressions from one text For each expression a weight based on its frequency is computed16 The frequency measure is multiplied by 2 in the case of a multi word expression A final weight for each represented category is then obtained after the addition of all related expression weights This list can be returned as it is or shortened by a thresholding function 14 5 3 Results The evaluation was conducted similarly as for the first metho d section 4 3 The best results are reported on table 2 As in section 4 3 intermediate results can be computed depending on the interest to promote recall or precision Table 2 Evaluation of ELP method Recall R Precision P F measure F1 Nb classes Threshold Best Recall 52 74 20 69 27 37 19 6 No Best F measure 37 97 30 30 29 43 9 8 Yes 6 Combination of Symbolic Methods The two metho ds detailed here can be used on their own However their combination may improve the classification results In order to test that hypothesis we implemented the same mixing metho dology as in 15 This pro cess consists in the merging of the resulting lists of classes by computing their weighted17 union Mix1 or intersection Mix2 before a possible thresholding To be comparable the weights were normalized values between 0 and 1 The list of classes can also be thresholded before their union Mix3 or intersection Mix4 These last two approaches are based on the lists of classes returned by each metho d to maximize F1 see sections 4 3 and 5 3 18 16 17 18 The use of a TF IDF did not bring significant improvement as in previous work 14 An hyperparameter balances the importance of each method merged method1 1 method2 with 0 1 and steps of 0 1 In these cases the use of the hyperparameter has no influence on the results because thresholding comes before balancing 206 L Kevers and J Medori 6 1 Results Combined metho ds results are presented in table 319 This approach clearly improves both recall and f measure Generally the union combination tends to improve the recall while the intersection combination or the thresholding pro cess tend to increase precision In our experiments the best results were always reached with the union of unshortened lists Mix1 For F1 maximization thresholding has to be done afterwards Table 3 Evaluation of symbolic methods combination Recall R Precision P F measure F1 Nb classes Threshold 1 Mix1 Threshold Method1 Method2 Best R 60 21 13 20 20 86 30 5 No Any 37 13 33 12 31 64 8 1 Yes 0 3 0 7 Best F1 Mix2 Threshold Method1 Method2 Best R 38 66 29 28 30 52 9 1 No Any 34 73 34 55 31 50 7 Yes 0 3 0 7 Best F1 Mix3 Threshold Method1 Threshold Method2 Best F1 43 28 20 59 27 90 14 7 Yes N A Mix4 Threshold Method1 Threshold Method2 Best F1 24 07 37 95 29 46 4 4 Yes N A The best recall increases to 60 21 Mix1 compared with 46 13 14 08 for metho d 1 and 52 74 7 47 for method 2 As for metho d 1 and metho d 2 the best recall is reached when returning all classes from the merged lists that is 30 5 classes on average 10 additional categories Among the co des retrieved by the combination metho d 64 21 were returned by both metho ds and the remaining 35 79 by one method or the other From this result we can conclude that the two metho ds complement each other well At 31 64 Mix1 the best f measure improvement is not as clear cut but it nevertheless outperforms metho d 1 28 00 3 64 and method 2 29 43 2 21 This result is mainly due to the increase of precision for both methods 5 78 for method 1 and 2 82 for metho d 2 while the recall remains basically the same for metho d 2 0 84 and improves for method 1 2 61 This greater precision reduces the number of suggested categories to 8 1 previously 8 6 for metho d 1 and 9 8 for metho d 2 without lowering the recall level The Mix2 approach which only keeps the intersection of both metho ds result lists has lower performance The highest recall is limited by the proportion of common co des returned by both metho ds 64 21 and is therefore lower than for original metho ds The maximization of the f measure gives a similar result 31 50 as for Mix1 and an increase with regard to metho d 1 and 2 Again the improvement comes mostly from the precision 7 2 for method 1 and 4 24 for method 2 However this increase is greater than in Mix1 due to the filter effect of the intersection and the number of suggested categories dropped to 7 19 As a consequence of their implementation thresholding before merging we only look at f measure maximization for Mix3 and Mix4 approaches Symbolic Classification Methods for PDS Encoding into ICD 207 For the last two combinations regarding to the original metho ds Mix3 turns out to be less efficient and Mix4 shows only very little improvements Finally a brief look at the rare co des i e those that are used less than 6 times in our test corpus shows that 35 of their o ccurrences 212 out of 603 were covered by the unshortened list resulting of the union of both metho ds 7 Discussion and Future Work The reported results have to be put into perspective First the evaluation was conducted on manual indexing As shown by several studies in particular 17 for medical indexing the maximum inter annotator agreement is often situated at approximately 70 Therefore the evaluation of an automatic classification method compared with manual annotation cannot reach the maximal recall and precision Secondly we saw that most of the information that needs to be enco ded is present in the PDS However an internal study in iSaint Luc showed that 15 to 20 of the codes assigned by the coders cannot be inferred from the PDS The MA metho d relies on the extraction of phrases indicating diagnoses and pro cedures As indicated in section 4 66 of the phrases highlighted by professional coders were extracted by the system This means that 34 of the phrases were not found and therefore no co de could be inferred from them This lowered the maximum recall value accordingly More time should be dedicated to the development of the graphs used in this mo dule Regarding the ELP method the automatic transformation of the basis resource into the extraction transducer can be improved further The phrases used to build the transducers have to be as short as possible to promote recall and as unambiguous as possible to promote precision while some category labels are very complex Rule based automatic enumeration parsing i e splitting turns out to be more difficult to adapt than the thesaurus used in a previous experiment 14 because of the various possible syntactic compositions This could be explained by the fact that thesaurus items are well designated concepts whereas nomenclature items can be viewed as indications to guide the category choice For both methods a better precision may be reached by weighting more efficiently extracted phrases according to the part of the do cument in which they o ccur intro duction conclusion past history and current illness description Finally it will be interesting to conduct an evaluation of the help effectively provided by the tool to the co ders As a conclusion we can outline that our symbolic metho ds prove their usefulness in the context of unannotated corpora where it is difficult to apply machine learning approaches Moreover we think they can successfully be mixed with learning algorithms when a training set is available Acknowledgements This work was partly supported by the CAPADIS and STRATEGO projects CAPADIS is funded by the government of the BrusselsCapital Region Belgium ISRIB STRATEGO WIST 2 project 616442 is funded by the government of Walloon Region Belgium 208 L Kevers and J Medori References 1 Ananiadou S McNaught J Introduction to text mining in biology In Text Mining for Biology and Biomedicine pp User Tailored Document Ralf Klabunde and Alexander Kornrumpf Ruhr Abstract In order to satisfy the informational demands of different users generated texts should be tailored to the respective user types Document planning may benefit from a formal modeling of the participating agents the generation system and the user within the framework of game theory We show how rhetorical structures map to speaker strategies and how a user model may be represented as a domain theory containing different hypotheses for the listener strategies Based on this we present an algorithm which simultaneously performs the tasks of message selection and document structuring Keywords natural language generation document planning game theory 1 Introduction During natural language generation NLG document planning is the first step in transforming non linguistic informational units database entries or concepts into a coherent text Document planning comprises two subtasks the selection of the information to be conveyed from the underlying data or knowledge base and merging the selected informational units to textual units Typically the latter subtask is performed by establishing rhetorical relations 6 between semantic representations for corresponding text spans The result is a tree structure for the entire do cument plan Obviously the variable information requirements of users result in different do cument plans with respect to content and structure We aim at a general game theoretic mo del of do cument planning where different contents for various user types are determined by differing assumption costs and utilities for the players involved The close link between Gricean pragmatics and game theory is well known since 5 but only recently game theoretic mo deling of information exchange became a flourishing area in pragmatics 7 Document planning is driven by pragmatic demands since goo d guidelines for selecting the information to be conveyed are the Gricean maxims ensuring effective communication 10 Since almost every game theoretic model of communication is an attempt to formalize the Gricean ideas we achieve a precise reformulation of some of the Gricean ideas for do cument planning H Loftsson E 210 R Klabunde and A Kornrumpf 2 A Game Theoretic Algorithm for Document Planning Our starting point is a normal form game N A u where N is a finite set of players A We start with the specification of the actions A of the normal form game N A u Speaker actions Despite the well known weaknesses of an analysis based on rhetorical structures we act on the assumption that rhetorical relations reflect the basic actions of S in order to select the messages and to structure the do cument Speaker actions play a ma jor role in the discourse planning algorithm presented in subsection 2 3 This algorithm do es not only realize h but g as well because we map our data to message types From the total amount of messages we receive only those messages will be taken into consideration that are linked by rhetorical relations Listener actions L s action set is closely linked to the preconditions and effects of the rhetorical relations used Listener actions are responsible for the update of the information state in the user model Ls interpretation task is to find an explanation for the information on the basis of his own beliefs Since interpretation is equivalent to finding an explanation why the information conveyed might be true User Tailored Document 211 Definition 2 Candidate hypothesis Be TA a domain theory and P the set of predicate symbols within TA Then H p P p does not occur in a head clause of TA is the set of abducibles for TA and H P H the set of non abducibles h H is called a candidate hypothesis Definition 3 Abduction Be TA a domain theory and H a set of observations to be explained Find a consistent explanation formula F with F F H In other words abduction means given a set of nonabducibles find a formula F which consists solely of abducibles and explains Since there is usually more than one explanation F which explains external criteria are needed to determine the best explanation This can be done by assigning costs to each candidate hypothesis h Abduction then becomes an optimization problem usually called weighted abduction Definition 4 Weighted or cost based abduction Be TA an abduction problem Minimize hF costs h subject to F F H Given a generated do cument plan the action set of L comprises all updates of his information state which explain why the document plan might be true 2 2 The Utility Function The central concept for every game theoretic mo del is the utility function All aspects we have mentioned so far lead to plausible utilities or payoffs for S and L respectively The utility function is not only the core concept the exact fixing of the payoffs is also the most problematic decision for every game While in economic scenarios the payoffs are often asso ciated with monetary values in our approach the payoffs represent the cognitive burden of the agents which is hard to quantify However the exact numerical utilities are not the crucial factor but the relation between the different payoffs Therefore we do not postulate that the exact numerical utilities bear any deep semantics The differences between the payoffs determine the preferences of the players and their best response to the actions of the other agent Let us assume that S has a set of do cument plans at its disposal which express the same data in different manners Furthermore L knows a set of hypotheses AL H which offer possible explanations for the data H may be computed from L s domain theory TL Communication between S and L requires that S chooses a rhetorical relation aS AS and conveys the relation and its arguments to L who in turn chooses a hypothesis aL AL as an interpretation The utility function provides a basis for the agents to make their choices Payoffs for L Cost based abduction actually requires only very little reformulation to fit into the framework of game theory since the notion of costs i e negative utility is already accounted for in that concept For reasons of computational feasibility in our approach L may only adhere to a single hypothesis which does not have to match all of the facts Hence in addition to costs we 212 R Klabunde and A Kornrumpf need a metric of how goo d a hypothesis fits into the observed facts We call the selection of a hypothesis on this basis naive abduction L s utility may be formulated in a similar way Definition 5 Naive abduction Be TA costs a cost based abduction problem Find h H such that h H matchTA h costs h matchTA h costs h Definition 6 Listener utility Be aS m aL h a strategy profile with the interpretation as given above Let TL be the domain theory of L and m the propositions covered by aS The utility of L is defined as follows uL m h p hp q matchTA p q 1 if q p matchTA p q 2 User Tailored Document 213 The utility function for S has to find a balance between the goal of leading L to a hypothesis that accounts for d and minimizing the complexity of the communicated document plan m 2 3 A Multi iteration Game Algorithm for Discourse Planning We are now able to achieve text plans with our game theoretic concepts As already mentioned our task is to compute h g d The algorithm given in Table 1 determines the relevant subset of all possible messages and the structure of the do cument plan simultaneously The algorithm combines only those messages by means of rhetorical relations that form together with a listener s update mechanisms a Nash equilibrium Table 1 A game theoretic document planning algorithm 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Pool all messages derived from d N S L AL H u uS uS R nil repeat AS speaker actions rhet relations which may link pairs of elements in Pool Pool m h pure strategy equilibrium of N A u if m Pool then Rm else E constituents of aS Pool Pool E Pool Pool m end if until R nil return R There are some important differences between this algorithm and the algorithm presented in 8 p 108 Therefore we will explain this algorithm in some detail Line 1 Instead of preselecting messages by means of heuristics the pool is initialized with all messages known to S Lines 214 R Klabunde and A Kornrumpf Lines 9 10 If S is in an equilibrium and realizes an existing rhetorical relation instead of constructing a new one the pool does not change Lines 3 Summary and Outlook To sum up we provided generic formal notations for the interlocutors their tasks action sets and the utility functions All definitions are grounded in wellknown theoretical frameworks and game theory allowed the formulation of the interplay of the relevant representations and processes rooted in these theories In addition to developing the formal mo del of do cument planning mentioned above we applied that model to the generation of do cument plans for different users of performance data The generated texts explain the output of a heart rate monitor HRM worn by a runner during his training for amateur atheletes and beginners For reasons of space we cannot go into the details 4 will provide a comprehensive overview References 1 Console L Dupre D Torasso P On the relationship between abduction and deduction Journal of Logic and Computation 1 5 Anaphora Resolution with Real Preprocessing Manfred Klenner Don Tuggener Angela Fahrni and Rico Sennrich Institute of Computational Linguistics Binzmuehlestrasse 14 CH 8050 Zurich klenner tuggener sennrich cl uzh ch angela fahrni swissonline ch Abstract In this paper we focus on anaphora resolution for German a highly inflected language which also allows for closed form compounds i e compounds without spaces Especially we describe a system that only uses real preprocessing components e g a dependency parser a two level morphological analyser etc We trace the performance drop occurring under these conditions back to underspecification and ambiguity at the morphological level A demanding subtask of anaphora resolution are the so called bridging anaphora a special variant of nominal anaphora where the heads of the coreferent noun phrases do not match We experiment with two different resources in order to find out how to cope best with this problem Keywords Anaphora Resolution Coreference Resolution Bridging Anaphora 1 Introduction Anaphora resolution is a resource intensive task In order to find out whether a noun phrase is an antecedent of another subsequent noun phrase the anaphor information from various preprocessing components are to be combined A morphological analyser is needed for number person and gender determination a tagger is required to deliver part of speech tags a parser to find grammatical functions and the embedding depth of noun phrases and finally semantic information is necessary to tackle the most difficult task namely bridging anaphora Bridging anaphora are nominal anaphora where the heads of the noun phrases do not match Take the following sequence Iceland is an interesting place to visit The land of ice and fire is famous for Here Iceland and land of ice and fire are coreferent In order to establish this coreference link the least a system has to know is that Iceland is a land Lexical resources such a WordNet or its German counterpart GermaNet do comprise this kind of information although not exhaustively The proper determination of coreference depends on the quality of these resources and the preprocessing units using them Thus a poor performance of a system for anaphora resolution can have multiple causes and often it is hard to tell which component or resource is to blame Therefore it is tempting to reduce this kind of noise to its minimum and to create idealised H Loftsson E 216 M Klenner et al conditions under which one can easily fix failures Instead of using a parser one could use a treebank and if the treebank also has morphological annotations why not use it as well This way one ends up with a system that expects perfect prepro cessing and whose empirical results no longer indicate its usefulness for real world applications This kind of simplifications are often made by current approaches to anaphora resolution One of the most unrealistic and simplifying idealisations is to use true mentions instead of all noun phrases True mentions are those markables that are according to a coreference gold standard part of a coreference chain The ma jority of noun phrases in a text however are not in a coreference set The determination whether a NP is anaphoric i e a true mention or not is a demanding problem the so called anaphoricity classification problem There are a few systems that incorporate anaphoricity classification the ma jority of systems leaves this as an implicit task to the anaphora resolution component Separate anaphoricity classification has not proven to be more successful than its implicit counterpart Anaphoricity determination of markables is a non trival task and cutting it away makes a system an artificial one We are not saying that experiments under idealised conditions are totally in vain We are just arguing that it doesn t help a lot to tune a system on the basis of gold standard information if one intends to switch to a real world system One never foresees the amount of noise that is introduced by real components In this article we intro duce a system for anaphora resolution for German that uses only real preprocessing components Gertwol a morphological analyser Pro3Gres a dependency parser GermaNet a German wordnet and Wortschatz Leipzig a lexical resource generated by statistical means As most approaches we cast anaphora resolution as pairwise classification we use TiMBL Daelemans et al 2004 as a machine learning tool Our system is filter based that is candidate pairs that do not fulfil linguistic filter criteria are sorted out We give empirical results and discuss the reason for the drop of performance from an idealised setting to a real world setting Also different filters have been investigated to determine the usefulness of lexical resources for the task of resolving bridging anaphora for German 2 Filter Based Pairwise Classification Approaches to pairwise classification of anaphora resolution differ among others in their pair generation mo dule Some systems generate every pair independent of the distance between two markables the noun phrases that might stand in a coreference relation Under a linguistic point of view this only makes sense for nominal anaphora A pronoun at the end of a text could hardly refer back to a noun phrase at the beginning of a text without further intervening chain links Moreover the problem with such an approach is the vast amount of negative instances it pro Anaphora Resolution with Real Preprocessing 217 antecedent of an anaphor is reached We use a fixed window of three sentences for pronominal anaphora and bridging anaphora while for named entities there is no restriction Each pair additionally must pass all applicable filters Filters depend on the part of speech of the antecedent anaphor candidate For instance personal pronouns must agree in person number and gender with its antecedent head whether this is a pronoun or a noun After morphological analysis we often have underspecified information at hand only For instance German ihr can be plural without gender restriction their or singular feminine her If no information is available e g for unknown nouns we take a disjunction of all allowed values Possessive pronouns only unify in person and gender e g Sie liebt ihre 218 M Klenner et al Here is the list of our features Salience of a grammatical function is estimated on the basis of the training set in the following way the number of cases a grammatical function realises a true mention divided by the total number of true mentions it is the conditional probability of a grammatical function given an anaphor The function sub ject is the most salient function followed by direct ob ject It has been noticed that the local perspective of pairwise classification yields problems Take the following markable chain Hillary Clinton she Angela Merkel she is compatible with Hillary Clinton Angela Merkel is compatible with she but Merkel and Clinton are incompatible Since transitivity is outside the scope of a pairwise classifier it might well classify both compatible pairs as positive without noticing that this leads to an implicit contradiction setting Clinton and Merkel to be coreferent In a former paper we have argued that coreference clustering based on the so called Balas order coupled with intensional constraints to ensure consistency of coreference sets performs best in order to remedy these problems Klenner and Ailloud 2009 In this paper we concentrate on the performance drop of the baseline system under the conditions of real preprocessing components We do not discuss problems of coreference clustering 3 Real Preprocessing Tools Fortunately good NLP tools are available for a number of languages For German a two level morphology program called Gertwol a fast and well performing partof speech tagger the TreeTagger Schmid 1994 and a fast and state of the art dependency parser the Pro3Gres parser are the components of our systems Additionally we have developed a named entity recognition based on pattern matching and Wikipedia entries It is evident that the quality of prepro cessing determines the quality of the rest namely the decision made by linguistic filters and the classification carried out by the machine learning classifier 3 1 Morphology with Gertwol We use Gertwol a commercial system based on two level morphology Gertwol is fast and also carries out noun decomposition which is rather useful since in Anaphora Resolution with Real Preprocessing 219 German compounds are realised as single wordforms closed form compounds e g Computerexperte computer expert Compounds which are quite frequent in German might become very complex but often the head of the compound is sufficient to semantically classify the whole compound via GermaNet For instance Netzwerkcomputerexperte expert for network computers is an expert and thus is animate The other important task of Gertwol is to determine number person and gender information of a word Unfortunately ambiguity rate is high since e g some personal pronouns are highly ambiguous For instance the German pronoun sie she might be singular feminine or plural without gender restriction The pronoun ich does not impose any gender restrictions and moreover often refers in reported speech to a speaker which is referred to in the text by a noun phrase in third person 3 2 Named Entity Recognition Our Named Entity Recognition NER is pattern based but also makes use of extensive resources We have a large list of international first names 53 000 where the gender of each name is given From Wikipedia we have extracted all multiword article names e g Berliner Sparkasse a credit institute from Berlin and if available their categories e g Treptower Park has Parkanlage in Berlin Bezirk Treptow Pro3GresDe is a hybrid dependency parser for German that is based on the English Pro3Gres parser cf Schneider 2008 It combines a hand written grammar and a statistical disambiguation module trained on part of the 1 For a full discussion of Pro3GresDe see Sennrich et al 2009 220 M Klenner et al The parser give access to the following features e g grammatical function depth of embedding subclause information 4 Empirical Evaluation We have carried out two series of experiments The first one is concerned with the costs of real prepro cessing compared to the use of gold standard information e g tree bank instead of parser We incrementally fix the reasons for the performance drop The second experiments are devoted to bridging anaphora and the impact of two main lexical resources for German GermaNet a German WordNet and Wortschatz Leipzig a statistically derived thesaurus 4 1 The Price of Real Preprocessing From the two pro cessing steps of coreference resolution pairwise classification and subsequent clustering only the first is of interest here It is the baseline performance drop that we are interested in This degradation occurs before clustering and it cannot be compensated by clustering operations The performance drop is measured in terms of save gold standard versus noisy real world components morphological functional and syntactic information The gold standard information stems from the Table 1 Performance Drop gold standard info morphological functional subclause real 61 49 59 01 58 20 58 01 68 55 69 78 69 12 70 89 55 73 51 12 50 56 49 01 F measure Precision Recall Anaphora Resolution with Real Preprocessing 221 anaphoric relations Our experiments have however shown that it is better to restrict the search than to generate each and every pair performance drops to a great extent the larger the window Finally the local perspective of pairwise classification do es not allow to take boundness restrictions into account For instance we know that third person personal pronouns and possessive pronouns as well are anaphoric i e must be bound there are only very few exceptions There is however no way to tell the learner this kind of prior knowledge Fortunately this shortcoming can be compensated at the subsequent clustering step where these markables can be forced to be bound to the best available anaphor Let us see how the performance is like if we take gold standard information especially perfect morphology perfect syntax and perfect functional information The f measure value is 61 49 about 3 5 above the real world setting Precision drops slightly 68 55 but recall significantly increases to 55 73 The reason for performance increase is the increase in recall How can we explain this Let us first see how the different gold standard resources contribute to this increase If we turn grammatical functions from parser given to gold standard given the increase on the baseline is small f measure raises from 58 01 to 58 20 Our dependency parser is goo d enough to almost perfectly replace gold standard information The same is true with syntactic information concerning the depth of embedding and subclause detection Here as well only a small increase occurs the f measure is 59 01 But if we add perfect morphology an increase of 3 5 pushes the results to the final 61 49 The reason for the increase in recall and f measure is our filter based metho d Only those pairs are generated that pass the filter If the morphology is noisy pairs erroneously might pass the filter and others pairs erroneously do not pass the filter The first one spoils precision the second hampers recall We were quite surprised that the replacement of syntactic and functional information by real components was not the problem Morphology is responsible for the drop 4 2 Filtering for Resolution of Bridging Anaphora In this section we show that using different morpho syntactic distance based and semantic filters derived from real resources the task of resolving bridging anaphora in a pairwise manner is far from being accomplished with satisfying results Filtering aims at reducing the number of negative instances but this has been hardly investigated regarding the ceiling or performance upper bound it pro duces The upper bound values given in Tab 2 indicate how many false negatives a filter produces i e how many real positives it filters out 2 We have further investigated these upper bounds see Tab 3 and found that they are either very low when using very restrictive strict filters or that the filters do not eliminate enough negative instances when used in a relaxed lax mo de Throughout our experiments we use the CEAF scorer presented in Luo 2005 2 We get a slight reduction in precision when using no filters because of a string matching issue when filtering out string matching multiword items 222 M Klenner et al Table 2 Upper Bound of the Morpho syntactic and Distance Filters Filter no filter diff regens anaphor definite number agreement all morphosynt filters dist limit 3 all Recall Precision F measure Pairs Reduction Positives 100 00 98 60 99 21 4869822 4924 0 10 99 87 98 53 99 11 4864018 0 10 4915 0 10 100 00 98 60 99 21 4401565 9 62 4913 0 11 93 91 94 64 94 00 3480538 28 53 4622 0 13 93 78 94 57 93 90 3110842 36 12 4602 0 15 68 36 80 54 72 46 818588 83 19 1697 0 21 63 31 76 82 67 81 520735 89 30 1579 0 30 We can see from Tab 2 that the morpho syntactic filters which perform well in resolving pronominal anaphora give good upper bounds but do not reduce the amount of negative instances sufficiently Subclause exclusion here diff regens determined through verb dependency which establishes a kind of c command in our dependency framework is not really that relevant for resolving bridging anaphora as antecedents are often not in the same sentence Perhaps surprising is the fact that 9 positive instances get deleted by this filter Such errors o ccur with real prepro cessing as parsing is not perfect A simple definiteness filter anaphor definite that checks if a candidate anaphor has an indefinite determiner German ein i e a or an or its morphological variants reduces the training instances by almost 10 without reducing the upper bound Number agreement filtering shows that there are 302 positive instances that do not agree in number Still this filter cuts down the number of instances by almost 30 The often used distance filter with a sentence window of 3 pro duces an acceptable upper bound and reduces the instance size by 83 91 This is still not enough however looking at the percentage of positives 0 21 For filtering based on semantic information we use Wortschatz Leipzig and GermaNet We apply head extraction and decomposition to composite nouns based on Gertwol morphological analysis in the case they are not found directly in the lexical resources For 54 593 83 1 of the 65 703 markables synonyms can be found in Wortschatz Leipzig WSL for 60 985 92 8 we can make a often ambiguous GermaNet GN classification The synonymy filter WSL checks if a mention is in the synonymy list of the other one or if they share a common synonym The GN filter checks if both mentions are in the same GN class if the class is ambiguous we check all and let the pair be generated if we find a match We investigate the upper bounds of the semantic filters in two ways see Tab 3 If for a mention no information has been derived we let it pass the filter lax or we delete it strict There are huge differences between the upper bounds and the percentages of positive instances between lax and strict filtering This suggests that although for quite a large number of markables semantic information can be retrieved it do es not allow us to use it for hard filtering without a significant drop in the upper bound ceiling This gets obvious when we combine the strict semantic Anaphora Resolution with Real Preprocessing Table 3 Upper Bound of the Semantic Filters Filter WSL strict WSL lax GN strict GN lax all filt lax all filt strict Recall Precision F measure Pairs 37 00 55 71 42 34 112921 72 94 83 35 76 38 1679610 57 98 73 52 63 21 1030441 81 36 89 03 84 04 1694157 36 86 53 97 41 93 97593 15 1 28 41 18 32 8953 Reduction 97 86 65 51 78 84 65 21 98 00 99 81 Positives 1590 1 41 3300 0 20 3283 0 32 4385 0 26 1013 1 04 441 4 93 223 constraints with the morpho syntactic and distance filters in all filt strict It is the only filter that generates a fairly reasonable percentage of positives but drops the upper bound immensely As one would expect a synonymy based filter WSL is more strict than a semantic class based constraint GN The trade off between the percentage of positives and the reduction of the upper bound holds equally for both of the semantic filters the more positives the lower the upper bound Filtering is a key element to successful resolution of bridging anaphora However our experiments show that filters based on morphological information and syntactical constraints do not sufficiently reduce the amount of negative instances in order to train a reasonable classifier The distance constraint is a goo d filter to tune the trade off between recall and precision although the distance values might be highly dependent on the test domain and genre On the other hand using the lexical resources for filtering based on semantic constraints heavily suffers from sparseness leading to a considerably lower upper bound These findings seem to suggest that pairwise classification is not the best technique for resolving bridging anaphora given a real anaphora resolution scenario We are currently carrying out experiments with an incremental approach where pairwise classification is done only between the last mentions of already established coreference sets and the anaphor candidate We hope to show that by recasting the problem of coreference resolution as an incremental clustering problem the issue of resolving bridging anaphora becomes less important because true mentions linked through a bridging relation can be merged by a pronoun between them 5 Related Work The work of Soon et al 2001 is a prototypical and often reimplemented machine learning approach in the paradigm of pair wise classification Our system has a similar baseline architecture and our features do overlap to a great extent Work on coreference resolution for German is rare most of it uses the coreference annotated treebank 224 M Klenner et al took this finding seriously and have tried to use Wikipedia to complement GermaNet we map Wikipedia multiword items via Wikipedia categories to GermaNet classes We also have experimented with a statistically derived lexical resource the Wortschatz Leipzig Hinrichs et al 2005 intro duce anaphora resolution only pronouns on the basis of a former version of the 6 Conclusion In this paper we have discussed the intricacies of anaphora resolution based on real prepro cessing components Our system makes extensive use of non statistical resources rule based dependency parsing a German wordnet Wikipedia twolevel morphology but at the same time is based on a state of the art machine learning approach We have traced the performance drop that o ccurs under this conditions back to its origin It is the morphology of German that yields the problem Although German counts as a highly inflected language underspecification and ambiguity prevail and are the main cause of degrading performance We have also evaluated the usefulness of two resources GermaNet and Wortschatz Leipzig Our experiments suggest that filtering for pairwise classification is not a successful technique if bridging anaphora are concerned Other metho ds for finding proper antecedent anaphor candidates are needed here Our initial experiments with an incremental model are promising our future work will proceed in this direction Acknowledgements Our project is funded by the Swiss National Science Foundation grant 105211 118108 References Daelemans et al 2004 Daelemans W Zavrel J van der Sloot K van den Bosch A TiMBL Tilburg Memory Based Learner 2004 Hamp and Feldweg 1997 Hamp B Feldweg H GermaNet a Lexical Semantic Net for German In Proc of ACL Workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications 1997 Anaphora Resolution with Real Preprocessing 225 Hinrichs et al 2005 Hinrichs E Filippova K Wunsch H A Data driven Approach to Pronominal Anaphora Resolution in German In Proc of RANLP Borovets Bulgaria 2005 Klenner and Ailloud 2008 Klenner M Ailloud E Enhancing Coreference Clustering In Johansson C ed Proc of the Second Workshop on Anaphora Resolution WAR II Bergen Norway NEALT Proceedings Series vol 2 2008 Klenner and Ailloud 2009 Klenner M Ailloud E Optimization in Coreference Resolution Is Not Needed A Nearly Optimal Zero One ILP Algorithm with Intensional Constraints In Proc of the EACL 2009 Luo 2005 Luo X On coreference resolution performance metrics In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing Association for Computational Linguistics Morristown NJ USA pp Automatic Construction of a Morphological Dictionary of Multi Word Units Cvetana Krstev1 Ranka 2 Faculty of Philology University of Belgrade Faculty of Mining and Geology University of Belgrade 3 Faculty of Mathematics University of Belgrade 1 Abstract The development of a comprehensive morphological dictionary of multi word units for Serbian is a very demanding task due to the complexity of Serbian morphology Manual production of such a dictionary proved to be extremely time consuming In this paper we present a procedure that automatically produces dictionary lemmas for a given list of multi word units To accomplish this task the procedure relies on data in e dictionaries of Serbian simple words which are already well developed We also offer an evaluation of the proposed procedure on several different sets of data Finally we discuss some implementation issues and present how the same procedure is used for other languages Keywords electronic dictionary Serbian morphology inflection multi word units noun phrases query expansion 1 Introduction We have been developing morphological electronic dictionaries of Serbian for natural language pro cessing for many years now Our e dictionaries follow the metho dology and format known as DELAS DELAF which is presented for French in 1 1 Serbian e dictionaries of simple forms have reached a considerable size they have a total of 122 000 entries 2 Although we are continually enlarging our e dictionaries of simple words we have taken a further step towards tackling the problem of multi word units MWUs In an intro duction in 3 supported by comprehensive references Savary states that MWUs are hard to define controversial linguistic ob jects Nevertheless it seems that many authors agree that MWUs a are composed of two or more graphical words b show some degree of morphological syntactic distributional or semantic non compositionality and c have unique and constant references A deeper discussion of the nature of MWUs is beyond the scope of our paper When dealing with MWUs we are taking a pragmatic approach and consider MWUs to be sequences of graphical units that for some reason have to be described and pro cessed as a unit 1 A comprehensive bibliography on e dictionaries for other languages developed using the same methodology is given at http www igm univ mlv fr unitex H Loftsson E Automatic Construction of a Morphological Dictionary of MWUs 227 For some pro ductive classes of MWUs like different types of numerals and named entities time and duration measures and currencies we have developed finite state transducers FSTs that rely on morphological e dictionaries of simple words to model these MWUs correctly 4 When applied to a text in automatic text analysis these FSTs asso ciate recognized MWUs with lemmas as well as with appropriate grammatical categories Both the asso ciated lemmas and grammatical categories are in the same format as the one provided by dictionaries of simple words Some other MWUs that are idiosyncratic in nature ask for a different description Namely these MWUs cannot be described by FSTs they have to be listed in an e dictionary in a similar way and for similar reasons as simple words That means that some regular form or a lemma has to be listed in a DELAC dictionary together with some additional information that would enable the generation of all inflected forms that is entries for a DELACF dictionary Several questions arise here a what should this regular form listed in a dictionary of lemmas be b what additional information is necessary and c how the generation of all forms is to be performed When trying to find the answers to these questions one has to keep in mind that the graphical units composing a MWU are themselves simple words that have their own inflectional behavior However simple words that represent components of a MWU do not inflect freely they have to conform to some combining rules In the case of Serbian these rules can be rather complex since simple words constituting a MWU like adjectives and nouns can inflect in several grammatical categories For instance civilni vojni rok civil military service is a multi word noun that inherits its gender from the constituent noun rok masculine in this case and it inflects for case but it does not inflect for number although the simple word rok do es The adjectives civilni and vojni agree with the noun rok in number case gender and animacy but the comparative and indefinite forms of these adjectives are not used in this MWU It is clear from this example that the combining rules for Serbian MWUs are by no means trivial After considering several options we concluded that Multiflex a finite state tool for MWUs developed by A Savary 5 suits our needs best This tool supports finite state transducers that can mo del a number of combining conditions As for the inflection of constituent simple words it relies completely on FSTs for the inflection of simple words This way the generation of all inflected forms with Multiflex is performed with two types of FSTs for inflection of simple words and for inflection of MWUs To that end a lemma for each simple word constituent that inflects in a MWU has to be provided as well as values of all grammatical categories of forms appearing in the MWU lemma its regular or dictionary form For the given example the entry in DELAC dictionary is civilni civilni A2 adms1g vojni vojni A2 adms1g rok rok N81u ms1q nc axaxn1 The information given in this entry allows automatic pro duction of all 26 inflected forms for the DELACF dictionary as for example the entry for the singular dative case civilnom vojnom roku civilni vojni rok N ms3q 228 C Krstev et al Although the Multiflex approach is theoretically well founded easy to understand and apply and it successfully solves many problems of MWU inflection for Serbian it is obvious from the given example that the pro duction of a single DELAC entry is a very tedious task As a matter of fact we initially produced only 30 DELAC entries from scratch Realizing that additional information within DELAC entries everything between the parenthesis in most cases already exists in dictionaries of simple words DELA we decided to develop a mo dule for our lexical resources management tool LeXimir an enhancement of its predecessor WS4LR 6 that would help in obtaining this information However due to homography of forms and homonymy of simple word entries the developer of a DELAC entry still needs to choose between several options offered by a dictionary look up provided by this tool For the above example the choice had to be made three times for the information about forms like adms1g for the first component and once for the simple word entry because there are two entries for rok in the Serbian DELAS one for service and one for ro ck music Following this approach only 3195 DELAC entries were pro duced in the past three years which we found very ineffective In a comprehensive analysis of several tools for MWU inflection description 3 the author mentions only one system FASTR that supports automated MWU lexicon creation 7 Since this system is based on an approach very different from DELA metho dology we developed our own pro cedure for automatic construction of DELAC entries from a given list of MWUs For an item such as civilni vojni rok this pro cedure produces the aforementioned MWU lemma However the pro duced list of lemmas has to be manually checked and some decisions still have to be made even when the pro cedure offers only one candidate MWU lemma Thus we have to stress that our dictionaries remain handcrafted resources as per categorization in 8 in terms of the information they offer and the metho dology used to produce them without statistical engineering 2 Analysis of Initial Data We based the development of the automated pro cedure for DELAC construction on the initial dictionary of MWUs which contained 3195 lemmas covering different part of speech as presented in Table 1 nouns adjectives adverbs conjunctions prepositions interjections As only MWU nouns and adjectives inflect they have an inflectional class co de assigned to them in DELAC Each inflectional class is asso ciated with one inflectional transducer as described in 9 that controls the pro duction of all inflected forms The forms lemmas ratio in Table 1 shows that a direct pro duction of DELACF dictionary is out of the question for Serbian though it may seem possible for some other languages For instance for English this ratio for MWU nouns is 1 90 whereas for French it is 1 29 The calculation is performed on the basis of dictionaries described in 10 and 11 that are part of the standard distribution of Unitex 12 a corpus processing system based on the finite state technology Some inflectional transducers are frequently needed like nc axn which is used for MWUs consisting of an adjective followed by a noun in our case 1208 Automatic Construction of a Morphological Dictionary of MWUs Table 1 Initial content of the Serbian morphological dictionary of MWUs POS Nouns Adjectives Other lemmas 2571 207 415 forms 49792 21045 forms lemmas 19 4 101 7 Inflectional classes super classes 67 16 28 9 229 MWUs while some apply to a single MWU In order to design our automated procedure we grouped all inflectional transducers into equivalence classes or super classes a super class consists of all transducers that use the same form of a MWU lemma that is the same information for the pro duction of inflectional forms This is also reflected in the convention we used for naming the inflectional transducers A stands for an adjective constituent N stands for a noun constituent X stands for a constituent that do es not inflect including a separator while some additional digits and letters may be added to differentiate transducers This is illustrated in Table 2 by six inflectional transducers all belonging to one super class nxn and used for the inflection of MWUs consisting of a noun followed by another noun where both nouns inflect and must agree in basic grammatical categories It should be noted that MWUs sharing the same inflectional class or superclass do not necessarily have the same syntactic structure and vice versa For instance film u boji color movie and bolest ludih krava mad cows disease share the same class nc n4x although the first MWU consists of a noun followed by a prepositional phrase and the second of a noun followed by a phrase in the genitive However from the inflectional point of view they both behave in the same way only the first component inflects whereas the second and the third do not On the other hand predsednik dr zave president of the state and advokat odbrane attorney of the defense both consist of a noun followed by a component in the genitive case but their inflectional behavior is different in the first MWU the second component can change in number whereas in the second MWU it does not inflect For that reason they belong to two different classes and superclasses nc nxn4 and nc n4x respectively In order to formulate our strategy for the production of MWU lemmas we analyzed the data in the initial dictionary looking for useful information We identified the additional information assigned to components of MWUs belonging to a particular inflectional class and vice versa we identified inflectional classes asso ciated with the same additional information For instance the class nc nxn2m explained in Table 2 is asso ciated with only one combination of grammatical categories characterized by the fact that the components differ in gender On the other hand some combinations of grammatical categories like ms1q ms1q two masculine inanimate nouns in the nominative singular were associated in DELAC with three classes nxn nxn2 and nxn3 see Table 2 the first one being the most frequent 230 C Krstev et al Table 2 The inflectional classes for MWUs belonging to the super class nxn Infl class nc nxn nc nxnf nc nxn3 nc nxn2 nc nxn4 Example Translation Explanation lekar aku ser obstetrician Both components inflect and agree in case and number kit ubica killer whale The gender of the second component changes in plural kamenfoundation The separating hyphen can be omitted temeljac stone Kongo CongoNeither of the components inflects for Brazavil Brazzaville number predsednik president The first component inflects for case dr zave of the state and number the second may or may not inflect for number Kne zevina Principality The second component does not necesMonako of Monaco sarily inflect nc nxn2m On the basis of this information we got the general idea which combinations of atomic data can we expect to be extracted from dictionaries of simple words and how they combine with inflectional classes However the obtained information was incomplete For components that do not inflect for a certain MWU there was no additional information in the MWU lemma For such components this information was subsequently searched in dictionaries of simple words It was thus possible to establish that in naliv pero ink pen the first component was not in the dictionary of simple words in bruto plata gross income the first component was an adjective that do es not inflect in mini suknja mini skirt the first component was a prefix All three examples belong to the same super class 2xn There are still cases when no additional information whether extracted from MWU lemmas themselves or subsequently from electronic dictionaries can help in deciding in favor of one inflectional class over other possibilities For instance this is the case for krem karamel caramel cream and kamen temeljac foundation stone both components in these MWUs are masculine inanimate nouns in the nominative singular but in the first one karamel is the head and the first component do es not inflect super class 2xn while in the second example kamen is the head and both components inflect super class nxn 3 Description of the Strategy The purpose of the analysis performed in the first step was not to pro duce a strategy for construction of MWU lemmas automatically it was rather used as a reference during the manual pro duction of a strategy in the form of XML documents The schema of these do cuments is presented in Figure 1 Our strategy consists of two XML do cuments one for MWU nouns and the other for MWU adjectives Each XML do cument consists of a sequence of rules that are grouped by the number of components in MWUs Each rule states the conditions that a MWU and its components have to satisfy in order to be placed in a particular inflectional class and to have a particular MWU lemma assigned to them In order to make our strategy easier to produce and maintain conditions for some Automatic Construction of a Morphological Dictionary of MWUs 231 Fig 1 The XML Schema for a strategy document rules were grouped into general and specific conditions where specific conditions are simply additions to general conditions One rule will illustrate this Rule ID 2 CFLX NC_AXN3 CflxGroup AXN RuleGenCond Word ID 1 POS A Flex true Case 1 Anim a Gen g Word ID 2 POS N Flex true Case 1 Anim a Gen g RuleGenCond RuleSpecCond ID 1 Example Ajfelova kula Word ID 1 Num s Cond PRE Word ID 2 Num s RuleSpecCond RuleSpecCond ID 2 Example poljski radovi Word ID 1 Case 1 Num p Word ID 2 Case 1 Num p RuleSpecCond RuleSpecCond ID 3 Example elektronsko poslovanje Word ID 1 Case 1 Num s Word ID 2 Case 1 Num s SinSem VN Coll HumColl RuleSpecCond Rule This rule classifies a MWU in the inflectional class nc axn3 asso ciated with adjective noun MWUs that do not inflect for number super class axn if the following conditions are satisfied a General conditions the first component is an adjective the second is a noun both components are listed in the nominative case they agree in gender and animacy whichever they are b Additional specific conditions 232 C Krstev et al Table 3 Overview of rules for nouns and adjectives with 2 to 7 components Components 2 3 4 5 6 7 Total Rules for nouns general special 26 25 14 6 4 1 76 59 85 50 54 29 9 286 Rules for adjectives general special 5 8 5 2 9 8 5 2 20 24 4 Analysis and Evaluation of the Strategy We have performed the analysis and evaluation of our strategy in two steps In the first step we applied the strategy to the same data that we used to pro duce Automatic Construction of a Morphological Dictionary of MWUs 233 it that is our initial DELAC dictionary In the second step we applied it to several lists of MWUs that we have collected from various sources After applying our strategy to the initial DELAC dictionary containing 2571 nouns and 207 adjectives the obtained results were manually validated When assessing the success of the strategy we adhered to the following a If for a given MWU one of the rules pro duced the correct lemma and assigned the correct inflectional class we considered the strategy as successful b If for a given MWU none of the rules satisfied a but one of the rules pro duced the correct lemma although the assigned inflectional class was not correct whereas the assigned super class was correct we considered the strategy as partially successful c If for a given MWU none of the rules satisfied a or b we considered that the strategy failed for that MWU d For each lemma where either a or b applied we also determined the rank of the accepted solution the higher on the list of offered solutions the more favorable the accepted solution Table 4 gives percentages of successfully pro duced noun and adjectives entries case a partially successful results case b and failures case c and also indicates the rank of these results rank 0 means no solution offered A failure means that either no solution was offered or none of the solutions was classified as a or b In either case the reason was that the strategy failed to cover a particular MWU structure in 52 cases or 20 of all failures or a MWU component that inflects was not in the dictionary of simple words in 255 cases or 80 of all failures The latter case o ccurred frequently due to MWUs representing proper names where components are often not words in Serbian e g Dar es Salam or due to the fact that some words are used only in MWUs like domali in domali prst next to little ring finger and are thus not listed in dictionaries of simple words The lowest rank of a successful result was 10 for nouns and 3 for adjectives In the second step we applied our strategy for nouns to several different lists of MWUs We have not applied our strategy for adjectives in this step simply because we have not collected enough new adjectives Our list of new MWU nouns came from several different sources the official list of MWU names of settlements in Serbia 236 MWUs extracted from a log file of a Serbian professional journal that deals with economic issues 162 from Verne s novel Around Table 4 Evaluation of the strategy applied to the initial DELAC dictionary Rank 0 1 2 3 4 10 Total Nouns c 5 50 6 28 0 08 Adjectives c 2 93 37 56 36 59 2 93 15 61 4 39 a 63 74 6 56 2 22 0 90 b Total 5 50 82 71 7 26 3 43 1 10 a b Total 2 93 53 17 40 98 2 93 0 00 100 00 12 69 0 62 1 21 0 20 73 42 14 72 11 87 100 00 77 07 20 00 2 93 234 C Krstev et al the world in 80 days 114 from the explanatory dictionary of Serbian under the letter R 604 As the analysis in the first step indicated that MWU proper names pro duce in general worse results we decided to separate these lists in two groups After removing those already in DELAC we got a list of MWU toponyms 206 and a list of MWU common nouns 784 Table 5 shows that in the case of common nouns for only 3 62 items on the list 28 items no satisfactory solution a or b was offered For toponyms this percent is much higher 38 61 accounting for 78 items on the list In the case of toponyms all cases of failure are due to the fact that components of toponyms were not simple words in Serbian e g Feja in Kriva Feja The lowest rank of a successful result was 6 for common nouns and 2 for toponyms Table 5 Evaluation of the strategy for nouns applied on a list of MWU common nouns and compound toponyms Rank 0 1 2 3 4 6 Total Common nouns b c Total 10 08 0 26 0 13 1 68 1 94 1 68 92 12 4 78 1 42 0 00 100 00 Toponyms c 24 75 13 86 a 80 23 4 39 1 29 a 48 02 8 91 1 00 b Total 24 75 65 35 8 91 0 00 1 00 3 47 85 92 10 47 3 62 57 92 3 47 38 61 100 00 Row 2 in Table 6 shows that less rules were used for common nouns in the second step than for nouns in the initial DELAC in the first step This is probably due to the fact that while building our initial DELAC and the inflectional classes we tried to find various structurally different examples Row 3 shows that for all subsets except adjectives the number of rules that were not used is greater than the number of used rules namely many rules were added to the strategy upon analogy with other rules As the number of rules does not affect the effectiveness of the pro cedure except the pro cessing time to a small degree we believe that unused rules should not be removed because they may prove useful in the future Indeed the subset common nouns used seven rules that were not used for the initial DELAC nouns while toponyms used one new rule The rules used most frequently for nouns row 5 are rules pertaining to adjective noun MWUs which are also the most frequent in the dictionary The rules that failed each time they were used row 7 are obvious candidates for deletion from the strategy Rules that were more unsuccessful than successful row 8 should probably also be reconsidered as for instance by reinforcing the conditions if possible The average number of solutions per item is rather low row 9 ranging from 1 4 for the common nouns to 3 7 for toponyms In some cases much larger sets of solutions were offered The leader is Velika Plana a small town in Serbia with 52 solutions offered Such a high number of solutions o ccurred due to the homography of both components However even in this case the correct solution Automatic Construction of a Morphological Dictionary of MWUs 235 Table 6 Strategy rules performance data for subsets N initial DELAC nouns A initial DELAC adjectives CN additional common nouns T additional toponyms N 1 2 3 4 5 6 7 8 9 10 11 12 13 A CN T Items 2571 207 784 206 Rules applied 85 19 56 33 Rules not applied 201 5 230 253 Applications of rules 4083 434 1060 769 Most frequently used rule nc axn ac 2x2 nc axn nc axn3 number of times applied 1499 108 589 144 Absolutely successful rules 26 9 19 1 Absolutely unsuccessful rules 9 1 16 26 Rules more unsuccessful than successful 36 6 23 29 Solutions item 1 6 2 1 1 4 3 7 Maximum solutions per item 38 6 19 52 80 64 54 77 91 85 68 43 Success 1st solution Success 2nd solution 7 58 42 21 4 73 11 84 Success 3rd solution 3 62 3 02 1 44 0 0 had a high rank namely the second solution for Velika Plana was correct It may seem reasonable to exclude some dictionaries of simple words from this procedure for instance dictionaries of personal names in order to alleviate similar problems However that might not be such a good idea these very dictionaries successfully pro cessed many items among them three very specific names of small towns in Serbia named after famous Serbian po ets and politicians Aleksa 5 Implementation and Usefulness Our pro cedure for automatic pro duction of DELAC entries is a mo dule of the LeXimir tool 6 which is written in C and operates on the NET platform It supports development maintenance and exploitation of various resources edictionaries wordnets and aligned texts A user of this tool need not use all of these resources or even possess them but those that exist are visible in all mo dules and can be exploited in a useful way The e dictionaries of simple and MWU words that we develop using LeXimir are used primarily within the Unitex system As Unitex is open source software distributed under the terms of LGPL we easily incorporated its mo dules in LeXimir for many tasks that involve manipulation of e dictionaries including dictionary look up used in the mo dule for automated pro duction of DELAC 236 C Krstev et al entries To manipulate the strategy in the form of XML do cuments our tool relies on W3C standard languages XQuery and XSLT supported by NET The user interface of the mo dule for automatic pro duction of DELAC lemmas is very friendly A user can choose files with lists of MWUs and a strategy and results are presented in a form of a table in which the user has only to check the correct solutions upon which a list of DELAC entries is produced Various debugging tools and preference selections are at the user s disposal It has already been shown that LeXimir can be used for languages other than Serbian and English Our new mo dule for pro duction of DELAC entries can also be successfully applied without any mo dification to other languages that have dictionaries of simple words in DELA thus directly supported by Unitex naturally corresponding XML do cuments representing the strategy for a particular language need to be created However the system can be easily mo dified to support other formats of simple words dictionaries because only the dictionary look up mo dule has to be changed The experiment is already in progress for Polish for which Multiflex is used for inflection of MWUs but simple word dictionaries are handled in a different database environment 13 Another tool WS4QE shortened for Work Station for Query Expansion was developed on basis of LeXimir and it enables expansion of queries submitted to the Google search engine 6 Integrated lexical resources enable mo difications of user queries for both monolingual and multi lingual search The main feature of WS4QE is that it enables inflection of simple words and MWUs supplied as keywords to Google Again Unitex and Multiflex are used for inflection However WS4QE goes one step further for free phrases supplied as key words having a structure covered by a MWU inflectional class the tool uses our strategy and acts upon the first offered solution which is the correct one in most cases 6 Conclusions The results that we have presented justify the efforts invested in designing our pro cedure because it allows for massive pro duction of DELAC entries We have already prepared a list of 25 000 MWUs extracted from the Serbian explanatory dictionary that we hope to be able to pro cess in a few months One important thing that remains to be done is the addition of semantic and or domain markers to MWU lemmas which have so far been systematically added only for proper names following the approach suggested in 14 We are considering several solutions including one proposed in 15 We envisage further development of our procedure We would like to allow MWUs to be components of other MWUs and components of free phrases as well This would keep the number of possible structures low and consequently reduce the number of inflectional classes and the number of rules in the strategy For instance a lemma for a MWU from the beginning of this article civilni vojni rok could in this case be civilni civilni A2 adms1g vojni rok vojni rok nc axn ms1q nc axn Automatic Construction of a Morphological Dictionary of MWUs 237 That is its second component could be a MWU itself vojni rok military service and not a simple word and it would remain in the most frequent adjectivenoun class This approach is already implemented in Multiflex but not in Unitex However DELAC entries that are already pro duced need not be revised References 1 Courtois B Silberztein M Collocation Extraction in Turkish Texts Using Statistical Methods Senem Kumova Metin1 and Bahar Karao glan2 Izmir University of Economics Engineering and Computer Science Faculty Izmir Turkey senem kumova ieu edu tr Ege University International Computing Institute Izmir Turkey bahar karaoglan ege edu tr 1 2 Abstract Collocation is the combination of words in which words appear together more often than by chance Since collocations are blocks of meaning they play an important role in natural language processing applications word sense disambiguation part of speech tagging machine translation etc In this study a corpus of Turkish is subjected to the following statistical techniques frequency of occurrence mutual information and hypothesis tests We have utilized both stemmed and surface form of corpus to explore the effect of stemming in collocation extraction The techniques are evaluated by recall and precision measures Chi square hypothesis test and mutual information methods have produced better results compared to other methods on Turkish corpus In addition we have found that a stemmed corpus facilitates discrimination between successful and unsuccessful collocation extraction methods Keywords Collocation collocation extraction 1 Introduction Collocations are conventional word combinations that co o ccur together so recurrently that they may not be regarded as random combination of words The term collocation has been first intro duced by an English linguist J R Firth in the book Modes of Meaning 1 in which he states that a word can be understoo d by the company it keeps and gives some examples to illustrate the notion of collocations In his further study he states Collocations of a given word are statements of the habitual or customary places of that word Later Sinclair a student of Firth defined collocation as the o ccurrence of two or more words within a short space of each other in a text 2 In contrast Hoey 3 gives a more statistical definition stating that a collocation is the appearance of two or more lexical items together with a probability that cannot be interpreted as random In Oxford Collocation Dictionary collocation is defined as the co o ccurrence of words to pro duce natural sounding speech and writing Since collocations are arbitrary and indefinite they have an important effect on meaning in text and speech As a result extracting of collocations supports a H Loftsson E Collocation Extraction in Turkish Texts Using Statistical Methods 239 wide range of natural language processing applications such as natural language generation machine translation word sense disambiguation part of speech tagging information retrieval computational lexicography corpus linguistic search and in some social studies through language 4 5 In order to serve for this wide range of applications many different metho ds of collocation exploration can be found in the literature which can be categorized as statistical and rule based metho ds Rule based methods depend especially on part of speech tagging information On the other hand statistical metho ds frequency measure mutual information 6 hypothesis testing etc are based on some kind of frequency measure to extract collocations in a given corpus Smadja s Xtract 7 and the techniques of Kita et al 8 and Shimohata et al 9 are also examples of methods known by the names of researchers In this study we have applied some statistical techniques to extract collocations in Turkish and compared the results using recall and precision measures We have utilized both stemmed and surface formed corpora to examine the effect of extensive agglutination in Turkish Although there are many studies on different languages including English Spanish Russian Chinese French to the best of our knowledge there is no corresponding study on Turkish in this concept We believe that our results may further open research in the field of agglutinative languages especially Turkish In section 2 the term collocation is presented In section 3 we have given previous work on Turkish collocations In section 4 collocation extraction techniques which are implemented in the study are briefly described In section 5 experimental setup which clarifies utilized corpora the base set and evaluation metho d is given Section 6 involves the implementation results Finally section 7 deals with the discussion of the above study 2 Collocation As it is evident from different definitions of collocation in recent works there are no known rules for the formation of collocations Although researchers do not have a total consensus on either the definition of collocation or the rules by which they are created common features collected from different studies may be listed and defined as in below Collocations are recurrent Of all properties which discriminates collocations from other word combinations recurrence is the easiest property to measure As a result almost all extraction techniques depend on some kind of frequency measure 4 6 7 10 11 Collocations are arbitrary and language specific There are no known rules that define which words collocate and how a word chooses a particular word or words from millions of different words in language to create a collocation For example strong is a common collocation with coffee in English But there is no clear explanation for the preference of this word instead of powerful Also collocations may change in different languages depending on the so cial or 240 S Kumova Metin and B Karao glan cultural behaviors of native speakers In Turkish strong coffee is called sert kahve the exact translation of the words to English gives hard coffee Collocations create a unit block in language In natural language processing applications considering sense or meaning integrity a unit block may be defined as a single word or a combination of words that has an individual meaning sense and may be regarded as a sentence or a constituent of a sentence Especially in applications such as word sense disambiguation part of speech tagging or machine translation detection of units is an important prepro cessing step that affects the whole performance of the proposed system For example the collocation lady killer means a man exceptionally attractive to women rather than one who kills them So for collocations the meaning of the whole is not the meaning of the parts Collocations are domain dependent There are many different domain specific collocations especially in particular sports medicine or science Smadja 7 gave the domain of sailing as an example Word combinations a dry suit or a wet suit do es not mean a suit that is literally dry or wet they are special types of suit which sailors use but these meanings are not obvious for even native speakers Since the definition of collocation is still a controversial issue in our study we assumed the following word combinations as collocations 3 Collocation Extraction in Turkish Turkish is a highly pro ductive language through extensive agglutination with a rich set of derivational and inflectional suffixes In a theoretic manner it is possible to derive millions of different word forms from just a particular lexeme in Turkish As a result computational linguistic applications have a high level of complexity of time and space In addition to this high level of complexity in applications language mo dels may need mo difications for surface formed corpus and stemmed corpus Collocation Extraction in Turkish Texts Using Statistical Methods 241 Recent work on Turkish collocations involves commonly linguistic studies discussing the importance of collocation notion in translation and second language education or examining the collocativity of a particular word in written Turkish texts 12 13 14 In the area of computational linguistics Oflazer et al 15 propose a rulebased multiword expression processor The pro cessor extracts multiword expressions in a morphologically analyzed corpus in which parts of speech and inflections are all tagged Multiword expressions are categorized in four different forms in the study by Oflazer et al lexicalized collocations semi lexicalized collocations non lexicalized collocations and multiword named entities Depending on the certain morphological patterns multiword collocations are retrieved by querying about 1100 rules 4 Some Collocation Extraction Techniques There are various statistical and rule based techniques for collocation extraction We have applied common statistical techniques to avoid the time and space complexity of the prepro cessing steps needed in rule based techniques In the following subsections utilized techniques frequency of o ccurrence mutual information and some of hypothesis tests t test log likelihood chi square will be briefly described 4 1 Frequency of Occurrence The frequency of o ccurrence metho d is the simplest and earliest approach on collocation extraction In this technique the frequency of bigrams or frequency of words that co o ccur in a given window designates whether the word combination is a collocation or not Combinations are ranked by the frequency to create a candidate list and the most frequent combinations are accepted as collocations Although some of the frequent candidates are collocations others are pairs of function words 5 To discard these frequent function word pairs filtering e g part of speech tagging is recommended in many existing studies 16 4 2 Mutual Information In information theory mutual information is defined as the quantity that measures the mutual dependence of the two variables In collocation extraction instead of two random variables the definition is mo dified for values of random variables Thus a new measure point wise mutual information is intro duced 6 10 If we write x and y for the first and the second word respectively point wise mutual information for them is given by I x y log2 P x y P x If the words x and y are independent of each other the probability of the words coming together must be equal to the multiplication of their own probabilities 242 S Kumova Metin and B Karao glan P x y P x To decide whether a word combination is a collocation it is necessary to prove that the joint o ccurrence of the words is more than coincidence The common approach showing the dependence between words is testing the hypothesis of independence Hypothesis testing methods attempt to reject the null hypothesis that states that words in combination are independent of each other The different metho ds testing null hypothesis used in literature are described as follows 4 4 Dunning s Log Likelihood Test Log likelihoo d metho d is a hypothesis testing approach presented by Dunning 11 In collocation discovery two alternative explanations for the o ccurrence frequency of bigram w1w2 is examined Hypothesis 1 P w2 w1 p P c2 c12 N c1 c1 c2 c12 are respectively the number of occurrences of w1 w2 and w1w2 N is the total number of words in the corpus Assuming a binomial distribution b k n x xk 1 x n k log likelihood ratio is then as follows log log b c12 c1 p b c2 c12 N c1 p L H 1 log L H 2 b c12 c1 p1 b c2 c12 N c1 p2 2 logL c12 c1 p logL c2 c12 N c1 p logL c12 c1 p1 logL c2 c12 N c1 p2 where L k n x xk 1 x n k Mood 1974 has shown that 2log has an asymptotically 2 distribution So if the calculated values 2log are less than Collocation Extraction in Turkish Texts Using Statistical Methods 243 2 value at given level of significance the null hypothesis of independence is accepted otherwise the hypothesis that states w1w2 is a collocation is accepted As a result the log likelihoo d method pro duces a statistic that tells how much more likely one hypothesis is than the other the higher the number the closer the candidate is to being a collocations The metho d is applied to all word combinations in the corpus and a ranked list is generated to extract collocations 4 5 The t Test In the t test null hypothesis states that sample is drawn from a normal distribution with mean The test looks at the differences between expected and observed means scaled by variance of the data As a result if the observed mean differs from expected mean null hypothesis is rejected The t statistics is computed as s2 N where f w2 N f w1w2 P w1w2 N The null hypothesis is P w1w2 P w1 2 test is a hypothesis testing technique presented by Pearson The technique does not require normally distributed probabilities as in t test The test is applied to 2x2 tables to compare observed frequencies with expected frequencies 244 S Kumova Metin and B Karao glan Table 1 2x2 table showing frequencies for words beyaz and saray w1 beyaz w2 saray 8 beyaz saray w2 saray 15820 e g beyaz to examine whether the null hypothesis of independence can be rejected The expected frequency representing independence are calculated and if the observed frequencies differ from expected frequency null hypothesis is rejected Table 1 includes the 2x2 frequency table of the words beyaz white saray palace The couple beyaz saray refers to The White House in English The null hypothesis in 2 test may be stated as beyaz and saray are independent The 2 statistic sums differences between observed Oij and expected values Eij in all cells of the table and scales the differences by the magnitude of the expected as follows Oij Eij 2 4 2 Eij where i is row and j is column index in the table The expected frequency of each cell is computed from the totals of rows and columns converted into proportions The 2 value is calculated for all word combinations in corpus and a ranked list is generated The combinations having higher values are accepted as being collocations 5 5 1 Experimental Setup The Base Data Set Collocation extraction is a corpus based application As a result ideally a collocation tagged corpus is required Due to the great sizes of corpora however it is impossible to tag manually all collocations in a corpus In many studies researchers extract a base set from corpora and implement the metho ds on this set The base set may be constructed in many different ways It may be constructed from a specific word combination considering part of speeches For example adjective noun pairs in the corpora may be selected to create a base set as in the study of Evert and Krenn 17 Or the set may be retrieved from a dictionary 18 In our study we have used a different approach to generate the base set We have retrieved all bigrams in the corpus excluding those across sentence boundaries Afterwards five techniques defined in previous chapter are applied to generate a ranked list of bigrams In the ranked lists candidates having higher scores are assumed to be collocations We selected the first best 200 candidates in each list to create the base set and tagged the set manually Thus it not only became Collocation Extraction in Turkish Texts Using Statistical Methods 245 possible to compare all metho ds based on the same data set but also all preprocessing steps to retrieve collocation candidates in the corpus were eliminated This study utilized the Bilkent corpus compiled as Bilkent University for computational linguistic studies 19 The corpus consists of articles from popular newspapers over an interval of several years It has been morphologically analyzed by a finite state machine sentence boundaries and stemmed forms of words have been tagged automatically 19 We have applied collocation extraction techniques to both the stemmed and surface form of corpus to examine the effect of stemming in Turkish The corpus has about 719665 words and 48268 sentences Since collocations are defined as frequently occurring word combinations we have eliminated word combinations o ccurring less than four times in the corpus before the application of statistical extraction techniques 5 2 Evaluation Method Evaluation of extraction techniques defined in previous chapters is performed by recall and precision which are frequently used as performance measures in information retrieval For collocation extraction recall may be defined as the fraction of the collocations in the corpus or the base set that is successfully retrieved Precision is the fraction of true collocations in the retrieved list of collocation candidates Taking to be collocations extracted from base set and to be the number of true collocations in the base set recall and precision may be defined as r p While presenting the precision and recall values we have applied the approach of Evert and Krenn 17 In the approach instead of computing the measures for only a single proportion of candidate list for example just for the whole set or just for the first N candidates recall and precision are computed for N highest ranked candidates where N may vary from 1 to the total number of candidates N 1 2 3 base data set This approach prevents misleading conclusions being drawn from a single value of N We have plotted graphs of precision and recall for whole base set 6 Results Through agglutination in Turkish a stem can occur in many different forms due to many possible different inflections All words in collocations are prone to agglutination especially those in final position As a result in the corpus the same collocation may occur with different surface forms and this variation reduces the total frequency of the collocation to the extent that collocation may completely lose recurrence property For example maliye bakanli gi is a collocation that 246 S Kumova Metin and B Karao glan can be translated as ministry of commerce to English However this collocation may occur in the forms such as maliye bakanli gina to the ministry of commerce or maliye bakanli ginin of the ministry of commerce maliye bakanli ginda in the ministry of commerce maliye bakanli giyla with the ministry of commerce Therefore the frequency of o ccurrence is widely spread across different forms This property induced us to expect that collocation extraction may give better results for stemmed corpora in which different word forms are all merged to one stem The implementation of extraction techniques on the Bilkent corpus has returned a larger base set for stemmed corpus 661 bigrams compared to surface formed corpus 507 bigrams as expected Base sets involve 53 5 and 49 8 true collocations respectively for surface and stemmed forms of the Bilkent corpus The proportions give us the baseline for the precision graphics As a result if one particular metho d gives lower values than the baseline for a particular N value it is said to be even worse than random selection Figures 1 and 2 show precision graphs of surface and stemmed Bilkent corpus respectively The horizontal axis in the graphs presents the percentage of base set completed In the graphs three important results are pointed out Firstly it is noteworthy that 2 and mutual information metho ds give consistently higher precision for both stem and surface form lists In contrast log likelihoo d t test and frequency measure metho ds perform even worse than the assumed baseline which is random selection Secondly in the stemmed corpus as expected precision values are higher and the gap between 2 and mutual information metho ds with the others are more apparent Finally it can be seen from the figures that the stemmed corpus gives a clearer indication of which metho ds are successful and which are not and the degree of difference between them Figures 3 and 4 show that 2 and mutual information metho ds reach higher scores of recall earlier compared to other metho ds supporting precision results Correlatively 2 and mutual information metho ds generate higher scores for true collocations and extract them earlier than other metho ds 1 0 9 0 8 0 7 Precision 0 6 0 5 0 4 0 3 0 2 0 1 0 0 20 40 Frequency of Occurence Mutual Information Log likelihood The t test Chi square test 60 80 100 Fig 1 Precision graph for Bilkent corpus surface form Collocation Extraction in Turkish Texts Using Statistical Methods 247 1 0 9 0 8 0 7 Precision 0 6 0 5 0 4 0 3 0 2 0 1 0 0 0 2 0 4 Frequency of occurence Mutual Information Log likelihood The t test Chi square test 0 6 0 8 1 Fig 2 Precision graph for Bilkent corpus stemmed form 1 0 9 0 8 0 7 Recall 0 6 0 5 0 4 0 3 0 2 0 1 0 0 20 40 60 80 100 Frequency of Occurence Mutual Information Log likelihood The t test Chi square test Fig 3 Recall graph for Bilkent corpus surface form 1 0 9 0 8 0 7 Recall 0 6 0 5 0 4 0 3 0 2 0 1 0 0 20 40 60 80 100 Frequency of occurence Mutual Information Log likelihood The t test Chi square test Fig 4 Recall graph for Bilkent corpus stemmed form 248 S Kumova Metin and B Karao glan 7 Discussion In this study some statistical methods were applied to Turkish corpus to retrieve collocations 2 and mutual information methods generated higher precision and recall values compare to other techniques Methods are both utilized on stemmed and surface form of corpus to explore the effect of agglutination in collocation extraction It is seen that the stemmed corpus generated results that are more effective in discriminating between successful and unsuccessful metho ds In a further work we hope to be able to improve metho ds of analysis in the light of the results of this study References 1 Firth J R Modes of Meaning Papers in Linguistics 1934 51 Oxford University Press Oxford 1957 2 Sinclair J M Corpus Concordance Collocation Oxford University Press Oxford 1991 3 Hoey M Patterns of Lexis in Text Oxford University Press Oxford 1991 4 Bisht R K Dhami H S Neeraj Tiwari N An evaluation of different statistical techniques of collocation extraction using a probability measure to word combinations Journal of Quantitative Linguistics 13 Collocation Extraction in Turkish Texts Using Statistical Methods 249 16 Justeson J S Katz S M Principled Disambiguation Discriminating Adjective Senses with Modified Nouns Computational Linguistics 21 1 1995 17 Evert S Krenn B Methods for the qualitative evaluation of lexical association measures In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics Toulouse France pp Towards the Design and Evaluation of ROILA A Speech Recognition Friendly Artificial Language Omar Mubin Christoph Bartneck and Lo e Feijs Department of Industrial Design Eindhoven University of Technology Den Dolech 2 5612 AZ Eindhoven The Netherlands o mubin c bartneck l m g feijs tue nl Abstract In our research we argue for the benefits that an artificially designed language that we call ROILA could provide to improve the accuracy of speech recognition given that it is constructed on speech recognition friendly principles We also contemplate the trade off effect of users investing some effort in learning such a language Initially we present the design and evaluation of the vocabulary of ROILA and subsequently we describe the ROILA grammar and the method by which we rationally chose grammar rules Our evaluation results indicated that the vocabulary of ROILA significantly outperformed English whereas we could not yet replicate similar trends while evaluating the grammar Keywords Artificial Languages Automatic Speech Recognition Sphinx 4 1 Introduction Recent research in speech recognition is gradually progressing towards altering the medium of communication in a bid to improve the quality of speech interaction As stated in 12 constraining language is a plausible metho d of improving recognition accuracy In 15 the user experience of an artificially constrained language was evaluated within a movie information dialog interface and it was concluded that 74 of the users found the constrained language interface to be more satisfactory than natural language interface The limitations prevailing in current automatic speech recognition technology for natural languages is an obstacle behind the unanimous acceptance of Speech Interaction Generally in speech interfaces the fo cus is on using natural language it may be time to explore a different balance in the form of a new language The field of handwriting recognition has followed a similar road map The first recognition systems for handheld devices such as Apple s Newton were nearly unusable Palm solved the problem by inventing a simplified alphabet called Graffiti which was easy to learn for users and easy to recognize for the device Using the same analogy we aim to design a Speech Recognition Friendly Artificial Language ROILA where an artificial language as defined by the Oxford Encyclopedia is a language deliberately invented or constructed In linguistics there are numerous artificial H Loftsson E Towards the Design and Evaluation of ROILA 251 languages for e g Esperanto Interlingua whose goal is easier communication amongst users however there has been little or no attempt to optimize a spoken artificial language for speech recognition In summary our research is constructed on the basis of two main goals Firstly the artificial language should be optimized for efficient automatic speech recognition and secondly there should be an attempt to make it learnable for a user two possibly contradictory requirements for e g users would prefer shorter words but shorter words would be harder to recognize 2 Vocabulary Design In order to obtain a group of phonemes that could be used to generate the vocabulary of ROILA we conducted a phonological overview of natural languages 10 Extending from our goal of designing a language that is easy to learn for humans we extracted a set of the most common phonemes present in the ma jor 13 natural languages of the world based on number of speakers We used the UCLA Phonological Segment Inventory Database UPSID 11 The database provides an inventory of the phonemes of 451 languages of the world We generated a list of phonemes that are found in 5 or more ma jor languages This resulted in a total of 23 phonemes Certain other constraints were employed to reduce this list further diphthongs were excluded and phonemes that had ambiguous behavior across languages were ignored Therefore the final set of 16 phonemes that we wished to use for our artificial language was in ArpaBet notation AE B EH F IH JH K L M N AA P S T AH W As a starting point for the first version of the vocabulary of ROILA we choose the artificial language Toki Pona 6 which caters for the expression of very simple concepts by just 115 words Therefore this number formed the size of the ROILA vocabulary In order to maintain a balance between our two research goals we set the word length to 4 5 and 6 characters with each word having 2 or 3 syllables rendering the following word types CVCV VCVC VCCV CVCVC VCVCV CVCVCV VCCVCV VCVCCV where V refers to a vowel and C to a consonant from our pool of 16 phonemes The 8 word types were simple extensions of words existing in Toki Pona based on the assumption that such words would be easy to learn and pronounce To define the scalable representation of the words we utilized a genetic algorithm that would converge to a vocabulary of words that would have the lowest confusion amongst them and in theory be ideal for speech recognition The genetic algorithm randomly initialized a vocabulary of N words for P vocabularies where each word was any one of the 8 afore mentioned word types The algorithm was then run for G generations with mutation and cross over being the two primary offspring generating techniques The fitness function was determined from data available in the form of a confusion matrix from 7 where the matrix provided the conditional probability of recognizing a phoneme pj by a speech recognizer when phoneme pi was said instead Therefore the confusion between any two words was determined by computing the probabilistic string edit distance as suggested 252 O Mubin C Bartneck and L Feijs in 1 The first ROILA vocabulary was generated by running the algorithm for P G 200 In order to have a benchmark of English words to compare against we set the English vocabulary as the meanings of the 115 Toki Pona words 2 1 Vocabulary Evaluation In order to evaluate ROILA 16 6 female voluntary participants were asked to record samples of every word from both English and ROILA The recordings were then passed offline through the Sphinx 4 4 speech recognizer Participants had various native languages but all were graduate students and hence had reasonable command over English Recordings were carried out using a high quality microphone Sphinx was tuned such that it was able to recognize ROILA by means of a phonetic dictionary however an acoustic mo del for English was used In addition we did not carry out any training on the acoustic mo del for ROILA One of the researchers conducted rounds of ROILA recordings until we had a pool of recordings that rendered a recognition accuracy of 100 These sample recordings of every word would be played out before participants recorded each ROILA word This was done to ensure that the native language of participants would not affect their ROILA articulations The experiment was carried out as a 2 condition within sub ject design where the language type English ROILA was the main independent variable The dependent variable was the total number of errors in recognition Words from both English and ROILA were randomly presented and the order of recording English or ROILA first was also controlled between participants We carried out a repeated measure ANOVA which revealed that language type did not have an effect F 1 9 0 758 p 0 41 Both ROILA and English performed equally in terms of accuracy 67 61 and 67 66 respectively Without any training data such accuracy is expected from Sphinx on test data 14 To judge if ROILA word structure had an effect on recognition accuracy we executed an analysis in which the type of word was the independent variable This factor had 2 levels namely CV or non CV type where CV type words were CVCV CVCVC and CVCVCV The ANOVA analysis revealed a nearly significant trend F 1 113 3 6 p 0 06 CV type words performed better on recognition on average 4 19 participants got such words wrong as compared to non CV type words where 5 75 participants got them wrong Therefore for our second iteration of the evaluation we generated a new vocabulary that comprised of CV type words only The genetic algorithm was run with the parameters G P 200 We had 11 4 female from the earlier 16 participants carry out recordings of the new vo cabulary using the same setup and pro cedure We did not have them record the English words again Participants would once again hear sample pronunciations The REMANOVA revealed that the new ROILA vocabulary significantly outperformed English F 1 10 4 86 p 0 05 see Figure 1 The accuracy for the 11 participants was English 65 11 and ROILA CV 71 11 This vocabulary was hence declared as the first ROILA vocabulary Towards the Design and Evaluation of ROILA 253 Fig 1 Average Errors Bar Chart for English and ROILA CV 3 Grammar Design In conjunction with conducting a phonological overview of artificial languages we also carried out a morphological overview of artificial languages individually and also in contrast to ma jor natural languages of the world 10 This aided us in identifying grammar features which were popular in both sets of languages We determined several grammatical categories based on properties defined in various linguistic encyclopedias 5 Gender numbering tense and aspect are some examples However within each category there were a number of options that we could choose from for e g should we have gender How many tenses should we have In order to make our choice we carried out a rationale decision making pro cess by utilizing the Questions Options and Criteria QOC technique 8 For this purpose we defined the following important criteria for every grammatical property Learnability defines whether the grammatical marking in question would be easy to learn or not Expected recognition accuracy defines the effect the grammatical marking would have on the anticipated word error rate given that the more constrained a grammar lower perplexity is the better it would be for recognition 9 Vocabulary size describes the effect the grammatical marking would have on increasing or decreasing the vocabulary size Expressive Ability of the language defines whether using the grammatical marking in question would actually enable speakers to express more concepts then they would have been unable to do so otherwise Efficiency simply relates the grammatical marking to how many words would be required to communicate any solitary meaning Acknowledgement within Natural and Artificial Languages states the popularity of the particular grammatical marking amongst each type of languages Appropriate weights were assigned to the criteria based on importance for e g learnability and expected recognition accuracy were assigned higher weights with recognition accuracy being given twice as much weight as learnability The total sum of the weights was 1 254 O Mubin C Bartneck and L Feijs Table 1 ROILA Example Sentences ROILA Sentence pito fosit bubas pito fosit jifi bubas pito fosit jifo bubas English Translation I am walking to the house I walked to the house I will walk to the house Literal Meaning I walk house I walk past tense marker house I walk future tense marker house All possibilities of each grammatical category were listed and every category was then ranked across the criteria by giving a number between 1 and 3 with 3 being the best fit The category which yielded the highest output was then chosen to be as the grammar category of choice After filling in a matrix we concluded firstly that the ROILA grammar would be of isolating type Affixes would not be added as this might alter the word structure hereby reducing their efficiency for speech recognition Therefore grammatical categories in ROILA would be represented by word markers see Table 1 At the end we arrived at the following properties Gender male female on the level of pronouns only and not nouns Numbering singular plural on the level of nouns Person references first second third on the level of pronouns Tense past present future and word order would be SVO 3 1 Grammar Evaluation In order to evaluate the grammar in terms of recognition we formulated some sample sentences N 30 based on a hypothetical interaction scenario for a dialog system These sentences were evaluated against their English semantic counterpart Sphinx 4 Language Models were created using the Sphinx Knowledge Base tool 13 An identical setup was followed as done in the evaluation of the vocabulary except that participants would now record sentences and not isolated words Participants would once again hear a sample voice as a guide of how to pronounce sentences The dependent variable was word accuracy a common metric to evaluate continuous speech recognition 3 with the independent variable yet again language type In the initial evaluation we conducted recording sessions with 8 participants However we were unable to achieve significant results in favor of ROILA as indicated by the REMANOVA results F 1 7 1 97 p 0 21 4 Discussion and Conclusion Our results revealed some interesting insights Firstly we were able to achieve improved speech recognition accuracy as compared to English for a relatively larger vocabulary Similar endeavors have only been carried out for a vocabulary size of 10 2 Secondly we quantitatively illustrated that CV type words perform better in recognition co articulation of CV syllables could be one explanation for that We must keep in mind several implications to our results Firstly participants recorded words without any training in ROILA whereas they were already Towards the Design and Evaluation of ROILA 255 acquainted with English Potentially by training participants in ROILA the accuracy could be further improved This effect was observed to be more pronounced when participants had to speak ROILA sentences which could explain the insignificant difference between ROILA and English in terms of the accuracy of speech recognition The acoustic mo dels of Sphinx are trained with dictation training data and from what we observed the ROILA sentence articulations of participants did not fall within the domain of dictation speech There were pauses between words and pronunciations were not smooth which could have been caused by the inexperience of the participants in ROILA In the future we aim to conduct more evaluation sessions of ROILA sentences after carrying out training with additional participants It may also be observed that the acoustic mo del of Sphinx was primarily designed for English yet our ROILA accuracy in the second vocabulary iteration were significantly better as compared to English a promising result indeed What we would also like to determine in the future is the magnitude of the difference between ROILA and English This could be accomplished by using the same English acoustic mo del for another natural language and comparing the differences in recognition accuracy between the three languages English ROILA and the second natural language We acknowledge the trade off factor of humans having to invest some energy in learning a new language like ROILA even though in various steps of the design pro cess we have tried to accommo date the aspect of human learnability and intro duce language features which were conducive to learnability In summary by designing an artificial language we are faced with the effort a user has to put in learning the language Nevertheless we wish to explore the benefits that an artificial language could provide if its designed such that it is speech recognition friendly This factor might end up outweighing the price a user has to pay in learning the language and would ultimately motivate and encourage them to learn it Another criticism that might be levied on ROILA is that many artificial languages were created already but not many people ended up speaking them Where our approach is different is that we aim to deploy and implement our artificial language in machines and once certain machines can speak the new language it could encourage humans to speak it as well In the future we aim to train participants in ROILA and evaluate it by deploying it in an interaction context We acknowledge that a meaningful so cietal application of our language would provide an extra gain in addition to recognition performance We aim to explore applications for children medical tasks or care robots1 Acknowledgements We would like to thank the reviewers for their helpful comments and feedback to revise the paper References 1 Amir A Efrat A Srinivasan S Advances in phonetic word spotting In The Tenth International Conference on Information and Knowledge Management pp 1 To know more about ROILA and its latest developments please visit http roila org 256 O Mubin C Bartneck and L Feijs 2 Arsoy E Arslan L A universal human machine speech interaction language for robust speech recognition applications In Sojka P Kope cek I Pala K eds TSD 2004 LNCS LNAI vol 3206 pp Time Expressions Ontology for Information Seeking Dialogues in the Public Transport Domain Agnieszka Mykowiecka Institute of Computer Science Polish Academy of Sciences J K Ordona 21 01 237 Warsaw Poland agn ipipan waw pl Abstract The paper presents an ontology of natural language temporal expressions which occur in dialogues led by users of a public transport call center It was elaborated on the basis of analysis of 500 transliterated dialogues and contains a multihierarchy of concepts representing semantics of time referring fragments of interlocutors utterances The paper contains also an analysis of frequencies of all types of time relevant expressions within the analyzed data Keywords temporal expressions in Polish time ontology 1 Introduction Time expressions occur in dialogues of nearly every type and for many of them resolving time references is crucial for successful communication One of the domains for which temporal relations are extremely important is information about public transport connections In this domain a description of time together with space and transportation lines organization issues are the most important areas of interest As representing time points or intervals and inferring about their interdependencies is crucial for natural language understanding no wonder that there are a lot of time models and temporal logics describing ways of operating on time points or H Loftsson E 258 A Mykowiecka a day and also imprecise notions like later However the annotation guidelines concern English lexical triggers and their usage for recognizing boarders and types of temporal phrases in texts in another natural language is limited One of the first complex solutions of the problem of linguistic temporal expressions interpretation was proposed within the Verbmobil project for the task of appointment scheduling 2 In recent years a lot of different ideas were explored in all the domains enumerated above 4 The research presented here aimed at collecting and organizing linguistic material concerning temporal linguistic constructions in Polish The motivation of this work was the fact that although in general time expressions in Polish are similar to English their exact formulations have to be described on the basis of real data The constructed set of concepts was used as a starting point for creating machine learning models for solving a problem of automatic recognition of temporal concepts in the chosen type of Polish dialogues In an experiment described in 7 of semantical labeling using CRF model for 15 types of time related concepts occurring in the test set the F measure ranged from 0 5 for time_rate_rel to 0 97 for at_hour and 1 0 for before_hour only concept names without values were evaluated The data which is our source of knowledge about the way people talk about time in the context of using public transport is a set of Polish dialogues collected at the Warsaw Transport Authority call Time Expressions Ontology for Information Seeking Dialogues 259 2 Temporal Expressions Ontology The ontology presented in the paper is a newly defined OWL resource There are two reasons for this solution First although a lot of effort has been put into making ontologies reusable using already defined ones like 8 is still very difficult and all problems connected with their full understanding projection into a chosen subdomain and expansion issues described for example in 9 still exist The second reason was connected with our 260 A Mykowiecka Fig 1 Upper part of the class hierarchy of all defined subclasses is given The second case in which a lot of class instances differing in values of a datatype property can occur is marked by the The fourth column contains the number of different natural language phrases which occurred within the corpus Their examples together with English translations for cases in which a name of a class is not sufficient for understanding the phrase are given in the last column As it is not easy to clearly linearise the multidimensional division the subsections which try to represent it are not always quite precise For most classes their exact placement in the hierarchy is given in Figure 1 The numbers given support the statement that in this domain relative temporal expressions are very frequent For example for about 2300 time concept occurrences there are 448 occurrences of TIME_REL objects which represent relative time of a day in intervals On the other hand the phrases used to represent them are not very diverse 41 type of phrases The most frequent notion of this class is Later 170 occurrences were expressed by only 6 types of phrases There are also quite a lot of partial expressions not only those giving hours without giving minutes but there are also a lot of expressions in which only the minute part is given 261 Such an expression can either mean an additional specification of an exact hour which was already given before or address every hour in a given time period The next observation is the fact that there are some domain related language expressions which are not likely to be found in general resources e g godziny szczytu peak hours Time Expressions Ontology for Information Seeking Dialogues Table 1 Temporal concepts statistics name TIME_DAY_LEVEL Points DATE DATE_ABS DATE_REL TIME_OF_WEEK_WRK HOLIDAY WORKINGDAY Intervals TIME_YEARPERIOD_ABS DATE_ABS_BEG DATE_ABS_END TIME_HOUR_LEVEL Points AFTER_HOUR AROUND_HOUR AT_HOUR AT_HOUR_MINPART AT_HOUR_PART BEFORE_HOUR IN_X_HOURS IN_X_MINUTES Intervals DAYTIME_PERIOD_BEG DAYTIME_PERIOD_END DAYTIME_PERIOD_SPAN TIME_DAYPERIOD _AFTERNOON _DAY _EVENING _MORNING _NIGHT _OFFPEAK _PEAKHOURS _WHOLEDAY TIME_REL _AtThisTime _Earlier _Earliest _Now _Early _Later _TheSame _Tomorrow _Today _TooEarly _TooLate Span TIME_SPAN TIME_SPAN_D TIME_SPAN_H RIDE_DURATION_H RIDE_DURATION_M RIDE_DURATION_REL WALK_DURATION Rates RATE_HOUR_ABS RATE_HOUR_REL _Freq _LessFreq _MoreFreq _NoFreq occurr different different examples subtypes phrases objects 261 18 14 4 62 30 32 30 22 2 2 2 8 2 2 2 1 1 9 2 8 1 11 10 1 20 4 16 pierwszy wrzenia 1 09 dzie urodzin birthday w dni nierobocze non working days w dni robocze working days 11 10 od pierwszego kwietnia from 1 04 1 do 31 sierpnia to 31 08 33 58 757 261 38 31 3 16 50 25 18 79 2 3 4 36 12 1 11 5 448 12 23 5 95 2 170 6 69 45 6 45 12 3 9 7 169 11 5 69 28 10 3 3 12 19 26 420 62 23 13 1 8 21 17 17 8 8 1 1 1 1 1 1 1 1 11 11 1 1 1 1 1 1 1 1 1 1 1 2 2 2 8 2 40 2 5 13 4 5 1 1 1 1 23 57 514 74 24 15 3 12 27 21 18 22 2 2 4 6 1 1 3 2 41 3 7 2 4 1 6 5 4 4 2 2 po dwudziestej after 20 okolo dziesitej around ten dziesita osiem eight past ten zero zero pitnacie po fifteen after dwunasta 12 00 przed dziesit before 10 za jak godzink za jakie 2 8 2 92 5 5 28 12 4 2 1 6 okolo tydzie around a week dziesi minut ten minutes mniej wiecej godzin around an hour maksimum czternacie minut dlugo long dwie trzy minuty minuta drogi dwie co godzin raz na 262 A Mykowiecka 3 Further Work In the paper the ontology for representing Polish phrases referring to temporal expressions used in one chosen domain was presented Two different directions of further usage of this resource are planned First the ontology will be utilised while implementing an algorithm for resolving time references in the original domain of public transport information Second the experiment with expanding this resource with time related notions which occur in a different domain of medical clinical notes is planned The next possible subject for further research could be a detailed comparison of type of phrases used in Polish dialogues with phrases used in another natural language in a similar context There is also planned to establish a method for automatic conversion of the elaborated annotation into TIMEX labels References 1 Ahn D van Rantwijk J de Rijke M A cascaded machine learning approach to interpreting temporal expressions In HLT NAACL ACL pp Reliability of the Manual Segmentation of Pauses in Natural Speech Raoul Oehmen Kim Kirsner and Nicolas Fay University of Western Australia Stirling Hwy Crawley 6008 Perth Western Australia raoul oehmen graduate uwa edu au Abstract Recent innovations regarding analysis of pauses in natural speech have necessitated the segmentation of increasingly small pause durations from the speech stream 1 Identifying pauses and pause durations relies on human judgement However the reliability of these judgements has yet to be established This study investigated the reliability of multiple segmentations of four speech files Results suggest that while inter analyst reliability is moderate intra analyst reliability was high Furthermore inter analyst variation appears to be related to the signal to noise ratio of the speech files A further analysis of the segmentation of one speech file demonstrated that a lack of reliability was associated with certain non speech vocalizations suggesting that reliability could potentially be increased with more precise guidelines for analysts Keywords Pauses Fluency Distributional Fitting Reliability 1 Introduction Traditional analyses of pause durations in spontaneous speech have applied arithmetic means to real time pause frequency distributions 2 similar to those shown on the left in Figure 1 However significant skew in these distributions makes this problematic 3 More recently the frequency of durations of pauses has been more accurately viewed in log time where pauses form two normal distributions 1 4 as shown on the right in Figure 1 These distributions can be mathematically modelled and descriptive statistics obtained They have been dubbed the short pause SP and long pause LP distributions and are thought to represent pauses related to articulatory and cognitive processes respectively 1 in line with traditional accounts 2 However prior to modelling pause frequency distributions pauses must first be identified from the speech stream For more than forty years a widespread disinterest in short articulatory pauses due in part to poor recording and analysis resolutions has led to the use of a variety of minimum pause duration thresholds the smallest pauses considered to be real Numerous different thresholds used across different studies have made comparisons difficult as changes in threshold affects almost every descriptive statistic 5 The pervasive 250msec threshold employed by GoldmanEisler 2 to exclude articulatory pauses was perhaps inaccurate as numerous pauses between 130 and 250msec have been shown to have both cognitive and expressive H Loftsson E 264 R Oehmen K Kirsner and N Fay Fig 1 Pause frequency distributions in both real time left and log time right for a typical speaker Note log time frequency distribution includes bimodal two distributional fit functions 6 Furthermore the junction between articulatory and cognitive pauses has been shown to be highly variable between individuals 1 Technological and theoretical advances have allowed more recent approaches to embrace articulatory pauses via the use of ultra short minimum pause thresholds no minimum 7 10 msec 4 and 20msec 1 This raises the additional question of the accuracy of such fine judgements by analysts in discerning vocalisations from brief silences Such judgements are necessary regardless of whether pauses are segmented from speech entirely by hand or whether a semi automated procedure is used e g 8 and 9 Semi automated procedures typically consider speech to be any time when an amplitude contour rises above some threshold usually set manually to the level of ambient noise 10 Thus manual and semi automated procedures differ only with regards to whether analysts pick a single best fit threshold or decide upon thresholds on a pause by pause basis While semi automated systems have an advantage in terms of consistency 6 their performance may be degraded under conditions of low signal to noise ratios caused by a range of factors including recording conditions stress and aphasia Furthermore while automated systems are reliant purely on the amplitude contour modern analysis equipment puts many more measures at the disposal of the manual analyst with none of the resolution problems inherent in early studies 2 Albeit with a few exceptions 4 11 reliability statistics have seldom been reported and no detailed analysis of variations between segmentations regardless of measurement techniques has been conducted to date This study will determine both Inter analyst and Intra analyst reliability in addition to investigating the causes of any observed variation 2 Method 2 1 Stimuli The stimuli to be analysed consisted of four spontaneous monologues elicited via a stimulus question averaging 1 86 minutes and containing on average 162 discernable pauses Each contained a different English first language adult speaker 3 females Reliability of the Manual Segmentation of Pauses in Natural Speech 265 and 1 male and each discussed different topics Files MB SK CN were recorded on dynamic unidirectional lapel microphones into portable digital recorders File RO was recorded using a head mounted dynamic unidirectional microphone in an acoustically controlled laboratory with a signal to noise S N ratio of 53 86 Files SK CN were recorded in a quiet laboratory setting and had S N ratios of 20 36 and 16 36 respectively while File MB was recorded in a home setting and had a S N ratio of 5 90 All files were recorded at a sampling frequency of 44100 Hz 2 2 Segmentation Four experienced pause analysts two qualified Speech Therapists one Linguist and one Psychologist segmented each of the sound files In addition each analyst segmented one of the four files a second time after an interval of roughly one week This provided five segmentations of each sound file and 20 segmentations in total Segmentation was conducted in PRAAT 12 via the procedure described in 13 This physical approach ignores linguistic considerations entirely requiring analysts only to place boundaries at those points in the file where a period of silence transitions into a period of speech and vice versa Thus each pause consists of two boundaries acknowledging the on off sequence of vocalisations that comprise speech Sounds that were clearly not associated with a speech act such as non stylistic coughs and audible movements of the muscles of articulation were excluded Analysis was conducted primarily via the visual modality making use of spectrographic information including amplitude and fundamental frequency contours but also by listening to the speech The minimum pause duration threshold employed in the present study was 10msec while the viewing window was set at 0 6 seconds 2 3 Distributional Analysis Pauses were modelled in an analogous fashion to that reported in 1 Pause durations were converted to logarithms and grouped into a series of bins forming pause frequency distributions for each speaker separately A Expectancy Maximisation algorithm 14 was then applied to each distribution fitting the best two distribution model to the data in an attempt to minimise log likelihood A number of statistics can then be derived from the modelled distributions including the means standard deviations and pause rates per minute of speech of the long and short pause distributions 3 Results Distributional statistics for each segmentation were obtained following application of the Expectancy Maximisation algorithm 14 Measures of variation were calculated for Inter analyst reliability between the different segmentations of the same file by different analysts by calculating the standard deviation of the four segmentations of each file for the Long and Short Pause distributional means Similarly measures of Intra analyst variation were calculated as the standard deviation of the repeat segmentations of one file by each analyst The measures are presented in Table 1 below and show intra analyst variability to be lower than Inter analyst variability Furthermore the data indicate that variation increases with signal to noise ratio This point is seen in Figure 2 where larger shaded areas representing variation between segmentations 266 R Oehmen K Kirsner and N Fay are associated with those files with lower signal to noise ratios This is further demonstrated by a Pearson correlation coefficient of r 0 60 between the variability in short pause means and signal to noise ratio albeit with only 4 data points Table 1 Signal to noise ratio and Inter Intra analyst distributional statistic variation for the short and long pause distribution mean of four speech files in log File MB Signal Noise Ratio Inter Intra SP Mean Variation LP Mean Variation SP Mean Variation LP Mean Variation 5 90 0 50 0 32 0 18 0 05 File CN 16 36 0 16 0 09 0 01 0 04 File SK 20 36 0 49 0 13 0 02 0 03 File RO 53 86 0 17 0 07 0 03 0 00 Fig 2 Long pause mean plotted against short pause mean for all segmentations of four speech files inter analyst reliability Shaded areas represent the spread of distributional parameters between analysts segmentation of the same file Despite File RO being the highest quality recording variation still remained between analysts A more detailed analysis of the placement of boundaries junctions between speech and pause between segmentations of this file was therefore conducted Two important analyses were conducted the reliability with which analysts find a particular junction agreement and the variability in msec of the positioning of boundaries between analysts Variation between positioning of analysts boundaries was calculated by taking the absolute root mean square difference as used in 4 For the present file Reliability of the Manual Segmentation of Pauses in Natural Speech 267 this was 5 83msec for all boundaries located by two or more analysts For voice offset boundaries speech changing to pause those followed by a short pause had significantly less variation compared to those followed by a long pause absolute root mean square difference of 4 41msec and 12 63 respectively t 61 2 65 p 0 01 With regards to agreement out of a total of 471 boundaries discovered by one or more analysts 66 03 of boundaries were located by all four analysts while 76 92 of boundaries were located by at least 3 of the 4 analysts Only 14 32 and 8 76 of boundaries were located by one or two analysts In an attempt to determine more specifically the cases of low agreement all speech surrounding boundaries was coded into phonemic categories Table 2 below shows the proportions of some of these events for both speech onset and speech offset boundaries Boundaries located by one or two analysts appear to be characterised by a disproportionately high percentage associated with preparatory movements for articulation e g sounds emitted by opening the mouth prior to vocalisation as well as boundaries associated with fricatives Similarly there appears to be a disproportionately low number of boundaries associated with plosives that were detected by only one analyst Table 2 Percentage of boundaries comprising each phonemic category at varying agreement levels separated by whether the boundary signifies the onset or offset of speech Category Nasal Plosive Fricative Long Vowel Short Vowel Prep Artic Onset of Speech after pause 1 4 2 4 3 4 4 4 0 00 4 76 0 00 1 96 12 12 33 33 42 31 41 18 24 24 19 05 15 38 18 30 9 09 4 76 3 85 7 19 12 12 9 52 7 69 22 88 21 21 14 28 19 23 1 31 Offset of Speech before pause 1 4 2 4 3 4 4 4 14 71 5 00 12 00 23 37 11 76 25 00 28 00 23 38 23 53 10 00 8 00 16 23 8 88 35 00 12 00 11 69 11 76 0 00 16 00 12 34 25 71 5 00 12 00 2 60 4 Discussion Results suggest that while intra analyst reliability is high inter analyst reliability is only moderate with sizable differences between segmentations that can be linked to the signal to noise ratio of the files With regards to intra analyst segmentations the data suggest that whatever reasoning or judgement is being used it is being applied consistently However in inter analyst segmentations an increase in variability at low S N ratios could be due to a decrease in the discriminability of the signal This would bring differences in analysts criterion or their propensity to say the signal was present to the fore in line with traditional Signal Detection Theory accounts 15 While these findings suggest that attention should be paid to the quality of recordings as well as using only a single analyst where possible it is clearly not feasible to insist on laboratory quality recordings Instead future research could investigate possible increases in accuracy associated with the use of additional variables e g pulse rates and formant frequency as well as changing spectrogram viewing settings A further detailed analysis of the positioning of boundaries in a single file revealed greater variation related to the positioning of boundaries proceeding long pauses pauses occurring after the completion of a motor plan compared to proceeding short 268 R Oehmen K Kirsner and N Fay pauses pauses occurring within a motor plan One possible explanation is that the occlusion of vocalisation within a motor plan is likely to be under stricter temporal constraints due to the following speech than at the completion of a motor plan This may lead to a more defined boundary and make analysis more precise Nonetheless variation in the placement of boundaries does not account for the extent of the variation between analysts statistics Variation is more likely to be due to imperfect agreement on the presence or absence of boundaries In particular it appears that disagreements are frequently caused by misclassification of non speech artefacts such as a preparatory articulation and audible breathing events as speech Such problems can likely be resolved with improved segmentation guidelines for analysts References 1 Kirsner K Dunn J Hird K Parkin T Clark C Time for a pause Speech Science Technology Melbourne 2002 2 Goldman Eisler F Psycholinguistics Experiments in spontaneous speech Academic Press London 1968 3 Quinting G Hesitation phenomena in adult aphasic and normal speech Mouton The Hague 1971 4 Rosen K M Kent R D Duffy J R Lognormal distribution of pause length in ataxic dysarthia Clinical Linguistics and Phonetics 17 Large Scale Language Modeling with Random Forests for Mandarin Chinese Speech to Text Ilya Oparin Lori Lamel and Jean Luc Gauvain LIMSI CNRS Spoken Language Processing Group B P 133 91403 Orsay cedex France oparin lamel gauvain limsi fr http www limsi fr Abstract In this work the random forest language modeling approach is applied with the aim of improving the performance of the LIMSI highly competitive Mandarin Chinese speech to text system The experimental setup is that of the GALE Phase 4 evaluation This setup is characterized by a large amount of available language model training data over 3 2 billion segmented words A conventional unpruned 4 gram language model with a vocabulary of 56K words serves as a baseline that is challenging to improve upon However moderate perplexity and CER improvements over this model were obtained with a random forest language model Different random forest training strategies were explored so as to attain the maximal gain in performance and Forest of Random Forest language modeling scheme is introduced Keywords language modeling random forest speech to text ASR STT Mandarin Chinese 1 Introduction The task of language mo deling is to create language mo dels LM that are able to capture the regularities of a natural language A language mo del is an inherent part of any modern speech to text STT system Language mo dels are used in STT systems along with acoustic mo dels AM and a pronunciation dictionary that links the them to perform large vocabulary continuous speech recognition The basis of modern language mo dels is the word N gram approach The omnipresent assumption of N grams is that the o ccurrence of a word can be predicted according to its immediate in case of bigrams or short range left context Word N grams appear extremely efficient in practice Frankly speaking despite the fact that it has already been many decades since they were introduced into the recognition field N grams are still the most common framework for language mo deling since their modeling capacity is hard to beat This work aims to improve over a baseline N gram mo del by using random forest LMs The peculiarity of this work is that the brute force N gram LM is trained on very large amounts of text data over 3 billion of word tokens H Loftsson E 270 I Oparin L Lamel and J L Gauvain without any pruning and cut off This model is thus robust and very challenging to improve upon The baseline recognition system is a competitive state of theart Mandarin STT system that was developed at LIMSI for and submitted to the GALE Phase 4 evaluation Language mo dels are usually evaluated by means of word error rate WER and perplexity Due to peculiarities of Mandarin Chinese it is common practice to measure speech recognition performance in terms of the character error rate CER rather than the traditional WER However WER or CER do not give an opportunity to compare LMs directly since WER CER results also include the impacts of both the acoustic mo del and the deco der Thus if the systems to be compared differ either in the acoustic component or in the deco der no LM comparison can be made The perplexity is widely used in speech recognition community to evaluate the LM independently from the full system P P 2H PM w1 w2 wm m 1 1 is a per word entropy PM is the probability assigned to a string of Where H words from a test corpus by a LM and m is the length of a string in words In the next section random forest approach RF to language mo deling is intro duced The data and experimental setup are described in Section 3 Perplexity and speech recognition accuracy results are given in Section 4 with conclusions and directions for future work presented in Section 5 2 2 1 Random Forest Language Models Decision Trees The decision tree DT mechanism for estimating probabilities of words following each other has long been known as an alternative to the N gram approach for language mo deling in STT 1 Several studies showed that stand alone DTs do not outperform traditional smoothed N gram mo dels 2 However with recent advances in language mo deling that extend the use of decision trees to that of random forests this research direction has reentered the research spotlight With the help of DTs it is possible to cluster together similar histories i e possible previous words to the one being predicted at the leaves of a tree Each leaf forms an equivalence class of histories that share the same probability distribution over words to predict Usually binary DTs are implemented in which sets of possible histories are split at every node with a yes no question If the predictor i e position in N gram history we ask questions about is the previous word a question looks like Is the previous word in the set S or S The data i e N grams corresponding to yes answers are propagated through one branch going out of a node the no data is passed along the other branch Actually a conventional N gram model can be represented as a special case of a tree mo del For example a bigram mo del may represented by a DT in which the yes set consists of one individual word at each node Ideally during the training phase all possible predictors and questions should be tried at each node to split the Large Scale Language Modeling with Random Forests 271 data and the best predictor question pair should be picked and stored for that node However in real life greedy algorithms have to be used A DT is constructed in a way to reduce the uncertainty about the event being predicted Thus entropy can naturally be used as the goo dness measure One should measure entropy for training data M in a node before split then w w C w S log C w S C S 2 A random forest is a collection of decision trees that include randomization in the tree growing algorithm The underlying assumption is that while one DT do es not generalize well to unseen data a set of randomized DTs interpolated 272 I Oparin L Lamel and J L Gauvain together might and actually should perform better Greedy algorithms are used at the stage of DT construction for choosing the best questions to split the data We also do not take into account questions asked at other nodes when searching for the one to be asked at a given node As a result trees are only locally but not globally optimal with respect to training data Randomized trees where the randomization is intro duced during tree construction i e finding the best questions to ask at each node are not locally optimal but the collection of them may be and actually is closer to the global optimum and thus these provide better results Different randomization schemes may be used to randomize DTs in order to form a RF The most commonly used metho ds are random predictor selection to ask questions about and random initialization of greedy algorithms used to find the best question in a node It should also be noted that the RF approach is also a promising framework to incorporate different sources of information into a language mo del 4 6 The RF models were shown to consistently outperform word based N gram mo dels for relatively small scale tasks e g the Wall Street Journal portion of Penn Treebank 2 4 5 7 reported improvements in recognition performance with random forest LMs trained on limited data that take account of morphological features for inflectional languages Improvements in recognition rate after rescoring N best lists generated with a conventional N gram mo del with a RF mo del were also reported for Mandarin Chinese for the GALE task with LMs trained on about 700 million word tokens of data 5 8 The setup used here is similar to the latter However our experiments are characterized by significantly larger training data size and a lower CER for the recognition baseline 3 3 1 Experiments Chinese Mandarin STT System The GALE Phase 4 Mandarin Chinese setup was used for the current experiments The system is a highly competitive one with a language mo del trained on large amounts of Mandarin Chinese data thus providing the system with robust linguistic estimates This makes improving upon the performance attained with this system a very challenging task Recognition vocabulary In written Chinese words are not separated by white spaces The natural solution is thus to make use of character based LMs or perform word segmentation as a pre pro cessing step The former was shown to be inferior to the latter 9 so the segmentation approach was taken in this work Due to ambiguity word segmentation in Chinese is not a trivial task as even native speakers of Chinese may disagree in certain cases 10 Several approaches to automatic word segmentation in Chinese exist 11 In this work we make use of the longest match algorithm based on the 56052 word vocabulary used in LIMSI STT systems developed for previous GALE Mandarin Chinese evaluations This is a simple greedy algorithm that tries to match the longest possible word according to the vocabulary adds a space after it and shifts to the Large Scale Language Modeling with Random Forests 273 next character after the space to search for another word The segmented files are used to train word based LMs The vocabulary also includes all individual Chinese characters This way we avoid the problem of having out of vo cabulary words in a text after the segmentation Decoding The speech recognizer is a development version of the LIMSI STT system used in the AGILE participation in the GALE 09 evaluation Word recognition has one deco ding chain with three passes The first pass generates a word lattice with cross word position dependent gender independent acoustic mo dels followed by consensus decoding with 4 gram and pronunciation probabilities 12 13 Unsupervised acoustic model AM adaptation is performed for each segment cluster using the CMLLR Constrained Maximum Likelihoo d Linear Regression and MLLR 14 techniques prior to the next deco ding pass The first deco ding pass is done with an MLP PLP f0 acoustic mo del the second uses a PLP F0 based mo del and the third pass also uses an MLP PLP f0 acoustic mo del All AMs are tied state left to right context dependent CD HMMs with Gaussian mixtures The triphone based CD phone mo dels are word independent but position dependent The tied states are obtained by means of a decision tree The mo dels all use speaker adaptive SAT and Maximum Mutual Information Estimation MMIE training They are trained on 1400 hours of manually transcribed broadcast news and broadcast conversation data distributed by LDC for use in the GALE program using both standard PLP and concatenated MLP PLP features For the PLP mo dels a maximum likelihoo d linear transform MLLT is also used The model sets cover about 49k phone contexts with 11 5k tied states and 32 Gaussians per state Silence is mo deled by a single state with 2048 Gaussians Initially speaker independent mo dels are trained on all of the available data and serve priors for Maximum a Posteriori MAP estimation of gender specific mo dels LM training data The LM training data consists of 48 different text sources in Mandarin Chinese These sources are collected by different institutions and are diverse in size genre and internal structure The data includes transcripts of broadcast news and broadcast conversations newspaper texts text collected from the web etc The description of the data available for the GALE Phase 3 evaluation can be found e g in 15 For the Phase 4 evaluation new text corpora of Mandarin Chinese became available Some corpora are entirely new some are the extended versions of the ones that existed before The new data were added to that used for the previous evaluations to train the language mo dels The total amount of new data is 590 32M words after segmentation It represents a 22 5 increase in the data available for training as compared to the data available for the previous evaluations 2 6G words resulting in a corpus with 3 2 billion word tokens Baseline Language Model The baseline LM is a word based 4 gram LM Individual LMs are first built for each of the 48 corpora These mo dels are smoothed 274 I Oparin L Lamel and J L Gauvain Table 1 Perplexity on different GALE evaluation sets Set Phase 3 LM Phase 4 LM dev07 181 184 eval07 206 206 dev08 194 192 dev07 eval07 dev08 193 194 dev09 234 211 dev09s 230 207 according to unmo dified interpolated Kneser Ney discount scheme 16 No cutoffs and pruning is imposed thus allowing the LMs to take account of all possible information These individual mo dels are subsequently linearly interpolated together with the interpolation weights tuned on dev09 data It should be noted that the dev data is never used it for RF tuning e g as held out data for controlling the DT growing pro cess This is done in order to keep the same test conditions and to not intro duce any biases As the number of individual mo dels is 48 one mo del is trained for each available corpora this small number of parameters do es not result is bias towards this data This is supported by the comparison with the previous baseline 4 gram mo del developed for the GALE Phase 3 evaluation That LM was tuned on the dev07 eval07 dev08 subset As can be seen from Table 1 current Phase 4 LM attains the same perplexity results on dev07 eval07 dev08 data even though it was not tuned on these and it performs significantly better on dev09 and dev09s data The GALE Phase 4 dev09 sets were used in this study to evaluate the performance of different mo dels A subset of dev09 called dev09s was also defined for this evaluation It constitutes about half of dev09 data The baseline N gram Phase 4 LM perplexity is 211 on dev09 and 207 on dev09s sets In the system submitted to Phase 4 evaluation word lattices generated with the baseline LM are subsequently rescored with the Neural Network language model NNLM 17 However in this study the NNLM was not applied in order to assess the improvement with RFLMs over N gram mo dels 3 2 Training of RF Models The SRILM compatible RF toolkit was used in these experiments with random forests 18 Growing DTs for large training data and large vocabularies is very computationally expensive We found it infeasible to do straightforward RF training for the 3 2 billion words data used to train the baseline N gram mo del Thus in our experiments we tried different strategies to train RF mo dels that would result in building RFLMs in reasonable time benefit from all the available data and improve the baseline at the same time An important feature of tree based models is that after an RF is grown i e the structure of constituent randomized DTs are established it is possible to Large Scale Language Modeling with Random Forests Table 2 Data chosen to estimate RF probabilities corpus word sampling rate sampled word bcm bnm data 19 42M 1 0 19 42M ng 315 52M 0 01 3 15M giga xin 366 96M 0 008 2 93M ibm sina 279 87M 0 01 2 79M giga cns 76 73M 0 03 2 28M 275 pour larger amounts of data down to the leaves The structure of a DT is a set of nodes and leaves together with questions that are assigned to each node A question for a binary word based DT is a position in history it is asked about e g 1 for an immediate left neighbor and words constituting yes no sets for this position Thus after a DT is grown we can propagate data down to the leaves if a particular N gram contains a word from a yes set at a specific position it is propagated along one branch if it contains a word from the no set the N gram is pushed along the other If a N gram contains a word that is in neither set the background N gram LM is used to bailout to estimate the probability Thus any N gram either ends up in the leaf or gets its probability from the backoff LM Pouring larger amounts of training data down to the leaves make probability estimations more robust since they are based on larger data RF on restricted data Decision tree training may be performed on restricted data This is the first experiment we ran before addressing the problem of making use of all available data The crucial point is thus to choose the training and heldout data that is likely to be representative of the test data For the GALE Mandarin Chinese task this is broadcast news Mandarin bnm and broadcast conversations Mandarin bcm transcribed data as it constitutes the target type of data in the evaluations The training data was chosen to contain all available bnm and bcm transcriptions except for the recent bcm and bnm data released during Phase 4 The latter was chosen as the heldout data used as a stopping criterion during the DT training phase After the structures of constituent DTs are defined the training data together with additional data is poured down to the leaves to get more robust probability estimates The additional data was taken from the remaining top four according to the interpolation weights text sources These text sources are quite large and thus were downsampled The resulting size corresponds to the weights inferred during interpolation of separate source N gram LMs to form the baseline N gram mo del see Table 2 Heldout data is usually used as a stopping criterion during the DT training phase In the SRILM compatible RFLM toolkit 18 the DTs are actually fully grown on the basis of the training data and then pruned according to gains on heldout data However in 8 it was shown that shallow RFs that contain DTs of limited depth have performance close to the RFs consisting of fully grown DTs We thus first compare the performance of RFs consisting of fully grown 276 I Oparin L Lamel and J L Gauvain and shallow DTs Another issue that needs evaluation is the number of DTs to form an RF Usually 100 or 50 randomized DTs are sufficient to train a RF The perplexity results for different RF configurations are presented in Table 3 The numbers 50 and 100 correspond to the number of randomized DTs that constitute a RF The second and third columns correspond to the performance of RFs as stand alone models while the last two columns show the perplexity when the RFs are interpolated with the baseline 4 gram LM As can be seen from this table while the RFs with DTs of maximum 1000 nodes appear to be too shallow the ones with 10000 nodes perform close to the fully grown and subsequently pruned trees There is also no really significant difference between RFs consisting of 50 and 100 trees trained on restricted data RF trained on different sources As already mentioned the baseline 4gram LM is obtained as result of interpolation of many sub LMs each being trained on one of 48 available Mandarin Chinese corpora The interpolation weights are tuned on dev09 data Applying the same strategy to RF construction seems a natural thing to do A problem arose in that the corpora are very large containing hundreds of millions words which was found infeasible to train RFs straightforwardly Thus a different strategy was utilized for these corpora Individual RFs are not trained for large corpora The DTs trained on restricted data are used instead The data of large corpora is poured down these trees Thus tree structures are the same as for the restricted data RF but the probability distributions in the leaves are estimated on the data from specific large corpora There are a total of 48 sources used to train Mandarin Chinese mo dels It was found to be feasible to train specific RFs for 34 of the sources Randomized DTs or to be more precise their structures in terms of nodes and questions asked in the nodes trained on restricted data see section 3 2 were used to pour down the counts for the larger 14 corpora Training RFs for specific sources consumes a lot of computational time and puts high demand on memory usage Training about 50 full grown RFs consisting of hundred DTs may keep busy a modern computational cluster with couple of dozens nodes for months The performance of shallow trees with maximum 10000 nodes was shown to be close to that of the full grown trees on restricted data At the same time such trees are much faster to train As a result we trained shallow DTs with a maximum of 10000 nodes for each individual corpora Another decision that was taken on the basis of these results is that 50 randomized DTs are basically enough to form a RF The final RF is obtained by interpolation of RFs corresponding to different sources We call such an RF a Forest of Random Forests FRF Table 3 Dev set perplexity for different RF configurations on dev09 set DT depth 50 DTs 100 DTs 50 DTs interp 100 DTs interp fully grown 279 4 276 1 206 8 206 4 10000 nodes 299 1 295 7 207 9 207 7 1000 nodes 358 1 356 1 210 7 210 7 Large Scale Language Modeling with Random Forests 277 4 Results Decision tree probabilities were discounted according to mo dified Kneser Ney scheme and used together with a corresponding Kneser Ney smoothed 4 gram LM The N gram LM is used as a backoff mo del This backoff mo del is trained on restricted data see section 3 2 A total of 50 randomized DTs form the random forest As already mentioned the perplexity of the dev09 and dev09s data sets are respectively 211 and 207 with the baseline 4 gram LM For dev09s set the perplexity with the RF trained on restricted data is 293 which is higher than that with the best interpolated N gram LM trained on all available data When these two are interpolated together the perplexity of this data set decreases to 201 corresponding to a 3 relative improvement However we are mostly interested in checking the results on dev09 since this set was used to tune the N gram mo dels for individual corpora in order to form the baseline N gram LM The dev09 perplexity with different RFLMs are given in Table 4 The random forests were trained on different sources as described in Section 3 2 The RF type RF corresponds to the RF trained on restricted data as described in Section 3 2 As for the dev09s setup the RF on its own performs worse than the N gram LM but a small gain in perplexity is observed when these mo dels are interpolated According to results obtained with RFLMs using smaller setups one would expect a perplexity reduction over N gram baseline with standalone RF mo dels if trained on the same data However in these restricted data experiments we by definition use much less data to train a RF and also do not make use of interpolation of LMs trained on the different data sources The FRF in Table 4 stands for the Forest of Random Forests that takes account of all available data with interpolation weights tuned on dev09 The stand alone perplexity of the FRF is 10 better than the perplexity of the RF However no further improvement after the interpolation with the baseline Ngram mo del was observed The RF minimum and maximum perplexities for individual corpora RFs are 302 and 414 The perplexities with the different individual N gram mo dels range from 485 to 2614 The perplexity distribution for individual corpora RFLMs are thus much flatter This must be due to the fact a backoff N gram LM plays significant role in RF probability estimation Another explanation of this fact is the shallow nature of randomized DTs that form RFs with the limitation of 10000 nodes that was imposed on the mo dels This makes individual mo dels smoother and as a result the interpolation of these mo dels is actually less promising as compared to that of individual N gram LM mo dels Having observed the interpolation weights for different RFs in FRF we found them considerably different from the weights of N gram mo dels that form the final N gram LM In FRF the RFs for the smallest corpora obtained unexpectedly large weights We presume the reason is as follows The mo dified KneserNey N gram LM trained on restricted data is used as a backoff mo del for the RF The Kneser Ney discount coefficients for 4 grams are rather high especially for the singleton 4 grams discount1 0 873236 discount2 1 476549 discount3 278 I Oparin L Lamel and J L Gauvain Table 4 Perplexity for differently trained RFLMs on dev09 set RF type Stand alone ppl Interpolated ppl RF 299 207 FRF 268 208 FRF with N gram weights 283 210 FRF without small corpora 279 208 RF modified KN discounts 334 208 FRF modified KN discounts 282 208 1 364567 Thus a lot of probability mass is taken from RF estimates and transferred to the N gram mo del that is used as a backoff mo del This effect is more severe in case an RF is trained on very small corpora in our setup there are four corpora that are several order of magnitude smaller than the others and thus contain many singleton 4 grams The smallest corpora got as much as 0 3 of the total weight which looks rather strange The backoff N gram LM thus contributes most to the final probability estimation At the same time the N gram backoff mo del is trained on restricted data performs well with the perplexity of 309 on dev09 Consequently at the stage of weight optimization for different RFs these mo dels may be given prominence This helps to reduce the perplexity of the stand alone FRF but not in interpolation with the baseline N gram LM In order to compensate for this effect we tried two experiments First we did not try to optimize weights for individual RFs but rather took the interpolation weights that were calculated to build the baseline N gram LM from N gram LMs corresponding to each of 48 corpora The results given in the row in FRF with N gram weights of Table 4 show that this approach do es not lead to an improvement Another possibility to compensate for the effect of unwanted backoff mo del prominence is eliminating small corpora RFs from the final interpolation We thus eliminated 4 of the 48 corpora The weights of the top corpora among the remaining 44 become less peaky but the results shown in the FRF without small corpora row in Table 4 give the impression this do es not solve the problem We also tried to hand edit the mo dified Kneser Ney discount co efficients for order 4 making them equal to 0 1 and then re estimate the individual RF mo dels This way we rely more on probability estimates for 4 grams provided by the RF mo dels and pass less probability mass to the backoff N gram LM The results for such mo dels are shown in the last two rows of Table 4 The lattices generated by the Mandarin GALE Phase 4 LIMSI STT system were rescored with the best RFLM the first one from the Table 4 The LM Table 5 CER of RF on dev09s set RF weight 0 00 0 10 0 15 0 20 0 25 0 30 0 50 1 00 CER 9 81 9 77 9 76 9 72 9 75 9 75 9 80 10 41 Large Scale Language Modeling with Random Forests 279 to generate these lattices is the baseline 4 gram LM described earlier As we already mentioned the lattices were not rescored with the neural network LM These results are presented in Table 5 The RF weight row corresponds to the weights given to the RFLM Small but significant improvement in CER over the baseline N gram mo del is observed with the RFLM 5 Conclusion and Future Work Improving over a robust state of the art STT system trained on large amounts of data is a very challenging task Many of the approaches that perform well on small and medium size tasks do not scale well to experiments on large data In this paper we presented results using random forest language mo dels to improve upon a well tuned competitive speech to text system for Mandarin Chinese Improvements both in perplexity and CER were observed However these improvements are significantly less impressive than those reported for smaller scale tasks This lessor degree of improvement can be expected for large scale tasks One can argue that the RF approach can actually be regarded as a sophisticated smoothing technique At the same time a baseline 4 gram LM with a comparatively small vocabulary of 56K words is trained on the very large corpus containing 3 2 billion words that makes the estimates provided by this mo del robust and rather reluctant to adding new ways of enhancements The results presented here are still preliminary Due to the very large size of training data and high computational demands imposed by a random forest LM several simplifications were made at the stage of RF construction E g the number of nodes was forced to be not much than 10000 while in fully grown trees it can be more than a million for very large corpora DT structures were not individually trained but only the corresponding counts were poured down the nodes of the trees trained on restricted data etc These simplifications may result in losing much of the potential gain that can be attained with RFLMs for example in the forest of random forests scenario Thus the ma jor direction of future work is performing efficient straightforward training of RF language mo dels on the same amounts of data available for N gram LM training Acknowledgments This work has been partially supported by OSEO under the Quaero program and by the GALE program Any opinions findings or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding organizations References 1 Bahl L R Brown P F de Souza P V Mercer R L A Tree Based Statistical Language Model for Natural Language Speech Recognition CSL 37 280 I Oparin L Lamel and J L Gauvain 3 Navratil J Jin Q Andrews W Campbell J P Phonetic Speaker Recognition Using Maximum Likelihood Binary Decision Tree Models In Proc of ICASSP 2003 Hon Kong pp Design and Evaluation of an Agreement Error Detection System Testing the Effect of Ambiguity Parser and Corpus Type Maite Oronoz Arantza IXA NLP Group University of the Basque Country maite oronoz a diazdeilarraza koldo gojenola ehu es http ixa si ehu es Abstract We present a system for the detection of agreement errors in Basque a language with agglutinative morphology and free order of the main sentence constituents Due to their complexity agreement errors are one of the most frequent error types found in written texts As the constituents concerning agreement can appear in any order in the sentence we have implemented a system that makes use of dependency trees of the sentence which abstract over specific constituent orders We have used Saroi a tool that obtains the analysis trees that fulfill a set of restrictions described by means of declarative rules This tool is applied to the output of two dependency analyzers MaltIxa data driven and EDGK rule based The system has been evaluated on two corpora a group of texts containing errors and another one composed of correct texts As a secondary result we have also estimated a measure of the impact of syntactic ambiguity on the quality of the results Keywords Grammar error ambiguity parsing 1 Introduction Detection of grammatical errors is a relevant area of study in computer assisted language learning and grammar checking This paper presents the implementation and evaluation of a system for the detection of agreement errors in Basque regarded as one of the most frequent kinds of error 1 iaz de Ilarraza et Referring to the pro cess of detecting grammatical errors H Loftsson E 282 M Oronoz A After the analysis of dependencies the system will make use of Saroi 3 a tool that given a set of dependency trees obtains those that fulfill the set of restrictions described by means of declarative rules Although the tool is useful for several types of tree inspection processes in this work we will use it for the detection of ungrammatical structures Saroi will be applied to the outputs of two dependency analyzers EDGK a knowledge based dependency parser 4 and MaltIxa 5 a data driven parser based on Maltparser a freely available and state of the art parser 6 For the evaluation of the system texts containing errors and correct texts from the Basque Dependency Treebank 7 will be used We are also concerned about the impact of morphosyntactic ambiguity in the quality of our system A lot of error detection has been carried out on English for which this kind of ambiguity is less of an issue but in morphologically rich languages a deep analysis of the influence of ambiguity in error detection is in our opinion fundamental Among the three main types of ambiguity that can be relevant to grammatical error treatment morphological syntactic and semantic our study will concentrate on measuring the effect of morphological and syntactic ambiguity in the results leaving aside semantic ambiguity The remainder of this paper is organized as follows After this intro duction section 2 relates our work to similar systems Section 3 comments on general aspects of agreement errors in Basque Section 4 will describe the linguistic resources used for the analysis of incorrect texts corpora and the main computational tools two dependency analyzers and Saroi a tool for tree inspection Section 5 will present the experiments performed and the main results obtained We conclude the paper in section 6 with our main contributions 2 A Bird s Eye View of Error Detection Techniques Approaches to grammatical error detection correction are difficult to compare due to mainly the following reasons i most of them concentrate on one error type and ii the lack of large available error corpora Choosing the more appropiate technique to the problem of error detection is not a trivial decision Empirical and knowledge based approaches can be used for this purpose Empirical approaches are suitable for error types related to the omission replacement or addition of elements For example Tetreault and Cho dorow 8 use machine learning techniques to detect errors involving prepositions in non native English speakers A deeply studied area using machine learning techniques is that of context sensitive spelling correction 9 where the ob jective is to detect errors due to word confusion e g to too Bigert and Knutsson 10 prove that precision is significantly improved when unsupervised metho ds are combined with linguistic information Regarding knowledge based methods many types of local syntactic errors have been detected by means of tools based on finite state automata or transducers such as Constraint Grammar CG 11 The Xerox Finite State Tool 12 or ad hoc systems Systems based on finite state techniques usually define error patterns enco ded in the form of rules which are applied to the analyzed texts Design and Evaluation of an Agreement Error Detection System 283 For global error treatment approaches based on context free grammars CFG or finite state techniques have been used For example CFG based systems have experimented with the relaxation of some constraints in the grammar 13 or have specially developed error grammars 14 Statistical parsers have also been used that get a measure of grammaticality 15 The relationship between ambiguity and error detection has been mentioned in very few o ccasions 16 17 Similarly to most NLP areas the development of tools for grammatical error detection finds ambiguity as a main obstacle for the design of efficient and accurate systems Birn 16 states that the errors accumulated through morphological and syntactic analysis make it difficult to detect grammatical errors 3 Agreement Errors in Basque Basque is an agglutinative language with free order among the elements of the sentence When classifying the errors related to agreement we can distinguish three types of contexts Table 1 Agreement error sbj subject obj object erg ergative abs absolutive Zentral nuklear r ak zakar erradiaktiboa eratzen dute Power station nuclear 0 the abs pl det rubish radioactive abs sg create aux sbj erg 3pl obj abs 3sg The nuclear power station create radioactive rubbish 284 M Oronoz A 4 4 1 General Linguistic Resources The Corpus The task of creating large sets of ungrammatical sentences is a necessary but time consuming activity A corpus of this type can be composed of sentences pro duced by language learners learner corpus or it can be taken from a general error corpus not necessarily produced in a language learning context 14 Although some approaches propose the automatic creation of ungrammatical sentences 18 we decided to use a set of genuine errors For evaluation we use two corpora 4 2 Syntactic Analysis The creation of NLP tools is a very expensive task so instead of preparing specially tailored resources for error processing we decided to use the existing systems in our group and perform the necessary adaptations to deal with illformed sentences For the analysis of the input texts we use the syntactic analysis chain for Basque 19 It is composed of three main components see figure 1 Design and Evaluation of an Agreement Error Detection System 285 Raw text Tokenizer Morphological analyzer Multi words MORFEUS Linguistic CG stochastic HMM disambiguation EUSTAGGER Named entities Shallow syntactic function disambiguation Noun and verb chains Morphosyntactic processing Chunking Level M1 M2 M3 M4 S1 S2 Linguistic features POS POS SubPOS POS SubPOS Case M3 rest of features SF nom verb chunks S1 main SF Method CG HMM CG HMM CG HMM CG CG CG EDGK Analyzed text MaltIxa Analyzed text Fig 1 The syntactic analysis chain for Basque and the disambiguation levels in it For the detection of agreement errors we applied Saroi a system developed to apply a set of query rules to dependency trees Saroi takes as input a group of analysis trees and a group of rules and obtains as output the dependency trees that fulfill the conditions described in the rules Its main general ob jective is the analysis of any linguistic phenomena in corpora 1 Labeled Attachment Score 286 M Oronoz A eratzen ncsubj zentral power station create agreement subj case n nk Detect ncsubj ncmod auxmod type nor nork ncsubj ncmod case ncobj zakar rubbish auxmod dute nor object ncmod nuklearrak nuclear case num per absolutive plural 3 ncmod erradiaktiboa radioactive case num per case num per absolutive singular 3 ergative singular 3 nork subject ERROR Fig 2 A rule left side detecting the agreement error in the dependency tree right side of the sentence in Basque Nuclear power station create radioactive rubbish Figure 2 shows an example of a rule that detects the error in the dependency tree of the same figure In the sentence the sub ject zentral nuklearrak nuclear power station in absolutive case and the auxiliary verb dute linked to the main verb eratzen create and which needs a sub ject in ergative do not agree Saroi uses as input the result of the syntactic analyzer see section 4 2 in which the relations between the elements of the sentence are ambiguous as a result of the remaining morphosyntactic ambiguity see figure 3 in which for example nuklearrak has 3 interpretations Then Saroi constructs all the set of non ambiguous trees starting from an initially ambiguous tree figure 3 The detection rules are applied to the expanded set of dependency trees eratzen 10 ncsubj ncobj auxmod 10 ncsubj ncobj auxmod 1 ncmod 2 6 ncmod 7 11 1 ncmod 2 10 ncsubj ncobj auxmod 6 ncmod 9 11 zentral 1 ncmod ncmod zakar 6 ncmod dute 11 nuklearrak erradiaktiboa 2 4 5 7 9 Fig 3 Ambiguous tree and some of its corresponding non ambiguous trees 5 Experiments In this section we will first comment on the experimental settings of the evaluation 5 1 and 5 2 and then we will present the results obtained 5 4 5 1 Preprocessing When using the predefined linguistic analysis chain for the detection of agreement errors we had to take several aspects into account Design and Evaluation of an Agreement Error Detection System 287 Considering the problems mentioned in section 5 1 and being concerned about the impact of ambiguity in the quality of our analyzers we followed these steps 1 We chose the best option for morphological and syntactic disambiguation 2 Once we decided the appropiate disambiguation level we evaluated the system using two corpora correct and error corpora An important remark regarding evaluation is that we will not apply the standard development refinement test cycle but instead we will follow a development test metho dology a design of error detection rules in Saroi and b evaluation This means that there will not be a second step for the refinement of the rules after examining their results on a development set Our aim was to test the effectiveness of a set of clean error detecting rules over different settings corpus parser and ambiguity In that respect the rules examine clear and possibly naive linguistic statements e g the sub ject and verb must agree in case and number This also means that there will be room for improvement of the results after adapting the error detection rules to the details of real and or noisy data 2 http www ei ehu es 288 M Oronoz A 5 3 Election of the Disambiguation Level Due to morphosyntactic and syntactic ambiguity a number of trees ranging from 1 to more than 100 are generated for each sentence Taking into account the combinations of morphosyntactic and shallow syntactic function disambiguation levels the best disambiguation criteria should be those that a detect the highest number of errors in ungrammatical sentences b give the lowest number of false alarms in grammatical sentences and c generate the lowest number of analysis trees for each sentence efficiency With this ob jective we followed two steps 1 First we chose the best morphosyntactic disambiguation level 2 Second after the morphosyntactic disambiguation level was fixed we selected the best option for shallow syntactic function disambiguation For that reason we selected a set of 10 ungrammatical sentences and their respective corrections one for each sentence that is a total of 20 sentences The sentences were analyzed with the eight disambiguation combinations3 giving the results shown in table 2 The two combinations that generate the lowest number of trees with acceptable detection and false alarm rates were those performing the deepest morphosyntactic disambiguation that is M34 S1 and S2 Table 2 Looking for the best morphosyntactic disambiguation combination Disambiguation combinations M1 S1 M2 S1 M3 S1 M4 S1 M1 S2 M2 S2 M3 S2 M4 S2 Number of trees 67 7 67 7 27 8 46 7 22 11 22 11 11 6 11 62 Errors in ungrammatical 5 5 6 6 5 5 6 6 False alarms in grammatical 0 0 1 1 0 0 1 0 Next we performed a deeper analysis to choose the best syntactic function disambiguation level S1 or S2 We soon realized that the grammar that assigns the dependency relations to correct texts need of relaxation when applied to ill formed ones For example in the sentence nik ez nago konforme I do not agree the word nik I was not tagged as subject as it carries the ergative case and the auxiliary verb asks for a sub ject in absolutive this is a constraint in the dependency grammar when assigning the subject tag We experimented relaxing all the conditions referred to the type of auxiliary in the rules assigning subject object and indirect object relations This relaxation is not performed for error detection this is done by means of error detection rules but it is necessary for the assigning of dependency relations to ungrammatical sentences Then in a second experiment we used a set of 75 sentences containing agreement errors together with their corrections The sentences were analyzed with the M3 S1 Relaxed M3 S1 NotRelaxed M3 S2 Relaxed and M3 S2 NotRelaxed combinations The best results were obtained with the M3 S2 Relaxed option 3 4 8 combinations 4 morphosyntactic 2 syntactic Although the M4 S2 combination in table 2 seems to be good it sometimes creates too many trees and in other cases it does not obtain any analysis tree Design and Evaluation of an Agreement Error Detection System 289 that is the option that disambiguates the most and with the relaxed dependency relation assignment A deeper study about the impact of ambiguity in error detection is described in 2 5 4 Evaluation of the System After these tests we noticed that the results are directly proportional to the parser s accuracy When the relations are wrongly assigned the detection of agreement errors is difficult Sometimes a false detection occurs that is an erroneous sentence is flagged as incorrect but with a rule that is not the expected one The rules mark the sentence as incorrect but they fail in the diagnosis Correct corpora We evaluated our system against the Basque Dependency Treebank Its relations are presumably perfect there is no need of a parser neither the problem of ambiguity nor partial parsing so the system should perform well This experiment served to evaluate the system on false alarms A subset containing 1906 trees was used After applying the detection rules 161 errors were flagged 8 45 of the corpus As this implies a high false alarm rate we made a detailed analysis table 3 finding out that Table 3 Evaluation results on the Basque Treebank Flagged by the system Numb From FA From treebank Not considered FA 90 55 9 4 72 FA 63 39 13 3 30 Real errors 8 4 97 Total 161 8 45 290 M Oronoz A Error corpora We also performed an evaluation of the system on error corpora using both EDGK and MaltIxa We applied the agreement detection rules to all the possible analysis trees of the sentences We calculated four results 1 Using a data driven parser MaltIxa M 2 The knowledge based parser EDGK E 3 MaltIxa and EDGK M E An error will be marked if it is flagged in the dependency trees obtained by MaltIxa and EDGK 4 MaltIxa or EDGK M E If an error is flagged on the output of either of the syntactic analyzers the sentence will be deemed erroneous Examining the results in table 4 we see that when applying the full set of error detection rules precision varies between 24 26 and 26 19 As could be expected the best precision results were reached with the option M E when the error is flagged in the trees analyzed by both analyzers the system is certain about the error However recall falls down 24 44 In general looking to both precision and recall the two best options seem to be M and M E In general the data driven parser gets better results with correct texts and it also behaves better with incorrect sentences showing a robust behaviour There are two error detection rules named two subj and two obj that account for most of the false alarms both with EDGK and MaltIxa These rules mark the attachment of two sub jects ob jects to a verb This phenomenon can occur as a consequence of a genuine agreement error but also because of an incorrect dependency analysis and is the reason for many false alarms We think that as the frequency of incorrect analysis trees is relatively high these rules cause more harm than good For that reason we perform three experiments to confirm this assumption In the second row of table 4 we show the results without considering the rule that detects two sub jects two subj In the third one the rule that checks the appearance of two ob jects is removed two obj Table 4 Agreement error detection with MaltIxa and edgk All the rules M E M M Without the rule M two subj E M M Without the rule M two obj E M M Without the rules M two subj E and M two obj M Number of errors Number of words Correctly detected 28 17 11 33 26 14 10 29 22 12 8 26 21 10 8 23 FA Detected P R F 81 109 25 68 62 22 36 35 53 70 24 28 37 77 29 56 31 42 26 19 24 44 25 28 103 136 24 26 73 33 36 45 59 85 30 58 57 77 39 99 35 49 28 57 31 11 29 78 21 31 32 25 22 22 26 31 73 102 28 43 64 44 39 45 55 77 28 57 48 88 36 06 39 51 23 52 26 66 24 99 19 27 29 62 17 77 22 21 75 101 25 74 57 77 35 61 33 54 38 88 46 66 42 41 19 29 34 48 22 22 27 02 10 18 44 44 17 77 25 38 42 65 35 38 51 11 41 81 45 4995 E E E E E E E E Design and Evaluation of an Agreement Error Detection System 291 and finally the last row shows the result of removing both rules The best results are obtained in the last case 44 44 precision in the M E option against the worse recall 17 77 and f score of 25 38 Considering precision and recall MaltIxa gives the best results 38 88 precision and 46 66 recall f score 42 41 6 Conclusions and Future Work In this work we have presented a set of experiments on agreement error detection applied to an agglutinative and free constituent order language For this we have used Saroi a tool built for the inspection of dependency trees The tool allows us to design restrictions by means of query rules to be applied on the output of dependency parsers In the evaluation we have experimented tuning the ambiguity of the analysis chain we have used two general purpose dependency parsers and two types of corpora When analyzing the Basque Dependency Treebank we have detected illformed dependency trees that is manual annotation mistakes Additionally we have evaluated our system regarding false alarms obtaining a false alarm rate of 3 30 Most of the alarms could be easily avoided improving the verb subcategorization schemas we use and in this way leaving a minimal false alarm rate In consequence we think that the precision of our grammar rules is high One of the main problems is the lack of coverage of the dependency analyzers When the trees are not syntactically well formed the system is more prone to signal a false alarm Any improvement in syntactic analysis will have a positive effect on the error detection system In the future we want to analyze how complementary are MaltIxa and EDGK and how they could be combined to obtain suitable analysis trees Working with real texts also led us to consider the problem of ambiguity The best results are obtained when using the deepest disambiguation level both morphosyntactic and syntactic This can be explained by the explosion in the number of trees when all the ambiguity is considered References 1 Zubiri I 292 M Oronoz A 7 Aduriz I Aranzabe M Arriola J M Atutxa A TectoMT Modular NLP Framework Charles University in Prague Institute of Formal and Applied Linguistics popel zabokrtsky ufal mff cuni cz Abstract In the present paper we describe TectoMT a multi purpose open source NLP framework It allows for fast and efficient development of NLP applications by exploiting a wide range of software modules already integrated in TectoMT such as tools for sentence segmentation tokenization morphological analysis POS tagging shallow and deep syntax parsing named entity recognition anaphora resolution tree to tree translation natural language generation word level alignment of parallel corpora and other tasks One of the most complex applications of TectoMT is the English Czech machine translation system with transfer on deep syntactic tectogrammatical layer Several modules are available also for other languages German Russian Arabic Where possible modules are implemented in a language independent way so they can be reused in many applications Keywords NLP framework linguistic processing pipeline TectoMT 1 Introduction Most non trivial NLP natural language pro cessing applications exploit several tools e g tokenizers taggers parsers that process data in a pipeline For developers of NLP applications it is beneficial to reuse available existing tools and integrate them in the pro cessing pipeline However it is often the case that the developer has to spend more time with the integration and other auxiliary work than with the development of new tools and innovative approaches The auxiliary work involves studying do cumentation of the reused tools compiling and adjusting the tools in order to run them on the developer s computer training models if these are needed and not included with the tools writing scripts for data conversions the tools may require different input format or enco ding resolving incompatibilities between the tools e g different tagsets assumed etc Such a work is inefficient and frustrating Moreover if it is done in an ad ho c style it must be done again for other applications The described drawbacks can be reduced or eliminated by using an NLP framework that integrates the needed tools so the tools can be combined into various pipelines serving for different purposes Most of the auxiliary work is already implemented in the framework and developers can fo cus on the more creative part of their tasks Some frameworks enable easy addition of thirdparty tools usually using so called wrappers and development of new mo dules within the framework H Loftsson E 294 M Popel and Z In this paper we report on a novel NLP framework called TectoMT 1 In Sect 2 we describe its architecture and main concepts Sect 3 concerns implementation issues Finally in Sect 4 we briefly describe and compare other NLP frameworks 2 2 1 TectoMT Architecture Blocks and Scenarios TectoMT framework emphasizes mo dularity and reusability at various levels Following the fundamental assumption that every non trivial NLP task can be decomposed into a sequence of subsequent steps these steps are implemented as reusable components called blocks Each block has a well defined and documented input and output specification and also a linguistically interpretable functionality in most cases This facilitates rapid development of new applications by simply listing the names of existing blocks to be applied to the data Moreover blocks in this sequence which is called scenario can be easily substituted with an alternative solution other blocks which attempts at solving the same subtask using a different approach or metho d 2 For example the task of morphological and shallow syntax analysis and disambiguation for English text consists of five steps sentence segmentation tokenization part of speech tagging lemmatization and parsing In TectoMT we can arrange various scenarios to solve this task for example Scenario A Sentence_segmentation_simple Penn_style_tokenization TagMxPost Lemmatize_mtree McD_parser Scenario B Each_line_as_sentence Tokenize_and_tag Lemmatize_mtree Malt_parser In the scenario A tokenization and tagging is done separately in two blocks Penn_style_tokenization and TagMxPost respectively whereas in the scenario B the same two steps are done in one block at once Tokenize_and_tag Also different parsers are used 3 1 2 3 http ufal mff cuni cz tectomt Scenarios can be adjusted also by specifying parameters for individual blocks Using parameters we can define for instance which model should be used for parsing Penn_style_tokenization is a rule based block for tokenization according to Penn Treebank guidelines http www cis upenn edu treebank tokenization html TagMxPost uses Adwait Ratnaparkhi s tagger 1 Tokenize_and_tag uses Aaron Coburn s Lingua EN Tagger CPAN module Lemmatize_mtree is a block for English lemmatization handling verbs noun plurals comparatives superlatives and negative prefixes It uses a set of rules about one hundred regular expressions inspired by morpha 2 and a list of words with irregular lemmatization McD_parser uses MST parser 0 4 3b 3 Malt_parser uses Malt parser 1 3 1 4 TectoMT Modular NLP Framework 295 TectoMT currently includes over 400 Applications in TectoMT correspond to end to end NLP tasks be they real end user applications such as machine translation or only NLP related experiments Applications usually consist of three phases 1 conversion of the input data to the TectoMT internal format possibly split into more files 2 applying a scenario i e a sequence of blocks to the files 3 conversion of the resulting files to the desired output format Technically applications are often implemented as Makefiles which only glue the three phases Besides developing the English Czech translation system 5 TectoMT was also used in applications such as 296 M Popel and Z 2 3 Layers of Language Description TectoMT profits from the stratificational approach to the language namely it defines four layers of language description listed in the order of increasing level of abstraction raw text word layer w layer morphological layer m layer shallow syntax layer analytical layer a layer and deep syntax layer layer of linguistic meaning tectogrammatical layer t layer The strategy is adopted from the Functional Generative Description theory 17 which has been further elaborated and implemented in the Prague Dependency Treebank PDT 18 We give here only a very brief summary of the key points Every do cument is saved in one file and consists of a sequence of sentences Each sentence is represented by a structure called bundle which stands for a bundle of trees Each tree can be classified according to TectoMT Modular NLP Framework 297 Fig 1 English Czech parallel text annotated on three layers of language description is saved in a TectoMT document each sentence in one bundle We show only a simplified representation of the trees in the first bundle 4 In the near future TectoMT will migrate to using ISO 639 language codes e g ar cs en de instead of full names 298 M Popel and Z Fig 2 Vauquois diagram for translation with transfer on tectogrammatical layer A layer and t layer structures are dependency trees so it is natural to handle them as tree data structures M layer structure is a sequence of tokens with asso ciated attributes which is handled in TectoMT as a special case of a tree all nodes except the technical root are leaves of the tree W layer raw text is represented as string attributes stored within bundles 3 3 1 TectoMT Implementation Design Decisions TectoMT is implemented in Perl programming language under Linux This do es not exclude the possibility of releasing platform independent applications made of selected components platform independent solutions are always preferred in TectoMT TectoMT modules are programmed in ob ject oriented programming style using inside out classes following 22 Some of the mo dules are just Perl wrappers for tools written in other languages especially Java and C TectoMT is a mo dern multilingual framework and it uses open standards such as Unico de and XML TectoMT is neutral with respect to the metho dology employed in the individual blocks fully stochastic hybrid or fully rule based approaches can be used 3 2 TectoMT Components and Directory Structure Each block is a Perl class inherited from TectoMT Block and each block is saved in one file The blocks are distributed into directories according to the languages and layers on which they operate For example all blocks for deep syntactic analysis of English i e for generating t trees from a trees are stored in a directory SEnglishA_to_SEnglishT In this paper we use for simplicity only short names TectoMT Modular NLP Framework 299 0 1 Fig 3 Components of the TectoMT framework of blocks so e g instead of the full name SEnglishA_to_SEnglishT Assign_grammatemes we write only Assign_grammatemes TectoMT is composed of two parts see Fig 3 The first part the versioned part which contains TectoMT core classes and utilities format converters blocks applications and in house tools is stored in an SVN repository so that it can be developed in parallel by more developers The second part the shared part which contains linguistic data resources downloaded third party tools and the software for visualization of TectoMT files Tree editor TrEd 23 is shared without versioning because a it is supposed to be changed rather additively b it is huge as it contains large data resources and c it should be automatically reconstructible simply by downloading and installing the needed components 3 3 Data Formats The internal TectoMT format tmt files is an XML with a schema defined in Prague Markup Language 24 It is similar to the format used for the Prague Dependency Treebank 2 0 18 but all representations of a textual do cument at the individual layers of language description are stored in a single file TectoMT includes converters for various formats and corpora e g Penn Treebank 25 CoNLL 12 EMILLE 26 PADT 27 Scenarios are saved in plain text files with a simple format that enables including of other scenarios 300 M Popel and Z 3 4 Parallel Processing of Large Data TectoMT can be used for pro cessing huge data resources using a cluster of computers 5 There are utilities that take care of input data distribution filtering parallel pro cessing logging error checks and output data collection In order to allow efficient and flawless processing input data i e corpora should be distributed into many tmt files with about 50 to 200 sentences per file Each file can be pro cessed independently so this metho d scales well to any number of computers in a cluster For example the best version of translation from English to Czech takes about 1 2 seconds per sentence plus 90 seconds for initial loading of blocks in memory per computer more precisely per cluster job 6 Using 20 computers in a cluster we can translate 2000 sentences in less than 4 minutes TectoMT was used to automatically annotate the parallel treebank CzEng 0 9 8 with 8 million sentences 93 million English and 82 million Czech words 4 Other NLP Frameworks In Tab 1 we summarize some properties of TectoMT and four other NLP frameworks 5 6 7 8 We use Sun Grid Engine http gridengine sunsource net Most of the initialization time is spent with loading translation and language models about 8 GiB Other applications presented in this paper are not so resourcedemanding so they are loaded in a few seconds http opennlp sourceforge net http weblicht sfs uni tuebingen de englisch index shtml TectoMT Modular NLP Framework 301 Table 1 Comparison of NLP frameworks Notes a ETAP 3 is a closed source project only a small demo is available developed since license for public use main prog language linguistic theory strictly rule based main applicatione uses deep syntax 5 Summary TectoMT is a multilingual NLP framework with a wide range of applications and integrated tools Its main properties are 302 M Popel and Z Acknowledgments This work was supported by the grants GAUK 116310 LC536 and FP7 ICT 2007 3 231720 EuroMatrix MSM0021620838 MSMT CR Plus We thank three anonymous reviewers for helpful comments References 1 Ratnaparkhi A A maximum entropy part of speech tagger In Proceedings of the conference on Empirical Methods in Natural Language Processing pp TectoMT Modular NLP Framework 303 13 Romportl J Zvy 304 M Popel and Z 29 Cunningham H Maynard D Bontcheva K Tablan V GATE an architecture for development of robust HLT applications In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics July 07 12 2002 30 Mel cuk I A Towards a functioning model of language Mouton 1970 31 Tyers F M Using Information from the Target Language to Improve Crosslingual Text Classification Gabriela Laboratory of Language Technologies National Institute for Astrophysics Optics and Electronics gabrielarr mmontesg villasen inaoep mx Faculty of Computer Science Autonomous University of Puebla dpinto cs buap mx 3 Department of Computer and Information Sciences University of Alabama at Birmingham solorio uab edu 1 2 Abstract Crosslingual text classification consists of exploiting labeled documents in a source language to classify documents in a different target language In addition to the evident translation problem this task also faces some difficulties caused by the cultural discrepancies manifested in both languages by means of different topic distributions Such discrepancies make the classifier unreliable for the categorization task In order to tackle this problem we propose to improve the classification performance by using information embedded in the own target dataset The central idea of the proposed approach is that similar documents must belong to the same category Therefore it classifies the documents by considering not only their own content but also information about the assigned category to other similar documents from the same target dataset Experimental results using three different languages evidence the appropriateness of the proposed approach Keywords Crosslingual text classification prototype based method unlabeled documents text classification 1 Introduction Text classification is the task of assigning do cuments into a set of predefined classes or topics 1 The leading approach for this task considers the application of machine learning techniques such as Support Vector Machines and H Loftsson E 306 G consists in exploiting labeled documents in a source language to classify do cuments in a different target language Because of the inherent language barrier problem of this approach most current CLTC metho ds have mainly addressed different translation issues In particular they have explored the translation from one language to another by means of machine translation approaches as well as by multilingual lexical resources such as dictionaries and ontologies 2 3 Although the language barrier is an important problem in CLTC it is not the only one It is clear that in spite of a perfect translation there are also some cultural discrepancies manifested in both languages that will affect the classification performance That is given that a language is the way of expression of a cultural and socially homogeneous community documents from the same category but different languages i e different cultures may concern very different topics As an example consider the case of news about sports from France in French and from USA in English while the first will include more do cuments about so ccer rugby and cricket the latter will mainly consider notes about baseball basketball and American football In order to tackle this problem recent CLTC metho ds have proposed to enhance the classification mo del by iteratively incorporating information from the target language into the training phase 4 5 6 their purpose is to obtain a classification mo del that is as close as possible to the target topic distribution The metho d proposed in this paper is a simple and inexpensive alternative for facing the problems caused the cultural discrepancies between both languages Different to previous iterative approaches it does not consider the mo dification or enrichment of the original classifier instead it attempts to improve the document classification by using more information to support the decision pro cess Mainly it is based on the idea that similar do cuments must belong to the same category and therefore it classifies the do cuments by considering their own information as usual as well as the information about the assigned category to other similar do cuments from the same target dataset In the following section we describe the proposed metho d for CLTC This metho d is based on the prototype based classification approach 7 but mo difies the traditional class assignment strategy in order to incorporate information from the set of similar do cuments Then in Section 3 we define the experimental configuration and show results in six different pairs of languages that demonstrate the usefulness of the proposed approach for CLTC Finally in Section 4 we present our conclusions and some ideas for future work 2 Prototype Based CLTC Method Given that prototype based classification is very simple and has demonstrated to consistently outperform other algorithms such as Using Information from the Target Language 307 training set is more similar to it and to its nearest neighbors from the same target language dataset Figure 1 shows the general schema of the proposed metho d It consists of four main pro cesses The first one carries out the translation of the training documents from the source language S to the target language T The second pro cess fo cuses on the construction of the class prototypes using the well known normalized sum technique 8 The third process involves the identification of the nearest neighbors for each do cument from the target language dataset DT Finally the fourth pro cess computes the classification for each do cument d DT considering information from their own and their neighbors Bellow we present a brief description of each one of these pro cesses Fig 1 General scheme of the proposed text classification method Document Translation Two basic architectures have been explored for CLTC one based on the translation of the target dataset to the source language and another one based on the translation of the training set to the target language We decided to adopt the latter option because training sets are commonly smaller than test sets and therefore their translation tend to be less expensive In particular the translation was achieved using the Worldlingo online translation machine1 Prototype Construction This pro cess carries out the construction of the class prototypes based on information from 1 http www worldlingo com es products services worldlingo translator html 308 G set organized in a predefined set of classes C and represented in their own term space it computes the prototype vector for each class ci C using Formula 1 Pi 1 d ci d d d ci 1 Nearest Neighbors Identification This process fo cuses on the identification of the k nearest neighbors for each do cument di from the target dataset DT refer to Formula 2 In order to do that we compute the similarity between two do cuments di and all other d in DT using the cosine formula refer to Formula 4 di Nk argmaxSj Sk d Sj sim d di 2 where Sk and sim are defined as follows Sk S S DT S k sim di dj Class Assignment In prototype based classification the class of a do cument d from the target dataset is traditionally determined by Formula 5 Our proposal extends this class assignment strategy by considering not only information from the do cument itself but also information about the assigned category to other similar do cuments from the same target dataset In particular given a do cument from the target dataset d DT in conjunction with its k nearest neighbors d we assign a class to d using Formula 6 Nk class d argmaxi sim d Pi 1 class d argmaxi sim d Pi 1 k where d nj Nk 5 inf d nj Using Information from the Target Language 309 3 3 1 Evaluation Datasets For the experiments we considered a subset of the Reuters RCV 1 Corpus 9 This subset considers three languages English French and Spanish and the news reports corresponding to four classes Crime Disasters Politics and Sports For each language we used 320 documents 80 per each class2 3 2 Evaluation Measure The evaluation of the performance of the proposed metho d was carried out by means of the F measure This measure is a linear combination of the precision and recall values from all class ci C It is defined as follows F M easure 1 C C i 1 7 Recall ci number of correct predictions of ci number of examples of ci number of correct predictions of ci number of predictions as ci 8 9 P recision ci 3 3 Baseline Experiments The goal of these experiments was to evaluate the performance of a traditional CLTC approach where do cuments from a source language are used to classify do cuments from a different target language For these experiments we applied the following standard pro cedure first we translated the training do cuments from the source language to the target language using Worldlingo then we constructed a classifier in the target language using the translated training set finally we used the built classifier to determine the class of each document from the target language dataset For the construction of the classifier we considered three of the most used metho ds for text classification namely 2 3 This corpus can be downloaded from http ccc inaoep mx mmontesg resources CLTC RCV Subset txt For NB and SVM we used the implementation and default configuration of WEKA 10 310 G Table 1 F measure results for six crosslingual experiments using a traditional CLTC approach Source language English English French French Spanish Spanish Target language French Spanish English Spanish English French Experiment PBC EF F ES S FE E FS S SE E SF F 0 616 0 814 0 956 0 879 0 851 0 790 NB 0 753 0 791 0 931 0 882 0 891 0 802 SVM 0 764 0 625 0 616 0 658 0 486 0 723 using the class assignment function described in Formula 5 Table 1 shows the Fmeasure results obtained by these metho ds in six crosslingual experiments which correspond to all possible pair combinations of the three selected languages From these results those by PBC are of special interest since our metho d is an extension of this approach 3 4 Results from the Proposed Method As described in Section 2 the main idea of the proposed method is to classify the do cuments by considering not only their own content but also information from other similar do cuments from the same target dataset Particularly we adapted the traditional prototype based approach PBC to capture this information refer to Formula 6 being a constant that determines the relative importance of both components Considering the proposed metho d we designed some experiments in such a way that we could evaluate the impact on the classification results caused by the selection of different values of as well as the impact caused by the usage of different number of neighbor do cuments into the class assignment process In particular we used 0 0 1 0 2 1 and k 1 30 Experiments showed that the best results were achieved when using small values of indicating that information from the neighbor documents is of great relevance On the other hand they could not indicate a clear conclusion about the appropriate number of neighbors since several different values allowed to obtain similar classification improvements Figure 2 shows some results of the proposed method in the six crosslingual experiments These results correspond to three different values of 0 0 1 and 0 2 This figure also shows the results from the traditional prototype based approach which correspond to our metho d results using 1 The achieved results indicate that the proposed metho d clearly outperforms the traditional prototype based approach In order to summarize the results from the experimental evaluation Table 2 presents the best results achieved by the proposed metho d Comparing these results against those from Table 1 it is possible to notice that our metho d outperformed all used classification algorithms in all except one of the crosslingual Using Information from the Target Language 311 a Experiment EF F b Experiment SF F c Experiment ES S d Experiment FS S e Experiment FE E f Experiment SE E Fig 2 F measure results of the proposed method in the six crosslingual experiments using different values of and numbers of neighbors k The straight line corresponds to the PBC baseline result 1 experiments demonstrating the usefulness of considering information from the target dataset in crosslingual text classification At this point it is important to clarify that several different configurations of our metho d as shown in Figure 2 allowed obtaining competitive classification results One example is the configuration defined by 0 1 and k 11 which 312 G also outperformed most baseline results as shown in the last column of Table 2 We evaluated the statistical significance of the best achieved results using the z test with a confidence of 95 a indicates that the improvement over the PCB is statistically significant whereas a indicates the same regarding the best baseline result 4 Conclusions and Future Work In addition to the evident translation problem crosslingual text classification CLTC also faces some difficulties caused by the cultural discrepancies manifested in both languages by means of different topic distributions In this paper we proposed a simple and inexpensive approach for facing this problem This approach is based on the idea that similar do cuments must belong to the same category and therefore it classifies the documents by considering their own information as usual as well as the information about the assigned category to other similar do cuments In particular we implemented the proposed approach using the prototypebased classification algorithm In our implementation the decision about the category of each do cument from the target language is determined by the class whose prototype calculated from the training set is more similar to it and to its nearest neighbors from the same target language dataset This way the proposed metho d determines the category of do cuments taking advantage of information from the two languages As future work we plan to carry out an extensive analysis of several crosslingual experiments using different languages and a larger number of do cuments to establish a simple criterion for determining the appropriate values for parameters and k Once defined this criterion we also plan to use the proposed approach in conjunction with a semi supervised metho d as the one described by Rigutini et al 4 Our goal is to enhance the selection of the do cuments that will be iteratively included in the training set and consequently to obtain a classification mo del that is as close as possible to the target language distribution Acknowledgments This work was done under partial support of CONACyTMexico project grants 83459 82050 106013 and 106625 and scholarship 239516 Using Information from the Target Language 313 References 1 Sebastiani F Machine learning in automated text categorization ACM Computing Surveys 34 Event Detection Using Lexical Chain Sangeetha S 1 R S Thakur2 and Michael Aro ck3 Department of Computer Applications National Institute of Technology 1 2 Abstract This paper describes a new architecture for event detection from text documents The proposed system correctly identifies the sentences that describe an event of interest to extract its participants It follows an unsupervised method for identifying the lexical chains from the raw sentences taken as a training data The lexical chain constructed using Wordnet lexicon is then used for identifying event mention The significance of the proposed system is it is the first system that applies lexical chain for event identification The entire architecture is divided into three tasks namely natural language pre processing lexical chain construction and event detection Keywords Event Extraction Lexical chain 1 Introduction Information extraction IE 1 is the process of extracting the structured information from the unstructured text IE systems were evaluated by Message Understanding Conferences MUC till 1998 Automatic content extraction ACE programme is the successor of MUC with the ob jective of developing extraction technology to support automatic pro cessing of source language data Event identification and characterization of ACE 3 programme identifies events by making use of event triggers Event trigger is the word that clearly expresses the o ccurrence of an event Event mention is the sentence in which the event is described An event comprises event participants which are the entities that participate in the event with different roles As the supervised machine learning requires a large set of event annotated data our approach uses unsupervised method for extracting the events The proposed metho d constructs the lexical chain from the un annotated or raw sentences of training do cuments Section 4 explains how the training do cuments are constructed Lexical chain holds the set of semantically related words of a given sentence or a document from which it was obtained Word net lexicon 15 is used for constructing lexical chain in the proposed work The significance of H Loftsson E Event Detection Using Lexical Chain 315 the proposed system is it is the first system that uses lexical chain for event extraction to the best of our knowledge The entire architecture is divided into three tasks namely natural language pre processing lexical chain construction and event detection The remaining sections of the paper are organized as follows The next section describes the related work in the field of information extraction generally and event extraction specifically Section 3 provides a description of proposed approach Section 4 explains how the training do cuments are constructed and partial experimental results and Section 5 concludes the paper 2 Related Work McCracken et al 2 had combined statistical and knowledge based technique for extracting events Its main fo cus was on summary report genre It had extracted the factual accounting of incidents from a person s life This resource had covered every instance of every verb in the corpus Xu et al 5 had developed a metho d for identifying event extent event trigger and event argument automatically using bootstrapping metho d The work had extracted the events from the Nobel Prize winning domain by obtaining extraction rules from the text fragments using binary relations as seeds Abuleil 4 proposed a metho d which extracted events by breaking each event into elements analysed and understood the syntax of each element identified the role played by each element in the event and how they formed relationship between related events David Ahn 7 broke down the task of extracting the events into subtasks such as anchor identification argument identification attribute assignment and event co reference Each task was performed with the help of machine learned classifier Some of the learners used are memory based learners and maximum entropy learners Aone et al 6 has identified events by tagging the text with name and noun phrase using pattern matching techniques and has resolved co references using rule based approach It did not consider the semantic relatedness All the above said existing systems extract the events without considering the semantic features of the text However consideration of meaning of the text improves the efficiency of event extraction and the information extraction on the whole The actual meaning of a sentence is identified from the meaning of individual words Our approach uses lexical chain for identifying the events because lexical chain holds the semantically related words The concept lexical chain 8 is based on the cohesion 9 which is a metho d for sticking different part of the text which is semantically related Lexical chaining has been used in various tasks such as text summarization 12 word sense disambiguation and web information retrieval 11 Naughton et al 10 stated that event identification using manual extraction of trigger terms from Lexicon such as Wordnet performs well Our approach improves this trigger by adding some more related words of the same concept by constructing lexical chain Naughton et al 10 manually extracted the terms related to an event type In our approach 316 Sangeetha S R S Thakur and M Arock we learn all the words used in the sentences which represent an event and also the related words from Wordnet semantic relations This includes more related words and will improve the result From the literature we have identified that a lexical chain holds set of semantically related words b to identify an event we need the words related to a particular event and also c to the best of our knowledge there is no event extraction system that uses lexical chain so far Hence we propose an architecture which identifies an event by constructing LC based on training sentences 3 Proposed Work The proposed architecture is broadly classified into three phases Fig 1 namely prepro cessing lexical chain construction and event detection Fig 1 Architecture Overview Pre processing Prepro cessing includes part of speech tagging stop word removal and named entity identification Collect the sentences describing a particular event of interest to form a single do cument and pass them as input to the prepro cessing stage We have adopted standard tree bank POS tagger for part of speech tagging and ACE named entity chunking available in 14 At the end of prepro cessing stage we obtain the tokenized tagged words from the training sentences along with the types of named entities Lexical Chain Construction The prepro cessing phase produces output as a list of words collected from all the training sentences These words are the candidate words for constructing lexical chain Several algorithms have been proposed for lexical chain We have used the algorithm developed by 8 with some enhancement using the Word net Event Detection Using Lexical Chain 317 lexicon Moriss and Hirst constructed lexical chain which includes only the words available in the text which are semantically related In our approach along with the words in the text we have included the words in the Word net lexicon that are used to establish the semantic relation between any two words in the text Moreover the proposed approach includes noun verb and adjective words as all these POS can act as an event trigger according to ACE 3 Since we are using the words in the lexical chain as a trigger word for identifying the event our pro cedure learns not only the semantically related words from the training data but also the related words from the Word net lexicon The pro cedure for constructing the lexical chain is given below All the senses of each candidate words are extracted Each sense of the word is represented by the set of words in its synonym at 0th level and hypernym hyponym meronym holonym at level 1 For each pair of candidate words each sense of the word Wi is compared with all other senses of word Wj If the match occurs the words Wi Wj and the list of matched words in the sense representation are included in the chain Along with the matched words path length with respect to the current level of matching and frequency of the word are also stored The frequent words with minimum path length are included in the lexical chain The list of all words forms overall LC From the set of LC s formed we manually select the LC which is related to the event As the sentences in the training do cuments are related to a particular event the required LC is always longer than other LC s constructed Event Detection and Extraction In a given do cument the sentences are searched for words in lexical chain Sentence score is calculated based on how many words of LC it contains The sentences with highest score are the sentences representing the type of event This metho d can be adopted to any type of event by selecting the training sentences denoting the particular event type We mark the event mention as relevant if its score is above threshold More number of words in the lexical chain reflects the large set of words related to the event which in turn identify large set of event mentions As a result the final precision and recall will improve Keeping this in mind we are pro ceeding our research and obtained initial set of results in Fig 2 which shows how redundancy in training do cument affects the number of words in the lexical chain To extract the event participants the event mention should be tokenized POS tagged stemmed and named entities should be recognized From the lexical chains patterns using preposition and type of named entities are identified to extract the required fields Then sub ject and ob ject of the sentence are identified which denote the agent and theme of the event which can be mapped to event participants 4 Experiment and Discussions We have collected 50 documents representing the education and administrative positions held by various international leaders from Wikipedia 13 The do cuments 318 Sangeetha S R S Thakur and M Arock are chosen in such a way that it uses different words to represent education and administrative positions details in each document Among the 50 do cuments we have used 25 do cuments for training and remaining 25 do cuments for testing The sentences in the training do cuments are manually separated into sentences describing education details and sentences describing details of administrative positions held and they are grouped under two different do cuments These do cuments are used as input to the preprocessing stage In our experiment first we have considered the do cument with the sentences related to education This system is under development Till now we have completed the construction of the lexical chain In this paper we illustrate the preliminary result of our proposed architecture If the number of words in lexical chain is greater we can identify the wide range event mentions which will improve the final precision and recall value Thus importance should be given to the number of words in the lexical chain Redundancy of context words in the training do cument is the main factor which is affecting the number of words in LC Our experimental result confirms it From the empirical analysis we have identified that if candidate words are less redundant in the same context the number of words in the lexical chain increases Whereas if redundancy is introduced in the context words of the training document number of words in the lexical chain decreases Fig 2 shows it as the preliminary result Fig 2 Lexical chaining vs redundancy in context words Event Detection Using Lexical Chain 319 5 Conclusion The proposed work provides an architecture that extracts the events by using lexical chains Most of the existing systems extract the events without considering the semantics features of the text However consideration of semantics of the text improves the efficiency of event extraction and the information extraction on the whole Our approach identifies the semantic relationship between words and constructs lexical chains based on the relationship Lexical chain is used to identify the event mention within the documents as relevant or irrelevant based on the sentence score From our empirical results we conclude that less redundant training sentences will acquire more number of unique words in the lexical chain and large set of words in lexical chain correctly identify large set of event mentions References 1 Cunningham H Information Extraction Automatic Encyclopaedia of Language and Linguistics Using Comparable Corpora to Improve the Effectiveness of Cross Language Information Retrieval Fatiha Sadat University of Quebec in Montreal Computer Science department 201 President Kennedy avenue Montreal QC Canada sadat fatiha uqam ca Abstract Large scale comparable corpora became more abundant and accessible than parallel corpora with the explosive growth of the World Wide Web From the Cross Language Information Retrieval point of view limitation of translation resources as well as ambiguity arising due to failure to translate query terms is largely responsible for large drops in the effectiveness below monolingual performance Therefore strategies on bilingual terminology extraction from comparable texts must be given more attention in order to enrich existing bilingual lexicons and thesauri and to enhance Cross Language Information Retrieval In the present paper we focus on the enhancement of CrossLanguage Information Retrieval using a two stage corpus based translation model that includes bi directional extraction of bilingual terminology from comparable corpora and selection of best translation alternatives on the basis of their morphological knowledge The impact of comparable corpora on the performance of the Cross Language Information Retrieval process is evaluated in this study and the results indicate that the effect is clearly positive especially when using the linear combination with bilingual dictionaries and JapaneseEnglish pair of languages Keywords Cross language information retrieval comparable corpora similarity translation disambiguation 1 Introduction Cross Language Information Retrieval CLIR deals with the problem of presenting an information retrieval task in one language and retrieving documents in one or several other languages The main methods for CLIR are presented in overviews by Oard Diekema 1998 and Pirkola et al 2001 Methods based on the translation of queries are categorized into either i dictionary based translation ii machine translation iii methods using parallel corpora and other approaches which are based on other existing linguistic resources such as the thesaurus However according to previous research Sadat et al 2002 the main problems which are largely responsible for large drops in the effectiveness of CLIR below monolingual performance are listed as follows First is the problem of inflection A commonly used method to deal with inflected words is to remove affixes from word forms The method is called stemming Morphological analysis also allows the normalization of words forms into H Loftsson E Using Comparable Corpora to Improve the Effectiveness 321 their base forms The second problem associated to CLIR is related to the translation ambiguity arising from polysemous words Third is the problem of compounds phrases multi words e g specialized vocabulary and their handling and failure of translation Compound words form an important part of natural language since compounding is a major way of forming new words From the information retrieval IR point of view compounds may be content bearing words in natural language sentences and therefore important for the retrieval result Hedlund 2002 The problem with compound handling for CLIR is acknowledged for many languages The forth problem encountered in CLIR is related to proper names named entities and other untranslatable words using existing translation tools bilingual dictionary and or machine translation Lexical coverage of existing bilingual dictionary limitation of general purpose dictionaries especially for specialized vocabulary and inexistence of translation tools for pairs of languages are among the merging problems that could be solved using large scale corpora In recent years two types of multilingual corpora have been an object of studies and research related to natural language processing and information retrieval parallel corpora and comparable corpora The parallel corpora are made up of original texts and their translations This allows texts to be aligned and used in applications such as computer aided translator training and machine translation systems This method could be expensive for any pair of languages or even not applicable for some languages which are characterized by few amounts of Web pages on the Web On the other hand non aligned comparable corpora more abundant and accessible resources than parallel corpora have been given a special interest in bilingual terminology acquisition and lexical resources enrichment Dagan Itai 1994 Dejean et al 2002 Diab Finch 2000 Fung 2000 Gaussier et al 2004 Kaji 2003 Koehn Graehl 2002 Nakagawa 2000 Peters Picchi 1995 Rapp 1999 Sadat et al 2003a Sadat et al 2003b Sadat et al 2003c Sadat 2004 Shahzad et al 1999 Tanaka Iwasaki 1996 Utsuro et al 2002 Utsuro et al 2003 Comparable corpora are defined as collections of texts from pairs or multiples of languages which can be contrasted because of their common features in the topic the domain the authors the time period etc Comparable corpora could be collected from downloading electronic copies of newspapers and articles on the WWW for any specified domain This paper intends to bring solutions to the problem of lexical coverage of existing bilingual dictionaries but also to the improvement of the performance of CLIR The main contributions concern the enhancement of CLIR by an automatic acquisition of bilingual terminology from comparable corpora that will help cope with the limitation of CLIR especially in the query disambiguation process as well as during the query expansion with related terms Furthermore this study could be valuable for the extraction of unknown words and their translation and thus the enrichment and enhancement of bilingual dictionaries Therefore we present in this paper an approach of learning bilingual terminology from textual resources other than bilingual dictionaries such as comparable corpora and evaluations on CLIR First we propose a twostage corpus based translation model for the acquisition of bilingual terminology from comparable corpora The first stage concerns the extraction of bilingual translations from the source language to the target language also from the target language to the source language The two results are combined for the purpose of disambiguation In the second stage the extracted translation alternatives are filtered on the basis of their 322 F Sadat morphological knowledge A linguistics based pruning technique is applied in order to compare source words and their target language translation equivalents on the basis of their part of speech tags Furthermore we present a combined translation model involving the comparable corpora and readily available bilingual dictionaries In our evaluations we used a large scale test collection on Japanese English and different weighting schemes of SMART retrieval system and confirmed the effectiveness of the proposed translation model in CLIR The remainder of the present paper is organized as follows Section 2 presents an overview of the proposed model Section 3 presents the two stage corpus based translation model Section 4 introduces a combination of different translation models Experiments and evaluations in CLIR are related in Section 5 Section 6 concludes the present paper 2 An Overview of the Proposed Model Throughout this paper we will seek to exploit and explore benefits from collections of news articles for the acquisition of bilingual terminology in order to enrich existing multilingual lexical resources and help cross the language barrier for information retrieval We rely on such comparable corpora for the extraction of bilingual terminology in the form of translations and or expansion terms i e words that will help the query expansion in CLIR First a linguistic preprocessing is performed on the comparable corpora in order to replace each term with its inflectional root to remove most plural word forms to replace each verb with its infinitive form to remove stop words and stop phrases and finally to extract content words such as nouns verbs adjectives adverbs and foreign words which will constitute the main target of our study Second the task of bilingual terminology extraction is accomplished by a twostage corpus based translation model which is described in detail in Section 3 Third a linear combination involving the comparable corpora and bilingual dictionaries is completed in order to select best translation candidates of the source terms of a given query Finally documents are retrieved in the target language 3 Two Stage Corpus Based Translation Model A two stage corpus based translation model Sadat et al 2003a Sadat et al 2003b Sadat et al 2003c which is based on the symmetrical criterion in addition to the assumption of similar collocation aims to find translations of the source word in the target language corpus but also translations of the target words in the source language corpus Linguistic resources were used in the two stage corpus based translation model as follows i a collection of news articles from Mainichi Newspapers 19981999 for Japanese and Mainichi Daily News 1998 1999 for English were considered as comparable corpora because of their common feature on the time period Documents of NTCIR 2 test collection were also considered as comparable corpora in order to cope with special features of the test collection during evaluations ii morphological analyzers ChaSen version 2 2 9 Matsumoto et al 1997 for texts in Japanese and OAK Sekine 2001 for English texts were used in linguistic processing iii Using Comparable Corpora to Improve the Effectiveness 1 323 EDR 1996 and EDICT bilingual Japanese English and English Japanese dictionaries were considered in the translation of context vectors of source and target languages Japanese words written in Katakana representing foreign words and proper names that were not found in the bilingual dictionaries were manually translated A transliteration process could be used in order to convert those words to their English equivalence 3 1 First Stage in the Proposed Translation Model The two stage corpus based translation model for the acquisition of bilingual terminology is described as follows 1 2 3 A simple bilingual terminology acquisition from source language to target language to yield a first simple translation model represented by similarity vectors SIMST A simple bilingual terminology acquisition from target language to source language to yield a second simple translation model represented by similarity vectors SIM T S Merge the first and second models to yield a two stage translation model based on bi directional comparable corpora and represented by similarity vectors SIMST The simple approach for bilingual terminology acquisition from comparable corpora is based on the assumption of similar collocation i e If two words are mutual translations then their most frequent collocates are likely to be mutual translations as well We follow strategies of previous researches Dejean et al 2002 Fung 2000 Rapp 1999 Sadat et al 2003a Sadat et al 2003b Sadat et al 2003c The approach is described as follows First word frequencies context word frequencies in surrounding positions here three words window are estimated following statistics based metrics Context vectors for each term in the source language are constructed As well context vectors for terms in the target language are constructed We use the loglikelihood ratio Dunning 1993 for the estimation of context frequencies for each pair of words in either the source language or target language as expressed in equation 1 LLR w i wj K11log K11N K12N K21N K22N K12log K21 log K22log C1R1 C1R2 C2R1 C2R2 1 where C1 K11 K12 C2 K21 K22 R1 K11 K21 R2 K12 K22 N K11 K12 K21 K22 K11 frequency of common occurrences of word wi and word wj in a specified window size of the monolingual corpus K12 corpus frequency of word wi in the corpus K11 K21 corpus frequency of word wj in the corpus K11 K22 N K11 K12 K21 Second context vectors of words in the source language are translated into the target language using a bilingual seed lexicon We consider all translation candidates keeping the same context frequency value as the source word This step requires a 1 http www csse monash edu au jwb wwwjdic html 324 F Sadat seed lexicon that will be enriched using the proposed bootstrapping approach of this paper The third step is the construction of similarity vectors for each pair of source word and target word Context vectors original and translated of words in both languages are compared using the cosine metrics Salton McGill 1983 as expressed in equation 2 Finally similarity vectors are normalized to yield a simple corpus based translation model Similarity wi w j v k k ik v jk 2 jk v v 2 ik k 2 where vik represents co occurrence frequencies of the source word wi with word wk The word wk is found in the translated context vectors of the source word wi vjk represents co occurrence frequencies of the target word wj with the word wk The word wk is found in the context vectors of the target word wj The merging strategy in the first stage of the two stage corpus based translation model is represented by the following equation 3 SIMST s t simST t s s t simST t s SIMST t s simT S s t SIMT S simST t s simST t s 3 Similarity vectors SIMST and SIMTS for the first and second models are constructed and merged to yield a bi directional acquisition of bilingual terminology from source language to target language The merging process will keep common pairs of source term and target translation s t which appear in SIMST as pairs s t but also in SIMTS as pairs t s to result in combined similarity vectors SIMST for each pair s t The product of similarity values simST t s and simTS s t of vectors SIMST and SIMTS consecutively will result in similarity values simST t s of vectors SIMST which will represent the first stage of the two stage corpus based translation model In further sections we name the simple approach for bilingual terminology acquisition from comparable corpora as simple corpus based translation and the translation model representing the first stage of the two stage corpus based translation as bi directional corpus based translation 3 2 Second Stage in the Proposed Translation Model Combining linguistic and statistical methods is becoming increasingly common in computational linguistics especially as more corpora become available Klavens Tzoukermann 1996 Sadat et al 2003c We propose to integrate linguistic concepts into the corpus based translation model Morphological knowledge such as Part ofSpeech POS tags context of terms etc could be valuable to filter and prune the extracted translation candidates The objective of the linguistics based pruning technique is the detection of terms and their translations that are morphologically close enough i e close or similar POS tags This proposed approach will select a fixed Using Comparable Corpora to Improve the Effectiveness 325 number of equivalents from the set of extracted target translation alternatives that match the Part of Speech of the source term POS tags are assigned to each source term Japanese via morphological analysis As well a target language morphological analysis will assign POS tags to the translation candidates We restricted the pruning technique to nouns verbs adjectives and adverbs although other POS tags could be treated in similar way For Japanese2 English pair of languages Japanese nouns MEISHI are compared to English nouns NN and Japanese verbs DOUSHI to English verbs VB Japanese adverbs FUKUSHI are compared to English adverbs RB and adjectives JJ while Japanese adjectives KEIYOUSHI are compared to English adverbs RB and adjectives JJ This is because most adverbs in Japanese are formed from adjectives Thus we select pairs of source term and target translation s t such as POS s NN and POS t MEISHI POS s VB and POS t DOUSHI POS s RB and POS t FUKUSHI or POS t KEIYOUSHI POS s JJ and POS t KEIYOUSHI or POS t FUKUSHI Note that Japanese vocabulary is frequently imported from other languages primarily but not exclusively from English The special phonetic alphabet here Japanese katakana is used to write down foreign words technical terms proper nouns and loanwords e g names of persons and entities Japanese foreign words were not pruned with the proposed linguistics based technique but could be treated via transliteration i e conversion of Japanese katakana to their English equivalence or to the alphabetical description of their pronunciation Knight Graehl 1998 Finally the generated translation alternatives are sorted in decreasing order by similarity values Rank counts are assigned in increasing order starting at 1 for the first sorted list item A fixed number of top ranked translation alternatives are selected and misleading candidates are discarded 4 Combining Different Translation Models Combining different translation models has showed success in previous research Dejean et al 2002 We propose a combined translation model involving comparable corpora and readily available bilingual dictionaries The proposed dictionary based translation model is derived directly from readily available bilingual dictionaries by considering all translation candidates of each source entry as equiprobable to yield a probabilistic translation model P2 t s The linear combination will involve the two probabilistic translation models P1 t s and P2 t s derived from the comparable corpora either the simple or the two stage model and readily available bilingual dictionaries respectively as follows 2 English POS tags NN refers to noun VB to verb RB to adverb JJ to adjective while Japanese POS tags MEISHI refers to noun DOUSHI to verb FUKUSHI to adverb and KEIYOUSHI to adjective with respect to their extensions 326 F Sadat P t s i pi t s Parameters 1 and 2 represent mixture weights of each translation source with i 1 Although the mixture weights could be adjusted using the EM algoi i rithm Dejean et al 2002 individual translation sources were assigned equiprobable weights in these preliminary evaluations Japanese vocabulary is frequently imported from other languages primarily but not exclusively from English Katakana the special phonetic alphabet is used to write down foreign words and loanwords example names of persons and other terms A probabilistic translation model representing the transliteration could be integrated in the combined model as well 5 Evaluation and Experiments Experiments have been carried out in order to measure the improvement of our proposal on bilingual terminology acquisition from comparable corpora on JapaneseEnglish tasks in CLIR i e Japanese queries to retrieve English documents 5 1 Evaluations on the Corpus Based Translation Model We considered the set of news articles as well as the abstracts of NTCIR 2 test collection as comparable corpora for Japanese English language pairs NTCIR2 contains abstracts from academic conference papers with much technical terms that may not be found in the standard dictionaries or in general domain news articles The abstracts of NTCIR 2 test collection are partially aligned more than half are Japanese English paired documents but the alignment was not considered in the present research in order to treat the set of documents as comparable Content words nouns verbs adjectives adverbs and Foreign words were extracted from English and Japanese corpora Context vectors were constructed for 13 552 481 Japanese terms and 1 517 281 English terms Similarity vectors were constructed for 96 895 255 Japanese English pairs of terms and 92 765 129 English Japanese pairs of terms Bi directional similarity vectors after merging and disambiguation resulted in 58 254 841 Japanese English pairs of terms 5 2 Evaluations on the Retrieval System Conducted experiments and evaluations were completed using a large scale test collection NTCIR 2 Kando 2001 SMART information retrieval system Salton 1971 which is based on vector model was used to retrieve English documents We used the monolingual English runs i e English queries to retrieve English documents and the bilingual Japanese English runs i e Japanese queries to retrieve English documents Topics of NTCIR 2 collection numbered 0101 to 0149 were considered and key terms contained in the fields title TITLE description DESCRIPTION and concept CONCEPT were used to generate 49 queries in Japanese and in English There is a variety of techniques implemented in SMART to calculate weights for individual terms in both documents and queries These weighting techniques are formulated by combining three parameters Term Frequency component Inverted Using Comparable Corpora to Improve the Effectiveness 327 Document Frequency component and Vector Normalization component The standard SMART notation to describe the combined schemes is XXX YYY The three characters to the left XXX and right YYY of the period refer to the document and query vector components respectively For example ATC ATN applies augmented normalized term frequency Bilingual translations were extracted from the collection of news articles using the simple translation model and the two stage translation model A fixed number p set to five of top ranked translation alternatives was retained for evaluations in CLIR Results and performances on the monolingual run as well as on the bilingual runs using the two stage corpus based translation model and the linear combination to bilingual dictionaries are illustrated in Table 1 Evaluations are based on the average precision differences in term of average precision of the monolingual counterpart and the improvement over the monolingual counterpart Retrieval methods are represented by the monolingual retrieval Mono dictionarybased translation DT the simple corpus based translation model SCT the bidirectional corpus based translation model BCT the two stage corpus based translation model TCT Linear combinations were represented by SCT DT for the combined simple corpus based translation and bilingual dictionaries BCT DT for the combined bidirectional corpus based translation and bilingual dictionaries TCT DT for the combined two stage corpus based translation and bilingual dictionaries Our interest Table 1 Evaluations on the proposed translation models using ATN NTC weighting schemes of SMART retrieval system Average Precision Monolingual and Improvement Mono DT SCT BCT TCT P 5 SCT DT BCT DT TCT DT 0 3368 0 2279 0 1417 0 1801 0 2008 0 2366 0 2721 0 2987 100 67 66 42 07 53 47 59 62 70 25 80 79 88 69 328 F Sadat in the present research is related to the evaluation of the proposed two stage translation model TCT and the combination to bilingual dictionaries TCT DT over the other retrieval methods As illustrated in Table1 the bi directional corpus based translation model BCT showed a better improvement in terms of average precision compared to the simple corpus based translation model SCT with 27 1 The two stage corpus based translation model TCT showed better performance in terms of average precision with 41 7 and 11 5 compared to SCT and to BCT respectively Linear combination of simple or comparable corpora and bilingual dictionaries showed better performances in terms of average precision compared to the models stand alone SCT DT showed 70 25 of the monolingual counterpart Mono 3 82 compared to the dictionary based translation DT and 66 97 compared to the simple corpus based translation SCT in the case of ATN NTC weighting scheme BCT DT showed better improvement with 80 79 of the monolingual counterpart Mono 19 4 of the dictionary based translation DT 51 08 of BCT and 15 of the combined SCT DT The proposed hybrid combination TCT DT of the two stage corpus based translation model and bilingual dictionaries showed the best performance with 88 69 of the monolingual retrieval in the case of ATN NTC weighting scheme TCT DT showed an improvement of 9 7 of the combined BCT DT and 26 24 of the combined SCT DT Furthermore the different improvements in term of average precision were noticed through all weighting schemes of SMART retrieval system with ATN NTC showing the best results for all the retrieval methods involved in the present study Thus key techniques used in the proposed two stage corpus based translation model for bilingual terminology acquisition from comparable corpora can be summarized as follows The acquisition of bilingual terminology from bi directional comparable corpora yields a significantly better result than using the simple model The bi directional corpus based translation model is considered as one kind of symmetric probabilistic model that provides a disambiguation of the extracted translation alternatives and helps improve the accuracy of translation extraction The bi directional approach is more effective than the simple approach in a way that it includes a disambiguation process for the extracted translation alternatives Evaluations in CLIR showed an improvement of 27 1 in terms of mean average precision for the bi directional corpus based translation model of the simple corpus based translation model the main average precision goes from 0 1417 to 0 1801 for the ATN NTC weighting scheme The approach based on bi directional comparable corpora largely affected the translation because related words could be added as translation alternatives or expansion terms Linguistics based pruning technique has allowed a great improvement in the effectiveness of CLIR Therefore morphological knowledge such as part of speech could provide a valuable resource in filtering and pruning the translation candidates Combining different translation models yields a significantly better result than using each model by itself Translation models based on comparable corpora and bilingual dictionaries have completed each other and their linear combination has provided a valuable resource for query translation expansion in CLIR and has allowed an improvement in the effectiveness of information retrieval Using Comparable Corpora to Improve the Effectiveness 329 6 Conclusion In the present paper we investigated the approach of extracting bilingual terminology from comparable corpora in order to enhance CLIR especially in the disambiguation and query expansion processes and possibly enrich existing bilingual lexicons We proposed a two stage corpus based translation model consisting of bi directional extraction of bilingual terminology and linguistic based pruning Among the drawbacks of the proposed translation process is the introduction of many noisy terms or wrongly translated terms however most of those terms could be considered as efficient for the query expansion in CLIR but not for the translation Combination of two stage corpus based translation model and bilingual dictionaries yields to better translations and an effectiveness of information retrieval could be achieved across Japanese and English languages Further extensions include an integration of a transliteration model in the hybrid combination especially for loanwords and foreign words of Japanese language Second possible extension is the decomposition of the large scale corpora into comparable pieces instead of taking the whole corpus as a single piece could be investigated in the future Third translation on a word by word basis is not applicable to all compounds and technical terms especially when dealing with Japanese language Thus the problem of multiword phrases and compounds should be solved Evaluations using other combinations and more efficient weighting schemes that are not included in SMART retrieval system such as OKAPI which showed great success in information retrieval are among the future subjects of our research on CLIR References 1 Buckley C Allan J Salton G Automatic Routing and Ad hoc Retrieval using SMART In Proceedings of the Second Text Retrieval Conference TREC 2 pp 330 F Sadat 8 Fox A E Shaw A J Combination of Multiple Searches In Proceedings of the Second Text Retrieval Conference TREC 2 pp Using Comparable Corpora to Improve the Effectiveness 331 27 Renders J M Dejean H Gaussier E Assessing Automatically Extracted Bilingual Lexicons for CLIR in Vertical Domains XRCE Participation in the GIRT Track of CLEF 2002 In Peters C Braschler M Gonzalo J eds CLEF 2002 LNCS vol 2785 Springer Heidelberg 2003 28 Sadat F Maeda A Yoshikawa M Uemura S Exploiting and Combining Multiple Resources for Query Expansion in Cross Language Information Retrieval IPSJ Transactions of Databases 43 SIG 9 TOD 15 Semi automatic Endogenous Enrichment of Collaboratively Constructed Lexical Resources Piggybacking onto Wiktionary Franck Sa jous1 Emmanuel Navarro2 Bruno Gaume1 Laurent 1 CLLE ERSS CNRS Abstract The lack of large scale freely available and durable lexical resources and the consequences for NLP is widely acknowledged but the attempts to cope with usual bottlenecks preventing their development often result in dead ends This article introduces a language independent semi automatic and endogenous method for enriching lexical resources based on collaborative editing and random walks through existing lexical relationships and shows how this approach enables us to overcome recurrent impediments It compares the impact of using different data sources and similarity measures on the task of improving synonymy networks Finally it defines an architecture for applying the presented method to Wiktionary and explains how it has been implemented Keywords Collaboratively Constructed Lexical Resources Endogenous Enrichment Crowdsourcing Wiktionary Random Walks 1 Introduction While emerging pro cesses of creation and diffusion keep increasing the pro duction of digital do cuments the tools to process them still suffer from a lack of acceptable linguistic resources for most languages We desperately need linguistic resources is claimed in 1 after arguing that it is not realistic to assume that large scale resources can all be developed by a single institute or a small group of people and concluding that a collaborative effort is needed and that sharing resources is crucial In this paper we propose a new metho d for developing lexical resources which could meet these needs and we apply it to Wiktionary 1 the free online dictionary The system we describe automatically computes semantic relations namely synonyms to be added or not to a lexical network after being validated or invalidated by contributors In Section 2 we take inventory of the usual approaches and point out the impediments that hinder the success of such pro cesses We then investigate new trends which could help overcome this shortcoming We outline in Section 3 the key points of our metho d based on a 1 http www wiktionary org H Loftsson E Semi automatic Enrichment of Collaborative Lexical Resources 333 semi automatic endogenous enrichment process We explain in Section 4 how we compute the candidate relations by random walks over various graphs and using several measures that we evaluate regarding our specific purpose We present the architecture built to carry out the whole enrichment validation system in Section 5 and we describe possible future extensions of out metho d in Section 6 2 2 1 Lexical Resource Building Context Princeton WordNet 2 is probably the only successful large scale project among lexical resource building attempts which is widely used The subsequent projects EuroWordNet 3 and BalkaNet 4 were less ambitious in terms of coverage Moreover these resources froze as soon as the projects ended while Princeton WordNet kept on evolving EuroWordNet s weaknesses have been underlined in 5 and automatic metho ds to add missing lexical relations have been proposed Existing resources have been used in 6 to build WOLF a free French WordNet Pattern based approaches were first proposed in 7 to harvest semantic relations from corpora and refined in 8 by reducing the need for human supervision All of the latter three automatic processes would require validation by experts to pro duce reliable results However the cost of this validation work makes it difficult to afford or results in resources that are not freely accessible The problems of time cost and availability are increasingly becoming a matter of concern in corpus linguistics an AGILE like metho d borrowed from Computer Science has been proposed in 9 to address the problem of simultaneously maximizing corpus size and annotations while minimizing the time and cost involved in the creation of corpora To tackle the availability issue and build free corpora a metho d relying on metadata to automatically detect copylefted web pages is described in 10 In the domain of lexical resource building metho ds relying on crowdsourcing may help overcome recurrent bottlenecks 2 2 Collaboratively Constructed Resources CCR It has been claimed in 11 that the accuracy of Wikipedia comes close to Britanica who criticized the criteria of the evaluation 12 A more mo derate study 13 has shown in a task measuring the semantic relatedness of words that resources based on the wisdom of crowds are not superior to resources based on the wisdom of linguists but that CCRs are strongly competitive It has also been demonstrated that crowds can outperform linguists in term of coverage Collaborative and so cial approaches to resource building do not rely only on colleagues or students but on random people who do not share the NLP researchers interest for linguistic resource building Therefore building sophisticated and costly infrastructures that are empty shells waiting to be filled presents the risk of being platforms that no one would visit Indeed in the current web landscape competition for visitors is difficult and empty shells as promising as 334 F Sajous et al they can be are not attracting many people Any infrastructure that underestimates and do es not answer this attractiveness issue is doomed to fail However there are at least two main tracks to follow in order to avoid this pitfall Gamers Some language resource builders have been successful in designing simple web games in which many people come to play just for fun For instance the French serious game Jeux de Mots 2 14 has been useful for collecting a great number of relations between words mostly non typed asso ciative relations but also better defined lexico semantic relations such as hypernymy meronymy etc However setting up an interesting game for collecting any kind of linguistic information is not easily feasible For instance domain specific resources might be harder to collect this way Secondly designing game play that really works is a difficult task in itself and it is likely that many game elicited resource initiatives will fail because of the game not being fun for random people Piggybackers Only a few collaborative or so cial infrastructures are really successful These resources and networks concentrate the ma jority of internet users Merely being asso ciated with one of these success stories affords the possibility of crowds of visitors Wiktionary and Wikipedia are probably the best examples The NLP community can offer some services to the users of these resources in order to take advantage of their huge amounts of visitors and contributors Significant steps towards such an architecture have been made in 15 16 Generalizing this approach to so cial networks while adding a gaming dimension is also possible and constitutes an interesting avenue to be explored Moreover simply adding plugins to existing solid and popular infrastructures requires much less effort and technical skill than setting up the whole platform though lots of technical difficulties o ccur to comply with and plug into these infrastructures 3 Outline of Proposal for a New Approach Taking into account the observations made in Section 2 and considering the benefits of using CCRs we propose a metho d for enhancing lexical resources that is reasonable in terms of time and cost based on i piggybacking onto Wiktionary ii computing similarity measures grounded on random walks through the graphs extracted from its lexical networks Sections 4 2 and 4 3 and iii giving an easy way for users to validate the candidate relations that we suggest 3 1 Wiktionary Wiktionary is a free multilingual collaborative dictionary including definitions semantic relations and translations a detailed presentation can be found in 15 16 Its intrinsic features fulfill some of our needs it is publicly available its growth is fast and continuous and as its content is based on crowdsourcing the 2 See http www lirmm fr jeuxdemots jdm accueil php Semi automatic Enrichment of Collaborative Lexical Resources 335 reasonable cost constraint turns euphemistic However what is the quality of resources constructed by naive speakers as compared to those built by skilled professional lexicographers A recent study 17 evaluated three German resources designed in different manners expert built GermaNet semi controlled OpenThesaurus and collaboratively edited German Wiktionary This comparison demonstrated that all resources have a similar topology3 and lexical coverage but different density of semantic relations for instance Wiktionary has fewer hypernyms hyponyms than GermanNet but clearly outperforms both other resources in term of antonymy relations Table 1 gives the number of common nouns verbs adjectives and undirected synonymy and translation links for the French and English Wiktionaries in 2008 and 2010 These figures relate to all lexemes found conversely in 16 only the lexemes connected by synonymy links have been counted Translation and synonymy links have been counted after the graphs have been symmetrized i e two way links are counted once Table 1 Growth of French and English Wiktionaries from year 2008 to 2010 2008 2010 Nouns Verbs Adj Nouns Verbs Adj Lexemes 38 973 6 968 11 787 106 068 As we can see the number of lexemes has seen a growth that makes Wiktionary for these languages comparable to commercial printed dictionaries in term of lexical coverage the French Petit Robert includes 60 000 entries and the Longman Dictionary of Contemporary English features 50 000 entries Moreover all the resources that capture some aspect of linguistic knowledge can prove to be useful and interesting So traditional resources and collaborative resources should both continue to be developed especially since as mentioned by 19 their content do es not overlap too much Regarding semantic relations we have shown the sparseness of the synonymy networks extracted from Wiktionary in 2008 16 Synonymy relations grew at slower rate than lexeme coverage which makes the 2010 graphs even more sparse To help fill this gap we present below an endogenous enrichment metho d 3 2 Endogenous Enrichment Our aim is to be able to propose for an existing semantic lexical network new relations that are potentially missing To propose new pairs of words which may be synonymous we compute a similarity measure between any two nodes lexemes of the network by applying random walks through already existing lexical relations Details of the different data sources graph mo deling and measures we use are given in Section 4 3 Extracted graphs are small worlds with a heavy tailed degree distribution see 18 336 F Sajous et al As the potential new synonyms we compute are to be validated by contributors and not automatically added to the initial resource our purpose is i to suggest candidates for the greatest number of lexemes and ii for a given lexeme to propose a finite list of candidates including at least some relevant ones In our case it is better to propose no candidate at all than irrelevant ones and the system is not meant to suggest all relevant candidates first because a contributor won t check an endless list and secondly our metho d is an iterative computation suggestion validation cycle Thus if a relevant candidate is not initially proposed it may be the next in the list of suggestions which may be shifted when a suggested candidate is chosen So as the relations added to the network will change its structure and as the computation of candidates will be repro cessed regularly after the release of a new dump in the case of Wiktionary this relevant candidate may be proposed after some iterations Thus recall will increase with successive iterations and we focus therefore more on precision 3 3 Validation The candidates that we compute are suggested to the contributors via an interface described in Section 5 If a contributor validates a suggestion the relation is added to Wiktionary No cross validation system in which a relation would be added only if several contributors validate it has been designed to keep close to the wiki principle we did not add any additional regulation 4 but as we ease the addition of synonyms we fairly give an easy way to remove them too 4 Similarity Elicitation This section presents the metho ds used to compute from existing lexical networks new synonymy relations to be added We rely on different kinds of data and similarity measures and compare the results obtained by evaluating them against expert built gold standards 4 1 Data Networks have been extracted from English and French Wiktionaries for nouns verbs and adjectives thus splitting the global structure of the dictionaries into mono part of speech subparts Given a language version of Wiktionary we consider only the article sections dedicated to entries in the language of interest e g the English lexemes of the English Wiktionary From these sections we extract the existing synonymy and translation links as well as the glosses 4 2 Bipartite Graphs Model In order to homogenize and simplify the description of the experiments each type of data we used will be mo delled as a undirected bipartite graph G V V E 4 For some insights into the autoregulation of the Wikiprojects ecosystem see 20 Semi automatic Enrichment of Collaborative Lexical Resources 337 where the set of vertices V will always denote the lexemes of the language and part of speech of interest whereas another set of vertices V will vary depending on the sources of data The set of edges E is such that E To propose new synonymy relations we compute the similarity between any possible pair of lexemes the vertices from the graphs described in the previous section The intent is to propose as candidates the pairs with the highest scores which are not already known as synonyms in Wiktionary We test various similarity measures all based on short fixed length random walks Such approaches for measuring the topological resemblance in graphs are intro duced in 18 21 This kind of methods is applied to lexical networks in 22 to compute semantic relatedness We consider a walker wandering at random in the undirected bipartite graph G V V E starting from a given vertex v At each step the probability for the walker to move from nodes i to j is given by the cell i j of the transition matrix P defined as follow P ij 1 d i if i j E 0 otherwise 1 where d i is the degree of incidence number of neighbours of vertex i Thus starting from v the walker s position after t steps is given by the distribution of 5 6 As we parse only the dump of the language of interest we find the oriented link v t t as a translation of v in v s article and symmetrize it into v t Having a more subtle model with oriented edges requires parsing all dumps of all languages http www ims uni stuttgart de projekte corplex TreeTagger 338 F Sajous et al probabilities Xt v v P t where v is a row vector of dimension V V with 0 anywhere except 1 for the column corresponding to vertex v We note Xt v u the value of the coordinate u of this vector which denotes as aforementioned the probability of reaching u afer t steps starting from v This is the first measure7 called simple we use other measures are based on this one simple v u Xt v u avg v u cos v u dot v u w V 2 3 4 5 Xt v u Xt u v 2 w V Xt v w Xt u w w V Xt v w 2 w V Xt u w 2 Xt v w Xt u w Xt v w log X if Xt u w 0 t u w otherwise ZKL v u w V Xt v w 6 cos and dot are respectively the classical cosine and scalar pro duct ZKL is a variant of the Kullback Leibler divergence intro duced in 22 Let C v G t sim be the ordered list of candidates computed on graph G with the similarity measure sim and a random walk of length t starting from v i sim v ui sim v ui 1 i sim v ui 0 7 C v G t sim u1 u2 un with i v ui EW s where EW s is the set of existing synonymy links in Wiktionary The experiments below consist in evaluating the relevancy of C v when G and sim vary t 2 will remain constant 8 4 4 Evaluation Method In view of our application cf Section 5 2 and given the criteria defined in Section 3 2 for each lexeme we consider that a suggested list of candidates is acceptable if it includes at least one relevant candidate Indeed a user can contribute provided that at least one good candidate occurs in the suggested list Thus the evaluation will broadly consist in counting for how many lexemes the system computes a suggested list with at least one relevant candidates Let GGS VGS EF S be a gold standard synonymy network where VGS is a set of lexemes and EGS 7 8 All these measures are not strictly speaking similarity indeed simple and zkl10 are not symmetric t has to be even and preliminary experiments have shown that the best results are obtained with 2 or 4 t 2 gives similar results and is less complex Semi automatic Enrichment of Collaborative Lexical Resources 339 against the gold standard s relations We only evaluate the suggested lists for the lexemes that are known by the gold standard i e v VGS Indeed if a lexeme v V do es not belong to the gold standard i e v V VGS we consider that it is a lexical coverage issue so one cannot deem whether a relation v c is correct or not 9 For the same reason for each lexeme v we remove from C v the candidates absent from the gold standard Finally we limit the maximum number of candidates to k 5 For each lexeme v V VGS we note k v the evaluable suggested list of candidates k k i ci C v VGS 8 k v c1 c2 ck with i sim v ci sim v ci 1 Please note that k v contains a maximum of k candidates but it may be smaller or even empty Note also that k v depends on the gold standard We note k v the set of correct candidates within k v k v c k v v c EGS 9 We define the set Nk of lexemes having at least one candidate being proposed of lexemes for which at least one correct candidate is proposed and the set Nk Nk v V VGS k v Nk v V VGS k v 10 To compare the efficiency of different data sources used to compute the candidates we measure Pk the ratio between the acceptable suggested lists and the lexemes for which suggestions are done and Rk the ratio between the number of suggested lists and the number of evaluable target lexemes Pk Nk Nk Rk Nk VGS V 11 Although Pk and Rk are not precision and recall measures they intuitively refer to the same notions and we adopt below abusively this terminology 4 5 Results Gold Standards We used Princeton WordNet to evaluate the candidates for English and DicoSyn10 for French The extraction of the synonymy networks from these resources repro duces what has been done in 16 Similarity measures Applying the different similarity measures presented in Section 4 3 shows that all give pretty similar results As an example the results obtained for the intersection of the gold standards and the English and French Wiktionaries nouns and verbs are reported in Table 2 The simple measure being as efficient as the others and having far less complexity further experiments have therefore been done using this measure 9 10 v may be a neologism or a domain specific word Less often it may be misspelling Any relation v c should therefore not be counted as false or true Dicosyn is a compilation of synonym relations extracted from seven dictionaries produced at ATILF and corrected at CRISCO units 340 F Sajous et al Table 2 P5 precision comparison for different data sources and measures Synonyms EN FR V N V N simple 41 4 32 4 58 6 47 3 avg 42 5 33 5 58 2 46 8 cos 43 4 34 6 60 2 47 9 dot 42 0 34 0 59 7 46 7 ZKL10 43 2 34 0 60 1 48 2 Translations EN FR V N V N 51 4 37 8 78 7 58 3 50 5 38 0 78 7 58 3 51 8 38 5 78 3 58 6 52 3 38 7 78 2 58 7 51 8 38 6 78 7 58 8 Syn EN V N 51 9 39 0 51 1 39 3 51 3 39 4 52 4 39 7 51 9 39 8 Trans FR V N 74 6 55 3 74 0 55 1 73 1 54 2 73 6 54 8 74 0 54 5 Data sources As we can see in Table 3 better results are obtained for French than for English This can be partly explained by the slightly lower density of the English networks cf Table 1 but is mainly due to the difference between the gold standards used networks extracted from WordNet are more sparse than the ones extracted from Dicosyn see 16 Morever Table 4 shows that some candidates rejected by the gold standards do not look unreasonable which makes it hard to draw definitive conclusions Nevertheless despite a potentially severe evaluation results look acceptable enough in view of our application The translations graph provides better precision than synonymy graphs This result was expected as in Wiktionary lexemes have more translation links than synonyms Moreover translations are often distributed over several languages which is more reliable than having a lot of translations into a given language The glosses graph s worse precision and higher recall was expected too almost all lexemes have glosses but information is less specific and we did not try any tricky edge weighting Combining synonyms and translations enables a better recall than with separated graphs and a similar precision for English For French it leads to a loss of precision compared to the translations only graph Table 3 Impact of different data sources on the simple similarity measure V 48930 196790 67649 41725 106068 17782 Synonyms Translations Syn Trans VGS V VGS P5 R5 P5 R5 P5 R5 21479 13742 46 3 24 9 53 5 23 4 53 7 34 6 117798 43236 32 4 17 1 37 8 24 9 39 0 32 4 11529 8890 41 4 33 2 51 4 43 5 51 9 53 8 9452 3958 61 2 24 9 76 1 19 8 69 6 34 2 29372 16084 47 3 23 2 58 3 22 2 55 3 35 4 9147 4037 58 6 22 3 78 7 36 8 74 6 45 8 Glosses P5 R5 26 1 98 3 14 9 98 9 27 0 99 9 32 2 96 1 20 7 99 4 41 1 99 4 Adj EN Nouns Verbs Adj FR Nouns Verbs Table 4 Example of propositions for nouns evaluated against gold standards GS in GS Yes No FR Yes No Propositions imprisonment captivity harmony peace filth dirt antipasto starter load burden possessive genitive rebirth renewal fool idiot dummy cheating fraud bypass circumvention dissimilarity variance pro benefit ouvrage travail renom gloire emploi fonction drapeau pavillon rythme cadence roulotte caravane chinois tamis drogue psychotrope fantassin bidasse force poigne salade bobard W C chiotte us tradition bisque soupe EN Semi automatic Enrichment of Collaborative Lexical Resources 341 5 Implementation The WISIGOTH Architecture In order to carry out our enrichment metho d we designed an architecture called WISIGOTH11 composed of a set of mo dules depicted in Fig 1 Fig 1 The WISIGOTH architecture 5 1 Computation of Candidates The first part of the architecture is made of a pro cessing pipeline which from a Wiktionary dump 12 builds the graphs intro duced in Section 4 2 and computes the candidate relations by applying the metho d described in Section 4 3 This pro cessing pipeline can be triggered each time a new dump is released or when a given threshold of edits has been registered 5 2 Suggestion and Validation of Candidates The interface we developed to suggest and validate or invalidate new relations materializes as a Firefox extension Once installed when a user browses the English or French Wiktionary the interface sends a request to the candidates service which returns for each known lexeme a list of potential synonyms Suggestion and Editing Next to each proposition appears a sign which triggers the automatic addition of the candidate as a synonym to the Wiktionary server As a contributor may want to add a synonym that has not yet been suggested we provide a free text area too Regardless of our enrichment metho d it enlarges the potential population of contributors not restricting it to wikico demasters acquainted with the underlying syntax As explained is Section 3 3 a sign is added to every synonym o ccuring in the page which handles the deletion of this synonym 11 12 WIktionarieS Improvement by Graph Oriented meTHods Wiktionaries dumps are available at http download wikipedia org 342 F Sajous et al Notification of editing Thus far wiktionaries dumps are released frequently Nevertheless to protect against irregular dumps which could result in a desynchronization between Wiktionary s current state and the lexical networks we extracted from it and therefore cause irrelevant suggestions the interface notifies our server the editing of synonyms Thus a remo delling of synonymy networks and a reprocessing of candidates may be done between two releases Storing these notifications will also later give us the opportunity to analyse which synonymy links look problematic e g a series of additions and deletions and how contributors behave 6 Conclusion and Future Work This paper has pointed out the problems usually encountered in the development of lexical resources It has shown how CCRs help overcome these difficulties and among them how we can take advantage of Wiktionary s infrastructure and content Nevertheless crowds are more prone to add new words than to provide semantic relations To encourage them we have designed a tool to assist collaborative editing by suggesting new synonyms to be validated We took the opportunity to compare the impacts of using different data sources and similarity measures The choice of the measure does not much affect the results whereas combining data sources permits us to gain precision or recall depending on the language Adding glosses to the Syn Trad graph presented and working on the weighting of the graphs edges should bring even better results Grounded on the topology of the graphs extracted from the lexical networks this system is language independent and moreover may be applied to other resources than Wiktionary contrary to methods like 23 which exploit the structure of hyperlinks between pages and are therefore bound to this resource It may help for example building WordNets that are still under construction as the Chinese one 24 Moreover not relying on other external resources makes this metho d endogenous and may be applicable to enhance lexical resources for under resourced languages When external resources are available for example stemming from distributional analysis over large corpora an exogenous enrichment mo dule can be coupled to our system and feed our edition interface A short term extension of this work will be the proposition of new translations by leveraging the same kind of graph mo del and similarity measures Linguistic observations should be done to characterize what other kinds of semantic relation than synonymy is captured by automatically computed relatedness Although we did not rely on a cross validation system for adding synonyms we think it could be useful to add a blacklist system to stop proposing a candidate judged as irrelevant by several contributors for a given target lexeme An interesting study would be the evaluation of the results of the endogenous enrichment pro cess at different stages of Wiktionary s growth This can be done by rebuilding the various past states of the lexical networks using the historical dump containing all articles revisions Such a study may show when it is appropriate to apply our metho d when we have enough material to start suggesting Semi automatic Enrichment of Collaborative Lexical Resources 343 new relations and when no more relevant relation is to be proposed and should be stopped Resources the Firefox extension presented in this paper and the structured data extracted from Wiktionary s dumps are publicly available at http redac univ tlse2 fr wisigoth References 1 Sekine S We desperately need linguistic resources 344 F Sajous et al 17 Meyer C M Gurevych I Worth its Weight in Gold or Yet Another Portable Extraction of Partially Structured Facts from the Web Andrew Salway1 Liadh Kelly1 Inguna 1 Centre for Digital Video Processing School of Computing Dublin City University Dublin 9 Ireland asalway lkelly gjones computing dcu ie 2 Tilde 75 Abstract A novel fact extraction task is defined to fill a gap between current information retrieval and information extraction technologies It is shown that it is possible to extract useful partially structured facts about different kinds of entities in a broad domain i e all kinds of places depicted in tourist images Importantly the approach does not rely on existing linguistic resources gazetteers taggers parsers etc and it ported easily and cheaply between two rather different languages English and Latvian Previous fact extraction from the web has focused on the extraction of structured data e g Building LocatedIn Town In contrast we extract richer and more interesting facts such as a fact explaining why a building was built Enough structure is maintained to facilitate subsequent processing of the information For example the partial structure enables straightforward template based text generation We report positive results for the correctness and interest of English and Latvian facts and for their utility in enhancing image captions Keywords Fact extraction multilingual information retrieval information extraction web image captioning 1 Introduction This paper proposes a novel fact extraction task which fills an important gap between current information retrieval IR and information extraction IE technologies in order to further exploit the vast quantities of multilingual information available on the web Search engines retrieve relevant web pages across diverse domains and across languages but the onus is on the user to read through and interpret the results By contrast IE systems provide structured facts and data from natural language texts which are amenable to further automated analysis and multi document summarization systems and question answering systems fuse information about an entity or topic of interest to reduce reading time However such systems are typically costly to port to new languages and the domains in which they work tend to be narrow and comprise only a small set of entity types and relations We believe that there are emerging applications H Loftsson E 346 A Salway et al such as automated image captioning and augmented reality which would benefit from exploiting information on the web across broad domains and multiple languages but which do not require fully structured information or the ma jority of all available information about an entity For example to automatically enhance an image caption we only require one interesting fact about the place in the image with enough structure for the fact to be inserted appropriately into a text generation template In sacrificing the requirements for full structure and comprehensive information about an entity we expect to gain considerably in broad coverage of domains and ease of porting between languages We elaborate these points in Section 2 as we define the Tell Me About task which is roughly to provide one or more of the most interesting facts about a given entity in a partially structured form that enables some further pro cessing and re use of the information Section 3 discusses related work in the fields of IR and IE with a focus on information extraction from the web multido cument summarization and question answering Section 4 presents a highly portable solution for extracting partially structured facts that exploits information redundancy on the web i e the fact that the same information about an entity is available in many forms on the web The crucial assumption is that at least one key fact about an entity will be expressed somewhere on the web in a simple form This means that we work with a few simple linguistic structures and shallow language processing and so the solution ports easily between languages We report positive results for the correctness and interest of facts in two rather different languages English and Latvian 128 facts each judged by an investigator and five sub jects Latvian is a highly inflected language nouns adjectives participles and verbs are all inflective and because of this rich morphology Latvian has quite free word ordering The utility of the Tell Me About task is demonstrated by enhancing the captions of tourist photographs using extracted facts for template based text generation with an evaluation of caption readability 90 image captions each judged by six sub jects In closing Section 5 considers generalising our solution to other domains and applications 2 The Tell Me About Task Let us elaborate on the details of this task and the motivation for it by considering one potential application automatic image captioning The number of digital images being archived in personal collections and shared in so cial image collections such as www flickr com and www panoramio com is increasing very rapidly When users view images from these collections it is desirable to have information describing each image available in a caption However people taking pictures will often either not know sufficient details about the place depicted in the image to do this effectively or will not take the time to do this so automated solutions are required There is also a burgeoning interest in augmented reality whereby a camera screen on a mobile device is updated automatically with caption like information about the place that the camera is pointed at Digital image capture devices are increasingly incorporating location sensing via Portable Extraction of Partially Structured Facts from the Web 347 GPS monitoring This can be combined with other image metadata such as the date and time of capture and cross referenced with geographic databases to generate simple descriptive captions for an image e g of the form North Bridge photographed in the afternoon 1 We see an opportunity to exploit the vast information content of the web in order to enhance such a caption with a key fact e g to output something like North Bridge which was built to link the New Town with the Old Town photographed in the afternoon Whilst we can be confident that information about many places is available in many languages on the web the challenge is to identify the most interesting facts for a given entity There is also the challenge of extracting information into partially structured facts that enable further pro cessing and re use of the information In the image captioning scenario simply adding whole sentences from the web to an existing caption would have unpredictable results for caption readability It could be that a long sentence contains information about more than one place so we need to identify just the relevant part of the sentence Also if we want to insert information into an existing caption i e into the middle of a sentence then we need to know something about how it phrased For the Tell Me About task we specify that facts should have the form of a triple Entity Cue Text Fragment where Cue is one of a fixed set of information cues loosely akin to relations and Text Fragment is a text fragment taken directly from a webpage such that Cue Entity Text Fragment reads naturally as a sentence e g North Bridge was built to link the New Town with the Old Town For template based image captioning this means we can for example insert information in a subclause starting with which for cues such as was built but removing which and the cue itself for cues like is The partial structure of the fact gives us control over text generation that we would not have if the fact was only a text fragment However because the right hand side of the fact is a text fragment and not another entity of fixed type as it would be in a standard IE template then the same cue can get quite different kinds of information allowing for much richer facts when available e g Hadrian s Wall was built in AD 122 130 on the orders of the Emperor Hadrian Hadrian s Wall was built to keep out the marauding Scottish To summarise the Tell Me About task proposed here is as follows Given the name of an entity and a specified language a list of facts about the entity should be returned in the form Entity Cue Text Fragment sorted with interesting facts ranked higher With regards to image captioning it is important to note that the place depicted in a photo may be one of very many different kinds of entity bridge monument beach church mountain plaza glacier etc Furthermore the most interesting aspect of one entity may not be the same as the most interesting aspect of another entity of the same type one church has spectacular stained glass windows another is known for an historical event that happened there a third offers amazing views from its tower Finally a caption for an image on a website may be required in many languages For these reasons as we discuss next current IE approaches are not appropriate 348 A Salway et al 3 Related Work Although we consider Tell Me About to be distinct from other natural language pro cessing tasks it do es clearly have similarity with established and well understood tasks within IR and IE The idea of ranking facts could be seen as similar to the ranking of documents for IR 2 and more specifically the retrieval and ranking of passages 3 Indeed snippets returned by web search engines are the starting point in our approach to fact extraction although by the end of the process the sorted facts are in a different order than the snippets ranked by the search engine The extraction of partially structured information makes our fact extraction look quite a lot like IE 4 but whilst we do specify a set of cues similar to relations we do not require the structuring of the right hand side text fragment into a template which would for example make relations between entities explicit We have found that this makes it possible to pursue quite a generic approach to fact extraction across broad domains and multiple languages whereas IE systems require non trivial amounts of work to be adapted to different kinds of entities and languages Question answering systems return facts typically in response to factoid questions with answers that are dates locations organizations people etc 5 However for a given entity it is not possible to anticipate what if any factoid question will give the most interesting information That said our approach to fact extraction shares assumptions about the redundancy of information on the web with some question answering techniques e g 6 Multi document summarization systems do something rather like the Tell Me About task when they select a set of informative sentences about an entity e g 7 but with a focus on more than just a few key facts and the need to pro duce coherent text as output such systems typically depend on quite extensive linguistic resources at a minimum training corpora that mitigate against porting easily between many languages Previous work on information extraction from the web rather than from domain specific collections of a single text type has achieved impressive quantities of facts at high levels of precision e g 1 million ranked facts with a prespecified relation at 75 98 Precision 8 Under the rubric of open information extraction which discovers relations as well as facts a precision of 88 has been reported 9 In related work the TextRunner system extracted over 500 million tuples from 120 million web pages 10 However much of this previous work has focused on the extraction of wholly structured data to specify relations between two entities e g facts of the form City CapitalOf Country PersonBornIn Year or Company Acquired Company Whilst this effectively enables the storage analysis and retrieval of millions of facts in relational databases these relatively simple facts are unlikely to be interesting for applications such as image captioning An online demonstration do es suggest that the TextRunner system 11 can provide facts with unstructured right hand sides but our impression is that low quality of information is the price for exceptionally broad coverage Furthermore with regards to portability between languages the approaches described by 9 and 10 rely on a linguistic analysis of how relations are expressed in English and on syntactic parsers Although the approach in 8 Portable Extraction of Partially Structured Facts from the Web 349 avoids syntactic analysis and parsing it nevertheless works with text that has been part of speech tagged and draws on existing word distribution data Taggers parsers and other linguistic resources are not available for many languages and so we have developed an approach that do es not need them 4 Our Approach to Fact Extraction Here we present a first solution for the Tell Me About task We show how given an entity in this case any kind of place we return a list of facts in the form Entity Cue Text Fragment ranked according to a score which is intended to promote interesting and true facts The approach is generic across a broad range of entities and requires minimal effort to port between languages It is based on two assumptions i the same information is expressed in many ways across the web so it is only necessary to look for it in a small number of relatively simple forms and ii overlaps between what is written on different web pages can be used to compute an interest correctness score to rank facts 4 1 Algorithm Given an entity steps I IV generate a list of facts about it I Get Snippets from Search Engine A series of queries is made to a web search engine we used Yahoo s BOSS API 12 Each query takes the form Entity Cue the use of double quotes indicates that only exact matches are wanted i e text in which the given entity and cue are adjacent A set of cues is manually specified to capture some common and simple ways in which information about the general kind of entity is expressed For places we used cues like is a is famous for is popular with was built Although we worked with around 40 cues including single plural and present past forms it seems that a much smaller number are responsible for returning the ma jority of high ranking facts in particular and perhaps unsurprisingly the generic is seems most pro ductive The query may also include a disambiguating term For example streets and buildings with the same name may occur in different towns so we can include a town name in the query outside the double quotes e g West Street is popular with Bridport For each query all the unique snippets returned by the search engine up to a specified maximum are pro cessed in the next step typically a snippet is a few lines of text from a webpage around the words that match the query often broken in mid sentence II Shallow Chunk Snippets to Make Candidate Facts Because all the information that we retrieve about the entity is expressed as Entity Cue then we can use a simple extraction pattern to obtain candidate facts from the retrieved snippets For both English and Latvian the gist of the pattern is BOUNDARY ENTITY CUE TEXT FRAGMENT BOUNDARY such that TEXT FRAGMENT captures the Text Fragment part of a fact The details of the pattern are captured in a regular expression on a language specific basis e g to specify boundary words and punctuation to allow optional words to 350 A Salway et al appear in between ENTITY and CUE and to reorder the elements for non SVO languages A successful match of the pattern on a snippet leads to the generation of a candidate fact using the extraction pattern in the Appendix the snippet text in London Big Ben was named after Sir Benjamin Hall matches giving the candidate fact Big Ben was named after Sir Benjamin Hall but The square next to Big Ben was named in 1848 do es not match III Filter Candidate Facts Four filters are used as a quality control the first two of which require language specific word lists built manually over a number of runs of the algorithm General filter words a candidate fact containing any of the given filter words is removed this can be used to remove potentially sub jective statements containing me my our amazing fantastic etc Invalid end words to catch some erroneous shallow chunking most likely due to noisy web data or to a badly cut search engine snippet this filter removes candidate facts ending in words such to from by etc Length of Text Fragment a threshold can be set to filter out candidate facts with text fragments shorter than the specified number of words it seems that shorter text fragments are more likely to lead to incomplete or incorrect facts Words all in capitals when this filter is turned on any candidate fact containing a word all in capitals is removed this is good for removing spam and content in an informal style but of course it also removes candidate facts containing acronyms IV Score and Sort Facts Our idea here is to rank facts at least coarsely so that we are more likely to get correct and interesting facts at the top The notions of correctness and interest are each problematic and difficult to unpick for the purposes of algorithm design and evaluation Here we exploit the overlap between candidate facts for the same Entity Cue pair to capture these notions to some extent For each Entity Cue pair a keyword frequency list is generated by counting the occurrence of all words in the Text Fragments for that pair words in a stop word file are ignored The score for each fact is then calculated by summing the Entity Cue frequencies of each word in the Text Fragment so that facts containing words that were common in other facts with the same Entity Cue will score highly If shorter facts are wanted then the sum is divided by the word length of the Text Fragment We see two main ways in which the sum score for a fact can get high i there are many overlapping Text Fragments for an Entity Cue pair so there are some high word frequencies and ii a fact contains more of those high frequency words than other facts Thus we hope to get high ranked facts with the most appropriate Cue for the Entity and the best Text Fragment for the Entity Cue pair To give an impression of how ranking works Figure 1 shows the top and bottom 10 facts returned for Eiffel Tower using the sum only scoring The top ranked facts are generally rich in correct information about the given entity In contrast incomplete and trivial facts end up low down the list We see that 4 of the top 10 facts have the Cue was built which seems like a goo d cue for interesting information about an historical monument The high ranking facts with this Cue include words like Paris 1889 international exhibition Portable Extraction of Partially Structured Facts from the Web 351 Eiffel Tower was built in 1889 for an international exhibition in Paris Eiffel Tower was named after an ingenious engineer whose design of the tower turned it into a reality and pride of the French nation Eiffel Tower is an iron tower built during 1887 1889 on the Champ de Mars beside the Seine River in Paris Eiffel Tower was one of the first tall structures in the world to contain passenger elevators Eiffel Tower was one of the landmarks visited by Luigi when he came to save Paris from invading Koopa Troopas Eiffel Tower was built by Gustave Eiffel for the International Exhibition of Paris of 1889 commemorating the centenary of the French Revolution Eiffel Tower was one of the first structures in the world to have passenger elevators Eiffel Tower was built in 1889 for the Universal Exposition celebrating the centenary of the French Revolution Eiffel Tower was built as a temporary structure for an exhibition in 1889 Eiffel Tower is named after its designer and engineer Alexandre Gustave Eiffel Eiffel Tower is built for the Paris exposition Eiffel Tower was famous enough for everyone to know Eiffel Tower is made up of a base Eiffel Tower was made for the Exposition Universelle Eiffel Tower is made of over 10 Eiffel Tower is made from 18 Eiffel Tower is made of 3 platforms Eiffel Tower is made with 2 Eiffel Tower is famous throughout the world Eiffel Tower is famous for a reason Fig 1 The top 10 and bottom 10 facts for the entity Eiffel Tower which are likely to appear after Eiffel Tower was built on many web pages the fact with all four of these words is ranked highest For Latvian the top ranked fact was Francijas 352 A Salway et al North Bridge was originally built in 1772 to connect the burgh with the Port of Leith to the north North Bridge was built to link the New Town with the Old Town Bahnhofstrasse is where well heeled bankers and perfectly coiffed ladies shop for designer clothing and gold watches Bahnhofstrasse is Zurich s main shopping avenue Durdle Door is a natural limestone arch on the Jurassic Coast near Lulworth in Dorset Durdle Door is a limestone arch The Matterhorn was one of the last Alpine mountains to be ascended due to its imposing shape and unpredictable weather The Matterhorn was first climbed in 1865 Fig 2 Pairs of facts about places The first is the top ranked fact with simple sum score the second is the top ranked with sum divided by number of words 4 2 Evaluation This evaluation used 68 place names in English and 60 place names in Latvian from around Europe We chose an even mixture of urban rural and famous not famous places from European cities London Riga Zurich and Dublin and countryside UK Latvia Switzerland and Ireland and various types of place churches statues mountains rivers etc For each place the top ranked fact was used for evaluation see Appendix for the settings used to generate facts Evaluating Correctness of Facts Each of the facts in English was rated as correct or incorrect by an investigator by searching for the fact on the web in the following manner If the fact was found on Wikipedia or an official tourist website for the region and on one other website or if the fact was found on three independent websites it was marked as correct If part of the fact was found on the web using this technique then the fact was marked as partially correct Otherwise the fact was marked as incorrect Due to the lack of coverage in Latvian on the web Latvian facts were rated as correct if they were located on Wikipedia or an official tourist website for the region or if they were known to be correct by the investigator For the English experiment 35 of the 68 facts were marked as correct 51 13 were partially correct 19 and 20 30 were incorrect Analysing the partially correct facts revealed that 11 of the 13 were incomplete facts e g Dridzis Lake is the deepest lake not only in Latgale here it looks like our chunking pattern cut too soon i e on the word but although a similar problem o ccurs occasionally with the way the search engine creates snippets The other two partially correct facts had spurious material at the end of the fact e g Mount Titlis is the largest winter sports paradise in Central Switzerland even the most demanding skiers the unusual punctuation is missed by our chunking pattern Analysing the 20 incorrect facts we found that only six of them were actually false for example a fact which was supposed to be about the National Museum in Zurich actually referred to a museum in Prague this is despite our use of Zurich as a disambiguating term Eight of the Portable Extraction of Partially Structured Facts from the Web Table 1 Responses from 5 subjects for 68 English facts and 60 Latvian facts English Latvian 5 5 subjects 3 5 said 5 5 subjects 3 5 said said Yes Yes said Yes Yes Is this the type of fact 26 68 53 68 14 60 38 60 you would expect to 38 78 23 63 read in a travel guide 353 incorrect facts were unreadable for example Daugava river is soon to be a prelude of things to come that would prove 2000 wasn t Cappellini s year which we put down to web noise The correctness of the remaining 6 incorrect facts was actually indeterminable e g Bastejkalns Park was renovated during last winter we have since added words with temporal reference like last to the filter words list as well as deictic words like this Similar to the English results for the Latvian evaluation 32 of the 60 facts were marked as correct 53 19 32 were partially correct and 9 15 were incorrect Evaluating the Interest of Facts Ten native English speakers were each presented with 34 English facts to rate Ten native Latvian speakers were each presented with 30 Latvian facts to rate In this way each fact was rated by 5 sub jects The lists of facts presented to sub jects were randomly chosen using a Latin square For each fact sub jects answered yes or no to the question Is this the type of fact you would expect to read in a travel guide The question is intended to get at the notion of interest in a way specific to our application scenario i e we assume users would be happy with travel guide like facts added to their image captions Results are summarised in Table 1 which indicates that more often than not our algorithm is pro ducing as its top ranked fact something that most people find acceptable as a fact for something like a travel guide Our evaluation criteria for fact correctness were rather strict note that a majority of sub jects rated more facts as interesting 78 English and 63 Latvian than we ourselves rated as correct around 50 for both As noted it seems that some relatively simple changes to our extraction patterns and word lists will improve our correctness score quite considerably so overall we are confident that the fundamentals of the approach are sound Importantly the approach was very cheap to port between languages Porting from English to Latvian required only a small mo dification to the extraction pattern and the translation of the cue set see Appendix other word lists were also translated but for Latvian these had little impact on results 4 3 Enhancement of Image Captions Our motivation for doing fact extraction was to add information about places into image captions which provides a scenario for evaluating the utility of the facts that we extract Sets of 30 image captions were created in English and Latvian for images depicting urban and rural places o ccurring in Ireland UK Latvia 354 A Salway et al and Switzerland Half the captions were in the form PLACE photographed in LOCATION The other half were in the form Photo taken near PLACE in LOCATION Half of the captions also had the time of day inserted into the sentence Photo taken in the TIME OF DAY near PLACE in LOCATION Each of the 30 English captions had a fact added in two different ways 1 insert fact as a subclause in the original sentence and 2 append the fact to the original caption as a new sentence This led to 60 enhanced English captions For 1 the string which CUE TEXT FRAGMENT was inserted after the place name in the caption Recall we are only able to insert information as a subclause which keeps the captions more compact because we have partially structured facts cf Section 2 For 2 a second sentence was formed by adding PLACE CUE TEXT FRAGMENT after the original caption Insertion as subclause was deemed inappropriate for Latvian so we had just 30 enhanced Latvian captions with facts added as sentences We ensured that correct facts were added because we wanted to concentrate on evaluating the readability of the enhanced captions The 60 enhanced English captions were presented to 6 native English speakers in random orders for judgment The 30 enhanced Latvian captions were presented to 6 native Latvian speakers For each enhanced caption sub jects answered yes or no to the question Does this sentence read naturally to you When facts were added as new sentences then a ma jority of sub jects deemed 29 30 97 of the enhanced image captions to be readable both for English and for Latvian The results English only for inserting facts as subclauses seemed to depend on the form of the original caption For 15 15 100 of captions with the form Place photographed in location a ma jority of sub jects judged the enhanced caption with fact inserted as subclause to be readable For the other caption form only 7 15 47 enhanced captions were judged readable by a ma jority of sub jects Upon inspection it seemed that these captions tended to be quite long already including additional temporal information so a further subclause even though grammatically correct became awkward to read 5 Conclusions To summarise a new kind of fact extraction task was defined and a solution to the task was evaluated for two rather different languages It was shown that it is possible to extract useful partially structured facts about different kinds of entity in a broad domain using a common approach that ports easily between languages in the absence of existing linguistic resources In contrast with traditional IR techniques we pro duce output that is more amenable to further automated pro cessing In contrast with traditional IE techniques our approach has the potential to cover much broader domains and many more languages Of course we need to try other kinds of language before making strong claims about portability Although Latvian is a free word order language the SVO order do es dominate so we were able to get good results with just one extraction pattern However even in languages with more variation in word ordering we expect that we could use just a few extraction patterns based around cue sets Portable Extraction of Partially Structured Facts from the Web 355 What is less clear to us is the ease with which we can port to other domains Whilst we found interesting facts about many different kinds of places were expressed using a relatively small number of common cues this may not be the case for all kinds of entities That said in preliminary work we got some encouraging facts about people and organizations using just a few cues Beyond the image captioning application and template based text generation we see potential for the Tell Me About task in other areas For some kinds of queries to search engines users may benefit from being presented with a few facts about their topic of interest we feel that our chunking of information and ranking of facts can add value to the snippets returned by a search engine Recently some search websites have started to offer something more like knowledge retrieval on top of information retrieval 13 14 and our impression is that our kind of fact extraction could contribute to such endeavours Acknowledgement The research reported in this paper is part of the project TRIPOD supported by the European commission under contract No 045335 References 1 Purves R S Edwardes A J Sanderson M Describing the Where improving image annotation and search through geography In First Intl Workshop on Metadata Mining for Image Understanding 2008 2 Baeza Yates R Ribeiro Neto B Modern Information Retrieval ACM Press New York 1999 3 Salton G Allan J Buckley C Approaches to passage retrieval in full text information systems In 16th ACM SIGIR pp 356 A Salway et al Appendix Settings Used for Evaluation Runs English Cues used in queries to search engine is was is the was the is a was a is an was an is in is on is by is next to is near to is known is famous is located is one of was built is made of is named was named is home to was home to is used was used was completed was destroyed was damaged is the site of was the site of was the scene of was made famous is the most is the biggest is the largest is the smallest is the oldest can be seen from is popular is popular with features offers is located by is located on is located in is famous for is known for was built by was built in was built for was built to is open Regular Expression for Shallow Chunking of Snippets the The s ENTITY s CUE s b and b b but b ENTITY and CUE are interpolated at run time captures the Text Fragment Filter words I my me mine you your yours we us ours another recently this also other further must should could sensational fun deserves excellent amazing wonderful miles kilometres m km minutes min mins hours hour probably actually possibly Scoring stop words the of is for a an and Invalid final words a the those these with by and but which that for like as Latvian For both languages the maximum snippets returned from search engine for a single query was 20 scoring metric was simple sum and score threshold 3 Passage Retrieval in Log Files An Approach Based on Query Enrichment Hassan Saneifar1 2 LIRMM Univ Montpellier 2 CNRS France 2 Satin Technologies France saneifar laurent poncelet mroche lirmm fr stephane bonniol satin tech com http www lirmm fr saneifar laurent poncelet mroche Abstract The question answering systems are considered the next generation of search engines This paper focuses on the first step of this process which is to search for relevant passages containing answers Passage Retrieval can be difficult because of the complexity of data log files in our case Our contribution is based on the enrichment of queries by using a learning method and a novel term weighting function This original term weighting function used within the enrichment process aims to assign a weight to terms according to their relatedness to the context of answers Experiments conducted on real data show that our protocol of primitive query enrichment make it possible to retrieve relevant passages Keywords Information Retrieval Question Answering Passage Retrieval Query Enrichment Context Learning System 1 1 Introduction Information Retrieval IR aims to find do cuments related to a topic specified by a user The topic is normally expressed as a list of specific terms However the needs of some application domains make Information Retrieval IR metho ds inefficient Indeed when the goal is to find specific and concise answers Information Retrieval systems are not relevant according to the considerable amount of documents that they retrieve as possibilities Moreover the information found in retrieved documents is not always correlated with the given query That is why Question Answering QA systems are an important research topic nowadays Question Answering systems aim to find a relevant fragment of a do cument which could be regarded as the best possible concise answer to a question given by user There are two main categories of QA systems 1 Open domain and 2 Restricted domain In the first case questions arise about general domains and sources of information are large corpora consisting of do cuments of several fields e g corpus of web pages The evaluation of open domain QA systems has been built since 1999 in TREC1 Text REtrieval Conference American evaluation campaigns 1 http trec nist gov H Loftsson E 358 H Saneifar et al In the second category QA systems are designed to answer questions in a specific area In this kind of QA systems information resources are technical and specialized documents The restricted domains called also closed domains have certain characteristics which make the metho ds of open domain QA become less useful 3 We detail these characteristics and some features of specialized textual data which make answer retrieval more difficult in Sect 2 The Passage Retrieval represents an important phase of QA pro cess To give an efficient and reliable definition a passage is defined as a fixed length sequence of words which can begins and ends anywhere in a do cument 9 The Passage Retrieval is the task of searching for passages which may contain the answer to a given question In this paper we present our work on passage retrieval in a specialized domain We deal with a particular type of complex textual data which are log files generated by Integrated Circuit IC design tools These log files are digital reports on configurations conditions and states of systems In this area checking the quality of pro ducts requires to answer some technical and specialized questions At this stage our goal is to find segments of the logs that contain the answers to the questions of quality check The particularity of such textual data i e log files and characteristics of restricted closed domains impact significantly the accuracy and performance of passage retrieval in this context We propose in this paper a passage retrieval system based on a new approach of query enrichment Our query enrichment process is based on a learning approach and a new weighting function which gives a score to terms of corpus according to their relatedness to the context of answers The enrichment pro cess takes place in two phases First we propose a metho d for learning the context of questions based on the notion of lexical world In the learning phase we identify the terms representing the context of questions Then the initial queries are enriched by these terms Secondly we propose an original term weighting function which aims at giving a high score to terms of corpus which have a significant probability to exist in the relevant passages The terms having the highest scores are included in the query in order to improve its enrichment Our approach is an interactive system based on relevant feedback We show that our approach gives satisfactory results on real data In Section 2 we present the specific characteristics of log files and also the limits of QA systems in restricted domains Existing work concerning the QA systems are presented in Sect 3 Section 4 presents some notions used in enrichment pro cesses and also the first phase of query enrichment In Section 5 we develop our approach of passage retrieval and query enrichment by presenting our novel term weighting function Experiments on real data are presented in Sect 6 2 Problem Study Log files generated by Integrated Circuits design tools are not systematically exploited in an efficient way despite the fact that they contain the essential Passage Retrieval in Log Files An Approach Based on Query Enrichment 359 information to evaluate the design quality The particular characteristics of logs described below make classical techniques of Natural Language Pro cessing NLP and Information Retrieval irrelevant 2 1 Information Retrieval and Log Files We consider log files as a kind of complex textual data i e containing multisource heterogeneous and multi format data Indeed in the design of Integrated Circuits different design tools can be used in the same time while each tool generates its own log files Therefore despite the fact that the logs of the same design level contain the same information their structures and vocabulary can vary significantly depending on the used design tool More precisely in order to report the same information each design tool uses its own vocabulary In this domain the questions queries are expressed using a vocabulary that do es not necessarily correspond to the vocabulary of all tools However a system should be able to answer the questions regardless of the type of tools that generated the log files We explain this issue with an example Consider the sentence Capture the total fixed STD cell as a given question query We pro duce two log files eg log a and log b by two different tools The answer to the question in log a is expressed as follows Std cell area 77346 sites non fixed 74974 fixed 2372 While the answer in log b is expressed in this line preplaced standard cell is 24678 As shown above the same information in two log files produced by two different tools is represented by different structures and vocabulary The keywords of the question i e Fixed STD cell exist in the answer extracted from log a while the answer from log b contains only the word cell Insofar as there is a dictionary asso ciating the word STD with standard we can also consider the word standard However by giving these two words as a query to an information retrieval system irrelevant passages of log b are retrieved standard cell seeds is 4567 Total standard cell length 0 4536 This can be explained by the fact that the question is expressed using the vocabulary of log a which is different from the vocabulary of log b In other words for a given question the relevant answers found in the logs of some tools do not necessarily contain the keywords of the question Therefore the initial query created by taking the keywords of question may be relevant to logs generated by a tool but irrelevant to logs generated by another tool2 whereas we aim to answer questions regardless type of tools generating log files The existence of question keywords or their syntactic variants in a passage is an important factor to assess the relevance of the passage The approaches 2 While all of these logs report the same information using different vocabularies 360 H Saneifar et al which are based on the notion of common terms between questions and passages are detailed in Sect 3 Moreover the performance of a QA system depends largely on redundant o ccurrences of answers in the corpus in which answers are seek 1 7 The metho ds developed for QA systems are generally based on the assumption that there are several instances of answers in corpus But information is rarely repeated in the log files of IC design tools This means that for a question there is only one o ccurrence of the answer in the corpus and thus one relevant passage containing the answer In addition design tools change over time often unexpectedly Therefore the format of the data in the log files changes which makes automatic data management difficult Moreover the language used in these logs is a difficulty that impacts information extraction methods Although the language used in these logs is English their contents do not usually comply with classic grammar In the pro cessing of log files we also deal with multi format data textual data numerical data alphanumerical and structured data e g table and data block There are also many technical words that contain special characters which are only understandable considering the domain documentation Due to these specific characteristics of log files NLP and IR metho ds developed for texts written in natural language are not necessarily well adapted to log files We therefore suggest the enrichment of initial queries in order to make them relevant to all types of logs generated by any kind of design tools We explain the query enrichment process in Sect 4 2 2 Passage Retrieval in Log Files The passages retrieval phase influences significantly the performance of QA systems because final answers are sought in the retrieved passages Most QA systems for a given question extract a large number of passages which likely contain the answer But an important point in QA systems is to limit as much as possible the number of passages in which the final answer extraction is performed Since we are situated in a very specialized domain high precision in the final answers i e the percentage of correct answers is a very important issue This implies that the passage retrieval system has to classify relevant passages based on a relevance score in the top positions among all retrieved candidate passages 3 Related Work Most passage retrieval algorithms depend on occurrences of query keywords in corpus 9 To enhance the query the use of morphological and semantic variants of query keywords is studied in 2 The reformulation of questions also referred to as surface patterns and paraphrases is a standard metho d used to improve the performance of QA The technique is based on identifying various ways of expressing an answer given a natural language question 5 For example for a question like Who founded the Passage Retrieval in Log Files An Approach Based on Query Enrichment 361 American Red Cross QA systems based on surface patterns seek reformulations like the founder of the American Red Cross is X or X the founder of the American Red Cross The question reformulation using surface patterns is also exploited in TREC9 and TREC10 8 and 5 present different approaches to take semantic variations semantic reformulations into account in order to complement the syntactic variants To find relevant passages 6 evaluates each passage using a scoring function based on the coverage of question keywords which exist also in the passage QA systems also use query expansion metho ds to improve performance of retrieval These metho ds can use the thesaurus 4 or be based on the incorporation of the most frequent terms in the m relevant do cuments Despite the satisfactory results achieved by the use of surface patterns and syntactic variants in mentioned work these metho ds are irrelevant in the context of log files according to the problems described in Section 2 Indeed the main reasons for irrelevancy of such metho ds are related to the fact that an answer is not reformulated in different ways in a corpus of log files Also there is a lack of redundancy of answers in corpus of logs In addition there are several technical and alphanumeric keywords in the domain of log files for which the use of syntactic or semantic variants appears to be complex or unmeaning 4 Passage Retrieval Based on Query Enrichment We propose in this paper a passage retrieval approach based on a new interactive pro cess of query enrichment The enrichment of query is based on a context learning pro cess and is asso ciated with a novel and original term weighting function Our protocol of context learning is designed to determine the context of a given question by analyzing the terms 3 co o ccurring around the question keywords in the corpus The new term scoring function proposed in this paper identifies the terms which are related to the answers The architecture of our approach consists of three main modules 1 Enrichment of the initial query by context learning 2 Enrichment by terms which are likely related to answer 3 Passage retrieval using the enriched query The first mo dule enriches the initial query extracted from a question in natural language This mo dule aims at making the initial query relevant to all types of logs which are generated by different tools At this step by learning the context of question we enrich the initial query by the most significant terms of the context The second mo dule is activated for a second enrichment of the query in order to obtain a higher accuracy At this phase we aim at identifying the terms which are likely related to answer in order to integrate them in the query For 3 In this paper the word term refers to both words and multi word terms of the domain 362 H Saneifar et al this purpose we propose a process of scoring of terms based on a new weighting function The weighting function is designed to give a score to terms according to their relatedness to answers We devote Section 5 to this topic In the third mo dule we seek the relevant passages in the logs generated by a tool different from the one which has been used in the learning phase That is we have two different corpora of logs The first one called training corpus is used in learning phase and the second corpus called test corpus is the corpus in which we retrieve the passages relevant to given questions The logs of the test corpus have structures and a vocabulary significantly different from the logs of the training corpus In our approach we look for specialized context called hereafter lexical world of question keywords In order to characterize and present in a relevant way the specialized context of keywords we use the terminological knowledge extracted from logs Before explaining the query enrichment pro cesses we develop the concept of lexical world and the use of terminological knowledge in the following subsections 4 1 Lexical World The lexical world of a term is a small fragment of do cument in which the term is seen We consider this fragment specialized context of the term because terms located around a term within a small fragment generally have a strong semantic and or contextual relations Therefore by determining the lexical world of a term in a do cument we identify terms that tend to appear around it in that do cument We do not put any limit on the size of lexical worlds eg a few lines a few words etc as it have to be determined pragmatically based on the type of do cuments In order to present the lexical world of a term several solutions are possible As a first solution we characterize the lexical world of a term by a set of words called also bag of words which are located around the term and present Noun Verb or Adjective parts of speech As a second solution the lexical world of a term is presented by co o ccurring words like bigrams of words i e any two adjacent words or multi word terms few adjacent words forming a meaningful term which are seen around the term We detail this point in the next section 4 2 Terminological Knowledge As mentioned above the lexical worlds can be characterized in different ways By words multi word terms or bigrams of words According to our experiments the multi word terms and words are more representative than bigrams of words Hence we create two types of lexical world 1 Consisting of words and 2 Consisting of multi word terms and words In order to determine the multi word terms we extract the terminology of logs using the metho d presented in 13 This metho d adapted to the specific characteristics of logs extracts the multi word terms according to syntactic patterns in the log files To choose the relevant and domain specific terms we use the terminology validation and filtering proto col presented in 12 We have finally the valid and relevant multi word terms to characterize lexical worlds Passage Retrieval in Log Files An Approach Based on Query Enrichment 363 4 3 Query Enrichment by Context Learning We explain here the first module of our query enrichment approach For a given question we look initially to learn the context of the question and characterize it by its most significant terms These terms represent at best the context of the question Thus it is expected that the passages corresponding to the question share some of these terms regardless of different kind of log files Firstly we seek lexical worlds of question keywords The found lexical worlds and the initial query are vectorized We use an IR system based on Vector Space VS mo del in order to select the lexical world most correlated to the initial query Then we choose the most representative terms of selected lexical world For this purpose we select the n terms having the highest tf idf scores 11 We get the first enriched query by inserting the selected terms in the initial one Since the next phase of query enrichment based on the novel weighting function is the main contribution of this paper we develop it in Section 5 Thus we explain in the following subsection how we look for relevant passages once initial queries are enriched 4 4 Passage Retrieval in Log Files We detail here the pro cess of finding relevant passages on the test corpus of log files First we segment the logs of the test corpus Segmentation is performed according to the structure of the text like data blocks tables separating lines etc Each segment is seen as a passage of log files containing potentially the answer Second we enrich the initial query in order to make it relevant to all types of log files We remind that we aim at adapting the initial query to vocabulary of all types of log files by our query enrichment approach Third we build a system of IR in order to find the relevant passages The IR system uses an indexing function and a similarity measure In this phase we experiment our IR system by using tf idf and Binary representation as indexing functions The Cosine and Jaccard measures are used as a similarity measure The IR system gives a relevancy score to every passage segment according to the enriched queries Then we order the passages based on their relevancy score and propose the top ranked passages to the user Several approaches of passage retrieval return a considerable number of candidate passages Our experiments conducted on real data assert that in more than 80 of the cases the relevant passage is located among the three topranked passages That is our approach often ranks the relevant passage among the three top candidate passages returned by a system as possible results 5 How to Find Terms Correlated to Answers To improve the relevance of the query to different types of logs we propose a second module of enrichment of the query In this mo dule we have as input the query enriched in the phase of context learning and the test corpus of logs in 364 H Saneifar et al which we seek the relevant passages different from the training corpus which is used in the context learning phase Our motivation is to find the terms in the logs of the test corpus which are likely to exist in the relevant passage and are therefore related to the answer For this purpose we offer here a term selection pro cess based on a new term weighting scoring function Terms will be selected based on their score obtained by this scoring function This function gives a score to each term based on three assumptions 4 The learning process is performed on the logs of the training corpus which are generated by a tool using a significantly different vocabulary and structures from the tool generating the logs of the test corpus Passage Retrieval in Log Files An Approach Based on Query Enrichment 365 The final score of a term that we call T RQ Term Relatedness to Query is calculated using the following formula TRQ lwf 1 idf According to the experiments the most relevant value of is 0 255 This means that we give more weight to the frequency of terms in the corpus and a smaller weight but which influences the final results to lwf We explain with an example the pro cess of selection of terms which are likely related to the answer Supposing Q Wa Wb Wd as a query enriched by the first mo dule learning phase and logb as a log file containing seven segments S1 Wa Wk Wm Wb S2 Wd Wk S3 Wz S4 Wa Wc We Wq S5 Wb We S6 Wz S7 Wb Wc Wk We consider that the border of lexical worlds border of the selected fragment of text around a given term corresponds to the border of segments i e a lexical world is not bigger than the corresponding segment Among sept segments there are fives which are asso ciated with terms of Q Thus we obtain S1 S2 S4 S5 S7 as the set of lexical worlds of question keywords The following lists shows the lexical worlds associated to each keywords of the question6 Wa S1 S4 Wb S1 S5 S7 Wd S2 Here for instance the word Wa in the query Q is associated with two lexical worlds S1 and S4 The idf score of the word Wk for example is equal to log 5 3 0 22 because the word Wk exists in three lexical worlds S1 S2 and S7 among fives The value of lwf for the word Wk in the segment S1 is calculated as following lwf2 1 1 log 3 2 5 8 Indeed there are two words in the query Q i e Wa and Wb which are associated with the lexical world S1 the lexical world in which the word Wk is located We note that for a given term the value of lwf depends on the lexical world in which the given term is located For example the value of lwf for the word Wk located in segment S2 is equal to lwf2 2 1 log 3 1 2 1 as there is just one keyword of the query Q asso ciated to S2 This means that Wk located in the segment S2 is less significant less related to the query than when it is located in the segment S1 Once the T RQ score of all terms of lexical worlds are calculated we identify the k highest scores and select the terms having these scores However among the terms selected based on their T RQ scores there are terms having the same score To distinguish these terms we assess their tendency to appear close to the keywords of the initial query In other words we seek to calculate the dependence of selected terms to the keywords of question in the context For this purpose we choose the Dice measure This statistical measure has a goo d performance 5 6 We justify the selected value of by presenting some results in http www lirmm fr saneifar experiments TRQ pdf Since a word can be used in different contexts i e different fragments of document it can be associated with several lexical worlds 366 H Saneifar et al for tasks of text mining 10 For a term T and a query keyword W we calculate the Dice measure as following Dice T W 2 T W T W For us T W number of times T and W o ccur together corresponds to the number of times where T and W are located in the same line in the corpus of logs x shows the total number of o ccurrences of x in the corpus Finally the value of Dice measure allows to distinguish the terms obtained in the previous step which have equal T RQ score The final score of these terms is obtained by the sum of Dice value and the T RQ score Note that we select at first the terms according to their T RQ score i e terms having highest T RQ score are selected If we have the terms having the same T RQ score we distinguish them by calculating their Dice values Finally as described above the system ranks the terms based on their T RQ scores considering Dice values for terms having the same score The system recommends to the user the k top ranked terms in order to enrich the query Our system is also able to integrate automatically the k top ranked terms into the query in an autonomous mode The enriched query EQmod2 will be used in passage retrieval 6 Experiments We test the performance of our approach on a corpus of log files from the real industrial world data from the Satin IP company There are 26 questions which are expressed in natural language and are extracted from standard check list Log files are segmented according to their structures blank lines tables data blocks etc Each segment is potentially a relevant passage Note that for a given question there is only one relevant passage segment in the corpus i e there is just one occurrence of the answer in the corpus The relevance of passages is evaluated as whether the final answer is situated in the passage The test corpus that we use for the experiments contains 625 segments and is about 950 KB Each segment consists of approximately 150 words The training corpus used in the phase of learning the context of questions consists of logs reporting the same information as logs of the test corpus but generated by a totally different nb Question i 1 1 rank answer Passage Retrieval in Log Files An Approach Based on Query Enrichment 367 Table 1 Percentage of question P n for which the relevant passage is ranked as n a performance obtained by using the not enriched queries initial queries b performance obtained by using the enriched queries a tf idf Cos Jac P 1 9 8 P 2 5 4 P 3 4 3 M RR 0 51 0 50 Binary Cos Jac 11 8 3 3 2 2 0 58 0 48 b tf idf Cos Jac P 1 20 18 P 2 3 1 P 3 1 3 M RR 0 83 0 78 Binary Cos Jac 19 13 0 4 2 1 0 79 0 65 In the experiments we measure the performance of passage retrieval using the enriched queries We aim at comparing the performance of passage retrieval using the enriched queries with the performance of passage retrieval using the initial queries not enriched That shows how our query enrichment approach improves the relevance of initial queries Tables 1a 1b present respectively the results of the performance of passage retrieval using the not enriched query and the enriched ones P n is the percentage of questions for which the relevant passage is ranked as n among the retrieved candidate passages as possibilities Here we show the results for the three first ranks As shown in Tab 1a by using the not enriched queries we obtain a M RR value equal to 0 58 in best conditions While by enriching the initial queries as mentioned in this paper the M RR improves significantly and reaches 0 83 in best conditions According to the results in the best configuration of the IR mo dule and by using the enrichment of queries the relevant passage is ranked in 76 of cases as the first passage among the candidate passages returned by the system Also in 92 of cases the relevant passage is located ranked among the three top ranked passages returned by the system when there are about 650 passages in the corpus 7 Conclusions We have presented a pro cess of double enrichment of initial queries in order to improve the performance of passage retrieval in log files The heterogeneity of the vocabulary and structures of log files and the fact that the keywords used in the questions expressed in natural language do not exist necessarily in the logs make the passage retrieval difficult Despite these characteristics our approach makes it possible to adapt an initial query i e list of question keywords to all types of corresponding log files whatever is their vocabulary According to the results by our query enrichment protocol which is based on our novel weighting function called T RQ Term Relatedness to Query we obtained a value of M RR equal to 0 83 while the value of M RR was equal to 0 58 by using the not enriched queries We plan to evaluate our system with other mo dels of Information Retrieval Improving the new term weighting function used in the second phase of query enrichment represents a ma jor point in the future work 368 H Saneifar et al References 1 Brill E Lin J Banko M Dumais S Ng A Data intensive question answering In Proceedings of the Tenth Text REtrieval Conference TREC pp Part of Speech Tagging Using Parallel Weighted Finite State Transducers Miikka Silfverberg and Krister Department of Modern Languages University of Helsinki Helsinki Finland miikka silfverberg krister linden helsinki fi Abstract We use parallel weighted finite state transducers to implement a part of speech tagger which obtains state of the art accuracy when used to tag the Europarl corpora for Finnish Swedish and English Our system consists of a weighted lexicon and a guesser combined with a bigram model factored into two weighted transducers We use both lemmas and tag sequences in the bigram model which guarantees reliable bigram estimates Keywords Weighted Finite State Transducer Part of Speech Tagging Markov Model Europarl 1 Introduction Part of Speech POS taggers play a crucial role in many language applications such as parsers speech synthesizers information retrieval systems and translation systems Systems which need to process a lot of data benefit from fast taggers Generally it is easier to find faster implementations for simple mo dels than for complex ones so simple models should be preferred when tagging speed is crucial We demonstrate that a straightforward first order Markov mo del is sufficient to obtain state of the art accuracy when tagging English Finnish and Swedish Europarl corpora Koehn 2005 The corpora were tagged using the Connexor fdg parsers H Loftsson E 370 M Silfverberg and K and the other one assigns weights for bigrams starting at o dd positions Both bigram mo dels are implemented as WFSTs The sentence WFST and bigram mo del WFSTs are combined using weighted intersecting composition Silfverberg and 2 Previous Research Statistical POS tagging is a common task in natural language applications POS taggers can be implemented using a variety of statistical mo dels including Hidden Markov Models HMM Church 1999 Brants 2000 and Conditional Random Fields Lafferty et al 2001 Markov mo dels are probably the most widely used technique for POS tagging Some older systems such as Cutting 1992 used first order mo dels but the accuracies reported were not very goo d E g Cutting 1992 report an accuracy of 96 for tagging English text Newer systems like Brants 2000 have used second order mo dels which generally lead to better tagging accuracy Brants 2000 reports accuracy of 96 46 for tagging the Penn Tree Bank More recent second order mo dels further improve on accuracy Collins 2002 reports 97 11 accuracy and Shen et al 2007 97 33 accuracy on the Penn Tree Bank We use lemmas in our bigram model as did Thede and Harper 1999 who used lexical probabilities in their second order HMM for tagging English and obtained improved accuracy 96 Part of Speech Tagging Using Parallel Weighted Finite State Transducers 371 3 Formulation of the POS Tagging Task In this section we formulate the task of Part of Speech POS tagging and describe probabilistic POS taggers formally By a sentence we mean a sequence of syntactic tokens s s1 sn and by a POS analysis of the sentence s we mean a sequence of POS analyzes t t1 tn We include lemmas in POS analyzes For each i the analysis ti corresponds to the token si in sentence s We denote the set of all sentences by S and the set of all analyzes by T A POS tagger is a machine which associates each sentence s with its most likely POS analysis ts To find the most likely POS analyzes for the sentence s the model estimates the probabilities for all possible analyzes of s using a distribution P For the sentence s and every possible POS analysis t the distribution P associates a probability P t s Keeping t fixed the mapping s P t s is a normalized probability distribution The most likely analysis ts of the sentence s is the analysis which maximizes the probability P t s i e ts arg max P t s t The distribution P can consist of a number of component distributions Pi each giving probability Pi s t for sentence s and analysis t The component probabilities are combined using some function F 0 1 n 0 1 to obtain P s t F Pi t s Pn t s The function F should be chosen in such a way that P is nonnegative and satisfies P t s 1 tT for each sentence s Often a convex linear function F is used to combine estimates given by the component models In such a case the model P is called a linear interpolation of the mo dels Pi 4 A Probabilistic First Order Model In this section we describe the idea behind our POS tagger We use a bigram mo del for POS tagging Thus the probability of a given tagging of a sentence is estimated using analyzes of word pairs Since we make use of extensive training material we may include lemmas in bigrams Although the training material is extensive the tagger will still encounter bigrams which did not o ccur in the training material or only occurred once or twice In such cases we want to use unigram probabilities for estimating the best POS analysis Hence we weight all analyzes using probabilities given by both the unigram and bigram models but weight bigram probabilities heavily while only giving unigram probabilities a small weight Hence unigram probabilities become significant only when bigram probabilities are very close to each other 372 M Silfverberg and K 4 1 The Unigram Model The unigram model emits plain unigram probabilities pu t sx for analyzes t given a word form sx we use the index x to signify that pu t sx is independent of the context of the word form sx Unigram probabilities are readily computed from training material The probability of the analysis t t1 tn given the sentence s s1 sn assigned by the unigram mo del is n Pu t s i 1 pu ti si In practice it is not possible to train the unigram model for all possible word forms in highly inflecting languages with pro ductive compounding mechanism such as Finnish or Turkish Instead the probabilities for analyzes given a word form need to be estimated using probabilities for words with similar suffixes For instance if the word form foresaw was not observed during training we can give it a similar distribution of analyzes as the word saw receives since saw shares a three letter suffix with foresaw In practice such estimation relying on analogy is accomplished by a so called POS guesser which seeks words with maximally long suffixes in common with an unknown word It then assigns probabilities for POS analyzes of the unknown word on basis of the analyzes of the known words Linden 2009a shows how a guesser can be integrated with a weighted lexicon in a consistent way 4 2 The Bigram Models We use two bigram mo dels Qo and Qe giving probabilities for bigrams starting at even and o dd positions in the sentence The estimates are built using plain bigram probabilities for tagging a word pair s1 and s2 with analyzes t1 and t2 respectively1 These probabilities pb t1 s1 t2 s2 are easily computed from a training corpus For an analysis t t1 t2k and a sentence s s1 s2k of even length 2k the models Qo and Qe give bigram scores k k 1 Qo t s i 1 pb t2i 1 s2i 1 t2i s2i Qe t s i 1 pb t2i s2i t2i 1 s2i 1 For an analysis t t1 t2k 1 and a sentence s s1 s2k 1 of o dd length 2k 1 the mo dels Qo and Qe give bigram scores k k Qo t s i 1 1 pb t2i 1 s2i 1 t2i s2i Qe t s i 1 pb t2i s2i t2i 1 s2i 1 In literature it is often suggested that one should instead compute probabilities of word form bigrams given POS analysis bigrams We cannot do this since we include lemmas in POS analyzes This makes the probability of a word form given a POS analysis either 0 or 1 since most analyzes only have one realization as a word form Part of Speech Tagging Using Parallel Weighted Finite State Transducers 373 4 3 Combining the Unigram and Bigram Models The standard way of forming a mo del from Pu Qo and Qe would be to use linear interpolation We do not want to do this since we aim to convert probabilities into penalty weights in the tropical semiring using the mapping p log p which is not compatible with sums Instead we take a weighted pro duct of powers of the component probabilities Hence we get a mo del P t s Pu t s wu Qo t s wo Qe t s we where wu we and wo are parameters which need to be estimated If each of the mo dels Pu Qe and Qo agree on the probability p of an analysis t given a sentence s we want P to give the same probability This is accomplished exactly when wu we wo 1 There does not seem to be any reason to prefer either of the mo dels Qe or Qo which makes it plausible to assume that we wo Hence an implementation of the model only requires estimating two non negative parameters the unigram parameter wu and the bigram parameter wb They should satisfy wu 2wb 1 It is possible that P t s will not be a normalized distribution when s is kept fixed but it can easily be normalized by scaling linearly with factor t P t s For the present implementation it is not crucial that P is normalized 5 Implementing the Statistical Model Using Weighted Finite State Transducers We describe the implementation of the POS tagger mo del using weighted finitestate transducers WFSTs We implement each of the components of the statistical mo del as a WFST which are trained using corpus data In order to speed up computations and prevent roundoff errors we convert probabilities p given by the mo dels into penalty weights in the tropical semiring using the transformation p log p In the tropical semiring the pro duct of probabilities pq translates to the sum of corresponding penalty weights log p log q The k th power of the probability p namely pk translates to a scaling of its weight k log p These observations follow from familiar algebraic rules for logarithms In our system tagging of sentences is performed in three stages using four different WFSTs The first two WFSTs a weighted lexicon and a guesser for unknown words implement a unigram mo del They pro duce weighted suggestions for analyzes of individual word forms The latter two WFSTs re score the suggestions using bigram probabilities The weights log p given by the unigram mo del and the bigram model are scaled by multiplying with a constant in order to prefer analyzes which are strong bigrams The scaled weights k log p are then added to give the total scoring of the input sentence This corresponds to multiplying the powers pk of the corresponding probabilities In the first stage we use a weighted lexicon which gives the five best analyzes for each known word form In initial tests the correct tagging for a known word 374 M Silfverberg and K could be found among the five best analyzes in over 99 of tagged word forms so we get sufficient coverage while reducing computational complexity For an unknown word x we use a guesser which estimates the probability of analyzes using the probabilities for analyzes of known words We find the set of known word forms W whose words share the longest possible suffix with the word form x We then determine the five best analyzes for the unknown word form x by finding the five best analyzes for words in the set W For each word si in a sentence s s1 sn we form a WFST Wi which is a disjunction of its five best analyzes t1 t5 according to the weights w si ti given by the unigram mo del In case there are less than five analyzes for a word we take as many as there are We then compute a weighted concatenation Ws of the individual WFSTs Wi The transducer Ws is the disjunction of all POS analyzes of the sentence s where each word receives one of its best five analyzes given by the unigram model To re score the analysis suggestions given by the lexicon and the guesser we use two WFSTs whose combined effect gives the bigram weighting for the sentence One of the mo del scores bigrams starting at even positions in the sentence and the other one scores bigrams starting at o dd positions Thus we give a score for all bigrams in the sentence without having to compute a WFST equivalent to the intersection of the models which might be quite large Using weighted intersecting composition Silfverberg and Using a tagged corpus we form a weighted lexicon L which re writes word forms to their lemmas and analyzes POS analyzes for a word form si are weighted according to their frequencies which are transformed into tropical weights In order to estimate the weights for words which were not seen in the training corpus we construct a guesser For an unknown word the guesser will try to construct a series of analyzes relying on information about the analyzes of known similar words Figure 1 shows an example guesser which can be constructed from a reversed weighted lexicon Guessing begins at the end of the word We allow guessing at a particular analysis for a word only if the word has a suffix agreeing with the analysis See Linden 2009a for more information on guessers 5 2 The Bigram Models To re score analyzes given by the unigram model we use two WFSTs whose combination serves as a bigram model The first one Be scores each known word form analysis bigram s2k s2k 1 and t1 t2 in the sentence starting at an Part of Speech Tagging Using Parallel Weighted Finite State Transducers 375 50 0 0 o o 0 g 0 0 0234 1 0 gs mon n 0 fni v 3 77 2 0 tab 3 0 g 4 5 d d 0 0 6 0 0 7 50 Fig 1 Guesser constructed from a weighted lexicon Guessing starts at the end of a word Skipping letters gives a high penalty and analyzes where equally many letters are skipped are weighted according to the frequency of the analyzes even position 2k according to the maximum likelihoo d estimate of the tag bigram t1 t2 w r t the word form bigram s2k s2k 1 The WFST Bo is similar to Be except it weights bigrams starting at odd positions s2k 1 s2k Given a word form pair s1 s2 we compute the probability P t1 s1 t2 s2 for each POS analysis pair t1 t2 These sum to 1 when w1 and w2 remain fixed Then we form a transducer B whose paths transform word form pairs s1 s2 into analysis pairs t1 t2 with weight log P t1 s1 t2 s2 Lastly we disjunct B with a default bigram which transforms arbitrary word form sequences to arbitrary analyzes with a penalty weight which is greater than the penalty received by all other transformations In addition to the mo del B we also compute a general word mo del W which transforms an arbitrary sequence of symbols into an arbitrary lemma and an analysis The word mo del W is used to skip words at the beginning and end of sentences From the transducers above we form the mo dels Be and Bo using weighted finite state operations Be W B W 0 1 and Bo B W 0 1 Here W 0 1 signifies an optional instance of W and tab cc tab 1 100 2 will tab v auxmod 3 6 8 tab 9 4 7 dog tab n nom sg 5 50 dog tab v inf 0 Fig 2 A small example of an even bigram model Be signifies an arbitrary symbol and signifies an arbitrary POS analysis symbol 376 M Silfverberg and K 5 3 Parsing Using Weighted Intersecting Composition In our system parsing a sentence S is in principle equivalent to finding the best path of the transducer S L Be Bo Since the intersection of Bo and Be could become prohibitively large we instead use intersecting composition Silfverberg and 6 Data In this section we describe the data used for testing and training the POS tagger For testing and training we used the Europarl parallel corpus Koehn 2005 The Europarl parallel corpus is a collection of pro ceedings of the European Parliament in eleven European languages The corpus has markup to identify speaker and some html markup which we removed to pro duce a file in raw text format We used the Finnish English and Swedish corpora Since the training and testing materials are the same for all three languages the results we obtain for the different languages are comparable We parsed the Europarl corpora using Connexor functional dependency parsers fi fdg for Finnish sv fdg for Swedish and en fdg for English Table 1 Some figures describing the test and training material for the POS tagger Language English Finnish Swedish Syntactic tokens 43 million 25 million 38 million Sentences 1 million 1 million 1 million POS tag sequences 122 2194 243 Table 1 describes the data used in training and testing the POS tagger We see that the fi fdg parser for Finnish emitted more than ten times as many tag sequences as sv fdg for Swedish or en fdg for English The en fdg parse emitted clearly fewest tag sequences Part of Speech Tagging Using Parallel Weighted Finite State Transducers 377 7 Training the Model We now describe training the model which consists of two phases In the first phase we build the weighted lexicon and guesser and the bigram mo dels In the second phase we estimate experimentally coefficients wu and wb which maximize the accuracy of the interpolated model P t s Pu t s wu Qo t s wb Qe t s wb Using a small material covering 1000 syntactic tokens we estimated wu 0 1 and wb 0 45 This shows that it is beneficial to weight the bigram mo del heavily which seems natural since bigrams provide more information than unigrams 100 1 GRAM 2 GRAM 95 90 ACCURACY 85 80 75 70 2 2 5 3 3 5 4 4 5 SIZE OF TRAINING CORPUS 10 n 5 5 5 6 Fig 3 The accuracy for the English POS tagger as a function of the size of training data We used between 102 and 106 sentences for training The lower curve displays the accuracy using only the unigram model whilst the upper curve displays the accuracy of the combined unigram and bigram model Figure 3 shows learning curves for the English language POS tagger using 102 to 106 sentences for training The lower curve displays accuracies for the unigram mo del and the upper curve shows the accuracy for the combined unigram and bigram mo del For the unigram model we can see that little improvement is obtained by increasing the training data from 104 sentences In contrast there is significant improvement 0 82 for the bigram mo del even when we move from 105 to 106 sentences 378 M Silfverberg and K 8 Evaluation We describe the metho ds we used to evaluate the POS tagger and the results we got We used ten fold cross validation to evaluate the POS tagger that is we split the training material in ten equally sized parts and used nine parts for training the model and the remaining part for testing Varying the tenth used for testing we trained ten POS taggers for each language For each of the languages we trained two sets of taggers One set used only unigram probabilities for assigning POS tags The other used both unigram and bigram probabilities We may consider the unigram taggers as a baseline For each tree languages we computed the average and standard deviation of the accuracy of the unigram and bigram taggers In addition we computed the Wilcoxon matched pairs signed ranks test for the bigram and unigram accuracies in all three languages The test does not assume that the data is normally distributed unlike the paired t test The results of our tests can be seen in table 2 Table 2 Average accuracies and standard deviations for POS taggers in Finnish English and Swedish The sixth column shows the improvement which results for adding the bigram model In the seventh column we show the results of the Wilcoxon matched pairs signed ranks test between unigram and bigram accuracies Language English Finnish Swedish Unigram Acc 93 10 94 38 94 12 0 09 0 07 0 20 Bigram Acc 98 29 96 63 97 31 0 01 0 03 0 11 Diff 5 19 2 25 3 19 Conf 99 8 99 8 99 6 9 Discussion and Future Work It is interesting to see that a bigram tagger can perform equally well or better than trigram taggers at least on certain text genres The mean accuracy 98 29 we obtained for tagging the English Europarl corpus is exceptionally high for example Shen et al 2007 report a 97 33 accuracy on tagging the Penn Tree Bank The improvement of 5 19 percentage points from the unigram model to the combined unigram and bigram model is also impressive There is also a clear improvement for Finnish and Swedish when the bigram mo del is used in tagging and accuracy for these languages is also high We had problems finding accuracies figures for statistical taggers of Finnish but for Swedish Megyesi 2001 reports accuracies between 94 and 96 which means that we get state of the art accuracy for Swedish Of course the Europarl corpus is probably more homogeneous than the Penn Tree Bank or the Brown Corpus both of which include texts from a variety of genres Furthermore tagging is easier because the en fdg parser only emits 122 different POS analyzes Still Europarl texts represent an important genre Part of Speech Tagging Using Parallel Weighted Finite State Transducers 379 because the EU is constantly producing written materials which need to be translated into all official languages of the union The accuracy for Finnish shows less improvement than English and Swedish We believe this is a result of the fact that Finnish words carry a lot of information but the bonds between words in sentences may be quite weak This conclusion is supported by the fact that unigram accuracy for Finnish is best of all three languages We do not believe that using trigram statistics would bring much improvement for Finnish Instead we would like to write a set of linguistic rules which would cover most typically occurring tagging errors Especially we would like to try out constraints which would mark certain analyzes as illegal in some contexts Such negative information is hard to learn using statistical metho ds Still it may be very useful so it could be provided by hand crafted rules Clearly our figures for accuracy need to be considered in relation to the tagging accuracy of the fdg parsers We did not succeed in finding a study on the POS tagging accuracy of the fdg parsers Instead we examined the POS tagging for one word per twenty thousand in the first tenth of the Europarl corpora for Finnish English and Swedish This amounted to 131 examined words for Finnish 219 examined words for English and 191 examined words for Swedish According to these tests the POS tagging accuracy of the fdg parsers for Finnish is 95 4 for English it is 97 3 and for Swedish it is 97 5 10 Conclusions We intro duced a model for a statistical POS tagger using bigram statistics with lemmas included We showed how the tagger can be implemented using WFSTs We also demonstrated a new way to factor a first order mo del into a mo del tagging bigrams at even positions in the sentence and another mo del tagging bigrams at o dd positions In order to test our mo del we implemented POS taggers for Finnish English and Swedish training them and evaluating them using Europarl corpora in the respective languages and Connexor fdg parsers We obtained a clear statistically significant improvement for all three languages when compared to the baseline unigram tagger At least for English and Swedish we obtain state of the art accuracy Acknowledgements We thank the anonymous referees We also want thank our colleagues in the Hfst team The first author is funded by Langnet Graduate School for Language Studies References Brants 2000 Brants T 380 M Silfverberg and K Collins 2002 Collins M Discriminative Training Methods for Hidden Markov Models Theory and Experiments with Perceptron Algorithms In EMNLP 2002 Cutting 1992 Cutting D Kupiec J Pedersen J Sibun P A Practical Part ofSpeech Tagger In Proceedings of the Third Conference on Applied Natural Language Processing 1992 Automated Email Answering by Text Pattern Matching Eriks Sneiders Department of Computer and Systems Sciences Stockholm University Forum 100 SE 16440 Kista Sweden eriks dsv su se Abstract Answering email by standard answers is a common practice at contact centers Our research assists this process by creating reply messages that contain one or several standard answers Our standard answers are linked to representative text patterns that match incoming messages The system works in three languages The performance was evaluated on two email sets the main advantage of our email answering technique is good correctness of the delivered replies Keywords Automated email answering automatic email response text message answering question answering text patterns 1 Introduction It is not unusual that an email flow to a contact center aka customer care center customer service contains frequently reoccurring inquiries therefore agents who communicate with customers use predefined response templates as draft answers Finding a predefined answer if it exists is a task that a computer can do Answering a generic question e g Can I pay with my Visa card would be easy More specific requests e g Please update my address in your customer database are less trivial Fortunately many companies have a web based self service system where a customer logs in and interacts with the system without mediation of a contact center agent Hence an automated answer can advise using the self service system where appropriate Katakis et al 1 present a good introduction to email management techniques and their application domains Most research has been done assuming personal use of email Automated message answering at contact centers has raised less interest Most email answering systems pursue the text classification approach A typical system perceives a message as a bag of words represented by a term vector with tf idf weights Normally the words are stemmed and stop words removed Further a typical system is trained on sample documents in predefined classes The most popular learning algorithms are Support Vector Machine and H Loftsson E 382 E Sneiders Weng and Liu 6 assign a set of representative concepts with weighted terms to each class of messages When a new message arrives its weight is calculated with respect to each concept set considering the terms in the message and their weights Very few email answering systems do text generation Marom and Zukerman 7 create new response texts by selecting the most representative sentences from previous responses to messages similar to the new incoming message Kosseim at al 8 follow the tradition of Information Extraction and operate a number of templates for capturing intention concepts named entities and relations When a new message arrives the system fills the templates performs semantic validation and generates the answer This paper introduces an email answering approach that has been used for sending replies without any human intervention as well as generating draft replies for contact center agents The system operates a database of standard answers and text patterns linked to these answers that record expected wordings in messages to be answered 2 Pattern Based Email Answering This research stems from an earlier work in automated FAQ answering in a restricted domain the system could answer about 70 of the queries 9 out of 10 answers correct 9 A natural step forward is to adapt the technique to answer larger pieces of text such as email messages 2 1 Question Templates The email answering system has a few standard answers that respond to the most frequent inquiries Each standard answer has a title that summarizes it in one sentence which helps to avoid confusion while reading the automatic reply if the answer does not quite correspond to the original message Furthermore each standard answer is linked to a number of question templates that record the expected text patterns of the future inquiries to be answered by this standard answer Fig 1 The syntax of our text patterns resembles that of regular expressions the text patterns are less rigorous though Each question template contains two patterns The required pattern matches a piece of message text if the message fits this standard answer The forbidden pattern must not match the message it detects details that disqualify the answer Please observe that the question templates are created before the actual email answering starts answ answ 2 answ 1 Required pattern Forbidden pattern Fig 1 Standard answers with their question templates Automated Email Answering by Text Pattern Matching 383 2 2 Steps of the Answering Process During the email answering process a new incoming message is split into paragraphs each paragraph into sentences each sentence into An empty synonym set matches everything therefore bright light matches bright and light with 0 3 any lexical items in between Phrases match compound words For example new paper matches new paper and newspapers Because compound words are popular in some languages such as Swedish we have special syntax for them e g news paper is equal to news paper 384 E Sneiders We define reoccurring pieces of text patterns as substitutes and reuse them For example we define ford volvo as car_name and use it in car_name car vehicle repair In order to minimize words stem ambiguity the stems are required to have at least five letters before the asterisk In the syntax for compound words a component must be at least four letters long or at least two if there is another component at least five letters long This has proved a sufficient trade off between the ambiguity of word stems and their ability to represent concepts 2 4 Test Beds Before we discuss real life examples of the text patterns let us introduce our test beds The email answering system was implemented at two contact centers The first one was an insurance company that employed fully automated email answering with 11 standard answers in Swedish The system scanned through all incoming messages If it could answer the message it sent a reply to the from and cc addresses of the original message The reply informed its reader that it was computer created and contained simple instructions how to reach a human agent if necessary Messages that could not be answered were passed to a human agent The second contact center worked for a telecom service provider It used 4 standard answers in Latvian Here email answering was not fully automated the system created draft reply messages for the agents of the contact center 2 5 Flexibility of a Text Pattern The following example illustrates what a text pattern may look like 2_4_hjul vad kosta A text pattern is more than a loose regular expression it embodies a question specific synonym dictionary and a quasi ontology Furthermore a combination of words and matching rules is delicate even small changes may influence the accuracy of detecting relevant texts Therefore at the current stage of our research management of the text patterns requires a manual effort In order to get an impression about the potential size of a text pattern let us see how many terms in a piece of query text i e a paragraph one pattern can match taking the insurance case see Section 2 4 as an example The 11 standard answers were linked to 161 required text patterns In average a text pattern matched 5 terms in a paragraph if the system delivered an answer Potentially the text patterns could match between 3 and 40 terms in a paragraph Fig 2 shows the largest top curve and smallest bottom curve number of terms in a query paragraph that a text pattern could possibly match for each of the 161 patterns lined up on the horizontal axis On the very left side there are text patterns that can match just over 40 query terms In the middle of the line up the text patterns can match up to 15 20 query terms The smallest text patterns on the right side can match just over 5 Automated Email Answering by Text Pattern Matching 45 40 35 30 25 20 15 10 5 0 161 required text patterns 385 terms in a query paragraph The bottom curve oscillates like an electrocardiogram with average 4 8 terms in a query paragraph that a text pattern must match as a minimum Why can one text pattern match a variable number of query terms A synonym set may contain phrases of different length The largest smallest number of matching terms can be reached if the system always selects the longest shortest synonym phrase or a basic lexical item instead of the shortest phrase in each synonym set Fig 2 suggests that a text pattern may contain a large number of parallel wordings that capture a variety of paraphrases of a meaningful statement Furthermore text patterns are built to match future messages and we are uncertain what exactly these messages will look like We are likely to compensate this uncertainty with some redundancy in the text patterns caused by guessing the future wordings Hereby the text patterns are more complex but also more expressive than just a set of keywords and a synonym dictionary 3 Spelling Correction Email texts are often untidy Tang et al 10 inspected more than five thousand webbased newsgroup messages in English and discovered that 73 2 of them needed paragraph normalization 85 4 needed sentence normalization 47 1 needed upperlower case restoration and 7 4 of the messages contained misspellings Dalianis 11 inspected spelling in another context of Internet based communication and found out that about 10 of the search queries submitted to a search engine of the Swedish tax authority were misspelled We did not count misspelled messages processed by our system yet we know that without spelling alterations our text pattern matching would fail to identify many pertinent messages because the matching rules embodied in these patterns are strict and even one letter wrong would result in a no match Furthermore our system faces three challenges First of all spelling alternatives are sought among words word stems and phrases from the text patterns rather than in off the shelf tools Number of matching query terms Fig 2 Number of query terms that can match a required text pattern 386 E Sneiders The second challenge is multiple languages The system works with texts in English Swedish Germanic Indo European languages and Latvian Baltic IndoEuropean language The third challenge is substitution of language specific character sets with the ISO8859 1 or ASCII character sets present on virtually all computers Of the three languages that our system works with Latvian emails are most exposed to character set substitution The Latvian alphabet has 33 letters of which 11 are not included in the ISO 8859 1 character set and may get replaced with English letters For example becomes a or aa becomes k or kj or kk etc There are no rules everything goes as long as people grasp the text In this case we deal with a deliberately altered syntax a pidgin language rather than misspellings The cure however is the If a standalone word stem not a part of a phrase is considered as a spelling alternative the corrected word must be no more than three letters longer than the stem in order to hinder replacement of a compound word with the stem of its first component Automated Email Answering by Text Pattern Matching 387 Phrases as spelling alternatives Phrases from the text patterns simple ones without other embedded phrases are the main spelling alternatives for misspelled compound words The system tests a phrase as a spelling alternative of a query word according to the following principle Let us consider an example and test 4 Performance Measurements Before the email answering system can start operating it is trained to recognize email texts that fit a given standard answer We say trained in quotes because this is not training as understood in machine learning We use some text filtering clustering and aggregation tools in order to group messages Then the messages are analyzed and text patterns created Today the text patterns are crafted manually increased automation of this process is further research and lies outside the scope of this paper Training messages Section 2 4 introduced our test 388 E Sneiders In the insurance case we had access to 5148 messages before the performance test many of these messages had been analyzed in order to manually create and adjust the text patterns In the telecom case we had access to two sets of messages Initially we had 4782 messages of which 706 messages corresponded to the 4 standard answers that the system included in its automated replies During the operation of the system but before the performance test we acquired 3768 more messages that were analyzed in order to manually adjust the text patterns Because the systems were running in production settings the training phase was the entire period the systems had been in operation While doing the training we paid more attention to correctness of the replies rather than recall We rather process a message manually than increase the risk of an incorrect reply Test messages The data for the performance measurements came from the systems logs We had no prior access to these messages we could not have used them in order to train the systems In the insurance case 3526 consecutive messages were analyzed In the telecom case 1314 consecutive messages were analyzed The correspondence between query messages and their automated replies was judged by humans third party observers 4 1 Precision Recall and Correctness We applied two evaluation criteria The first criterion was precision and recall calculated for each query message separately The second criterion was correctness of the replies actually given We distinguished between Table 1 shows precision and recall calculated for each query message separately Let us explain the insurance case From the 3526 inspected messages 395 messages got recall 1 20 messages got recall 0 5 179 messages got recall 0 In total 594 messages 395 20 179 had relevant answers in the system s database and some recall value The average Automated Email Answering by Text Pattern Matching Table 1 Precision and recall of the automatic replies Precision value Insurance case 1 0 67 0 5 0 33 Average 0 987 for 415 queries Telecom case 1 0 5 Average 0 98 for 124 queries 119 5 1 0 5 0 Total 124 Average 0 758 for 161 query 120 4 37 Total 161 404 1 9 1 Total 415 Average 0 682 for 594 queries Total 594 1 0 5 0 395 20 179 Num queries with the precision value 389 Recall value Num queries with the recall value Table 2 Correctness of the automatic replies Fully correct Insurance case 371 76 65 Telecom case 111 76 03 6 4 11 9 6 16 4 2 74 16 10 96 146 100 28 5 79 24 4 96 20 4 13 41 8 47 484 100 Technically correct Partial recall 1 Partial recall 1 Incorrect Total A sharp eyed reader has probably noticed that numbers in Table 1 and Table 2 do not match Let us take the insurance case in order to explain these numbers 371 fully answered plus 24 20 partially answered queries make 415 total precision queries Incorrectly answered queries have either zero recall or do not have any recall and precision values at all Technically correct replies were judged as incorrect when precision and recall where calculated because these queries should not have been answered Therefore technically correct replies did not get any recall and precision values Both implementations of the system show surprisingly similar correctness and average precision percentages despite different languages and knowledge domains Correctness of the replies is good From all the answered queries 390 E Sneiders around 76 were answered fully around 85 were answered fully or partially around 90 got the issues identified fully or partially fully correct technically correct partial replies 4 2 Performance Figures in Context The performance figures are easier to grasp if observed in the context of related systems mentioned in the introduction whose performance measurement methods are similar to those of ours Message classification Busemann et al 2 managed to choose the right category in 56 23 cases using Support Vector Machine Weng and Liu 6 reached top performance i e the highest F value at 62 77 recall and 77 52 precision by finding representative terms in query messages and weighing these terms with respect to message classes Mapping a message to a standard answer Malik et al 5 made a human equivalent selection of answer templates in 61 cases human equivalent or incomplete selection in 73 4 by first extracting a set of representative questions for each standard answer training and then mapping these questions to questions in query messages test calculating taxonomy distance between words Information extraction and answer text generation Kosseim et al 8 delivered 66 4 correct answers by applying Information Extraction templates and performing semantic validation of the extracted information These figures give us an intuitive insight into viable quality of email answering In order to make any formal claims which system performs better we have to test the systems using the same input data and applying the same conditions 5 Advantages and Limitations Following are the advantages of our text pattern based automated email answering The system demonstrates a good correctness of the replies and a superior ability to identify questions and problem statements relevant to standard answers if the message is answered The text patterns operate in isolated narrow single answer knowledge domains where they are self sufficient Therefore the system can start operation with rendering one standard answer and the number of standard answers can gradually rise to as many as needed Because each text pattern is autonomous the system can easily handle several questions in one query message and put several standard answers into one reply message Because the text patterns are self sufficient the system has a low technological threshold or barrier that impedes its deployment i e the system operates without components such as part of speech taggers stemmers generic or domain ontologies We do need some tool support to analyze training messages however as stated in further research Automated Email Answering by Text Pattern Matching 391 Because of the low technological threshold our approach is affordable for rare domains small businesses and small languages where open source linguistic and knowledge representation tools as well as technologically advanced manpower are not readily available We have tested the system for English Swedish and Latvian and consider it portable to most European languages Our approach is most advantageous in settings where correctness of the replies is crucial where we want to maximize the end users experience where a list of ten candidate answers is not an option For example in fully automated email answering without any human mediation Especially advantageous our approach is for email flows with a high ratio of reoccurring inquiries such as the email flow in 4 where 9 standard answers cover 72 of all messages Still our automated email answering approach has at least two limitations First it is designed for narrow and stable domains only It should not be considered for text classification tasks in arbitrary text collections Second at the current stage of our research development of the text patterns is manual which lessens the practical value of our technique until at least partial automation of this process is achieved 6 Conclusions and Further Research Our automated email answering system maps incoming messages to standard answers by matching text patterns linked to the standard answers The technique was designed for narrow and stable domains such as an email flow at a contact center The main advantage of our technique is good correctness of the delivered replies Performance evaluation on two email collections in two languages showed that about 85 of the messages could be answered fully or partially about 90 had their questions and problem statements correctly identified We consider the technique applicable to the majority of the European languages Currently we are working on performance comparison between our system and Support Vector Machine References 1 Katakis I Tsoumakas G Vlahavas I Email Mining Emerging Techniques for Email Management In Vakali A Pallis G eds Web Data Management Practices Emerging Techniques and Technologies pp 392 E Sneiders 2 Busemann S Schmeier S Arens R G Message classification in the call center In Proc Sixth Conference on Applied Natural Language Processing pp A System to Control Language for Oral Communication Laurent Spaggiari1 and Sylviane Cardey2 Human Factors Dept EDYDNX section 527 M0151 0 Airbus Operations SAS 316 route de Bayonne F 31060 Toulouse France 2 Centre 1 Abstract In this paper we discuss the use of controlled languages not for written texts but for oral communication which has never been done before and this in safety critical domains Interference between languages could effectively cause accidents due to misunderstanding of messages whatever they are We discuss how firstly we could automatically detect eventual possibilities of misunderstanding due to mispronunciation or bad interpretation and secondly how to prevent these problems by using controlled languages We show that our methodology which is intensional in nature is much more productive than working in extension Keywords Controlled language oral communication language interferences 1 Introduction In this paper we discuss the use of controlled languages not for written texts but for oral communication which has never been done before and this in safety critical domains Interference between languages could effectively cause accidents due to misunderstanding of messages whatever they are Though research concerning interferences between different languages has been carried out at Airbus Operations SAS and in Centre 2 Natural Language Whether it be oral written or signed a communication will be considered as successful and efficient when the received message complies with the mental process used for reconstructing and interpreting the information within the message However natural language not only allows everyone to create many variations for the same H Loftsson E 394 L Spaggiari and S Cardey 3 Controlled Languages Contrary to natural language controlled languages CLs are favoured by industry because they refer to systems that limit the number of core vocabulary words of applicable grammar and stylistic rules Industry does not need Shakespeare or Chaucer industry needs clear concise communicative A System to Control Language for Oral Communication 395 4 The Oral Aspect Creators of CLs usually base their grammar restrictions on well established writing principles e g write short sentences with only one topic avoid passive form Furthermore despite the fact that these languages do not have many rules in common 7 they do share one main characteristic they deal with the written aspects of language and not with the oral It is exactly this oral aspect that we address here The fact is that messages are not only read but can also be heard using synthetic recorded voices in nuclear plants and airports for example Because the receiver of the message may not have the same mother tongue as the one he she hears and because one cannot expect him her to master it a syntactically and lexically controlled message may not be sufficient Indeed when looking at the following pairs one can easily imagine the potential consequences in case of a misunderstanding increase the temperature versus decrease the temperature the gear is uplocked versus the gear is unlocked In French for an Anglophone dessus and dessous will sound the same As a further example ambiguities can result from English phonemes not present in Thai as is illustrated in Table 1 8 We do not enter into the complexity of the matter here but we can already see that half for example becomes harp as well as ball which is pronounced born Table 1 Ambiguities resulting from English phonemes not present in Thai the phonetic transcriptions are in SAMPA English phonemes T D f v s z l r Sound in Thai t d d p w d t n l Ambiguities birth bird they day half harp vine wine bus bud buzz but ball born free flee 5 A System to Help When Creating Sentences That Are to Be Pronounced Based on the observations in the previous section we have devised a system that can help when creating sentences that are to be pronounced This system has the ability to detect within a list not only all the homophones e g night knight and the minimal pairs e g brake brain but also quasi homophones e g increase decrease for a proposed word according to the source language North American English 396 L Spaggiari and S Cardey 5 1 The Database The database we used for checking the pronunciation is the Carnegie Mellon University Pronouncing Dictionary also known as cmudict This dictionary is a public domain machine readable pronunciation dictionary for North American English that contains over 130 000 words and their phonetic transcriptions The pronunciation of the words is encoded using a modified form of the Arpabet system each phoneme having a unique code e g ABRACADABRA AE2 B R AH0 K AH0 D AE1 B R AH0 This dictionary is used in different projects such as the Festival speech synthesis system and also the CMU Sphinx speech recognition system As a result of this database our system was able to retrieve all the words with the same pronunciation However we also wanted to obtain quasi homophones e g increase decrease from this list So we devised an algorithm that looks at the phonetic differences between words 5 2 The Algorithm The algorithm performs the following steps Calculation of the number of phonemes for the submitted word 11 for abracadabra Retrieval from the database of all the words that have o The same number of phonemes o The same number 1 of phonemes o The same number 1 of phonemes Calculation of the similarity number of different phonemes in the same order between the submitted word and the retrieved words Calculates the proximity between the two phonetic strings using the Levenshtein distance function Levenshtein distance is defined as the minimum number of necessary characters to be changed inserted or modified for transforming a string into another one It is commonly used for spelling checking speech recognition DNA analysis etc The system works by requesting a check for a specific word one by one So it is not possible to get statistics on the numbers of pairs of words retrieved for a specific language The algorithm is able to retrieve any string of any length monosyllabic to very long words The number of retrievals decreases with the length of the submitted word Also the words retrieved are sometimes irrelevant different enough to be not mistaken This is due to the fact that we weakened the constraints Indeed a difference of 2 phonemes is considered in our algorithm Also we did consider the phoneme itself as a whole and not as a sum of phonological features We think the retrievals will be more relevant when we will look at the divergences of phonological features and only consider one phoneme of difference instead of 2 as at present 5 3 Oral Communication Involving Different Mother Tongues The receiver of the message can have a different mother tongue from English Consequently some phonemes may not exist for him or her In this case what will the receiver understand Assuming the fact that he she will reconstruct words using existing phonemes in their own language the system should replace the phonemes by A System to Control Language for Oral Communication 397 those existing in another language and check in the database if homophones or quasi homophones exist To do this we devised a resource that gives for each language Arabic French and Chinese for the moment the list of non existing English phonemes and their counterpart s Table 2 illustrates an extract from this table Table 2 Non existing English phonemes and their counterpart s extract Phonemes Ax Ax Ax From English to Replaced by Aa Ae Ao Language Arabic Arabic Arabic As a result of this resource our system is now able depending on the language selected to reconstruct the pronunciation and then retrieve all the words with the same pronunciation in English An illustration of the system s interface showing the results for tomato is shown in Fig 1 Fig 1 Screen shot of the System to Help when Creating Sentences that are to be Pronounced for the word tomato 398 L Spaggiari and S Cardey 6 Results and Improvements The system behaves as intended because it retrieves all the homophones for a term feel F IY1 L fiel feil foell it retrieves all the minimal pairs for a term feel F IY1 L fail fall feat feed fees fell file fill foal foil fool foul fowl full peal peel it retrieves quasi homophones for a term depending on the language selected thought TH AO1 T for Chinese fought sawed sod sought and for French fought sought taught taut tot Using this information one can easily decide when creating a spoken message if one can use a word or if one should change it e g use reduce instead of decrease or even reformulate the whole sentence However when looking just at English some of the words that were retrieved could be avoided as they are different enough not to be mistaken We think that the reasons for this lie in the constraints insufficient we have applied for calculating the similarity Indeed we performed this calculation by counting the number of different phonemes between two words Also to be able to retrieve for example increase decrease we had to consider an acceptable number of 2 differences whatever they are As a consequence many words with 2 differences are retrieved A better way would be to take into account for each phoneme its phonological features and to count the different ones to get a much more precise result For example we would continue to take a difference of 2 phonemes as a maximum but reducing this time the maximum number of differences allowed between features we would reduce the numbers of results The 2 different phonemes in increase decrease share the same phonological features except one nasal vs oral Another improvement would consist in considering the whole sentence that is to say to consider the assimilations that occur between the words once these are put together Finally the phonetics of the reconstructed sentences could be saved in an ssml file 9 This file can easily be enriched with plenty of information concerning prosody and style voice emphasis break pitch speaking rate and volume of the speech output text structure etc This would allow us to use this file as an entry to obtain these phonetic strings pronounced by a synthetic voice for example Microsoft US English Anna An illustration of the system s interface enriched for voice synthesis word by word only is shown in Fig 2 A System to Control Language for Oral Communication 399 Fig 2 Screen shot of the System to Help when Creating Sentences that are to be Pronounced enriched for voice synthesis 7 Conclusion We have seen in this paper why language interferences have to be avoided and we have proposed a methodology and a system to find and solve these problems Some work about interference had already been done in our laboratory with native speakers but our system automatically detecting possible interference revealed itself much more efficient Much work still has to be done at the level of the boundaries between words but the methodology which consists in working at the level of phonemes and distinctive features rather than trying to find individual words seems to be more productive and easier to generalise for solving the problems of interferences which are due to bad pronunciation or bad interpretation The methodology used allows tracing back to the cause of the problems which is essential in safety critical applications The results of this research can be applied to different domains This is because the specific data i e lexicon by domain is tested against the general pronunciation dictionary as we cannot know the level of English people have So the methodology is not domain dependant and can be applied to any specific domain as long as the domain has its own dictionary 400 L Spaggiari and S Cardey References 1 Goyvaerts P Controlled English Curse or Blessing A User s Perspective In Proceedings of CLAW 1996 Leuven Belgium March 26 27 pp Robust Semi supervised and Ensemble Based Methods in Word Sense Disambiguation Anders Centre for Language Technology University of Copenhagen Njalsgade Abstract Mihalcea 1 discusses self training and co training in the context of word sense disambiguation and shows that parameter optimization on individual words was important to obtain good results Using smoothed co training of a naive Bayes classifier she obtains a 9 8 error reduction on Senseval 2 data with a fixed parameter setting In this paper we test a semi supervised learning algorithm with no parameters namely tri training 2 We also test the random subspace method 3 for building committees out of stable learners Both techniques lead to significant error reductions with different learning algorithms but improvements do not accumulate Our best error reduction is 7 4 and our best absolute average over Senseval 2 data though not directly comparable is 12 higher than the results reported in Mihalcea 1 Keywords co training tri training word sense disambiguation 1 Introduction Word sense disambiguation WSD is the task of deciding which sense a word has in a particular context The Senseval 2 shared task provides labeled data that can be used for supervised learning of WSD of 29 English nouns This data set was used by Mihalcea 1 in a line of experiments using inference from unlabeled data in addition to the Senseval 2 data namely instances drawn from the British National Corpus The baseline in Mihalcea 1 is a naive Bayes classifier trained on the labeled data She then considers the potential of self training and co training algorithms for making use of unlabeled data 4 She first shows that performance is very sensitive to parameter setting but nevertheless co training leads to a significant error rate reduction of 9 8 with a global parameter setting which specifies the number of iterations growth size and pool size In particular co training is run twice with a pool of 5000 data points selecting the 50 points most confidently labeled In this work we consider a parameter free semi supervised learning algorithm introduced in Li and Zhou 2 The algorithm is described in Sect 2 Sect 3 H Loftsson E 402 A intro duces a method for constructing ensembles of classifiers that form robust and accurate end classifiers namely random subspaces 3 In Sect 4 we apply tri training and random subspaces to supervised classifiers trained on Senseval 2 data incl naive Bayes decision stumps PART and logistic boosting In Sect 5 we discuss our results and conclude The reported prepro cessing of the data in Mihalcea 1 is very complicated and the prepro cessed data is no longer available Mihalcea p c Consequently results reported here are not directly comparable Moreover some test examples in the Senseval 2 have multiple labels and to simplify things we only evaluate our algorithms on test examples with a unique label 2 Tri training This section presents the tri training algorithm originally proposed by Li and Zhou 2 Let L denote the labeled data and U the unlabeled data Assume that three classifiers c1 c2 c3 same learning algorithm have been trained on three bootstrap samples of L In tri training an unlabeled datapoint in U is now labeled for a classifier say c1 if the other two classifiers agree on its label i e c2 and c3 Two classifiers inform the third If the two classifiers agree on a labeling there is a good chance that they re right The algorithm stops when the classifiers no longer change The three classifiers are combined by ma jority voting Li and Zhou 2 show that under certain conditions the increase in classification noise rate is compensated by the amount of newly labeled data points The most important condition is that the three classifiers are diverse If the three classifiers are identical tri training degenerates to self training Diversity is obtained in Li and Zhou 2 by training classifiers on bootstrap samples In their 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 for i 1 3 do Si bootstrap sample L ci train classifier Si end for repeat for i 1 3 do for x U do Li if cj x ck x j k i then Li Li x cj x end if end for ci train classifier L Li end for until none of ci changes apply majority vote over ci Fig 1 Tri training Li and Zhou 2005 Robust Semi supervised and Ensemble Based Methods in WSD 403 experiments they consider classifiers based on the C4 5 algorithm BP neural networks and naive Bayes classifiers The algorithm is sketched in a simplified form in Figure 1 see Li and Zhou 2 for all the details Tri training has to the best of our knowledge not been applied to WSD before but it has been applied to other NLP classification tasks incl Chinese chunking 5 and question classification 6 3 Random Subspaces Random subspaces was introduced in Ho 3 The idea is simple Randomly select a subset of the components of the feature vector and train a classifier In other words from a d dimensional data set we project n new k dimensional data sets by random projection Each new data set is given as input to the learning algorithm and the base classifiers are combined to form a stronger end classifier by ma jority voting One weakness with this metho d is that some of the subspaces may lack the ability to separate different classes For this reason we expect random subspaces to perform well with boosting algorithms see 4 4 1 Experiments Data The data sets were prepared to be as close as possible to the data sets used in Mihalcea 1 Briefly put we use a small set of local features incl context word forms and POS tags and a larger set of global features i e a small bag of content words Unlike Mihalcea 1 we do not use collocations as global features and we only use a single pool of unlabeled data points rather than pruning them by collocations See Mihalcea 1 for more details The complete list of words used in Mihalcea 1 i e the nouns in the Senseval 2 data Table 1 English nouns in Senseval 2 art circuit hearth restraint authority day holiday sense bar bum detention dyke lady material space stress chair channel child church facility fatigue feeling grip mouth nation nature post yew The average number of labeled examples for each word is about 100 with an additional pool for testing of about half that figure The average number of unlabeled examples in our experiments is 14 355 compared to 7 085 in Mihalcea 1 Mihalcea 1 prunes the unlabeled data by collocations to avoid noise but using the raw unlabeled data makes it easier to repro duce our results 404 A 4 2 Learning Algorithms We consider three baseline learning algorithms in our experiments Sect 3 motivated our choice of using logistic boosting 8 Briefly put logistic boosting considers the more well known boosting algorithms as generalized additive mo dels and applies the cost functional of logistic regression Naive Bayes is the standard choice in WSD and is known to perform reasonably well on Senseval 2 data We also ran our algorithms on decision stumps because they are fundamental to a wide range of learning algorithms incl logistic boosting Finally we include a rule based learning algorithm PART 9 because of their intuitive nature and potential usefulness in more descriptive computational linguistics 4 3 Results Our results using the tri training algorithm on Senseval 2 data are listed below We use self training until convergence as our semi supervised baseline see e g Abney 4 We only present average accuracy rather than accuracies on the 29 individual words is the absolute difference between our baseline and tritraining resp random subspaces Table 2 Results for tri training learner LogitBoost naive Bayes PART DecisionStump baseline self training tri training 65 56 66 39 66 43 0 87 64 33 64 09 62 74 1 59 60 37 60 57 60 84 0 47 58 23 58 59 58 86 0 63 Tri training leads to reasonable error reductions when applied to logistic boosting 2 5 and PART 1 2 in light of our relatively strong baselines but not when applied to naive Bayes Our results using random subspaces on Senseval 2 data are listed here Table 3 Results for random subspaces baseline learner LogitBoost 65 56 64 33 naive Bayes 60 37 PART DecisionStump 58 23 bagging random subspaces 66 80 68 11 63 61 64 01 62 68 63 16 56 62 58 65 2 55 32 2 79 0 42 The result obtained by random subspaces over logistic boosting even takes us considerably beyond what can be obtained with linear support vector machines We use bagging as our ensemble based baseline Robust Semi supervised and Ensemble Based Methods in WSD 405 Combining Tri Training and Random Subspaces Somewhat surprisingly performance degrades if we try to combine tri training and random subspaces We tri trained a random subspace model with logistic boosting but accuracy dropped by more than 2 percentage points to 66 00 compared to our random subspaces baseline 5 Conclusion Tri training and random subspaces are by and large robust metho ds for boosting supervised classifiers in the context of word sense disambiguation Naive Bayes is probably too stable to be amenable to tri training although Li and Zhou 2 report positive results and it is clear why random subspaces hurts the performance of naive Bayes Since naive Bayes implements a strong independence assumption between features and consult each feature independently there is little to gain from randomly constructed subspaces Both tri training and random subspaces work particularly well with logistic boosting It seems logistic boosting helps overcome the potential weaknesses of random subspaces and it is unlike support vector machines for example sensitive enough to bootstrap samples to be amenable to tri training Generally we reported competitive results on the Senseval 2 68 11 and obtained a 7 4 error reduction wrt logistic boosting Since the data set is not directly comparable to the data set used in Mihalcea 1 we plan to reimplement smoothed co training for direct comparison We also plan to compare tri training to semi supervised support vector machines 10 References 1 Mihalcea R Co training and self training for word sense disambiguation In CONLL Boston MA 2004 2 Li M Zhou Z H Tri training exploiting unlabeled data using three classifiers IEEE Transactions on Knowledge and Data Engineering 17 11 The Effect of Semi supervised Learning on Parsing Long Distance Dependencies in German and Swedish Anders Center for Language Technology University of Copenhagen Njalsgade Abstract This paper shows how the best data driven dependency parsers available today 1 can be improved by learning from unlabeled data We focus on German and Swedish and show that labeled attachment scores improve by 1 5 2 5 Error analysis shows that improvements are primarily due to better recovery of long distance dependencies Keywords dependency parsing semi supervised learning long distance dependencies 1 Introduction Rimell et al 2 argue that long distance dependencies are particularly interesting in parser evaluation since they provide a strong test of the parser s knowledge of grammar and since recovering long distance dependencies is necessary to completely represent the underlying predicate argument structure of the sentence useful for applications such as question answering and information extraction Rimell and Clark show that state of the art constituent parsers have accuracies below 50 on a new dataset of English unbounded dependencies The purpose of this paper is two fold i It is shown that it is possible to improve the accuracy of the best available dependency parsers for German and Swedish by learning from unlabeled data ii It is shown that improvements are primarily due to better recovery of long distance dependencies In particular it is shown that a novel semi supervised learning algorithm called generalized tri training is able to improve labeled attachment scores LASs on standard datasets by 1 72 German and 2 36 Swedish in general If we limit attention to long distance dependencies 7 however increases in F score are even more dramatic i e 5 09 German and 8 58 Swedish Semi supervised learning of structured variables is a difficult problem that has received considerable attention recently but most results have been negative 3 This paper uses stacked learning 4 to reduce structured variables i e dependency graphs to multinomial variables i e attachment and labeling decisions which are easier to manage in semi supervised learning scenarios H Loftsson E The Effect of Semi supervised Learning 407 Ensemble based metho ds such as stacked learning are used to reduce the instability of classifiers to average out their errors and to combine the strengths of diverse learning algorithms Ensemble based metho ds have attracted a lot of attention in dependency parsing recently 5 6 7 1 8 9 Nivre and McDonald 7 were first to intro duce stacking in the context of dependency parsing This paper applies a generalization of tri training 10 a form of co training that trains an ensemble of three learners on labeled data and runs them on unlabeled data to two classification problems attachment and labeling that together approximate dependency parsing Semi supervised dependency parsing has attracted a lot of attention recently 11 12 13 but there has to the best of our knowledge been no previous attempts to apply tri training or related combinations of ensemble based and semi supervised metho ds to any of these tasks except for the work of Sagae and Tsujii 14 However tri training has been applied to Chinese chunking 15 and question classification 16 We compare generalized tri training to the original tri training algorithm and to semi supervised support vector machines 17 Sect 2 first intro duces the dependency parsing problem and defines stacked learning Stacked learning is then generalized to dependency parsing and we describe how stacked dependency parsers can be further stacked as input for two end classifiers that can be combined to pro duce dependency structures These two classifiers will learn multinomial variables attachment and labeling from a combination of labeled data and unlabeled data using a generalization of the tri training algorithm presented in Li and Zhou 10 Sect 2 also intro duces generalized tri training Sect 3 describes our experiments We describe the data sets and how the unlabeled data were prepared Sect 4 presents our results Sect 5 presents an error analysis and shows that improvements are primarily due to better recovery of long distance dependencies and Sect 6 concludes the paper 2 2 1 Background Dependency Parsing Dependency parsing mo dels a sentence as a tree where words are vertices and grammatical functions are directed edges dependencies Each word thus has a single incoming edge except one called the root of the tree Dependency parsing is thus a structured prediction problem with trees as structured variables Each sentence has exponentially many possible dependency trees Our observed variables are sentences with words labeled with part of speech tags The task for each sentence is to find the dependency tree that maximizes an ob jective function which in our case is learned from a combination of labeled and unlabeled data More formally a dependency tree for a sentence x w1 wn is a tree T 0 1 n A with A 408 A is projective if every vertex has a continuous projection i e if and only if for every arc i j A and node k V if i k j or j k i then there is a subset of arcs i i1 i1 i2 ik 1 ik A such that ik k The German and Swedish data sets used in our experiments below have 27 8 resp 9 8 non projective dependency trees 2 2 Stacked Dependency Parsing Stacked generalization or simply stacking was first proposed by Wolpert 4 Stacking is an ensemble based learning metho d where multiple weak classifiers are combined in a strong end classifier The idea is to train the end classifier directly on the predictions of the input classifiers Say each input classifier ci with 1 i n receives an input x and outputs a prediction ci x The end classifier then takes as input x c1 x cn x and outputs a final prediction c0 x c1 x cn x Training is done by crossvalidation In sum stacking is training a classifier on the output of classifiers Stacked learning can be generalized to structured prediction tasks such as dependency parsing Architectures for stacking dependency parsers typically only use one input parser but otherwise the intuition is the same the input parser is used to augment the dependency structures that the end parser is trained and evaluated on Nivre and McDonald 7 first showed how the MSTParser 18 and the MaltParser 19 could be improved by stacking each parser on the predictions of the other Martins et al 1 generalized their work considering more combinations of parsers and stacking the end parsers on non local features from the predictions of the input parser e g siblings and grand parents In this work we parse the German and Swedish data sets from the CONLL X Shared Task and use three stacked dependency parsers for each language parser1 p1 parser2 p2 parser3 p3 Ge mst2 malt mst2 D malt mst1 E Sw mst2 mst2 D malt mst2 D malt mst1 A The notation malt mst2 means that the second order MSTParser has been stacked on MaltParser The capital letters refer to feature configurations Configuration A only stacks the level 1 parser on the predicted edges of the level 0 parser along with the input features Configuration D stacks a level 1 parser on several non local features of the predictions of the level 0 parser the predicted edge siblings grand parents and predicted head of candidate mo difier if predicted edge is 0 Configuration E stacks a level 1 parser on the features in configuration D and all the predicted children of the candidate head The chosen parser configurations are those that performed best in Martins et al 1 There are two reasons that our input parsers perform slightly worse than those reported on in Martins et al 1 i We use about 5 000 tokens of the training data for development ii For both datasets we used projective rather than pseudoprojective parsing in MaltParser For MSTParser level 0 we also The Effect of Semi supervised Learning 409 reduced training time by iterating three times over the German data rather than 10 times as in Martins et al 1 2 3 Stacking Stacked Dependency Parsing The input features of the input classifiers in stacked learning x can of course be removed from the input of the end classifier It is also possible to stack stacked classifiers This leaves us with four strategies for recursive stacking namely to constantly augment the feature set with level n classifiers trained on the predictions of the classifiers at all n 1 lower levels with or without the input features x or simply to train a level n classifier on the predictions of the level n 1 classifiers with or without x In this work we stack stacked dependency parsers by training classifiers on the output of three stacked dependency parsers and POS tags Consequently we use one of the features from x since this led to better results on development data Note that we train classifiers and not parsers on this new level 2 The reduction is done the following way First we train a classifier on the relative distance from a word to its head to induce attachments For example we may obtain the following features from the predictions of our level 1 parsers label p1 p2 p3 POS 1 1 1 1 NNP 0 0 0 0 VBD In the second row all input parsers p1 3 in column label p1 p2 p3 POS SBJ SBJ SBJ SBJ NN ROOT ROOT ROOT COORD VBN 2 4 Generalized Tri training Tri training was originally intro duced in Li and Zhou 10 The metho d involves three learners that inform each other Let L denote the labeled data and U the unlabeled data Assume that three classifiers c1 c2 c3 have been trained on L In the original algorithm the three 410 A classifiers are obtained by applying the same learning algorithm to three bootstrap samples of the labeled data but in generalized algorithms three different learning algorithms are used An unlabeled datapoint in U is labeled for a classifier say c1 if the other two classifiers agree on its label i e c2 and c3 Two classifiers inform the third If the two classifiers agree on a labeling we assume there is a good chance that they are right In the original algorithm learning stops when the classifiers no longer change in generalized tri training a fixed stopping criterion estimated on development data is used The three classifiers are combined by ma jority voting Li and Zhou 10 show that under certain conditions the increase in classification noise rate is compensated by the amount of newly labeled data points The most important condition is that the three classifiers are diverse If the three classifiers are identical tri training degenerates to self training As already mentioned Li and Zhou 10 obtain this diversity by training classifiers on bootstrap samples In their experiments they consider classifiers based on decision ive Bayes inference trees BP neural networks and 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 for i 1 3 do ci train classifier li L end for repeat for i 1 3 do for x U do Li if cj x ck x j k i then Li Li x cj x end if end for ci train classifier L Li end for until stopping criterion is met apply ci Fig 1 Generalized tri training Our predictions are those of the random forests classifier after a fixed number of rounds optimized on development data On development data this led to slightly better results than ma jority votes The Effect of Semi supervised Learning 411 3 3 1 Experiments Data We use the German and Swedish datasets from the CONLL X Shared Task i e the TIGER treebank 21 and Talbanken05 22 The TIGER treebank contains 700 000 tokens or 39 200 sentences and Talbanken05 contains 191 000 tokens or 11 000 sentences We use the official train test splits with one important exception we use the first approx 5 000 lines in the training data as development data The unlabeled data were the Leipzig Corpora Collection corpora for German and Swedish except we only used the first 75 000 sentences for the German corpus 3 2 POS Tags The unlabeled data were POS tagged using the freely available SVMTool 23 model 4 left right left The German data set contains 52 POS tags and the Swedish data set contains 37 POS tags The accuracy of SVMTool on the two data sets is about 95 3 3 Algorithm Once our data has been prepared we train the stacked dependency parsers listed in Sect 2 3 and use them to parse our development data our test data and our unlabeled data This gives us three sets of predictions for each of the three data sets From each triad of predictions we construct two data sets one for our attachment classifier say data set A and one for our dependency labeler say data set B Using 5 fold cross validation we train three classifiers on the development test data in A and B The entire architecture can be depicted as follows tri training nb stacking mst2 mst2 malt mst2 malt mst1 stacking mst2 malt mst1 forests tree We first stack three dependency parsers as described in Martins et al 1 We then stack three classifiers on top of these dependency parsers and POS tags a 412 A on development data and unlabeled data in A and a stopping criterion for our dependency label variable on development data and unlabeled data in B The stopping criteria are used when we update the classifiers trained on our test data in 5 fold cross validation 3 4 Baselines The best of the stacked input parsers is of course our natural baseline Since we have generalized tri training we include the original tri training algorithm as a semi supervised baseline The original tri training algorithm is run with the same decomposition and the same features as our generalized tri training algorithm We use the two learning algorithms of the three originally used in Li and Zhou 10 that we had available namely naive Bayes and C4 5 Finally we include S3VMs as a semi supervised baseline Since S3VMs pro duce binary classifiers and one vs many combination would be very time consuming we train a binary classifier that produces a probability that any candidate arc is correct and do greedy head selection We optimized the feature set and included a total of seven features head POS dependent POS dependent left neighbor POS distance direction predictions of the three classifiers 4 Results Our results on the German and Swedish data sets are presented in Figure 2 We first list the individual parsers malt and mst2 and the stacked parsers Since Martins et al 1 found that for German the level 0 parser mst2 outperformed any configuration of mst2 mst2 there is no figure for mst2 mst2 for German The best input parser for German is mst2 i e the second order MSTParser whereas the best input parser for Swedish is malt mst2 i e the second order MSTParser stacked on MaltParser with feature configuration D see Sect 2 3 Generalized tri training leads to highly significant improvements on both data sets p 0 001 The row tri training lists the results of the initial greedy head selection whereas tri training MST lists the results of reparsing using CLE to pro duce well formed dependency trees Reparsing hurts labeled attachment score LAS a bit but differences are small Since using the Eisner algorithm for reparsing 24 led to a slightly better result for Swedish than using CLE we report both results We only applied our semi supervised baselines to unlabeled parsing but it is quite evident that these learning strategies seem less promising than generalized tri training S3VMs seem to average out the input parsers for German and lead to a relatively small 0 5 improvement for Swedish The original tri training algorithm leads to scores well below any of the input parsers for Swedish but to a considerable improvement for German The Effect of Semi supervised Learning German malt mst2 malt mst2 malt mst1 s3vms orig tri training nb orig tri training C4 5 tri training tri training MST tri training excl punct Martins et al 2008 excl punct Swedish malt mst2 mst2 mst2 malt mst2 malt mst1 s3vms org tri training nb org tri training C4 5 tri training tri training MST tri training Eisner tri training excl punct Martins et al 2008 excl punct LAS 80 08 84 25 81 40 81 35 85 97 85 88 85 84 87 44 LAS 83 29 81 95 82 32 83 50 83 45 85 86 85 71 85 79 85 94 85 16 UAS 82 70 87 20 84 18 84 16 84 51 89 06 89 22 90 24 90 13 90 69 UAS 87 52 87 46 87 98 87 96 87 87 88 45 87 22 87 36 91 32 91 16 91 09 91 95 LA LAS p value 88 02 91 38 89 94 90 06 92 55 1 72 0 0005 92 55 1 61 0 0009 91 53 LA 87 54 87 41 87 20 88 24 88 12 90 12 90 12 90 12 89 15 LAS p value 413 2 36 0 0001 2 21 0 0001 2 29 0 0001 Fig 2 Results on the German and Swedish data sets Scores are including punctuation unless otherwise noted and p value is difference with respect to best input parser The results in Martins et al 2008 are incomparable since they did not take out 5000 sentences for development and since we did not convert the treebanks into projective trees before training MaltParser 5 Error Analysis and Discussion Our error reductions in LAS over the best of our stacked input parsers are 11 16 for German and 12 01 for Swedish in unlabeled attachment score UAS it is 21 42 resp 19 55 The most striking difference between our errors and those committed by our best input parser is their distribution across dependency length as illustrated in Figure 3 Inference from large amounts of unlabeled data seems to make our parser much better at predicting and labeling long distance dependencies and dependencies of the root node In general generalized tri training improves LASs by 1 72 German and 2 36 Swedish but if we limit attention to long distance dependencies 7 increases in F score are even more dramatic i e 5 09 German and 8 58 Swedish 414 A F score 97 63 94 64 91 16 88 99 91 19 0 71 0 62 1 33 3 09 5 09 F score 95 12 95 47 92 92 87 10 89 85 3 62 1 40 1 16 4 54 8 58 Fig 3 Recall precision and F score binned on dependency length Number of tokens is number of dependencies of length n in the gold standard Scores are including punctuation is the difference between system and baseline F scores Our errors and those committed by our best input parser seem to be distributed over POS tags in much the same way Distributions over dependency labels do shed some more light on what kind of long distance dependencies our parsers learn to recover Consider the recall precision and F score of labeled attachment of the 10 most frequent dependency relations in German excl ROOT in Figure 4 and the same results for the 10 most frequent dependency relations in Swedish excl ROOT in Figure 5 It is evident that ma jor improvements are primarily due to improvements with coordinating conjunctions punctuations and complements that can move relatively freely in and across clauses e g sub jects clausal ob jects and other ob jects In Swedish adverbs and some postnominal mo difiers move rather freely and we see big improvements here as well Since we train on less material than Martins et al 1 taking out 5000 sentences for development our point of departure is significantly worse than theirs The best stacking configuration for Swedish i e stacking the second order MSTParser on MaltParser with features D has a LAS of 85 16 on the full training section with projectivization and deprojectivization but only a LAS of 83 93 on our smaller subset excl punct Consequently the fact that tri training leads to results considerably better than those reported in Martins et al 1 0 78 excl punct says something about the potential of inference from unlabeled data Since we greedily select the best head for each word our output is not guaranteed to be wellformed dependency trees This is similar to Zeman and Zabokrtsky 25 The percentage of cyclic structures produced by our ensemble is in both cases below 5 Surdeanu and Manning 9 observe similar figures in ensemblebased greedy head selection Reparsing only leads to a small decrease in LAS The Effect of Semi supervised Learning 415 tag AG CD CJ MNR MO NK OA OC PUNC SB tok 150 129 172 153 772 1721 206 220 808 425 meaning genitive attribute coord conjunction conjunct postnominal mod modifier noun kernel mod accusative object clausal object punctuation subject Baseline rec 73 33 73 64 67 44 58 17 72 28 96 22 74 27 90 45 84 28 86 35 prec 76 39 71 97 67 05 54 27 74 40 95 61 72 17 85 04 84 28 82 84 F score 74 83 72 80 67 24 56 15 73 32 95 91 73 20 87 66 84 28 84 56 System rec 82 00 75 19 69 19 58 82 76 55 95 64 73 30 92 27 86 76 87 53 prec 72 35 74 62 67 61 54 55 76 16 96 31 72 25 89 82 86 65 88 15 F score 76 87 74 90 68 39 56 60 76 35 95 97 72 77 91 03 86 70 87 84 2 04 2 10 1 15 0 45 3 03 0 06 0 43 3 37 2 42 3 28 Fig 4 Recall precision and F score of labeled attachment score of 10 most frequent dependency relations in German tag AA AT CC DT ET IP OO PA SS tok 184 266 234 220 556 342 312 284 677 508 meaning coord conjunction other adverbial nom pre modifier conjuncts determiner other nom postmod period other object prep compl other subject Baseline rec 91 30 62 41 96 15 79 09 94 96 72 22 91 35 85 92 95 57 90 55 prec 89 84 64 59 96 15 79 82 90 26 73 73 91 35 78 96 94 04 89 67 F score 90 56 63 48 96 15 79 45 92 55 72 97 91 35 82 29 94 80 90 11 System rec 95 65 67 67 96 58 82 27 95 32 82 16 100 00 89 08 96 01 93 70 prec 94 62 67 92 95 76 83 80 92 66 79 15 100 00 84 05 95 73 91 54 F score 95 13 67 79 96 17 83 03 93 97 80 63 100 00 86 49 95 87 92 61 4 57 4 31 0 02 3 58 1 42 7 66 8 65 4 20 1 07 2 50 Fig 5 Recall precision and F score of labeled attachment score of 10 most frequent dependency relations in Swedish Generalized tri training leads to much better results than the other two semisupervised learning algorithms The original tri training algorithm gives a big improvement for German but for Swedish it makes things much worse S3VMs do not lead to improvements but seem to average out the ensemble used in the stacking 6 Conclusion This paper showed how the stacked dependency parsers intro duced in Martins et al 1 can be improved by inference from unlabeled data Briefly put we stacked three diverse classifiers on triads of stacked dependency parsers and let them label unlabeled data for each other in a co training like architecture Our error 416 A reductions in LAS over the best of our stacked input parsers were 12 24 for German and 12 01 for Swedish in UAS it was 21 42 resp 19 55 Error analysis shows that improvements are primarily due to better recovery of long distance dependencies Generalized tri training improves LASs by 1 72 German and 2 36 Swedish but if we limit attention to long distance dependencies 7 increases in F score are even more dramatic i e 5 09 German and 8 58 Swedish References 1 Martins A Das D Smith N Xing E Stacking dependency parsers In EMNLP Honolulu Hawaii 2008 2 Rimell L Clark S Steedman M Unbounded dependency recovery for parser evaluation In EMNLP Singapore 2009 3 Abney S Semi supervised learning for computational linguistics Chapman and Hall Boca Raton 2008 4 Wolpert D Stacked generalization Neural Networks 5 The Effect of Semi supervised Learning 417 20 Breiman L Random forests Machine Learning 45 Shooting at Flies in the Dark Rule Based Lexical Selection for a Minority Language Pair Linda Wiechetek1 Francis M Tyers2 and Thomas Omma3 Giellatekno Romssa Universitehta Norway linda wiechetek uit no Dept Lleng i Sist Inform Universitat d Alacant Spain ftyers dlsi ua es 3 Divvun 1 2 Abstract This paper presents a set of rules which form the prototype lexical selection component of a rule based machine translation system between two closely related minority languages North 1 Introduction North H Loftsson E Shooting at Flies in the Dark 419 Both languages have phonemic orthographies with differences resulting from differing conventions in standardisation These differences can largely be handled by rules thus a bilingual dictionary between the two was created from scratch by simply converting the orthography This process provides an adequate lexicon but when inspecting the resulting translations with a native Lule b c The languages also diverge on the lexical level and although the automatically constructed bilingual lexicon often provides adequate translations in many cases word use is actually quite different In some cases historical word roots are different in other cases words in one of the languages have acquired a new sense which does not exist in the other language or appear in specific syntactic or semantic construction which is resolved differently in the other language The need for lexical selection2 came up when seemingly straightforward translations were not accepted by Lule 1 2 The latter part of the lexicalised construction is originally a genitive too Lexical selection is defined by 1 as the principled selection of a lexical items and b the syntactic structure for input constituents based on lexical semantic pragmatic and discourse clues available in the input 420 L Wiechetek F M Tyers and T Omma 2 Objectives The objectives behind the development of a machine translation MT system between the two languages are largely guided by the sociolinguistic situation Following 2 applications of machine translation can be divided in two main groups with different requirements assimilation that is to enable a user to understand what the text is about and dissemination that is to help in the task of translating a text to be published Assimilation may be possible even when the text is far from being grammatically correct however for dissemination the effort needed to correct post edit the text must be lower than the effort needed to translate it from scratch A majority to minority language system will mainly be used for dissemination purposes where post editing the output should be faster than translating from scratch and intelligibility is less important In a minority to majority language system on the other hand intelligibility is the main goal as MT is mainly used for assimilation for instance to answer vital questions such as what are they writing about me in the minority language newspaper The system described in this paper falls outside the usual 3 Technical Background This section gives a brief overview of the two main technologies used in the construction of the prototype system 4 Apertium 5 a rule based machine translation platform and Constraint Grammar 3 a rule based framework for the disambiguation and annotation of text 3 1 Apertium The Apertium platform was originally aimed at the Romance languages of the Iberian peninsula but has also been adapted for other language pairs such 3 4 5 http avvir no The prototype system may be tested online at http victorio uit no cgi bin francis index php http www apertium org Shooting at Flies in the Dark 421 as Welsh 4 and Basque 5 The whole platform both programs and data is available from the project website under the GPL licence 6 The engine largely follows a shallow transfer approach to machine translation 6 Finite state transducers 7 are used for lexical processing first order hidden Markov models HMM and optional Constraint Grammar are used for partof speech tagging and finally multi stage finite state based chunking is used for structural transfer SL text deformatter morph analyser constraint grammar lexical selection lexical transfer structural transfer chunker interchunk interchunk postchunk morph generator reformatter TL text Fig 1 Modular architecture of the Apertium MT platform Bold indicates adjustments made for the North As this paper focuses on the lexical selection aspect a more detailed description of the pipeline figure 1 will not be made 3 2 Constraint Grammar The formalism used for both disambiguation and annotation is Constraint Grammar which is a linguistically based approach used for the bottom up analysis of running text The 6 7 http www fsf org licensing licenses gpl html http visl sdu dk constraint_grammar html 422 L Wiechetek F M Tyers and T Omma The lexical selection module is also implemented in Constraint Grammar and annotates words which are ambiguous in translation in a disambiguated source language sentence with references to their translation in the target language This method is inspired by other MT systems including rule based lexical selection such as the Dan2Eng system 8 which successfully uses 17 000 handwritten lexical transfer rules 4 4 1 Lexical Selection Potential Candidates The bilingual North Within the Constraint Grammar semantic information is encoded in semantic sets within the lexical selection module The bilingual lexicon specifies one or more alternative translations the default labelled with S0 and the alternatives Shooting at Flies in the Dark 423 labelled with consecutive numbers from one The rules make use of morphological syntactic and semantic information Rules were inspired by comments by a native speaker of Lule The rule selecting the translation vuoras for boaris old makes use of the fact that personal pronouns in first and second person usually denote a human Syntactic information is specifically used with the polysemous verb orrut stay seem which translates into vuojnnet before a noun adjective in essive case8 or a predicative as in example 3 3 Orru leamen buorre North The last type of constraints are narrower lexical or even idiosyncratic constructions The adjective buorre good is translated into jasskat before particular nouns such as iesdovdu self confidence Example 4 shows an example of this kind of constraint in the example North Semantic information is used in a number of rules The noun 8 9 The essive case expresses a temporary state or quality Psych verbs are those verbs which designate a psychological state or process 424 L Wiechetek F M Tyers and T Omma of suhttat get angry at where it gets a metaphorical meaning which is not conveyed by the word SET ANIMAL ealga rievssat SET HUMAN Prop Mal Prop Fem Prop Sur Fig 2 Two constraint grammar lexical selection rules to select between two translations of boaris old and orrut stay seem The second rule selects sense 1 S1 of the intransitive verb IV orrut stay seem if there is a subject predicative SPRED one position to the right as in example 5 a Orru buorre North b 5 Evaluation For the evaluation the North 10 The corpus of test sentences may be downloaded from http www dlsi ua es ftyers sme smj testsentences tar gz Shooting at Flies in the Dark 425 non equivalent translations were considered but only equivalent translations were included when calculating the percentage of correct translations Equivalent constructions are those where the lexical item is translated by a possible equivalent of the same part of speech Derivations which do not change the lexical category of the word e g Noun Noun and compounds are permitted In some cases it was difficult to decide whether the translation is a possible equivalent or a different word As is the case with the North 7 Rather than aiming at a system that translates 6 into 7 the aligned sentence should be discarded in favour of a more literal translation Some potential Lule 426 L Wiechetek F M Tyers and T Omma Table 1 Evaluation of the lexical selection rules over the New Testament The first column gives the word in North Word 6 Discussion As can be seen in table 1 the rules perform best on words where the non default scope is quite narrow such as the rule for buorre good where the non default is only picked in some lexical contexts Bad performance of some of the other rules is due to the selection of the wrong default as e g in the case of boaris old the existence of several variants that have not been considered and might be even restricted to a Biblical context as in muitalit tell where giehttot and not subtsastit or mujttalit get most hits and difficulties in excluding synonymy It is also due to the inclusion of various potentially deviating contexts in the total number of equivalent sentences as in the case of muitalit tell Categorising the rules with regard to their linguistic level and complexity the simple lexical rules referring to nearly idiosyncratic contexts are written very quickly and make up the ones performing best with the exception of the rule for Shooting at Flies in the Dark 427 11 1 Corinthians 3 16 428 L Wiechetek F M Tyers and T Omma The New Testament examples showed that even in a seemingly straightforward word pair the realisation in text can diverge in both directions This may result in several alternative translations partly synonymous Writing lexical selection rules does not only help to pick the correct equivalent but also to acquire knowlege about the correct equivalent Even though North 7 Conclusion The paper has explored the use of lexical selection in machine translation to improve lexical choice in translation In the case of such little researched language pairs as North Shooting at Flies in the Dark 429 References 1 Pustejovsky J Nirenburg S Lexical selection in the process of language generation In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics Morristown NJ USA pp Author Index Infante Lopez Gabriel Isahara Hitoshi 162 39 127 Jean Louis Ludovic 150 Johannsen Anders 401 Jones Gareth J F 345 Kanzaki Kyoko 162 Karanasou Panagiota 167 Karao glan Bahar 238 Karlsson Stefan 179 Kelly Liadh 345 79 Fahrni Angela 215 Fay Nicolas 263 Feijs Loe 250 Fellbaum Christiane D Ferret Olivier 150 Forcada Mikel L 121 Gaume Bruno 332 Gauvain Jean Luc 269 Gojenola Koldo 281 Groves Declan 121 HaCohen Kerner Yaakov Haji c Jan 1 2 138 432 Author Index Penkale Sergio 121 305 Villase nor Pineda Luis 39 85 305 Villatoro Tello 418 293 