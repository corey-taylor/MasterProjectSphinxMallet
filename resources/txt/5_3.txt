Author manuscript published in SLT Workshop on Spoken Language Technology United States 2012 TOWARDS A NEW SPEECH EVENT DETECTION APPROACH FOR LANDMARK BASED SPEECH RECOGNITION Stefan Ziegler Bogdan Ludusan Guillaume Gravier CNRS IRISA Campus de Beaulieu 35042 Rennes France ABSTRACT In this work we present a new approach for the classification and detection of speech units for the use in landmark or eventbased speech recognition systems We use segmentation to model any time variable speech unit by a fixed dimensional observation vector in order to train a committee of boosted decision stumps on labeled training data Given an unknown speech signal the presence of a desired speech unit is estimated by searching for each time frame the corresponding segment that provides the maximum classification score This approach improves the accuracy of a phoneme classification task by 1 7 compared to classification using HMMs Applying this approach to the detection of broad phonetic landmarks inside a landmark driven HMM based speech recognizer significantly improves speech recognition Index Terms speech event detection landmark driven ASR 1 INTRODUCTION In state of the art hidden Markov model based HMM automatic speech recognition ASR speech is modeled as a sequence of phone segments often referred to as the beadson a string model of speech 1 In contrast to that acousticphonetic or event based approaches to speech recognition model speech as a stream of asynchronous phonetic events which have to be further processed to obtain a higher level speech representation 2 3 4 5 Many of these approaches require the detection of phonetic events as time instances referred to as landmarks There are numerous studies concerning the detection of phonetic events in the speech signal which can be divided into two general groups The first group uses expert knowledge to derive detection rules from various signal representations e g 2 The second group uses a classification and detection approach where labeled data is converted into the desired phonetic feature representation and classifiers are trained to map acoustic observations onto phonetic speech This work was partially supported by the Agence National de la Recherche in the framework of the ASH project Furthermore we would like to thank Christian Raymond for advices concerning the use of his boosting software units e g 3 4 Employing these classifiers sequentially on the speech signal results in detection functions indicating the presence of the desired speech units These classifiers operate usually on a frame basis extracting a fixed dimensional observation vector for each frame eventually considering a small context window On the one hand this fixed observation space enables the use of non linear and non parametric classifiers like support vector machines 3 4 or multilayer perceptrons for classification which possess good discrimination abilities and can be trained efficiently On the other hand predicting speech units by observing a fixed size fraction of the speech signal does only partly model time variable speech units including phonemes and many articulatory feature units We are interested in the problem of extracting a fixeddimensional observation vector from time variable speech units to discriminate them using a classifier that requires a fixed number of observations To obtain this observation vector we use a maximum likelihood segmentation method to force each labeled speech unit into three spectrally homogeneous parts from which the observation vector is extracted In our work this observation vector is used to train speech units with boosted decision trees and to predict unknown segments during testing To obtain a detection function indicating the locations of a speech event by its local maxima we search for each frame the segment that maximizes the classification score at this frame This method is applied to a recently proposed landmark driven ASR framework that uses phonetic landmarks to guide the search in an HMM based ASR system 6 The paper is organized as follows First we present the details of our proposed method and review boosting with an ensemble of weak learners as well as landmark driven ASR Evaluation is carried out on a phoneme classification task and on landmark driven speech recognition experiments and we conclude with an outlook on future work 2 PROPOSED SYSTEM FOR SPEECH EVENT DETECTION In event based speech recognition phonetic speech units are often trained and classified at the frame level The classification scores of successively predicted frames correspond to a hal 00758424 version 1 28 Nov 2012 Fig 1 Diagram of the proposed method of using segmentation to extract a fixed dimensional observation vector from a labeled speech unit training or an unknown segment testing detection function that indicates the presence of the speech event usually by its magnitude Landmarks corresponding to single time instances marking the most salient points of speech events are then obtained by post processing the detection profiles varying from simple peak picking to processing with statistical models e g 3 To capture spectral transitions and temporal information usually a concatenation of parametrized speech frames create a fixed dimensional observation space for each frame for classification But since speech units are commonly modeled as time variable segments a fixed number of frames will either capture only fractions of the spectral information of the speech unit or contain misleading information about previous or following speech events If the detection function is obtained by inaccurate modeling of the desired speech event it is difficult to recover the loss in information by post processing noisy and erroneous detection profiles In the following we present a method that overcomes this shortcomings by extracting a fixed dimensional observation vector x from a given time variable sequence of parametrized speech Y y1 yt section 2 1 corresponding to any desired speech unit We are able to keep a fixed dimensional observation space since we reduce every speech segment to three subsegments similar to a three state HMM This observation vector enables the use of any desired classification method while generalizing all time variable speech units In our case we apply a committee of boosted decision stumps to train a classier F x section 2 2 F x is then used to generate a detection function dk dk 1 dk t for k desired speech units with the profile of dk indicating the presence of this speech unit in the unknown signal For each k th detection function a set of lk time instances Lk t1 tlk is extracted representing the exact locations of the speech event associated with speech unit k section 2 3 2 1 Maximum likelihood segmentation applied to speech units In the following we apply the maximum likelihood speech segmentation approach described in 7 to the segmentation of speech units Given the parametrized frame based representation of a labeled speech unit Y y1 yn for example a sequence of Mel frequency cepstral coefficients MFCCs vectors where n corresponds to the length in frames of the speech unit we aim at finding the segment borders b2 and b3 which segment the unit into i 3 segments yb1 yb2 1 yb2 yb3 1 and yb3 yn with b1 1 The optimal segment borders are considered to be the borders minimizing the intra segment distortion of each segment As proposed in 7 we measure the intra segment distortion as the accumulation of distances from each frame inside the segment to the segment 3 bi 1 1 b2 b3 arg min b2 b3 i 1 n bi yn hal 00758424 version 1 28 Nov 2012 1 This corresponds to a shortest path problem which can be solved for each segment using dynamic programming The observation vector x for each speech unit is obtained by first concatenating the centroids of the obtained segments T Fig 2 Simplified example illustrating the extraction of a detection function dk from a given set of segments Each segment is displayed with its prediction score The maximum scores at each frame t are mapped onto dk t to train a classifier Hj x on each partition The final classification consists in averaging the output scores of the full committee of classifiers a segment and x s e to the observation vector extracted from this segment The minimum number of frames in a segment is i 3 The obtained detection function indicates the positions of the k th speech event in the speech signal by its local maxima Since we put most effort into the accurate modeling of the speech units we expect clear indications of the speech events without requiring sophisticated post processing The following simple peak picking algorithm can be applied to a system of discrete speech units like phonemes or broad phonetic classes BPCs Given the detection functions for k speech units we extract landmarks in three steps 1 We first collect the time instances corresponding to local maxima for each detection function dk and associate each local maximum t with its magnitude dk t Since a local maximum corresponds to a segment of at least three frames the central frame t of a segment is always selected as the time instance t of the local maximum 2 If several units k share the same local maximum t only the landmark with the highest magnitude dk t is kept at t 3 Some of the remaining landmarks might correspond to a local maximum at a very low magnitude Therefore at each landmark all units k with a score dk t bigger than the landmark magnitude are also activated at that time instance Afterwards each set Lk t1 tlk contains the final collection of lk discrete time instances signaling the presence of the k th speech event If a local maximum t of speech unit k already corresponds to the highest score dk t at that time frame step 3 will have no effect If not this step will merge several speech units into broader units effectively signaling the possible presence of several units at the same time 3 LANDMARK DRIVEN HMM BASED ASR We use the presented speech event detection approach to extract broad phonetic landmarks for a recently proposed landmark driven HMM based ASR framework 6 In this framework broad phonetic landmarks are used to guide the Viterbi decoding of the first pass of a HMM based speech recognizer which we briefly summarize in the following The Viterbi algorithm searches the best path reaching state j t by computing a score S j t max S i t 1 log aij log p y t j R j t i hal 00758424 version 1 28 Nov 2012 F x J j 1 Hj x J 2 2 3 From classification to detection As stated in the introduction of section 2 F x is used to obtain a detection function dk dk 1 dk t If F x is trained to predict single frames yt of parametrized speech dk t directly corresponds to the prediction score of class k at frame t Since our classifier provides segment and not framebased prediction scores we provide a new method to generate detection profiles from a collection of predicted segments Intuitively in order to obtain dk t one has to search for the segment containing t that maximizes the likelihood of the given speech unit at frame t Given a collection of overlapping segments with each segment corresponding to a hypothetical speech unit we can directly compare the score of variable length segments using our proposed observationvector A collection of segments does not correspond to a graph so that single segments might not have a connection to preceding or subsequent segments see simplified example in Figure 2 With this work being a preliminary study we do not consider the task of finding a collection of suitable prior segments which is desirable to reduce the computational costs for generating dk t Instead we use an exhaustive collection of segments by predicting the observations extracted from all possible segments inside the speech signal up to a maximum segment length of 300ms Using fast classification like boosting and an effective implementation for the solution of equation 1 this stays computationally feasible The value of the detection function dk t at t is calculated as follows dk t max Fk x s e s t e e s i 1 3 Fk x is the score of the k th detection function given the observation x s and e correspond to the first and last frame of 4 where log aij is the transition probability and log p yt j corresponds to the acoustic likelihood R j t enhances paths through states j that are compatible with phonetic landmarks by R j t j to triphones checking the compatibility is trivial since each state can be directly linked to one BPC j t is a binary indicator that becomes 1 if state j is compatible with the landmark at time t and 0 if there is no landmark at all Rmax limits the influence of landmarks onto the overall score and has to be determined experimentally 4 EXPERIMENTS Our experiments focus on two objectives First we are interested in evaluating whether our proposed approach can capture the acoustic information of speech units and compare the classification performance to existing approaches Second the proposed approach will be used to extract broad phonetic landmarks to guide the search of an HMM ASR system as described in section 3 4 1 Experimental setup The speech corpus used in the experiments corresponds to radio broadcast news in the French language from the ESTER2 broadcast transcription evaluation campaign 10 Our training set consists of over 200 hours of speech the development set of 3 hours and the test set of 4 5 hours from 4 broadcast shows radio Africa1 radio Inter radio RFI radio TVME The test set is divided into J 12 non overlapping partitions for a committee of 12 classifiers Classification experiments are run on the development set while speech recognition is carried out on the test set Speech labels are obtained by forced alignment The speech recognizer used for recognition experiments is a two pass system with the first pass generating a word graph and the second pass rescoring the previously obtained graph using more complex models We use the tool bonzaiboost1 for the training of the ensemble of weak classifiers 4 2 Phoneme classification While landmark detection is primarily used for the detection of phonetic events our proposed method is designed to learn any time variable speech unit This includes phonetically motivated units as well as phonemes We decided to do classification experiments on a phoneme classification task since this is a challenging task considering the fine acoustic distinctions between phonemes Our French phoneme alphabet consists of 40 phonemes including 5 non speech events HMM monophone models serve as the baseline phoneme classifier because of their time warping ability which makes them perfectly suitable for discriminating time varying speech units The models employed correspond to monophone 3state left to right HMMs with 64 diagonal covariance Gaussian components per state HMMs are trained on the full 1 developed by Christian http bonzaiboost gforge inria fr Fig 4 Proposed observation vector for the phoneme classification experiments On the left the observation vector extracted for each phoneme as proposed in this paper classifier 2 3 4 and 6 On the right the observation vector as a concatenation of subsequent speech frames at the center of each phoneme classifier 1 trained on 1 12 of training set classifier boosting concatenated frames depth 2 boosting proposed depth 1 boosting proposed depth 2 boosting proposed depth 3 trained on full training set classifier HMM 64 gaussians committee proposed J 12 depth 2 hal 00758424 version 1 28 Nov 2012 1 2 3 4 accuracy 55 7 57 8 63 0 62 8 5 6 accuracy 65 9 67 6 Table 1 Classifiers of the phoneme classification task The classifier is described by its classification method e g boosting and observation vector e g proposed Raymond available at training set using the same speech parametrization as the boosted ensembles which are 39 dimensional MFCC vectors composed of 13 MFCC coefficients with first and second order derivations To compare the abilities of the classifiers to capture the acoustic properties of each phoneme we evaluate on a pure classification task i e all observation vectors were extracted using the known phoneme borders obtained by forced alignment either for training or predicting the associated phoneme HMM classification was performed by force aligning each model to the known phoneme borders and comparing the obtained likelihoods First we trained four classifiers on only one partition i e 1 12 th of the training data for comparison before creating the full committee of 12 classifiers The first classifier uses the three concatenated frames at the temporal center of each phoneme as observation vector x for training and testing see Figure 4 Training and predicting only the center of each phoneme is supposed to minimize errors due to coarticulation effects We compare this approach to our proposed method which we run using weak learners of different depth classi hal 00758424 version 1 28 Nov 2012 Fig 3 Detection profiles of the four broad phonetic units vowels glides plosives voiced and fricatives voiced for the french word aujourd hui uttered during a broadcast news show and its corresponding broad phonetic annotation On the left every frame of the detection function was predicted using classifier 1 see Table 1 On the right the proposed method was used to generate the detection score at each frame using classifier 3 While vowels plosive and fricative are well indicated for both classifiers the detection function for classifier 1 is more noisy especially for the vowels Glides seem to be more clearly represented on the right side which can be due to the fact that glides correspond to slow articulatory movements which can be difficult to capture by concatenated frames fier 2 3 and 4 For all experiments the number of boosting rounds was limited to 3000 Table 1 displays the classification accuracy on the development set as the percentage of correctly classified phonemes Using the proposed observation vector for boosting increases the performance by 7 3 using decision trees with depth 2 compared to training on concatenated frames While boosting on one partition of the training set does not outperform the accuracy of HMMs applying the committee of 12 boosted weak ensembles improves by 1 7 compared to HMMs 4 3 Landmark driven ASR To use our phoneme classifier for broad phonetic landmark detection we simply derived a collection of detection functions dj for j 7 BPCs vowels nasals glides fricatives voiced and unvoiced and plosives voiced and unvoiced by scoring every frame dj t with the score dk t of the phoneme k classifier 3 6 574k 608k 20 2 18 0 12 0 8 8 20 20 number of landmarks phoneme errors missed phonemes AVG BPC size phonemes 1 1 352k 67 7 4 6 13 Table 2 Statistics of extracted landmarks on the whole development set for three classifiers AVG BPC size corresponds to the average number of phonemes provided at each landmark that has the maximum score among all phonemes of this BPC at frame t We extracted landmarks as proposed in section 2 3 for classifiers 3 and 6 from Table 1 For classifier 1 we equally extracted landmarks but using the detection function obtained by predicting each individual frame Figure 3 compares de broadcast Inter RFI TVME Africa1 WER baseline 18 7 17 6 24 2 31 5 WER landmark driven 18 6 17 3 23 8 30 8 Table 3 Speech recognition performance driven by broad phonetic landmarks Landmarks are extracted using classifier 6 tection profiles obtained by predicting single frames using classifier 1 and by employing our proposed method classifier 3 Table 2 contains information about the landmark accuracy A phoneme error corresponds to a phoneme with at least one landmark that misclassifies this phoneme As one can expect directly extracting local maxima from frame based detection functions leads to more than twice as much landmarks compared to the proposed method because of the noisy detection profile see also Figure 3 resulting in many incorrect landmarks It should be noted that this is partly due to our landmark extraction algorithm which is designed to avoid any post processing like smoothing or heuristic peak picking algorithms The high average number of active phonemes at each landmark is due to many landmarks that consist of several merged BPCs For the landmark driven ASR experiments according to section 3 we used the landmarks obtained with classifier 6 First the development set was used to tune Rmax see equation 4 The optimal Rmax was then employed for the landmark driven decoding on the test set The results in Table 3 show an improvement for all 4 broadcast shows tested compared to the baseline which does not include landmarks The improvement in word error rate WER varies from 0 1 radio Inter to 0 7 radio Africa1 The overall WER of the test set was 23 1 compared to a 23 5 baseline and a Wilcoxon signed rank showed it to be significant at the 5 level Compared to the WER obtained by the phonetically guided decoding presented in 6 where BPCs were trained on selected frames and decoding was guided by predicting the BPC of every frame no broadcast show of the test set was degraded by the use of broad phonetic information 5 CONCLUSIONS In this work we presented a new method for the detection of variable length speech units We used segmentation to obtain a fixed dimensional observation vector for each speech unit to train a committee of boosted decision stumps for speech unit classification To detect speech units in an unknown signal we searched for each time frame the corresponding segment that provides the maximum classification score for the desired speech unit This approach improved the accuracy of a phoneme classification task compared to HMM phoneme classification as well as the WER of a hybrid HMM based landmark driven ASR framework compared to its HMMbased baseline With the promising results we were able to obtain by employing our proposed framework there are several directions for future research First the proposed fixed dimensional observation space could be refined by considering different segmentation and attribute extraction strategies Second the step from classification to detection by evaluating all possible segments should be replaced by an efficient search for a suitable collection of prior segments While boosting decision stumps performed well on the presented classification task applying other classification techniques might further improve speech unit classification and detection 6 REFERENCES 1 M Ostendorf Moving beyond the beads on a string model of speech in Proc of ASRU 1999 1999 pp hal 00758424 version 1 28 Nov 2012 