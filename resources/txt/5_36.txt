Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 CHAPTER 10 Audio Visual Automatic Speech Recognition An Overview Gerasimos Potamianos Chalapathy Neti Human Language Technologies Department IBM Thomas J Watson Research Center Yorktown Heights NY 10598 USA e mail fgpotam cnetig us ibm com Juergen Luettin Robert Bosch GmbH Automotive Electronics D 7152 Leonberg Germany e mail Juergen Luettin de bosch com Iain Matthews Robotics Institute Carnegie Mellon University Pittsburgh PA 15213 USA e mail iainm cs cmu edu INTRODUCTION We have made significant progress in automatic speech recognition ASR for well defined applications like dictation and medium vocabulary transaction processing tasks in relatively controlled environments However ASR performance has yet to reach the level required for speech to become a truly pervasive user interface Indeed even in clean acoustic environments and for a variety of tasks state of the art ASR system performance lags human speech perception by up to an order of magnitude Lippmann 1997 In addition current systems are quite sensitive to channel environment and style of speech variations A number of techniques for improving ASR robustness have met limited success in severely degraded environments mismatched to system training Ghitza 1986 Nadas et al 1989 Juang 1991 Liu et al 1993 Hermansky and Morgan 1994 Neti 1994 Gales 1997 Jiang et al 2001 Clearly novel non traditional approaches that use orthogonal sources of information to the acoustic input are needed to achieve ASR performance closer to the human speech perception level and robust enough to be deployable in field applications Visual speech is the most promising source of additional speech information and it is obviously not affected by the acoustic environment and noise Human speech perception is bimodal in nature Humans combine audio and visual information in deciding what has been spoken especially in noisy environments The visual modality benefit to speech intelligibility in noise has been quantified as far back as in Sumby and Pollack 1954 Furthermore bimodal fusion of audio and visual stimuli in perceiving speech has been demonstrated by the McGurk effect McGurk and MacDonald 1976 For example when the spoken sound ga is superimposed on the video of a person uttering ba most people perceive the speaker as uttering the sound da In addition visual speech is of particular importance to the hearing impaired Mouth movement is known to play an important role in both sign language and simultaneous communication between the deaf Marschark et al 1998 The hearing impaired speechread well and possibly better than the general population Bernstein et al 1998 There are three key reasons why vision benefits human speech perception Summerfield 1987 It helps speaker audio source localization it contains speech segmental information that supplements the audio and it provides complimentary information about the place of articulation The latter is due to the partial or full Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 AUDIO AUDIO FEATURE EXTRACTION AUDIO ONLY ASR t VIDEO t VISUAL FRONT END FACE DETECTION MOUTH LOCALIZATION LIP TRACKING VISUAL FEATURE EXTRACTION AUDIO VISUAL FUSION AUDIO VISUAL ASR VISUAL ONLY ASR AUTOMATIC SPEECHREADING Figure 1 The main processing blocks of an audio visual automatic speech recognizer The visual front end design and the audio visual fusion modules introduce additional challenging tasks to automatic recognition of speech as compared to traditional audio only ASR They are discussed in detail in this chapter visibility of articulators such as the tongue teeth and lips Place of articulation information can help disambiguate for example the unvoiced consonants p a bilabial and k a velar the voiced consonant pair b and d a bilabial and alveolar respectively and the nasal m a bilabial from the nasal alveolar n Massaro and Stork 1998 All three pairs are highly confusable on basis of acoustics alone In addition jaw and lower face muscle movement is correlated to the produced acoustics Yehia et al 1998 Barker and Berthommier 1999 and its visibility has been demonstrated to enhance human speech perception Summerfield et al 1989 Smeele 1996 The above facts have motivated significant interest in automatic recognition of visual speech formally known as automatic lipreading or speechreading Stork and Hennecke 1996 Work in this field aims at improving ASR by exploiting the visual modality of the speaker s mouth region in addition to the traditional audio modality leading to audio visual automatic speech recognition systems Not surprisingly including the visual modality has been shown to outperform audio only ASR over a wide range of conditions Such performance gains are particularly impressive in noisy environments where traditional acoustic only ASR performs poorly Improvements have also been demonstrated when speech is degraded due to speech impairment Potamianos and Neti 2001a and Lombard effects Huang and Chen 2001 Coupled with the diminishing cost of quality video capturing systems these facts make automatic speechreading tractable for achieving robust ASR in certain scenarios Hennecke et al 1996 Automatic recognition of audio visual speech introduces new and challenging tasks compared to traditional audio only ASR The block diagram of Figure 1 highlights these In addition to the usual audio front end feature extraction stage visual features that are informative about speech must be extracted from video of the speaker s face This requires robust face detection as well as location estimation and tracking of the speaker s mouth or lips followed by extraction of suitable visual features In contrast to audio only recognizers there are now two streams of features available for recognition one for each modality The combination of the audio and visual streams should ensure that the resulting system performance is better than the best of the two single modality recognizers and hopefully significantly outperform it Both issues namely the visual front end design and audio visual fusion constitute difficult problems and they have generated significant research work by the scientific community Indeed since the mid eighties over a hundred articles have concentrated on audio visual ASR with the vast majority appearing during the last decade The first automatic speechreading system was reported by Petajan 1984 Given the video of the speaker s face and by using simple image thresholding he was able to extract binary black and white mouth images and subsequently mouth height width perimeter and area as visual speech features He then developed a visual only recognizer based on dynamic time warping Rabiner and Juang 1993 to rescore the best two choices of the output of the baseline audio only system His method improved ASR for a single speaker isolated word recognition task on a 100 word vocabulary that included digits and letters Petajan s work generated significant excitement and soon various sites established research in audio visual ASR Among the pioneer sites was the group headed by Christian Beno it at the Institute Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 de la Communication VISUAL FRONT ENDS FOR AUTOMATIC SPEECHREADING As it was briefly mentioned in the Introduction see also Figure 1 the first main difficulty in the area of audio visual ASR is the visual front end design The problem is two fold Face lips or mouth tracking is first required followed by visual speech representation in terms of a small number of informative features Clearly the two issues are closely related Employing a lip tracking algorithm allows one to use visual features such as mouth height or width Adjoudani and Beno it 1996 Chan et al 1998 Potamianos et al 1998 or parameters of a suitable lip model Chandramohan and Silsbee 1996 Dalton et al 1996 Luettin et al 1996 On the other hand only a crude detection of the mouth region is sufficient to obtain visual features using transformations of this region s pixel values that achieve sufficient dimensionality reduction Bregler Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 et al 1993 Duchnowski et al 1994 Matthews et al 1996 Potamianos et al 2001b Needless to say robust tracking of the lips or mouth region is of paramount importance for good performance of automatic speechreading systems Iyengar et al 2001 Face Detection Mouth and Lip Tracking The problem of face and facial part detection has attracted significant interest in the literature Graf et al 1997 Rowley et al 1998 Sung and Poggio 1998 Senior 1999 In addition to automatic speechreading it has applications to other areas such as visual text to speech Cohen and Massaro 1994 Chen et al 1995 Cosatto et al 2000 person identification and verification Jourlin et al 1997 Wark and Sridharan 1998 Face Detection and Mouth Region of Interest Extraction A typical algorithm for face detection and facial feature localization is described in Senior 1999 This technique is used in the visual front end design of Neti et al 2000 and Potamianos et al 2001b when processing the video of the IBM ViaVoiceTM audio visual database described later Given a video frame face detection is first performed by employing a combination of methods some of which are also used for subsequent face feature finding A face template size is first chosen an 11 11 pixel square here and an image pyramid over all permissible face locations and scales given the video frame and face template sizes is used to search for possible face candidates This search is constrained by the minimum and maximum allowed face candidate size with respect to the frame size the face size increment from one pyramid level to the next the spatial shift in searching for faces within each pyramid level and the fact that no candidate face can be of smaller size than the face template In Potamianos et al 2001b the face square side is restricted to lie within 10 and 75 of the frame width with a face size increase of 15 across consecutive pyramid levels Within each pyramid level a local horizontal and vertical shift of one pixel is used to search for candidate faces In case the video signal is in color skin tone segmentation can be used to quickly narrow the search to face candidates that contain a relatively high proportion of skin tone pixels The normalized red green blue values of each frame pixel are first transformed to the hue saturation color space where skin tone is known Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Figure 2 Region of interest extraction examples Upper rows Example video frames of eight subjects from the IBM ViaVoiceTM audio visual database described in a later section with superimposed facial features detected by the algorithm of Senior 1999 Lower row Corresponding mouth regions of interest extracted as in Potamianos et al 2001b to occupy a largely invariant to most humans and lighting conditions range of values Graf et al 1997 Senior 1999 In the particular implementation all face candidates that contain less than 25 of pixels with hue and saturation values that fall within the skin tone range are eliminated This substantially reduces the number of face candidates depending on the frame background speeding up computation and reducing spurious face detections Every remaining face candidate is subsequently size normalized to the 11 11 face template size and its greyscale pixel values are placed into a 121 dimensional face candidate vector Each such vector is given a score based on both a two class face versus non face Fisher linear discriminant and the candidate s distance from face space DFFS i e the face vector projection error onto a lower 40 dimensional space obtained by means of principal components analysis PCA see below All candidate regions exceeding a threshold score are considered as faces Among such faces at neighboring scales and locations the one achieving the maximum score is returned by the algorithm as a detected face Senior 1999 Once a face has been detected an ensemble of facial feature detectors are used to estimate the locations of 26 facial features including the lip corners and centers twelve such facial features are marked on the frames of Figure 2 Each feature location is determined by using a score combination of prior feature location statistics linear discriminant and distance from feature space similar to the DFFS discussed above based on the chosen feature template size such as 11 11 pixels Before incorporating the described algorithm into our speechreading system a training step is required to estimate the Fisher discriminant and eigenvectors PCA for face detection and facial feature estimation as well as the facial feature location statistics Such training requires a number of frames manually annotated with the faces and their visible features When training the Fisher discriminant both face and non face or facial feature and non feature vectors are used whereas in the case of PCA face and facial feature only vectors are considered Senior 1999 Given the output of the face detection and facial feature finding algorithm described above five located lip Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Figure 3 Examples of lip contour estimation by means of active shape models Luettin et al 1996 Depicted mouth regions are from the Tulips1 audio visual database Movellan and Chadderdon 1996 and they have been extracted preceding lip contour estimation contour points are used to estimate the mouth center and its size at every video frame four such points are marked on the frames of Figure 2 To improve ROI extraction robustness to face and mouth detection errors the mouth center estimates are smoothed over twenty neighboring frames using median filtering to obtain the ROI center whereas the mouth size estimates are averaged over each utterance A size normalized square ROI is then extracted see 1 below with sides M N 64 see also Figure 2 This can contain just the mouth region or also parts of the lower face Potamianos and Neti 2001b Lip Contour Tracking Once the mouth region is located a number of algorithms can be used to obtain lip contour estimates Some popular methods are snakes Kass et al 1988 templates Yuille et al 1992 Silsbee 1994 and active shape and appearance models Cootes et al 1995 1998 A snake is an elastic curve represented by a set of control points The control point coordinates are iteratively updated by converging towards the local minimum of an energy function defined on basis of curve smoothness constraints and a matching criterion to desired features of the image Kass et al 1988 Such an algorithm is used for lip contour estimation in the speechreading system of Chiou and Hwang 1997 Another widely used technique for lip tracking is by means of lip templates employed in the system of Chandramohan and Silsbee 1996 for example Templates constitute parametrized curves that are fitted to the desired shape by minimizing an energy function defined similarly to snakes B splines used by Dalton et al 1996 work similarly to the above techniques as well Active shape and appearance models construct a lip shape or ROI appearance statistical model as discussed in following subsections These models can be used for tracking lips by means of the algorithm proposed by Cootes et al 1998 This assumes that given small perturbations from the actual fit of the model to a target image a linear relationship exists between the difference in the model projection and image and the required updates to the model parameters An iterative algorithm is used to fit the model to the image data Matthews et al 1998 Alternatively the fitting can be performed by the downhill simplex method Nelder and Mead 1965 as in Luettin et al 1996 Examples of lip contour estimation by means of active shape models using the latter fitting technique are depicted in Figure 3 Visual Features Various sets of visual features for automatic speechreading have been proposed in the literature over the last 20 years In general they can be grouped into three categories a Video pixel or appearance based ones b Lip contour or shape based features and c Features that are a combination of both appearance and Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 shape Hennecke et al 1996 In the following we present each category in more detail Possible feature post extraction processing is discussed at the end of this section Appearance Based Features In this approach to visual feature extraction the image part typically containing the speaker s mouth region is considered as informative for lipreading i e the region of interest ROI Such region can be a rectangle containing the mouth and possibly include larger parts of the lower face such as the jaw and cheeks Potamianos and Neti 2001b or the entire face Matthews et al 2001 Often it can be a three dimensional rectangle containing adjacent frame rectangular ROIs in an effort to capture dynamic speech information at this early stage of processing Li et al 1995 Potamianos et al 1998 Alternatively the ROI can correspond to a number of image profiles vertical to the lip contour Dupont and Luettin 2000 or be just a disc around the mouth center Duchnowski et al 1994 By concatenating the ROI pixel greyscale Bregler et al 1993 Duchnowski et al 1994 Potamianos et al 1998 Dupont and Luettin 2000 or color values Chiou and Hwang 1997 a feature vector is obtained For example in the case of an M N pixel rectangular ROI which is centered at location mt nt of video frame Vt m n at time t the resulting feature vector of length d MN will be after a lexicographic ordering 1 xt f Vtm n mt bM 2c m mt dM 2e nt bN 2c n nt dN 2e g 1 This vector is expected to contain most visual speech information Notice that approaches that use optical flow as visual features Mase and Pentland 1991 Gray et al 1997 can fit within this framework by replacing in 1 the video frame ROI pixels with optical flow estimates Typically the dimensionality d of vector 1 is too large to allow successful statistical modeling Chatfield and Collins 1991 of speech classes by means of a hidden Markov model HMM for example Rabiner and Juang 1993 Therefore appropriate transformations of the ROI pixel values are used as visual features Movellan and Chadderdon 1996 for example use low pass filtering followed by image subsampling and video frame ROI differencing whereas Matthews et al 1996 propose a nonlinear image decomposition using image sieves for dimensionality reduction and feature extraction By far however the most popular appearance feature representations achieve such reduction by using traditional image transforms Gonzalez and Wintz 1977 These transforms are typically borrowed from the image compression literature and the hope is that they will preserve most relevant to speechreading information In general a D d dimensional linear transform matrix is sought such that the transformed data vector t t contains most speechreading d elements To obtain matrix L training examples are given denoted by l information in its D l 1 L A number of possible such matrices are described in the following P P y Px x Principal components analysis PCA This constitutes the most popular pixel based feature representation for automatic speechreading Bregler et al 1993 Bregler and Konig 1994 Duchnowski et al 1994 Li et al 1995 Brooke 1996 Tomlinson et al 1996 Chiou and Hwang 1997 Gray et al 1997 Luettin and Thacker 1997 Potamianos et al 1998 Dupont and Luettin 2000 The PCA data projection achieves optimal information compression in the sense of minimum square error between the original vector t and its reconstruction based on its projection t however appropriate data scaling constitutes a problem in the classification of the resulting vectors Chatfield and Collins 1991 In the PCA implementation of Potamianos et al 1998 the data are scaled according to their inverse variance and their correlation matrix is Chatfield and Collins 1991 Press et al 1995 computed Subsequently is diagonalized as where 1 d has as columns the eigenvectors of and is a diagonal matrix containing the eigenvalues of Assuming that the D largest such eigenvalues are located at the j1 jD diagonal positions the data projection matrix is PCA j1 jD Given a data vector t this is first element wise mean and variance normalized and subsequently its feature vector is extracted as t PCA t y x A a a R R R AA a R R P a x y P x Discrete cosine wavelet and other image transforms As an alternative to PCA a number of popular linear image transforms Gonzalez and Wintz 1977 have been used in place of for obtaining speechreading P Throughout this work boldface lowercase symbols denote column vectors and boldface capital symbols denote matrices In addition denotes vector or matrix transpose and diag det denote matrix diagonal and determinant respectively 1 Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 features For example the discrete cosine transform DCT has been adopted in several systems Duchnowski et al 1994 Potamianos et al 1998 Nakamura et al 2000 Neti et al 2000 Scanlon and Reilly 2001 Nefian et al 2002 the discrete wavelet transform DWT Daubechies 1992 in others Potamianos et al 1998 and the Hadamard and Haar transforms by Scanlon and Reilly 2001 Most researchers use separable transforms Gonzalez and Wintz 1977 which allow fast implementations Press et al 1995 when M and N are powers of 2 typically values M N 16 32 or 64 are considered Notice that in each case matrix can have as rows the image transform matrix rows that maximize the transformed data energy over the training set Potamianos et al 1998 or alternatively that correspond to a priori chosen locations Nefian et al 2002 P Linear discriminant analysis LDA The data vector transforms presented above are more suitable for ROI compression than ROI classification into the set of speech classes of interest For the latter task LDA Rao 1965 is more appropriate as it maps features to a new space for improved classification LDA was first proposed for automatic speechreading by Duchnowski et al 1994 There it was applied directly to vector 1 LDA has also been considered in a cascade following the PCA projection of a single frame ROI vector or on the concatenation of a number of adjacent PCA projected vectors Matthews et al 2001 LDA assumes that a set of classes C such as HMM states is a priori chosen and in addition that the training set data vectors l l 1 L are labeled as cl 2 C Then it seeks matrix LDA such that the projected training sample f LDA l l 1 L g is well separated into the set of classes C according to a function of the training sample within class scatter matrix W and its between class scatter matrix B Rao 1965 These matrices are given by x 2 C is the class empirical probability mass function where Lc respectively L and 1 if i j 0 otherwise in addition c and c denote the class sample mean and i j c l c l 1 covariance respectively and finally c2C Prc c is the total sample mean To estimate LDA the are generalized eigenvalues and right eigenvectors of the matrix pair B W that satisfy B W first computed Rao 1965 Golub and Van Loan 1983 Matrix 1 d has as columns the generalized eigenvectors Assuming that the D largest eigenvalues are located at the j1 jD diagonal positions of then LDA j1 jD It should be noted that due to 2 the rank of B is at most jCj 1 where jCj denotes the number of classes the cardinality of set C hence D jCj 1 should hold In addition the rank of the d d dimensional matrix W cannot exceed L jCj therefore having insufficient training data with respect to the input feature vector dimension d is a potential problem 2C In 2 Prc Lc L c c SW X Prc P x P c and SB m X Prc m m m m c c S S c 2C 2 m m S S F f f P S F S F P f f S S Maximum Likelihood Data Rotation MLLT In our speechreading system Potamianos et al 2001b LDA is followed by the application of a data maximum likelihood linear transform MLLT This transform seeks a square non singular data rotation matrix MLLT that maximizes the observation data likelihood in the original feature space under the assumption of diagonal data covariance in the transformed space Gopinath 1998 Such a rotation is beneficial since in most ASR systems diagonal covariances are typically assumed when modeling the observation class conditional probability distribution with Gaussian mixture models The desired rotation matrix is obtained as P PMLLT arg max P f det PL Y det diag P 2C c P 2 g Lc c Gopinath 1998 This can be solved numerically Press et al 1995 Notice that LDA and MLLT are data transforms aiming in improved classification performance and maximum likelihood data modeling Therefore their application can be viewed as a feature post processing stage and clearly should not be limited to appearance only visual data Shape Based Features In contrast to appearance based features shape based feature extraction assumes that most speechreading information is contained in the shape contours of the speaker s lips or more generally Matthews et al Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 1 1 ORIGINAL OUTER LIP CONTOUR FEATURES NORMALIZED SEQUENCE 81926 1 0 9 0 8 0 7 0 6 WIDTH w h w 1 2 HEIGHT h 0 5 AREA 20 RECONSTRUCTED CONTOURS FOR VARIOUS NUMBERS OF FOURIER COEFFICIENTS USED 3 0 4 0 3 20 40 60 80 FRAME t 100 120 140 Figure 4 Geometric feature approach Left Outer lip width and height Middle Reconstruction of an estimated outer lip contour upper part from 1 2 3 and 20 sets of its Fourier coefficients lower part clockwise Right Three geometric visual features displayed on a normalized scale tracked over the spoken utterance 81926 of the connected digits database of Potamianos et al 1998 Lip contours are estimated as in Graf et al 1997 2001 in the face contours e g jaw and cheek shape in addition to the lips Two types of features fall within this category Geometric type ones and shape model based features In both cases an algorithm that extracts the inner and or outer lip contours or in general the face shape is required A variety of such algorithms were discussed above Lip geometric features Given the lip contour a number of high level features meaningful to humans can be readily extracted such as the contour height width perimeter as well as the area contained within the contour As demonstrated in Figure 4 such features do contain significant speech information Not surprisingly a large number of speechreading systems makes use of all or a subset of them Petajan 1984 Adjoudani and Beno it 1996 Alissali et al 1996 Goldschen et al 1996 w h S in terms Given a set of vectors 3 PCA can be used to identify the optimal orthogonal linear transform PCA of the variance described along each dimension resulting in a statistical model of the lip or facial shape see xS x y x y xK yK 1 1 2 2 3 P Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Figure 5 Statistical shape model The top four modes are plotted left to right at 3 standard deviations around the mean These four modes describe 65 of the variance of the training set which consists of 4072 labeled images from the IBM ViaVoiceTM audio visual database Neti et al 2000 Matthews et al 2001 Figure 5 To identify axes of genuine shape variation each shape in the training set must be aligned This is achieved using a similarity transform translation rotation and scaling by means of an iterative procrustes analysis Cootes et al 1995 Dryden and Mardia 1998 Given a tracked lip contour the extracted visual S S Note that vectors 3 can be the output of a tracking algorithm based on features will be S PCA B splines for example as in Dalton et al 1996 y P x Joint Appearance and Shape Features Appearance and shape based visual features are quite different in nature In a sense they code low and highlevel information about the speaker s face and lip movements Not surprisingly combinations of features from both categories have been employed in a number of automatic speechreading systems In most cases features from each category are just concatenated For example Chan 2001 combines geometric lip features with the PCA projection of a subset of pixels contained within the mouth Luettin et al 1996 as well as Dupont and Luettin 2000 combine ASM features with PCA based ones extracted from a ROI that consists of short image profiles around the lip contour Chiou and Hwang 1997 on the other hand combine a number of snake lip contour radial vectors with PCA features of the color pixel values of a rectangle mouth ROI A different approach to combining the two classes of features is to create a single model of face shape and appearance An active appearance model AAM Cootes et al 1998 provides a framework to statistically combine them Building an AAM requires three applications of PCA S com a Shape eigenspace calculation that models shape deformations resulting in PCA matrix PCA puted as above see 3 A b Appearance eigenspace calculation to model appearance changes resulting in a PCA matrix PCA of the ROI appearance vectors If the color values of the M N pixel ROI are considered such vectors are A r g b r g b r g b 4 1 1 1 2 2 2 MN MN MN P P x similar to vectors 1 c Using these calculation of a combined shape and appearance eigenspace The latter is a PCA matrix A S PCA on training vectors P where is a suitable diagonal scaling matrix Matthews et al 2001 The aim of this final PCA is to remove the redundancy due to the shape and appearance correlation and to create a single model that compactly describes shape and the corresponding appearance deformation W A xS PS xA S xA W PPCA PCA yA W yS Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Figure 6 Combined shape and appearance statistical model Center row Mean shape and appearance Top row Mean shape and appearance 3 standard deviations Bottom row Mean shape and appearance 3 standard deviations The top four modes depicted left to right describe 46 of the combined shape and appearance variance of 4072 labeled images from the IBM ViaVoiceTM audio visual database Neti et al 2000 Matthews et al 2001 Such a model has been used for speechreading in Neti et al 2000 and Matthews et al 2001 An example of the resulting learned joined model is depicted in Figure 6 A block diagram of the method including the dimensionalities of the input shape and appearance vectors 3 and 4 respectively their PCA projections S A and the final feature vector A S A S A S is depicted in Figure 7 PCA y y y P x Visual Feature Post Extraction Processing In an audio visual speech recognition system in addition to the visual features audio features are also extracted from the acoustic waveform For example such features could be mel frequency cepstral coefficients MFCCs or linear prediction coefficients LPCs typically extracted at a 100 Hz rate Deller et al 1993 Rabiner and Juang 1993 Young et al 1999 In contrast visual features are generated at the video frame rate commonly 25 or 30 Hz or twice that in case of interlaced video Since feature stream synchrony is required in a number of algorithms for audio visual fusion as discussed in the next section the two feature streams must attain the same rate Typically this is accomplished whenever required either after feature extraction by simple element wise linear interpolation of the visual features to the audio frame rate as in Figure 7 or before feature extraction by frame duplication to achieve a 100 Hz video input rate to the visual Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 1 64 1 64 1 1 4096 1 1 DCT FEATURES 24 t INTERPOLATION TO 100 Hz DCT 24 4096 1 APPEARANCE INFO OR FEATURE MEAN NORMALIZATION yt xt J 15 E 1 6000 1 186 1 186 1 PCA 6000 1 1 134 AAM FEATURES 197 1 86 1 1 1 41 41 SHAPE INFO PCA 187 197 86 LDA 1 41 MLLT 1 1 41 V 1 11 PLDA V PMLLT V PCA 134 41 ot Figure 7 DCT versus AAM based visual feature extraction for automatic speechreading followed by visual feature post extraction processing using linear interpolation feature mean normalization adjacent frame feature concatenation and the application of LDA and MLLT Vector dimensions as implemented in the system of Neti et al 2000 are depicted front end Occasionly the audio front end processing is performed at the lower video rate Another interesting issue in visual feature extraction has to do with feature normalization In a traditional audio front end cepstral mean subtraction is often employed to enhance robustness to speaker and environment variations Liu et al 1993 Young et al 1999 A simple visual feature mean normalization FMN by element wise subtraction of the vector mean over each sentence has been demonstrated to improve appearance feature based visual only recognition Potamianos et al 1998 2001b Alternatively linear intensity compensation has been investigated preceding the appearance feature extraction by Vanegas et al 1998 A very important issue in the visual feature design is capturing the dynamics of visual speech Temporal information often spanning multiple phone segments is known to help human perception of visual speech Rosenblum and Salda na 1998 Borrowing again from the ASR literature dynamic speech information can be captured by augmenting the visual feature vector by its first and second order temporal derivatives Rabiner and Juang 1993 Young et al 1999 Alternatively LDA can be used as a means of learning a transform that optimally captures the speech dynamics Such a transform is applied on the concatenation of consecutive feature vectors adjacent and including the current frame see also Figure 7 i e on x t yt bJ c yt yt 2 dJ 2e 1 5 with J 15 for example as in Neti et al 2000 and Potamianos et al 2001b Clearly and as we already mentioned LDA could be applied to any category of features discussed The same holds for MLLT a method that aims in improving maximum likelihood data modeling and in practice ASR performance For example a number of feature post processing steps discussed above including LDA and MLLT have been interchangeably applied to DCT appearance features as well as to AAM ones in our visual front end experiments during the Johns Hopkins workshop as depicted in Figure 7 Neti et al 2000 Matthews et al 2001 Alternate ways of combining feature post extraction processing steps can easily be envisioned For example LDA and MLLT can be applied to obtain within frame discriminant features Potamianos and Neti 2001b which can then be augmented by their first and second order derivatives or followed by LDA and MLLT across frames see also Figure 11 Finally an important problem in data classification is the issue of feature selection within a larger pool of candidate features Jain et al 2000 In the context of speechreading this matter has been directly addressed in the selection of geometric lip contour based features by Goldschen et al 1996 Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Summary of Visual Front End Algorithms We have presented a summary of the most common visual feature extraction algorithms proposed in the literature for automatic speechreading Such techniques differ both in their assumptions about where the speechreading information lies as well as in the requirements that they place on face detection facial part localization and tracking On the one extreme appearance based visual features consider a broadly defined ROI and then rely on traditional pattern recognition and image compression techniques to extract relevant speechreading information On the opposite side shape based visual features require adequate lip or facial shape tracking and assume that the visual speech information is captured by this shape s form and movement alone Bridging the two extremes various combinations of the two types of features have also been used ranging from simple concatenation to their joint modeling Comparisons between features within the same class are often reported in the literature Duchnowski et al 1994 Goldschen et al 1996 Gray et al 1997 Potamianos et al 1998 Matthews et al 2001 Scanlon and Reilly 2001 Unfortunately however comparisons across the various types of features are rather limited as they require widely different sets of algorithms for their implementation Nevertheless Matthews et al 1998 demonstrate AAMs to outperform ASMs and to result in similar visual only recognition to alternative appearance based features Chiou and Hwang 1997 report that their joint features outperform their shape and appearance feature components whereas Potamianos et al 1998 as well as Scanlon and Reilly 2001 report that DCT transform based visual features are superior to a set of lip contour geometric features However the above results are reported on single subject data and or small vocabulary tasks In a larger experiment Matthews et al 2001 compare a number of appearance based features with AAMs on a speaker independent LVCSR task All appearance features considered outperformed AAMs however it is suspected that the AAM used there was not sufficiently trained Although much progress has been made in visual feature extraction it seems that the question of what are the best visual features for automatic speechreading that are robust in a variety of visual environments remains to a large extent unresolved Of particular importance is that such features should exhibit sufficient speaker pose camera and environment independence However it is worth mentioning two arguments in favor of appearance based features First their use is well motivated by human perception studies of visual speech Indeed significant information about the place of articulation such as tongue and teeth visibility cannot be captured by the lip contours alone Human speech perception based on the mouth region is superior than perception on basis of the lips alone and it further improves when the entire lower face is visible Summerfield et al 1989 Second the extraction of certain highly performing appearance based features such as the DCT is computationally efficient Indeed it requires a crude mouth region detection algorithm which can be applied at a low frame rate whereas the subsequent pixel vector transform is amenable to fast implementation for suitable ROI sizes Press et al 1995 These facts enable the implementation of real time automatic speechreading systems AUDIO VISUAL INTEGRATION FOR SPEECH RECOGNITION Audio visual fusion is an instance of the general classifier combination problem Jain et al 2000 In our case two observation streams are available audio and visual modalities and provide information about speech classes such as context dependent sub phonetic units or at a higher level word sequences Each observation stream can be used alone to train single modality statistical classifiers to recognize such classes However one hopes that combining the two streams will give rise to a bimodal classifier with superior performance to both single modality ones Various information fusion algorithms have been considered in the literature for audio visual ASR for example Bregler et al 1993 Adjoudani and Beno it 1996 Hennecke et al 1996 Potamianos and Graf 1998 Rogozan 1999 Teissier et al 1999 Dupont and Luettin 2000 Neti et al 2000 Chen 2001 Chu and Huang 2002 The proposed techniques differ both in their basic design as well as in the adopted terminology The architecture of some of these methods Robert Ribes et al 1996 Teissier et al 1999 is motivated by models of human speech perception Massaro 1996 Massaro and Stork 1998 Berthommier 2001 In Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 FUSION TYPE Feature fusion One classifier is used Decision fusion Two classifiers are used AUDIO VISUAL FEATURES 1 Concatenated features 2 Hierarchical discriminant features 3 Enhanced audio features Concatenated features CLASSIFICATION LEVEL Sub phonetic early 1 Sub phonetic early 2 Phone or word intermediate 3 Utterance late Table 1 Taxonomy of the audio visual integration methods considered in this section Three feature fusion techniques that differ in the features used for recognition and three decision fusion methods that differ in the combination stage of the audio and visual classifiers are described in more detail in this chapter most cases however research in audio visual ASR has followed a separate track from work on modeling the human perception of audio visual speech Audio visual integration techniques can be broadly grouped into feature fusion and decision fusion methods The first ones are based on training a single classifier i e of the same form as the audio and visual only classifiers on the concatenated vector of audio and visual features or on any appropriate transformation of it Adjoudani and Beno it 1996 Teissier et al 1999 Potamianos et al 2001a In contrast decision fusion algorithms utilize the two single modality audio and visual only classifier outputs to recognize audio visual speech Typically this is achieved by linearly combining the class conditional observation log likelihoods of the two classifiers into a joint audio visual classification score using appropriate weights that capture the reliability of each single modality classifier or data stream Hennecke et al 1996 Rogozan et al 1997 Potamianos and Graf 1998 Dupont and Luettin 2000 Neti et al 2000 In this section we provide a detailed description of some popular fusion techniques from each category see also Table 1 In addition we briefly address two issues relevant to automatic recognition of audiovisual speech One is the problem of speech modeling for ASR which poses particular interest in automatic speechreading and helps establish some background and notation for the remainder of the section We also consider the subject of speaker adaptation an important element in practical ASR systems Audio Visual Speech Modeling for ASR Two central aspects in the design of ASR systems are the choice of speech classes that are assumed to generate the observed features and the statistical modeling of this generation process In the following we briefly discuss both issues since they are often embedded into the design of audio visual fusion algorithms Speech Classes for Audio Visual ASR The basic unit that describes how speech conveys linguistic information is the phoneme For American English there exist approximately 42 such units Deller et al 1993 generated by specific positions or movements of the vocal tract articulators Only some of the articulators are visible however therefore among these phonemes the number of visually distinguishable units is much smaller Such units are called visemes in the audio visual ASR and human perception literatures Stork and Hennecke 1996 Campbell et al 1998 Massaro and Stork 1998 In general phoneme to viseme mappings are derived by human speechreading studies Alternatively such mappings can be generated using statistical clustering techniques as proposed by Goldschen et al 1996 and Rogozan 1999 There is no universal agreement about the exact partitioning of phonemes into visemes but some visemes are well defined such as the bilabial viseme consisting of phoneme set f p b m g A typical clustering into 13 visemes is used by Neti et al 2000 to conduct visual speech modeling experiments and is depicted in Table 2 In traditional audio only ASR the set of classes c 2C that need to be estimated on basis of the observed feature Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Silence Lip rounding based vowels Alveolar semivowels sil sp ao ah aa er oy aw hh uw uh ow ae eh ey ay ih iy ax l el r y Alveolar fricatives Alveolar Palato alveolar Bilabial Dental Labio dental Velar s z t d n en sh zh ch jh p b m th dh f v ng k g w Table 2 The 44 phoneme to 13 viseme mapping considered by Neti et al 2000 using the HTK phone set Young et al 1999 sequence most often consist of sub phonetic units and occasionly of sub word units in small vocabulary recognition tasks For LVCSR a large number of context dependent sub phonetic units are used obtained by clustering the possible phonetic contexts tri phone ones for example by means of a decision tree Deller et al 1993 Rabiner and Juang 1993 Young et al 1999 In this chapter such units are exclusively used defined over tri or eleven phone contexts as described in the Experiments section For automatic speechreading it seems appropriate from the human visual speech perception point of view to use visemic sub phonetic classes and their decision tree clustering based on visemic context Such clustering experiments are reported by Neti et al 2000 In addition visual only recognition of visemes is occasionly considered in the literature Potamianos et al 2001b Visemic speech classes are also used for audio visual ASR at the second stage of a cascade decision fusion architecture proposed by Rogozan 1999 However the use of different classes for its audio and visual only components complicates audio visual fusion with unclear performance gains Therefore in the remainder of this section identical classes and decision trees are being used for both modalities HMM Based Speech Recognition The most widely used classifier for audio visual ASR is the hidden Markov model HMM a very popular method for traditional audio only speech recognition Deller et al 1993 Rabiner and Juang 1993 Young et al 1999 Additional methods also exist for automatic recognition of speech and have been employed in audio visual ASR systems such as dynamic time warping DTW used for example by Petajan 1984 artificial neural networks ANN as in Krone et al 1997 hybrid ANN DTW systems Bregler et al 1993 Duchnowski et al 1994 or hybrid ANN HMM ones Heckmann et al 2001 Various types of HMMs have also been used for audio visual ASR such as HMMs with discrete observations after vector quantization of the feature space Silsbee and Bovik 1996 or HMMs with non Gaussian continuous observation probabilities Su and Silsbee 1996 However the vast majority of audio visual ASR systems and to which we restrict the presentation in this chapter employ HMMs with a continuous observation probability density modeled as a mixture of Gaussian densities Typically in the literature single stream HMMs are used to model the generation of a sequence of audioonly or visual only speech informative features f ts g of dimensionality Ds where s A V denotes the audio or visual modality stream The HMM emission class conditional observation probabilities are modeled by Gaussian mixture densities given by o Pr ot j c s Xw Ks c k 1 sck ND ot ms c k ss c k s s 6 c 2 C whereas the HMM transition probabilities between the various classes are given by rs fPr c0 j c00 c0 c002C g The HMM parameter vector is therefore as rs bs where bs f ws k ms k ss k k 1 Ks c 2C g 7 In 6 and 7 c 2 C denote the HMM context dependent states whereas mixture weights ws k are positive for all classes c c c c c Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 AV Enh AUDIO AUDIO FEATURE EXTRACTION 1 1 o A t AV Concat 1 ENH 1 101 1 1 P ENH AV MSE 60 60 t 60 60 o AEnh t o AClean t VIDEO t 1 VISUAL FEATURE EXTRACTION 1 ot 41 V AV HiLDA 101 ot AV LDA 1 101 1 MLLT 1 1 60 1 P LDA AV 60 60 P MLLT AV 60 60 o HiLDA t Figure 8 Three types of feature fusion considered in this section Plain audio visual feature concatenation AV Concat hierarchical discriminant feature extraction AV HiLDA and audio visual speech enhancement AV Enh adding to one Ks c denotes the number of mixtures and ND is the D variate normal distribution and a diagonal covariance matrix its diagonal being denoted by with mean m oms s The expectation maximization EM algorithm Dempster et al 1977 is typically used to obtain maximum j likelihood estimates of 7 Given a current HMM parameter vector at EM algorithm iteration j s a re estimated parameter vector is obtained as a as 1 arg max a Q as a j O j j s s In 8 s denotes training data observations from L utterances l l 1 L and Q the EM algorithm auxiliary function defined as Rabiner and Juang 1993 8 O O j represents 9 Q a0 a00 j O s X X Pr O clj a0 L s l l 1 cl 00 log Pr O l clj a s In 9 l denotes any HMM state sequence for utterance l Replacing it with the best HMM path reduces EM to Viterbi training Deller et al 1993 As an alternative to maximum likelihood discriminative training methods can instead be used for HMM parameter estimation Bahl et al 1986 Chou et al 1994 c Feature Fusion Techniques for Audio Visual ASR As already mentioned feature fusion uses a single classifier to model the concatenated vector of timesynchronous audio and visual features or appropriate transformations of it Such methods include plain feature concatenation Adjoudani and Beno it 1996 feature weighting Teissier et al 1999 Chen 2001 both also known as direct identification fusion Teissier et al 1999 and hierarchical linear discriminant feature extraction Potamianos et al 2001a The dominant and motor recording fusion models discussed by Teissier et al 1999 also belong to this category as they seek a data to data mapping of either the visual features into the audio space or of both modality features to a new common space followed by linear combination of the resulting features Audio feature enhancement on basis of either visual input Girin et al 1995 Barker and Berthommier 1999 or concatenated audio visual features Girin et al 2001b Goecke et al 2002 falls also within this category of fusion under its general definition adopted above In this section we expand on three feature fusion techniques schematically depicted in Figure 8 Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Concatenative Feature Fusion V Given time synchronous audio and visual feature vectors A t and t with dimensionalities DA and D V respectively the joint concatenated audio visual feature vector at time t becomes V oAV oA t t ot o o 2 RD 10 where D DA D V As with all feature fusion methods i e also for vectors 11 and 12 below the generation process of a sequence of features 10 is modeled by a single stream HMM with emission probabilities see also 6 Pr oAV j c t Xw Kc k 1 ck ND oAV t mck sck for all classes c 2 C Adjoudani and Beno it 1996 Concatenative feature fusion constitutes a simple approach for audio visual ASR implementable in most existing ASR systems with minor changes However the dimensionality of 10 can be rather high causing inadequate modeling in 6 due to the curse of dimensionality Chatfield and Collins 1991 The following fusion technique aims to avoid this by seeking lower dimensional representations of 10 Hierarchical Discriminant Feature Fusion The visual features contain less speech classification power than audio features even in the case of extreme noise in the audio channel see Table 4 in the Experiments section One would therefore expect that an appropriate lower dimensional representation of 10 could lead to equal and possibly better HMM performance given the problem of accurate probabilistic modeling in high dimensional spaces Potamianos et al 2001a have considered LDA as a means of obtaining such a dimensionality reduction Indeed the goal being to obtain the best discrimination among the classes of interest LDA achieves this on basis of the data and their labels alone without a priori bias in favor of any of the two feature streams LDA is subsequently followed by an MLLT based data rotation see also Figure 8 in order to improve maximum likelihood data modeling using 6 In the audio visual ASR system of Potamianos et al 2001a the proposed method amounts to a two stage application of LDA and MLLT first intra modal on the original audio MFCC and visual DCT features and then inter modal on 10 as also depicted in Figure 11 It is therefore referred to as HiLDA hierarchical LDA The final audio visual feature vector is see also 10 AV PAV oAV oHiLDA PMLLT LDA t t 11 One can set the dimensionality of 11 to be equal to the audio feature vector size as implemented by Neti et al 2000 Audio Feature Enhancement Audio and visible speech are correlated since they are produced by the same oral facial cavity Not surprisingly a number of techniques have been proposed to obtain estimates of audio features utilizing the visual only modality Girin et al 1995 Yehia et al 1998 Barker and Berthommier 1999 or joint audiovisual speech data in the case where the audio signal is degraded Girin et al 2001b Goecke et al 2002 The latter scenario corresponds to the speech enhancement paradigm Under this approach the enhanced auAEnh can be simply obtained as a linear transformation of the concatenated audio visual dio feature vector t feature vector 10 namely as o AV oAV oAEnh PENH t t AV AV AV AV consists of D dimensional row vectors pAV where matrix PENH p p pDA i 1 DA and has dimension DA D see also Figure 8 1 2 12 for i Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 AV is by considering the approximation AEnh AClean in the A simple way to estimate matrix ENH t t denotes clean audio features available in addition to visual Euclidean distance sense where vector AClean t and noisy audio vectors for a number of time instants t in a training set T Due to 12 this becomes equivalent to solving DA mean square error MSE estimations P o o o pAV arg min i p P AV Equations 13 result to D systems of Yulefor i 1 DA i e one per row of the matrix ENH A Walker equations that can be easily solved using Gauss Jordan elimination Press et al 1995 A more AV by using a Mahalanobis type distance instead of 13 is considered sophisticated way of estimating ENH by Goecke et al 2002 whereas non linear estimation schemes are proposed by Girin et al 2001b and Deligne et al 2002 t2T Xo P AClean t i p oAV t 2 13 Decision Fusion Techniques for Audio Visual ASR Although feature fusion techniques for example HiLDA have been documented to result in improved ASR over audio only performance Neti et al 2000 they cannot explicitly model the reliability of each modality Such modeling is extremely important as speech information content and discrimination power of the audio and visual streams can vary widely depending on the spoken utterance acoustic noise in the environment visual channel degradations face tracker inaccuracies and speaker characteristics In contrast to feature fusion methods the decision fusion framework provides a mechanism for capturing the reliability of each modality by borrowing from classifier combination literature Classifier combination based on their individual decisions about the classes of interest is an active area of research with many applications Xu et al 1992 Kittler et al 1998 Jain et al 2000 Combination strategies differ in various aspects such as the architecture used parallel cascade or hierarchical combination possible trainability static or adaptive and information level considered at integration abstract rank order or measurement level i e whether information is available about the best class only the top n classes or the ranking of all possible ones or the scores likelihoods of them In the audio visual ASR literature examples of most of these categories can be found For example Petajan 1984 rescores the two best outputs of the audio only classifier by means of the visual only classifier a case of cascade static rank order level decision fusion Combinations of more than one categories as well as cases where the one of the two classifiers of interest corresponds to a feature fusion technique are also possible For example Rogozan and Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 asynchrony between the two HMMs and c Intermediate integration typically implemented by means of the product HMM Varga and Moore 1990 or the coupled HMM Brand et al 1997 which force HMM synchrony at the phone or word boundaries Notice that such terminology is not universally agreed upon and our reference to early or late integration at the temporal level should not be confused with the feature vs decision fusion meaning of these terms in other work Adjoudani and Beno it 1996 Early Integration The State Synchronous Multi Stream HMM In its general form the class conditional observation likelihood of the multi stream HMM is the product of the observation likelihoods of its single stream components raised to appropriate stream exponents that capture the reliability of each modality or equivalently the confidence of each single stream classifier Such model has been considered in audio only ASR where for example separate streams are used for the energy audio features MFCC static features as well as their first and possibly second order derivatives as in Hernando et al 1995 and Young et al 1999 or for band limited audio features in the multi band ASR paradigm Hermansky et al 1996 as in Bourlard and Dupont 1996 Okawa et al 1999 and Glotin and Berthommier 2000 among others In the audio visual domain the model becomes a two stream HMM with one stream devoted to the audio and another to the visual modality As such it has been extensively used in small vocabulary audio visual ASR tasks Jourlin 1997 Potamianos and Graf 1998 Dupont and Luettin 2000 Miyajima et al 2000 Nakamura et al 2000 In the system reported by Neti et al 2000 and Luettin et al 2001 the method was applied for the first time to the LVCSR domain Given the bimodal audio visual observation vector AV t the state emission score it no longer represents a probability distribution of the multi stream HMM is see also 6 and 10 Pr oAV t jc Notice that 14 corresponds to a linear combination in the log likelihood domain In 14 s c t denote the stream exponents weights that are non negative and in general are a function of the modality s the HMM state c 2 C and locally the utterance frame time t Such state and time dependence can be used to model the speech class and local environment based reliability of each stream The exponents are often constrained to A c t V c t 1 or 2 In most systems they are set to global modality only dependent values i e s s c t for all classes c 2C and time instants t with the class dependence occasionly being preserved i e s c s c t for all t In the latter case the parameters of the multi stream HMM are see also 6 7 and 14 s2fA Vg k 1 Y Xw Ks c o s c k Ds t N o m s k s s k s c c sct 14 aAV aAV f A V c 2C g where aAV r bA bV c c 15 consists of the HMM transition probabilities single stream components r and the emission probability parameters bA and bV of its The parameters of AV can be estimated separately for each stream component using the EM algorithm namely 8 for s 2 fA Vg and subsequently by setting the joint HMM transition probability vector equal to the audio one i e A or alternatively to the product of the transition probabilities of the two HMMs i e diag A V see also 7 The latter scheme is referred to in the Experiments section as AV MSSep An obvious drawback of this approach is that the two single modality HMMs are trained asynchronously i e using different forced alignments whereas 14 assumes that the HMM stream components are state synchronous The alternative is to jointly estimate parameters AV in order to enforce state synchrony Due to the linear combination of stream log likelihoods in 14 the EM algorithm carries on in the multi stream HMM case with minor changes Rabiner and Juang 1993 Young et al 1999 As a result r a r r r r a 1 arg max Q a a j OAV aAV AV a j j 16 can be used a scheme referred to as AV MS Joint Notice that the two approaches basically differ in the E step of the EM algorithm Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 AUDIO HMM STATES VISUAL HMM STATES COMPOSITE HMM STATES Figure 9 Left Phone synchronous state asynchronous multi stream HMM with three states per phone and modality Right Its equivalent product composite HMM black circles denote states that are removed when limiting the degree of within phone allowed asynchrony to one state The single stream emission probabilities are tied for states along the same row column to the corresponding audio visual state ones In both separate and joint HMM training the remainder of parameter vector AV consisting of the stream exponents needs to be obtained Maximum likelihood estimation cannot be used for such parameters and discriminative training techniques have to be employed instead Jourlin 1997 Potamianos and Graf 1998 Nakamura 2001 Gravier et al 2002a The issue is discussed later Notice that HMM stream parameter and stream exponent training iterations can be alternated in 16 a Intermediate Integration The Product HMM It is well known that visual speech activity precedes the audio signal by as much as 120 ms Bregler and Konig 1994 Grant and Greenberg 2001 which is close to the average duration of a phoneme A generalization of the state synchronous multi stream HMM can be used to model such audio and visual stream asynchrony to some extent by allowing the single modality HMMs to be in asynchrony within a model but forcing their synchrony at model boundaries instead Single stream log likelihoods are linearly combined at such boundaries using weights similarly to 14 For LVCSR a reasonable choice for forcing synchrony constitute the phone boundaries The resulting phone synchronous audio visual HMM is depicted in Figure 9 for the typical case of three states used per phone and modality Recognition based on this intermediate integration method requires the computation of the best state sequences for both audio and visual streams To simplify decoding the model can be formulated as a product HMM Varga and Moore 1990 Such model consists of composite states 2 CC that have audio visual emission probabilities of a form similar to 14 namely Pr oAVj c t Y Xw Ks cs s2fA Vg k 1 c s cs k ND ot m s c k s s c k s s s s s cs t 17 where cA cV Notice that in 17 the audio and visual stream components correspond to the emission probabilities of certain audio and visual only HMM states as depicted in Figure 9 These single stream emission probabilities are tied for states along the same row or column depending on the modality therefore the original number of mixture weight mean and variance parameters is kept in the new model However this is usually not the case with the number of transition probability parameters fPr 0 j 00 0 002CC g as additional transitions between the composite states need to be modeled Such probabilities are often 00 Pr c0V j 00 in which case the resulting product HMM is typically factored as Pr 0 j 00 Pr c0 Aj referred to in the literature as the coupled HMM Brand et al 1997 Chu and Huang 2000 2002 Nefian et al 2002 A further simplification of this factorization is sometimes employed namely Pr 0 j 00 0 00 Pr c0A j c00 A Pr cV j cV as in Gravier et al 2002b for example which results in a product HMM with the same number of parameters as the state synchronous multi stream HMM c c c c c c c c c c c Given audio visual training data product HMM training can be performed similarly to separate or joint multi stream HMM parameter estimation discussed in the previous subsection In the first case the composite model is constructed based on individual single modality HMMs estimated by 8 and on transition Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 probabilities equal to the product of the audio and visual only ones In the second case referred to as AV MSPROD in the experiments reported later all transition probabilities and HMM stream component parameters are estimated at a single stage using 16 with appropriate parameter tying In both schemes stream exponents need to be estimated separately In the audio visual ASR literature product or coupled HMMs have been considered in some small vocabulary recognition tasks Tomlinson et al 1996 Dupont and Luettin 2000 Huang and Chen 2001 Nakamura 2001 Chu and Huang 2002 Nefian et al 2002 where synchronization is sometimes enforced at the word level and recently for LVCSR Neti et al 2000 Luettin et al 2001 Gravier et al 2002b It is worth mentioning that the product HMM allows the restriction of the degree of asynchrony between the two streams by excluding certain composite states in the model topology In the extreme case when only the states that lie in its diagonal are kept the model becomes equivalent to the state synchronous multi stream HMM see also Figure 9 Late Integration Discriminative Model Combination A popular stage of combining audio and visual only recognition log likelihoods is at the utterance end giving rise to late intergration In small vocabulary isolated word speech recognition this can be easily implemented by calculating the combined likelihood for each word model in the vocabulary given the acoustic and visual observations Adjoudani and Beno it 1996 Su and Silsbee 1996 Cox et al 1997 Gurbuz et al 2001 However for connected word recognition and even more so for LVCSR the number of possible hypotheses of word sequences becomes prohibitively large Instead one has to limit the log likelihood combination to the top n best only hypotheses Such hypotheses can be generated by the audio only HMM an alternative audio visual fusion technique or can be the union of audio only and visual only n best lists In this approach the list of n best hypotheses for a particular utterance f 1 2 n g are first forcedaligned to their corresponding phone sequences i fci 1 ci 2 ci Ni g by means of both audio and start end ti j s for s 2 fA Vg visual only HMMs Let the resulting phone c i j boundaries be denoted by ti j s j 1 Ni and i 1 n Then the audio visual likelihoods of the n best hypotheses are computed as h h h h Pr h i Pr LM h i LM where Pr LM i denotes the language model LM probability of hypothesis i The exponents in 18 can be estimated using discriminative training criteria as in the discriminative model combination method of Beyerlein 1998 and Vergyri 2000 The method is proposed for audio visual LVCSR in Neti et al 2000 and it is referred to as AV DMC in the Experiments section h s2fA Vg j 1 Y Y Pr o t 2 t Ni s t i j s ti j s start end j ci j s c i j 18 h Stream Exponent Estimation and Reliability Modeling We now address the issue of estimating stream exponents weights when combining likelihoods in the audio visual decision fusion techniques presented above see 14 17 and 18 As already discussed such exponents can be set to constant values computed for a particular audio visual environment and database In this case the audio visual weights depend on the modality and possibly on the speech class capturing the confidence of the individual classifiers for the particular database conditions and are estimated by seeking optimal system performance on matched data However in a practical audio visual ASR system the quality of captured audio and visual data and thus the speech information present in them can change dramatically over time To model this variability utterance level or even frame level dependence of the stream exponents is required This can be achieved by first obtaining an estimate of the local environment conditions and then using pre computed exponents for this condition or alternatively by seeking a direct functional mapping between environment estimates and stream exponents In the following we expand on these methodologies In the first approach constant exponents are estimated based on training data or more often on held out data Such stream exponents cannot be obtained by maximum likelihood estimation Potamianos and Graf 1998 Nakamura 2001 Instead discriminative training techniques have to be used Some of these methods Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 seek to minimize a smooth function of the minimum classification error MCE of the resulting audio visual model on the data and employ the generalized probabilistic descent GPD algorithm Chou et al 1994 for stream exponent estimation Potamianos and Graf 1998 Miyajima et al 2000 Nakamura et al 2000 Gravier et al 2002a Other techniques use maximum mutual information MMI training Bahl et al 1986 such as the system reported by Jourlin 1997 Alternatively one can seek to directly minimize the word error rate of the resulting audio visual ASR system on a held out data set In the case of global exponents across all speech classes constrained to add to a constant the problem reduces to one dimensional optimization of a non smooth function and can be solved using simple grid search Miyajima et al 2000 Luettin et al 2001 Gravier et al 2002a For class dependent weights the problem becomes of higher dimension and the downhill simplex method Nelder and Mead 1965 can be employed This technique is used by Neti et al 2000 to estimate exponents for late decision fusion using 18 A different approach is to minimize frame misclassification rate by using the maximum entropy criterion Gravier et al 2002a In order to capture the effects of varying audio and visual environment conditions to the reliability of each stream utterance level and occasionly frame level dependence of the stream weights needs to be considered In most cases in the literature exponents are considered as a function of the audio channel signal to noise ratio SNR and each utterance is decoded based on the fusion model parameters at its SNR Adjoudani and Beno it 1996 Meier et al 1996 Cox et al 1997 Teissier et al 1999 Gurbuz et al 2001 This SNR value is either assumed known or estimated from the audio channel Cox et al 1997 A linear dependence between SNR and audio stream weight has been demonstrated by Meier et al 1996 An alternative technique sets the stream exponents to a linear function of the average conditional entropy of the recognizer output computed using the confusion matrix at a particular SNR for a small vocabulary isolated word ASR task Cox et al 1997 A different approach considers the audio stream exponent as a function of the degree of voicing present in the audio channel estimated as in Berthommier and Glotin 1999 The method was used at the Johns Hopkins summer 2000 workshop Neti et al 2000 Glotin et al 2001 and is referred to in the Experiments section as AV MS UTTER The above techniques do not allow modeling of possible variations in the visual stream reliability since they concentrate on the audio stream alone Modeling such variability in the visual signal domain is challenging and instead it can be achieved using confidence measures of the resulting visual only classifier For example Adjoudani and Beno it 1996 and Rogozan et al 1997 use the dispersion of both audio only and visualonly class posterior log likelihoods to model the single stream classifier confidences and then compute the utterance dependent stream exponents as a closed form function of these dispersions Similarly Potamianos and Neti 2000 consider various confidence measures such as entropy and dispersion to capture the reliability of audio and visual only classification at the frame level and estimate stream exponents on basis of held out data Such exponents are held constant within confidence value intervals Audio Visual Speaker Adaptation Speaker adaptation is traditionally used in practical audio only ASR systems to improve speaker independent system performance when little data from a speaker of interest are available Gauvain and Lee 1994 Leggetter and Woodland 1995 Neumeyer et al 1995 Anastasakos et al 1997 Gales 1999 Adaptation is also of interest across tasks or environments In the audio visual ASR domain adaptation is of great importance since audio visual corpora are scarce and their collection expensive Given few bimodal adaptation data from a particular speaker and a baseline speaker independent HMM one wishes to estimate adapted HMM parameters that better model the audio visual observations of the particular speaker Two popular algorithms for speaker adaptation are maximum likelihood linear regression MLLR Leggetter and Woodland 1995 and maximum a posteriori MAP adaptation Gauvain and Lee 1994 MLLR obtains a maximum likelihood estimate of a linear transformation of the HMM means while leaving covariance matrices mixture weights and transition probabilities unchanged and it provides successful adaptation with a small amount of adaptation data rapid adaptation On the other hand MAP follows the Bayesian paradigm for estimating the HMM parameters MAP estimates of HMM parameters slowly converge to their EM obtained estimates as the amount of adaptation data becomes large however such a convergence is slow and therefore MAP is not suitable for rapid adaptation In practice MAP is often used Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 in conjunction with MLLR Neumeyer et al 1995 Both techniques can be used in feature fusion Potamianos and Neti 2001a and decision fusion models discussed above Potamianos and Potamianos 1999 in a straightforward manner One can also consider feature level front end adaptation by adapting for example the audio only and visual only LDA and MLLT matrices and in case HiLDA fusion is used the joint audio visual LDA and MLLT matrices Potamianos and Neti 2001a Experiments using these techniques are reported in a later section Alternative adaptation algorithms also exist such as speaker adaptive training Anastasakos et al 1997 and front end MLLR Gales 1999 and can be used in audio visual ASR Vanegas et al 1998 Summary on Audio Visual Integration We have presented a summary of the most common fusion techniques for audio visual ASR We first discussed the choice of speech classes and statistical ASR models that influence the design of some fusion algorithms Subsequently we described a number of feature and decision integration techniques suitable for bimodal LVCSR and finally briefly touched upon the issue of audio visual speaker adaptation Among the fusion algorithms discussed decision fusion techniques explicitly model the reliability of each source of speech information by using stream weights to linearly combine audio and visual only classifier log likelihoods When properly estimated the use of weights results in improved ASR over feature fusion techniques as reported in the literature and demonstrated in the Experiments section Potamianos and Graf 1998 Neti et al 2000 Luettin et al 2001 In most systems reported such weights are set to a constant value over each modality possibly dependent on the audio only channel quality SNR However robust estimation of the weights at a finer level utterance or frame level on basis of both audio and visual channel characteristics has not been sufficiently addressed Furthermore the issue of whether speech class dependence of stream weights is desirable has also not been fully investigated Although such dependence seems to help in late integration schemes Neti et al 2000 or small vocabulary tasks Jourlin 1997 Miyajima et al 2000 the problem remains unresolved for early integration in LVCSR Gravier et al 2002a There are additional open questions relevant to decision fusion The first concerns the stage of measurement level information integration i e the degree of allowed asynchrony between the audio and visual streams The second has to do with the functional form of stream log likelihood combination as integration by means of 14 is not necessarily optimal and it fails to yield an emission probability distribution Finally it is worth mentioning a theoretical shortcoming of the log likelihood linear combination model used in the decision fusion algorithms considered In contrast to feature fusion such combination assumes class conditional independence of the audio and visual stream observations This appears to be a non realistic assumption Yehia et al 1998 A number of models are being investigated to overcome this drawback Pavlovic 1998 Pan et al 1998 AUDIO VISUAL DATABASES A major contributor to the progress achieved in traditional audio only ASR has been the availability of a wide variety of large multi subject databases on a number of well defined recognition tasks of different complexities These corpora have often been collected using funding from U S government agencies for example the Defense Advanced Research Projects Agency and the National Science Foundation or through wellorganized European activities such as the Information System Technology program funded by the European Commission or the European Language Resources Association The resulting databases are available to the interested research groups by the Linguistic Data Consortium LDC or the European Language resources Distribution Agency ELDA for example Benchmarking research progress in audio only ASR has been possible on such common databases In contrast to the abundance of audio only corpora there exist only few databases suitable for audio visual ASR research This is because the field is relatively young but also due to the fact that audio visual databases pose additional challenges concerning database collection storage and distribution not found in the audio Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 only domain For example computer acquisition of visual data at full size frame rate and high image quality synchronous to the audio input requires expensive hardware whereas even highly compressed visual data storage consumes at least an order of magnitude more storage space than audio making widespread database distribution a non trivial task Although solutions have been steadily improving and becoming available at a lower cost these issues have seriously hindered availability of large audio visual corpora Additional difficulties stem from the proprietary nature of some collected corpora as well as privacy issues due to the inclusion of the visual modality Most existing audio visual databases are the result of efforts by few university groups or individual researchers with limited resources Therefore most of these corpora suffer from one or more shortcomings Chibelushi et al 1996 2002 Hennecke et al 1996 They contain a single or small number of subjects affecting the generalizability of developed methods to the wider population they typically have small duration often resulting in undertrained statistical models or non significant performance differences between various proposed algorithms and finally they mostly address simple recognition tasks such as small vocabulary ASR of isolated or connected words These limitations have caused a growing gap in the state of the art between audio only and audio visual ASR in terms of recognition task complexity To help bridge this gap we have recently completed the collection of a large corpus suitable for audio visual LVCSR which we used for experiments during the Johns Hopkins summer 2000 workshop Neti et al 2000 Some of these experiments are summarized in the following section In the remainder of this section we give an overview of the most commonly used audio visual databases in the literature Some of these sets have been used by multiple sites and researchers allowing some algorithm comparisons However benchmarking on common corpora is not widespread Subsequently we describe the IBM ViaVoiceTM audio visual database and additional corpora used in the experiments reported in the next section Overview of Small and Medium Vocabulary Audio Visual Corpora The first database used for automatic recognition of audio visual speech was collected by Petajan 1984 Data of a single subject uttering 2 10 repetitions of 100 isolated English words including letters and digits were collected under controlled lighting conditions Since then several research sites have pursued audiovisual data collection Some of the resulting corpora are discussed in the following A number of databases are designed to study audio visual recognition of consonants C vowels V or transitions between them For example Adjoudani and Beno it 1996 report a single speaker corpus of 54 V1CV2 CV1 non sense words three French vowels and six consonants are considered Su and Silsbee 1996 recorded a single speaker corpus of aCa non sense words for recognition of 22 English consonants Robert Ribes et al 1998 as well as Teissier et al 1999 report recognition of ten French oral vowels uttered by a single subject Czap 2000 considers a single subject corpus of V1CV1 and C1VC1 non sense words for recognition of Hungarian vowels and consonants The most popular task for audio visual ASR is isolated or connected digit recognition Various corpora allow digit recognition experiments For example the Tulips1 database Movellan and Chadderdon 1996 contains recordings of 12 subjects uttering digits one to four and has been used for isolated recognition of these four digits in a number of papers Luettin et al 1996 Movellan and Chadderdon 1996 Gray et al 1997 Vanegas et al 1998 Scanlon and Reilly 2001 The M2VTS database although tailored to speaker verification applications also contains digit 0 to 9 recordings of 37 subjects mostly in French Pigeon and Vandendorpe 1997 and it has been used for isolated digit recognition experiments Dupont and Luettin 2000 Miyajima et al 2000 XM2VTS an extended version of this database containing 295 subjects has recently been completed in the English language Messer et al 1999 Additional single subject digit databases include the NATO RSG10 digit triples set used by Tomlinson et al 1996 for isolated digit recognition and two connected digits databases reported by Potamianos et al 1998 and Heckmann et al 2001 Finally two very recent databases suitable for multi subject connected digit recognition have been collected at the University of Illinois at Urbana Champaign a 100 subject set with results reported in Chu and Huang 2000 and Zhang et al 2000 and at Clemson University the 36 subject CUAVE dataset Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Figure 10 Example video frames of ten subjects from the IBM ViaVoiceTM audio visual database The database contains approximately 50 hrs of continuous dictation style audio visual speech by 290 subjects collected with minor face pose lighting and background variation Neti et al 2000 as discussed in Patterson et al 2002 Isolated or connected letter recognition constitutes another popular audio visual ASR task German connected letter recognition on data of up to six subjects has been reported by Bregler et al 1993 Bregler and Konig 1994 Duchnowski et al 1994 and Meier et al 1996 whereas Krone et al 1997 work on singlespeaker isolated German letter recognition Single or two subject connected French letter recognition is considered in Alissali et al 1996 The IBM ViaVoiceTM Audio Visual Database To date the largest audio visual database collected and the only one suitable for speaker independent LVCSR is the IBM ViaVoiceTM audio visual database The corpus consists of full face frontal video and audio of 290 subjects see also Figure 10 uttering ViaVoiceTM training scripts i e continuous read speech with mostly verbalized punctuation dictation style The database video is of a 704480 pixel size interlaced captured in color at a rate of 30 Hz i e 60 fields per second are available at a resolution of 240 lines and it is MPEG2 encoded at the relatively high compression ratio of about 50 1 High quality wideband audio is synchronously collected with the video at a rate of 16 kHz and at a relatively clean audio environment quiet office with some background computer noise resulting in a 19 5 dB SNR The duration of the entire database is approximately 50 hours and it contains 24 325 transcribed utterances with a 10 403 word vocabulary from which 21 281 utterances are used in the experiments reported in the next section In addition to Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 AUDIO 25 msec MFCC EXTRA CTION 1 24 t FEATURE MEAN NORMALIZATION t 1 24 1 1 216 LDA JA 9 216 1 60 1 1 60 MLLT 1 60 1 t 10 msec shift 100 Hz t 1 64 1 64 4096 1 1 E P LDA A 60 P MLLT A o A t 60 VIDEO 4096 t 1 100 1 1 101 DCT 1 P DCT V o AV t LDA 1 60 1 1 60 AV MLLT 1 60 1 100 P LDA 101 AV 60 P MLLT 60 ROI EXTRACTION PROCESSING AT 60 Hz o HiLDA t t INTERPOLATION FROM 60 TO 100 Hz 1 FEATURE MEAN NORMALIZATION 100 V DCT 1 100 1 100 LDA 1 30 1 1 30 30 V DCT t 1 1 30 1 1 690 MLLT P LDA P MLLT LDA 30 1 41 1 1 41 V 41 MLLT 1 41 E JV 23 P LDA 690 V 1 o V t 41 P MLLT Figure 11 The audio visual ASR system employed in some of the experiments reported in this chapter In addition to the baseline system used during the Johns Hopkins summer 2000 workshop a larger mouth ROI is extracted within frame discriminant features are used and a longer temporal window is considered in the visual front end compare to Figure 7 HiLDA feature fusion is employed LVCSR a 50 subject connected digit database has been collected at IBM in order to study the visual modality benefit to a popular small vocabulary ASR task This DIGITS corpus contains 6689 utterances of 7 and 10 digit strings both zero and oh are used with a total duration of approximately 10 hrs Furthermore to allow investigation of automatic speechreading performance for impaired speech Potamianos and Neti 2001a both LVCSR and DIGITS audio visual speech data of a single speech impaired male subject with profound hearing loss have been collected In Table 3 a summary of the above corpora is given together with their partitioning used in the experiments reported in the following section AUDIO VISUAL ASR EXPERIMENTS In this section we present experimental results on visual only and audio visual ASR using mainly the IBM ViaVoiceTM database discussed above Some of these results have been obtained during the Johns Hopkins summer 2000 workshop Neti et al 2000 Experiments conducted later on both these data as well as on the IBM connected digits task DIGITS are also reported Potamianos et al 2001a Goecke et al 2002 Gravier et al 2002a In addition the application of audio visual speaker adaptation methods on the hearing impaired dataset is also discussed Potamianos and Neti 2001a First however we briefly describe the basic audio visual ASR system as well as the experimental framework used The Audio Visual ASR System Our basic audio visual ASR system utilizes appearance based visual features that use a discrete cosine transform DCT of the mouth region of interest ROI as described in Potamianos et al 2001b Given the video of the speaker s face available at 60 Hz it first performs face detection and mouth center and size estimation employing the algorithm of Senior 1999 and on basis of these it extracts a size normalized 64 64 greyscale pixel mouth ROI as discussed in a previous section see also Figure 2 Subsequently a two dimensional separable fast DCT is applied on the ROI and its 24 highest energy coefficients over the training data are retained A number of post processing steps are applied on the resulting static feature Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Speech Recognition Training set Held out set Adaptation set Test set condition task Utter Dur Sub Utter Dur Sub Utter Dur Sub Utter Dur Sub Normal LVCSR 17111 34 55 239 2277 4 47 25 855 2 03 26 1038 2 29 26 DIGITS 5490 8 01 50 670 0 58 50 670 0 58 50 529 0 46 50 Impaired LVCSR N A N A 50 0 11 1 50 0 11 1 DIGITS N A N A 80 0 08 1 60 0 06 1 Table 3 The IBM audio visual databases discussed and used in the experiments reported in this chapter Their partitioning into training held out adaptation and test sets is depicted number of utterances duration in hours and number of subjects are shown for each set Both large vocabulary continuous speech LVCSR and connected digit DIGITS recognition are considered for normal as well as impaired speech The IBM ViaVoiceTM database corresponds to the LVCSR task in the normal speech condition For the normal speech DIGITS task the held out and adaptation sets are identical For impaired speech due to the lack of sufficient training data adaptation of HMMs trained in the normal speech condition is considered vector namely linear interpolation to the audio feature rate from 60 to 100 Hz feature mean normalization FMN for improved robustness to lighting and other variations concatenation of 15 adjacent features to capture dynamic speech information see also 5 and linear discriminant analysis LDA for optimal dimensionality reduction followed by a maximum likelihood data rotation MLLT for improved statistical data modeling The resulting feature vector V t has dimension 41 These steps are described in more detail in the Visual front end section of this chapter see also Figure 7 Improvements to this DCT based visual front end have been proposed in Potamianos and Neti 2001b including the use of a larger ROI a withinframe discriminant DCT feature selection and a longer temporal window see Figure 11 During the Johns Hopkins summer workshop and in addition to the DCT based features joint appearance and shape features by means of active appearance models AAMs have also been employed In particular 6000 dimensional appearance vectors containing the normalized face color pixel values and 134 dimensional shape vectors of the face shape coordinates are extracted at 30 Hz and are passed through two stages of principal components analysis PCA The resulting static AAM feature vector is 86 dimensional and it is post processed similarly to the DCT feature vector see Figure 7 resulting to 41 dimensional dynamic features o In parallel to the visual front end traditional audio features are extracted at a 100 Hz rate that consist of mel frequency cepstral coefficients MFCCs and its energy Rabiner and Juang 1993 Deller et al 1993 Young et al 1999 The obtained static feature vector is 24 dimensional and following FMN LDA on 9 adjacent frames and MLLT it gives rise to a 60 dimensional dynamic speech vector A t as depicted in Figure 11 The audio and visual front ends provide time synchronous audio and visual feature vectors that can be used in a number of fusion techniques discussed in a previous section The derived concatenated audio visual vector AV has dimension 101 whereas in the HiLDA feature fusion implementation the bimodal LDA generates t with a reduced dimensionality 60 see also Figure 11 features HiLDA t o o o In all cases where LDA and MLLT matrices are employed audio visual only and audio visual feature extraction by means of HiLDA fusion we consider jCj 3367 context dependent sub phonetic classes that coincide with the context dependent states of an available audio only HMM that has been previously developed at IBM for LVCSR trained on a number of audio corpora Polymenakos et al 1998 The forced alignment Rabiner and Juang 1993 of the training set audio based on this HMM and the data transcriptions produces labels cl 2 C for the training set audio visual and audio visual data vectors l l 1 L Such labeled vectors can then be used to estimate the required matrices LDA MLLT as described in the Visual front end section of this chapter P P x The Experimental Framework The audio visual databases discussed above have been partitioned into a number of sets in order to train and evaluate models for audio visual ASR as detailed in Table 3 For both LVCSR and DIGITS speech tasks in Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Modality Visual Remarks DCT DWT PCA AAM WER 58 1 58 8 59 4 64 0 Modality Acoustic None Remarks WER MFCC noisy 55 0 Oracle 31 2 Anti oracle 102 6 LM best path 62 0 Table 4 Comparisons of various visual features three appearance based features and one joint shape and appearance feature representation for speaker independent LVCSR Neti et al 2000 Matthews et al 2001 Word error rate WER is depicted on a subset of the IBM ViaVoiceTM database test set of Table 3 Visual performance is obtained after rescoring of lattices that have been previously generated based on noisy at 8 5 dB SNR audio only MFCC features For comparison characteristic lattice WERs are also depicted oracle anti oracle and best path based on language model scores alone Among the visual speech representations considered the DCT based features are superior and contain significant speech information the normal speech condition the corresponding training sets are used to obtain all LDA and MLLT matrices required the phonetic decision trees that cluster HMM states on basis of phonetic context as well as to train all HMMs reported The held out sets are used to tune parameters relevant to audio visual decision fusion and decoding such as the multi stream HMM and language model weights for example whereas the test sets are used for evaluating the performance of the trained HMMs Optionally the adaptation sets can be employed for tuning the front ends and or HMMs to the characteristics of the test set subjects In the LVCSR case the subject populations of the training held out and test sets are disjoint thus allowing for speaker independent recognition whereas in the DIGITS data partitioning all sets have data from the same 50 subjects thus allowing multi speaker experiments Due to this fact the adaptation and held out sets for DIGITS are identical For the impaired speech data the duration of the collected data is too short to allow HMM training Therefore LVCSR HMMs trained on the IBM ViaVoiceTM dataset are adapted on the impaired LVCSR and DIGITS adaptation sets see Table 3 To assess the benefit of the visual modality to ASR in noisy conditions in addition to the relatively clean audio condition of the database recordings we artificially corrupt the data audio with additive non stationary speech babble noise at various SNRs ASR results are then reported at a number of SNRs ranging within 1 5 19 5 dB for LVCSR and 3 5 19 5 dB for DIGITS with all corresponding front end matrices and HMMs trained in the matched condition In particular during the Johns Hopkins summer 2000 workshop only two audio conditions were considered for LVCSR The original 19 5 dB SNR audio and a degraded one at 8 5 dB SNR Notice that in contrast to the audio no noise is added to the video channel or features Many cases of visual noise could have been considered such as additive noise on video frames blurring frame rate decimation and extremely high compression factors among others Some preliminary studies on the effects of video degradations to visual recognition can be found in the literature Davoine et al 1997 Williams et al 1997 Potamianos et al 1998 These studies find automatic speechreading performance to be rather robust to video compression for example but to degrade rapidly for frame rates below 15 Hz The ASR experiments reported next follow two distinct paradigms The results on the IBM ViaVoiceTM data obtained during the Johns Hopkins summer 2000 workshop employ a lattice rescoring paradigm due to the limitations in large vocabulary decoding of the HTK software used there Young et al 1999 namely lattices were first generated prior to the workshop using the IBM Research decoder Hark with HMMs trained at IBM and subsequently rescored during the workshop by trained tri phone context dependent HMMs on various feature sets or fusion techniques using HTK Three sets of lattices were generated for these experiments and were based on clean audio only 19 5 dB noisy audio only and noisy audio visual at the 8 5 dB SNR condition HiLDA features In the second experimental paradigm full decoding results obtained by directly using the IBM Research recognizer are reported For the LVCSR experiments 11 phone context dependent HMMs with 2 808 context dependent states and 47 k Gaussian mixtures are used whereas for DIGITS recognition in normal speech the corresponding numbers are 159 and 3 2 k for single stream models Decoding using the closed set vocabulary 10 403 words and a trigram language model is employed for LVCSR this is the case also for the workshop results whereas the 11 digit zero to nine including Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Audio Condition Audio only AV Concat FF AV HiLDA FF AV DMC DF Clean 14 44 16 00 13 84 13 65 12 95 Noisy 48 10 40 00 36 99 Audio Condition AV MS Joint DF AV MS Sep DF AV MS PROD DF AV MS UTTER DF Clean 14 62 14 92 14 19 13 47 Noisy 36 61 38 38 35 21 35 27 Table 5 Test set speaker independent LVCSR audio only and audio visual WER for the clean 19 5 dB SNR and a noisy audio 8 5 dB condition Two feature fusion FF and five decision fusion DF based audio visual systems are evaluated using the lattice rescoring paradigm Neti et al 2000 Glotin et al 2001 Luettin et al 2001 oh word vocabulary is used for DIGITS with unknown digit string length Visual Only Recognition The suitability for LVCSR of a number of appearance based visual features and AAMs was studied during and after the Johns Hopkins summer workshop Neti et al 2000 Matthews et al 2001 For this purpose noisy audio only lattices were rescored by HMMs trained on the various visual features considered namely 86dimensional AAM features as well as 24 dimensional DCT PCA on 32 32 pixel mouth ROIs and DWT based features All features were post processed as previously discussed to yield 41 dimensional feature vectors see Figure 7 For the DWT features the Daubechies class wavelet filter of approximating order 3 is used Daubechies 1992 Press et al 1995 LVCSR recognition results are reported in Table 4 depicted in word error rate WER The DCT outperformed all other features considered Notice however that these results cannot be interpreted as visual only recognition since they correspond to cascade audio visual fusion of audio only ASR followed by visual only rescoring of a network of recognized hypotheses For reference a number of characteristic lattice WERs are also depicted in Table 4 including the audio only at 8 5 dB result All feature performances are bounded by the lattice oracle and anti oracle WERs It is interesting to note that all appearance based features considered attain lower WERs e g 58 1 for DCT features than the WER of the best path through the lattice based on the language model alone 62 0 Therefore such visual features do convey significant speech information AAMs on the other hand did not perform well possibly due to severe undertraining of the models resulting in poor fitting to unseen facial data As expected visual only recognition based on full decoding instead of lattice rescoring is rather poor The LVCSR WER on the speaker independent test set of Table 3 based on per speaker MLLR adaptation is reported at 89 2 in Potamianos and Neti 2001b using the DCT features of the workshop Extraction of larger ROIs and the use of within frame DCT discriminant features and longer temporal windows as depicted in Figure 11 result in the improved WER of 82 3 In contrast to LVCSR DIGITS visual only recognition constitutes a much easier task Indeed on the multi speaker test set of Table 3 a 16 8 WER is achieved after per speaker MLLR adaptation Audio Visual ASR A number of audio visual integration algorithms presented in the fusion section of this chapter were compared during the Johns Hopkins summer 2000 workshop As already mentioned two audio conditions were considered The original clean database audio 19 5 dB SNR and a noisy one at 8 5 dB SNR In the first case fusion algorithm results were obtained by rescoring pre generated clean audio only lattices at the second condition HiLDA noisy audio visual lattices were rescored The results of these experiments are summarized in Table 5 Notice that every fusion method considered outperformed audio only ASR in the noisy case reaching up to a 27 relative reduction in WER from 48 10 noisy audio only to 35 21 audio visual In the clean audio condition among the two feature fusion techniques considered HiLDA fusion Potamianos et al 2001a improved ASR from 14 44 audio only to a 13 84 audio visual WER however concatenative fusion degraded performance to 16 0 Among the decision fusion algorithms used the product HMM AV MS PROD with jointly trained audio visual components Luettin et al 2001 improved performance Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 25 90 LVCSR AUDIO ONLY 20 DIGITS AUDIO ONLY AV Enhanced AV Concat 15 WORD ERROR RATE WER 80 70 AV Enhanced 60 50 40 30 20 10 0 0 5 7 5 dB GAIN AV Concat AV HiLDA AV MS Joint 7 dB GAIN AV HiLDA AV MS Joint 10 0 5 10 15 20 0 5 10 15 20 SIGNAL TO NOISE RATIO SNR dB SIGNAL TO NOISE RATIO SNR dB Figure 12 Comparison of audio only and audio visual ASR by means of three feature fusion AV Concat AV HiLDA and AV Enhanced algorithms and one decision fusion AV MS Joint technique using the full decoding experimental paradigm WERs vs audio channel SNR are reported on both the IBM ViaVoiceTM test set speaker independent LVCSR left as well as on the multi speaker DIGITS test set right of Table 3 HiLDA feature fusion outperforms alternative feature fusion methods whereas decision fusion outperforms all three feature fusion approaches resulting in an effective SNR gain of 7 dB for LVCSR and 7 5 dB for DIGITS at 10 dB SNR Potamianos et al 2001a Goecke et al 2002 Gravier et al 2002a Notice that the WER range in the two graphs differs to a 14 19 WER In addition utterance based stream exponents for a jointly trained multi stream HMM AV MS UTTER estimated using an average of the voicing present at each utterance further reduced WER to 13 47 Glotin et al 2001 achieving a 7 relative WER reduction over audio only performance Finally a late integration technique based on discriminative model combination AV DMC of audio and visual HMMs Beyerlein 1998 Vergyri 2000 Glotin et al 2001 produced a WER of 12 95 amounting to a 5 reduction from its clean audio only baseline of 13 65 this differs from the 14 44 audio only result due to the rescoring of n best lists instead of lattices Notice that for both clean and noisy audio conditions the best decision fusion method outperformed the best feature fusion technique considered In addition for both conditions joint multi stream HMM training outperformed separate training of the HMM stream components something not surprising since joint training forces state synchrony between the audio and visual streams To further demonstrate the differences between the various fusion algorithms and to quantify the visual modality benefit to ASR we review a number of full decoding experiments recently conducted for both the LVCSR and DIGITS tasks and at a large number of SNR conditions Potamianos et al 2001a Goecke et al 2002 Gravier et al 2002a All three feature fusion techniques discussed in the relevant section of this chapter are compared to decision fusion by means of a jointly trained multi stream HMM The results are depicted in Figure 12 Among the feature fusion methods considered HiLDA feature fusion is superior to both concatenative fusion and the enhancement approach In the clean audio case for example HiLDA fusion reduces the audio only LVCSR WER of 12 37 to 11 56 audio visual whereas feature concatenation degrades performance to 12 72 the enhancement method obviously provides the original audio only performance in this case Notice that these results are somewhat different to the ones reported in Table 5 due to the different experimental paradigm considered In the most extreme noisy case considered for LVCSR 1 5 dB SNR the audio only WER of 92 16 is reduced to 48 63 using HiLDA compared to 50 76 when feature concatenation is employed and 63 45 when audio feature enhancement is used Similar results hold for DIGITS recognition although the difference between HiLDA and concatenative feature fusion ASR is small possibly due to the fact that HMMs with significantly less Gaussian mixtures are used and the availability of sufficient data to train on high dimensional concatenated audio visual vectors The comparison Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Method j Task Modality Unadapted MLLR MAP MAP MLLR Mat MAP Mat MAP MLLR LVCSR AU VI AV 116 022 136 359 106 014 52 044 110 166 42 873 52 376 101 215 44 199 47 624 95 027 41 216 52 928 98 674 46 519 50 055 93 812 41 657 AU 52 381 3 770 3 373 2 381 3 968 2 381 DIGITS VI 48 016 16 667 12 103 10 516 8 730 8 531 AV 24 801 0 992 1 190 0 992 1 190 0 992 Table 6 Adaptation results on the speech impaired data WER of the audio only AU visual only VI and audio visual AV modalities using HiLDA feature fusion is reported on both the LVCSR left table part and DIGITS test sets right table of the speech impaired data using unadapted HMMs trained in normal speech as well as a number of HMM adaptation methods All HMMs are adapted on the joint speech impaired LVCSR and DIGITS adaptation sets of Table 3 For the continuous speech results decoding using the test set vocabulary of 537 words is reported MAP followed by MLLR adaptation and possibly preceded by front end matrix adaptation Mat achieves the best results for all modalities and for both tasks considered Potamianos and Neti 2001a between multi stream decision fusion and HiLDA fusion reveals that the jointly trained multi stream HMM performs significantly better For example at 1 5 dB SNR LVCSR WER is reduced to 46 28 compared to 48 63 for HiLDA Similarly for DIGITS recognition at 3 5 dB the HiLDA WER is 7 51 whereas the multi stream HMM WER is significantly lower namely 6 64 This is less than one third of the audio only WER of 23 97 A useful indicator when comparing fusion techniques and establishing the visual modality benefit to ASR is the effective SNR gain measured here with reference to the audio only WER at 10 dB To compute this gain we need to consider the SNR value where the audio visual WER equals the reference audio only WER see Figure 12 For HiLDA fusion this gain equals approximately 6 dB for both LVCSR and DIGITS tasks Jointly trained multi stream HMMs improve these gains to 7 dB for LVCSR and 7 5 dB for DIGITS at 10 dB SNR Full decoding experiments employing additional decision fusion techniques are currently in progress In particular intermediate fusion results by means of the product HMM are reported in Gravier et al 2002b Audio Visual Adaptation We now describe recent experiments on audio visual adaptation in a case study of single subject audio visual ASR of impaired speech Potamianos and Neti 2001a As already indicated the small amount of speech impaired data collected see Table 3 is not sufficient for HMM training thus calling for speaker adaptation techniques instead A number of such methods described in a previous section are used for adapting audioonly visual only and audio visual HMMs suitable for LVCSR The results on both speech impaired LVCSR and DIGITS tasks are depicted in Table 6 Notice that due to poor accuracy on impaired speech decoding on the LVCSR task is performed using the 537 word test set vocabulary of the dataset Clearly the mismatch between the normal and impaired speech data is dramatic as the Unadapted table entries demonstrate Indeed the audio visual WER in the LVCSR task reaches 106 0 such large numbers occur due to word insertions whereas the audio visual WER in the DIGITS task is 24 8 in comparison the normal speech per subject adapted audio visual LVCSR WER is 10 2 and the audio visual DIGITS WER is only 0 55 computed on the test sets of Table 3 We first consider MLLR and MAP HMM adaptation using the joint speech impaired LVCSR and DIGITS adaptation tests Audio visual only and audio visual performances improve dramatically as demonstrated in Table 6 Due to the rather large adaptation set MAP performs similarly well to MLLR Applying MLLR after MAP improves results and it reduces the audio visual WER to 41 2 and 0 99 for the LVCSR and DIGITS tasks respectively amounting to a 61 and 96 relative WER reduction over the audio visual Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 unadapted results and to a 13 and 58 relative WER reduction over the audio only MAP MLLR adapted results Clearly therefore the visual modality dramatically benefits the automatic recognition of impaired speech We also apply front end adaptation possibly followed by MLLR adaptation with the results depicted in the Mat MAP MLLR entries of Table 6 Although visual only recognition improves the audio only recognition results fail to do so As a consequence audio visual ASR degrades possibly also due to the fact that in this experiment audio visual matrix adaptation is only applied to the second stage of LDA MLLT SUMMARY AND DISCUSSION In this chapter we provided an overview of the basic techniques for automatic recognition of audio visual speech proposed in the literature over the past twenty years The two main issues relevant to the design of audio visual ASR systems are First the visual front end that captures visual speech information and second the integration fusion of audio and visual features into the automatic speech recognizer used Both are challenging problems and significant research effort has been directed towards finding appropriate solutions We first discussed extracting visual features from the video of the speaker s face The process requires first the detection and tracking of the face mouth region and possibly the speaker s lip contours A number of mostly statistical techniques suitable for the task were reviewed Various visual features proposed in the literature were then presented Some are based on the mouth region appearance and employ image transforms or other dimensionality reduction techniques borrowed from the pattern recognition literature in order to extract relevant speech information Others capture the lip contour and possibly face shape characteristics by means of statistical or geometric models Combinations of features from these two categories are also possible Subsequently we concentrated on the problem of audio visual integration Possible solutions to it differ in various aspects including the classifier and classes used for automatic speech recognition the combination of single modality features vs single modality classification decisions and in the latter case the information level provided by each classifier the temporal level of the integration and the sequence of such decision combination We concentrated on HMM based recognition based on sub phonetic classes and assuming time synchronous audio and visual feature generation we reviewed a number of feature and decision fusion techniques Within the first category we discussed simple feature concatenation discriminant feature fusion and a linear audio feature enhancement approach For decision based integration we concentrated in linear log likelihood combination of parallel single modality classifiers at various levels of integration considering the state synchronous multi stream HMM for early fusion the product HMM for intermediate fusion and discriminative model combination for late integration and we discussed training the resulting models Developing and benchmarking feature extraction and fusion algorithms requires available audio visual data A limited number of corpora suitable for research in audio visual ASR have been collected and used in the literature A brief overview of them was also provided followed by a description of the IBM ViaVoiceTM database suitable for speaker independent audio visual ASR in the large vocabulary continuous speech domain Subsequently a number of experimental results were reported using this database as well as additional corpora recently collected at IBM Some of these experiments were conducted during the summer 2000 workshop at the Johns Hopkins University and compared both visual feature extraction and audio visual fusion methods for LVCSR More recent experiments as well as a case study of speaker adaptation techniques for audio visual recognition of impaired speech were also presented These experiments showed that a visual front end can be designed that successfully captures speaker independent large vocabulary continuous speech information Such a visual front end uses discrete cosine transform coefficients of the detected mouth region of interest suitably post processed Combining the resulting visual features with traditional acoustic ones results in significant improvements over audio only recognition in both clean and of course degraded acoustic conditions across small and large vocabulary tasks as well as for both normal and impaired speech A successful combination technique is the multi stream HMM based decision fusion approach or the simpler but inferior discriminant feature fusion HiLDA method This chapter clearly demonstrates that over the past twenty years much progress has been accomplished in capturing and integrating visual speech information into automatic speech recognition However the visual modality has yet to become utilized in mainstream ASR systems This is due to the fact that issues Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 of both practical and research nature remain challenging On the practical side of things the high quality of captured visual data which is necessary for extracting visual speech information capable of enhancing ASR performance introduces increased cost storage and computer processing requirements In addition the lack of common large audio visual corpora that address a wide variety of ASR tasks conditions and environments hinders development of audio visual systems suitable for use in particular applications On the research side the key issues in the design of audio visual ASR systems remain open and subject to more investigation In the visual front end design for example face detection facial feature localization and face shape tracking robust to speaker pose lighting and environment variation constitute challenging problems A comprehensive comparison between face appearance and shape based features for speakerdependent vs speaker independent automatic speechreading is also unavailable Joint shape and appearance three dimensional face modeling used for both tracking and visual feature extraction has not been considered in the literature although such an approach could possibly lead to the desired robustness and generality of the visual front end In addition when combining audio and visual information a number of issues relevant to decision fusion require further study such as the optimal level of integrating the audio and visual loglikelihoods the optimal function for this integration as well as the inclusion of suitable local estimates of the reliability of each modality into this function Further investigation of these issues is clearly warranted and it is expected to lead to improved robustness and performance of audio visual ASR Progress in addressing some or all of these questions can also benefit other areas where joint audio and visual speech processing is suitable Chen and Rao 1998 such as speaker identification and verification Jourlin et al 1997 Wark and Sridharan 1998 ACKNOWLEDGEMENTS We would like to acknowledge a number of people for particular contributions to this work Giridharan Iyengar and Andrew Senior IBM for their help with face and mouth region detection on the IBM ViaVoiceTM and other audio visual data discussed in this chapter Rich Wilkins and Eric Helmuth formerly with IBM for their efforts in data collection Guillaume Gravier currently at IRISA INRIA Rennes for the joint multistream HMM training and full decoding on the connected digits and LVCSR tasks Roland Goecke currently at the Australian National University for experiments on audio visual based enhancement of audio features during a summer internship at IBM REFERENCES Adjoudani A and Beno it C 1996 On the integration of auditory and visual parameters in an HMM based ASR In Stork D G and Hennecke M E Eds Speechreading by Humans and Machines Berlin Germany Springer pp Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Bahl L R Brown P F DeSouza P V and Mercer L R 1986 Maximum mutual information estimation of hidden Markov model parameters for speech recognition Proc International Conference on Acoustics Speech and Signal Processing Tokyo Japan pp Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Cootes T F Taylor C J Cooper D H and Graham J 1995 Active shape models their training and application Computer Vision and Image Understanding 61 1 Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Goldschen A J Garcia O N and Petajan E D 1996 Rationale for phoneme viseme mapping and feature selection in visual speech recognition In Stork D G and Hennecke M E Eds Speechreading by Humans and Machines Berlin Germany Springer pp Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Krone G Talle B Wichert A and Palm G 1997 Neural architectures for sensorfusion in speech recognition Proc European Tutorial Workshop on Audio Visual Speech Processing Rhodes Greece pp Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Neumeyer L Sankar A and Digalakis V 1995 A comparative study of speaker adaptation techniques Proc European Conference on Speech Communication and Technology Madrid Spain pp Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Rowley H A Baluja S and Kanade T 1998 Neural network based face detection IEEE Transactions on Pattern Analysis and Machine Intelligence 20 1 