Data Min Knowl Disc 2010 21 Using interesting sequences to interactively build Hidden Markov Models Szymon Jaroszewicz Received 20 March 2009 Accepted 6 March 2010 Published online 7 April 2010 The Author s 2010 Abstract The paper presents a method of interactive construction of global Hidden Markov Models HMMs based on local sequence patterns discovered in data The method is based on finding interesting sequences whose frequency in the database differs from that predicted by the model The patterns are then presented to the user who updates the model using their intelligence and their understanding of the modelled domain It is demonstrated that such an approach leads to more understandable models than automated approaches Two variants of the problem are considered mining patterns occurring only at the beginning of sequences and mining patterns occurring at any position both practically meaningful For each variant algorithms have been developed allowing for efficient discovery of all sequences with given minimum interestingness Applications to modelling webpage visitors behavior and to modelling protein secondary structure are presented validating the proposed approach Keywords Interesting 1 Introduction Sequence databases are at the heart of several important applications such as Web Mining bioinformatics or speech analysis and recognition Mining frequent sequences is an important approach to analysis of such data Unfortunately as is the case with most frequent pattern based approaches sequence mining typically produces thousands of frequent sequences which are very difficult for the user to analyze due not only to Responsible editor Johannes 123 Using interesting sequences to interactively build Hidden Markov Models 187 their quantity but also to redundancies between them This paper addresses the problem by incorporating an explicit global model of background knowledge into the sequence mining problem The model helps identify truly interesting patterns taking into account what is already known about the data The user provides a possibly empty description of their current knowledge about the domain and a database from which new knowledge is to be discovered Knowledge description has the form of a global probabilistic model from which precise probabilistic inferences can be made In Jaroszewicz and Simovici 2004 Jaroszewicz and Scheffer 2005 and Jaroszewicz et al 2009 a Bayesian network was used for this purpose due to its flexibility understandability as well as the fact that it represents a full joint probability distribution making inference possible In the case of sequence data similar advantages are shared by Hidden Markov Models HMMs Rabiner 1989 Welch 2003 and for that reason they have been used to model background knowledge in the presented approach Based on the model and the data interesting patterns are discovered A pattern is defined to be interesting if its frequency in the data differs significantly from that predicted by the global model The patterns are then presented to the user whose task is to interpret them and update the model The new updated model is then used again together with the data to find a new set of interesting patterns The procedure is repeated several times until the desired quality of the global model has been reached The approach to mining interesting sequences presented in this paper follows the same cycle First sequences are discovered whose frequency in a database differs significantly from the predictions of the HMM Such sequences are considered interesting and are shown to the user who updates the HMM by adding more hidden states representing new underlying behavior by modifying lists of possible output symbols in existing states or by adding new transitions between states The HMM parameters are then retrained using the Baum Welch algorithm a variant of the Expectation Maximization approach and the process is repeated In Jaroszewicz et al 2009 it has been demonstrated that interactive model construction using human intelligence in the process gives models which represent the domain much better than models built using fully automated methods Similar conclusions have been reached in this paper The advantage of this approach is that the new states added to the HMMs have clear user defined meaning The lists of symbols emitted in each state are also much shorter and more coherent The resulting model is thus understandable and all hidden states have clear interpretations As our experiments have shown this is usually not possible with automatic methods An additional benefit is that the Baum Welch algorithm turned out to work much better with hand built models resulting in much faster convergence and better avoidance of local optima A drawback of the proposed method is that it requires intensive human involvement and sometimes a significant effort is needed to explain the discovered interesting patterns However it is argued that such an approach is usually necessary if meaningful internal structure is to be obtained The approach has been tested on the web server log of the National Institute of Telecommunications in Warsaw author s employer The application proved that the proposed approach is highly practical and produces accurate models which are 123 188 S Jaroszewicz understandable and easy to interpret Another application which is presented in this paper is about modelling protein secondary structure using publicly available data 2 Related research There has been a significant amount of work on mining frequent patterns in sequence data full discussion is beyond the scope of this work see for example thorough overviews in Laxman and Sastry 2006 and in Han et al 2007 An idea of incorporating background knowledge defined as a formal model giving well defined predictions into the pattern discovery process has already been suggested in Hand 2002 The paper presented a high level framework of which the current approach and that in Jaroszewicz and Simovici 2004 Jaroszewicz and Scheffer 2005 Jaroszewicz et al 2009 can be considered a specific case The paper Hand 2002 is very general and does not give specific details on how the background knowledge should be represented or what discovery methods should be used Section 5 of Laxman and Sastry 2006 describes approaches to testing significance of sequence patterns based on comparing with a background model The purpose however is to test statistical significance and the models are thus simple based on independence assumption and fixed throughout the discovery process user s background knowledge and intervention do not come into the picture as they do in our approach In Laxman and Sastry 2005 a separate small HMM with special structure is built for each temporal pattern episode Such small HMMs are later used for significance testing The use of a global model being a mixture of small episode models is alluded to but not developed further In Prum et al 1995 a similar approach has been used to derive expected probabilities of DNA sequences Zaiane and his collegues Zaiane et al 2007 Satsangi and Zaiane 2007 have worked on discovering so called contrast sequences This is similar to a single stage of the interactive model building process described here except that two datasets are compared while in our approach one of the datasets is replaced by an HMM As a result Zaiane et al s overall methodology and the discovery algorithms are very different from methods proposed in this paper There has been some related work in the fields of biological sequence modelling and speech recognition Low Kam et al 2009 where a set of unexpected patterns is found whose probability as predicted by an HMM is low The idea however is more in line with Laxman and Sastry 2006 than with the approach proposed here No explicit model building is considered and in fact the exact structure of the model used is not described presumably it is just a simple Markov model over the symbols The model does not change during the discovery process Furthermore contrary to our approach stationarity assumption is made which is not suitable for shorter sequences In Spiliopoulou 1999 and Li et al 2007 background knowledge is also incorporated into the sequence mining process However it is stored as a set of rules and there is no global probabilistic model Unexpectedness is defined in a syntactic rather than probabilistic fashion See Jaroszewicz and Simovici 2004 and Jaroszewicz et al 2009 for a discussion of drawbacks of rule based representations and advantages of having a unified model 123 Using interesting sequences to interactively build Hidden Markov Models 189 A somewhat related technique is adaptive HMM learning Huo et al 1995 Lee and Gauvin 1996 Huo and Lee 1997 The approach is essentially based on placing a prior on the HMM parameters The prior is usually obtained using the Empirical Bayes procedure Huo et al 1995 Robbins 1956 This allows for example for tuning a speech recognition system to a new speaker using a prior based on parameters obtained for several other speakers While there are similarities to our approach e g the prior plays the role of background knowledge the method is fundamentally different There is no notion of an interesting pattern model parameters are updated based on new data only The update is meant to adapt the model to idiosyncrasies of new data while using older data as a prior not to build a single general model In our approach there is a single dataset which remains static it is the interesting patterns used to update the model which change from iteration to iteration Also contrary to our approach in adaptive HMM learning the update is done automatically and only to model s parameters not its structure Model s understandability is not an explicit goal of adaptive HMM learning Another related technique is semisupervised learning of HMMs Inoue and Ueda 2003 Ji et al 2009 where labeled examples may be viewed as user provided guides modifying the model built on unlabeled data The nature of the process is clearly different from the proposed approach This work is based on an earlier paper Jaroszewicz 2008 by the author but has been significantly expanded All parts related to mining interesting sequences starting at arbitrary point in time are new So is the application to modelling protein secondary structure Comparison with automatically built models has also been significantly expanded 3 Definitions and notation Let us begin by describing the mathematical notation used Vectors are denoted by lowercase boldface Greek letters and matrices with uppercase boldface roman letters All vectors are assumed to be row vectors explicit transposition is used for column vectors Superscripts are used to denote time or more precisely position counting from the beginning of a database sequence and subscripts to denote elements of vectors t denotes i th element of the vector at time t Since matrices will not change e g i with time in our applications superscripts on matrices will denote matrix power e g P t is the matrix P raised to the power t parentheses around the matrix are added to further avoid confusion Let us fix a set of symbols O o1 om All sequences will be constructed over this set It will also play the role of the set of output symbols of all HMMs Sequences of symbols will be denoted with bold lowercase letters s and d where s will denote temporary sequences being worked on and d sequences in the database s denotes the length of sequence s Following our convention st is the symbol of s at time t or more precisely the symbol at distance t from the beginning of the sequence with the first symbol being at index 0 Further if o O then so will denote a sequence 123 190 S Jaroszewicz obtained by appending o to s and os the sequence obtained by prepending o at the beginning of s An empty sequence will be denoted with 3 1 Hidden Markov Models Let us now describe HMMs which will be used to represent knowledge about systems with discrete time Only most important facts will be given full details can be found in literature Rabiner 1989 Welch 2003 A HMM is a modification of a Markov Chain such that the state the model is in is not directly visible Instead every state emits output symbols according to its own probability distribution For example while modelling website visitors behavior the internal states could correspond to visitor s intentions e g wants to find a specific content and output symbols to the pages he she actually visits Formally an HMM is a quintuple Q O 0 P E where Q q1 qn is 0 a set of states O o1 om the set of output symbols 0 0 1 n a vector of initial probabilities for each state and P an n by n transition matrix where Pi j is the probability of a transition occurring from state qi to state q j Finally E is an n by m emission probability matrix such that Ei j gives the probability of observing an output symbol o j provided that the HMM is in state qi Where convenient we will use shorthand notation Eio to denote the probability of emitting symbol o in state qi Notice that the vector t of state probabilities at time t can be computed as t 0 P t Similarly E is the vector of probabilities of observing each symbol provided that is the vector of state probabilities We will now discuss how to determine the probability that a given sequence of output symbols is produced by an HMM For that aim a key concept of forward probabilities will be introduced full details can be found in Rabiner 1989 and Welch 2003 Let s be a sequence of symbols t s Further let z t denote the state the HMM is in at time t The forward probabilities of a sequence s in the HMM are defined as s i Pr u 0 s0 u 1 s1 u t 1 st 1 z t qi where u 0 u 1 u t 1 is the sequence of symbols output by the HMM at times 0 1 t 1 respectively and s0 s1 st 1 are symbols at respective positions in the sequence s Simply speaking this is the probability that the HMM has emitted the sequence s and ended up in state qi at time t that is after emitting the last symbol of s Grouping the forward probabilities for all ending states we obtain a vector s s 1 s 2 s n The probability that the HMM emits a sequence s starting at t 0 can be computed by summing the elements of s An important property of the probabilities is that they can be efficiently computed using dynamic programming by extending the sequence symbol by symbol using the following formula 123 Using interesting sequences to interactively build Hidden Markov Models 0 i i n 191 so i j 0 s j E jo P ji 1 Another important problem related to HMMs is estimating their parameters starting transition and emission probabilities based on a given sequence database This is usually achieved using the Baum Welch algorithm which is a variant of the Expectation Maximization method The details can be found in Rabiner 1989 and Welch 2003 and will be omitted here The Baum Welch algorithm only guarantees convergence to a local minimum and has been reported to be slow In practice we have noticed the algorithm is very dependent on the emission probabilities matrix E If output symbols convey a reasonable amount of information about internal states the algorithm converges quickly and reliably if on the other hand this information is highly ambiguous the process is slow and often ends up in a local optimum This issue is discussed in detail and illustrated by experiments in later sections where it will be argued that estimating parameters of interactively built HMMs is significantly easier than for fully connected ones 4 Discovery of interesting sequences starting at the beginning of database sequences We begin with a more detailed description of how background knowledge is being represented As mentioned above user s knowledge about the problem is encoded using a HMM However the model will usually be much sparser than fully connected models used in most HMM applications The user needs to provide the hidden states of the model typically new states are added incrementally during the discovery process For every hidden state the user needs to specify possible transitions symbols which can be emitted from that state and whether the state can be an initial state i e can have a nonzero starting probability The user thus provides the structure of the model but not the starting transition and emission probabilities which will be estimated from data using the Baum Welch algorithm The key component of the interactive global HMM construction using the proposed framework is finding interesting sequences of output symbols There are several ways to formulate the concept of an interesting sequence This paper will provide two such formulations In this section we will present an approach which assumes that mined sequences are prefixes of database sequences i e they start at t 0 and in the following section we will extend the approach to sequences starting at arbitrary locations within database sequences Both approaches have important practical applications examples of which will be shown in later sections Let the provided database D contain N sequences of symbols each starting at t 0 D d1 d N 123 192 S Jaroszewicz Let s be a sequence of symbols also starting at time t 0 Denote by Pr HMM s the probability that the sequence s is generated by HMM starting at time t 0 Analogously the probability of observing that sequence in data is defined as Pr D s d D s is a prefix of d D that is as the percentage of sequences in the database of which s is a prefix A sequence whose probability of occurrence is greater than or equal to is called frequent The interestingness of s is defined analogously to Jaroszewicz and Simovici 2004 as I s Pr D s Pr HMM s 2 that is as the absolute difference between the probabilities predicted by the HMM and observed in data A sequence is called interesting if its interestingness is not lower than It is easy to see that the following observation holds Observation 1 If I s then either Pr D s or Pr HMM s That is for a sequence to be interesting it is necessary for it to be frequent either in data or in the HMM This observation motivates an algorithm for finding all interesting symbol sequences starting at t 0 for a user specified minimum interestingness threshold The algorithm is shown in Fig 1 its key steps are described in detail in the following paragraphs 4 1 Finding frequent sequences in data There are several algorithms for finding frequent sequences in data Han et al 2007 The situation analyzed in this section is much simpler however since all sequences Fig 1 The algorithm for finding all interesting sequences starting at t 0 123 Using interesting sequences to interactively build Hidden Markov Models 193 Fig 2 An algorithm for finding all frequent sequences in a HMM are assumed to start at t 0 A significantly less complicated approach is therefore used First all sequences in D are sorted in lexicographical order and scanned sequentially When scanning the i th sequence di the longest common prefix p of di and di 1 is computed Counts of all prefixes of p are incremented by 1 All prefixes of di 1 which are longer than p will never appear again because of the lexicographical scanning order so those whose support is lower than are removed and those whose support is greater than or equal to are added to the result set The scanning process then moves on to the next record 4 2 Finding frequently occurring sequences in a HMM A more interesting problem is to find all sequences which are emitted by an HMM with probability greater or equal to a given threshold This part has been implemented in the style of a depth first version of the well known Apriori algorithm Agrawal et al 1993 The sequences are built symbol by symbol beginning with the empty sequence We use the fact that appending additional symbols to a sequence cannot increase its probability of occurrence so if a sequence is found to be infrequent all sequences derived from it can be skipped To efficiently compute the probability of each sequence being emitted by the HMM we use the forward probabilities which can be efficiently updated using Eq 1 The updating is performed simultaneously with appending symbols to sequences and the forward probabilities are simply passed down the search tree The algorithm is shown in Fig 2 The approach is very efficient since computing probabilities in HMMs can be done much more efficiently than computing supports in large datasets 5 Discovery of interesting sequences starting at arbitrary time In this section we present a modification of the above approach to mining patterns which can start at any point in time not necessarily at t 0 For this to be possible some of the definitions introduced above need to be modified In order to avoid 123 194 S Jaroszewicz unnecessarily complicated notation this section will intentionally redefine some of the symbols used before The definitions of support in data and in the HMM become somewhat more involved since they need to account for the fact that each sequence may appear at several moments in time First we need to define the support of s in a single database sequence d suppd s 0 t d s s0 dt s1 dt 1 N supp D s i 1 suppdi s that is as the number of occurrences of s in the whole database Let Pr HMM t s denote the probability that the HMM emits the sequence s with s0 emitted at time t Define the support of a sequence s with time horizon T as suppHMM T s T s t 0 Pr HMM t s that is as the expected number of occurrences of the sequence s in the first T symbols emitted by the HMM Notice that this quantity can indeed be interpreted as statistical expectation since expectation of a sum is a sum of expectations even for dependent random variables Since the support of a sequence in an HMM will be compared with its support in data we introduce the following definition Definition 1 The support of sequence s in a HMM with respect to a database D d1 d N is defined as supp HMM D N s i 1 suppHMM di s Intuitively this is the expected support of s in data if the data were generated from the HMM The sum in the formula comes from the fact that in order to generate D the model HMM would have been used N times to generate N sequences each of length equal to the corresponding sequence in D The expected support of s in i th sequence in D would then be equal to suppHMM di s To define the interestingness of a sequence s we need to compare its support in the database D with analogous support computed based on the HMM s expectations I s supp D s suppHMM D s N i 1 di 3 123 Using interesting sequences to interactively build Hidden Markov Models 195 When the database contains only a single sequence D d the definition simplifies to I s suppd s suppHMM d s d Let us now make an observation which will allow us to find all sequences with given minimum interestingness threshold Observation 2 If I s then either N supp D s i 1 di or suppHMM D s N di i 1 This motivates an algorithm for finding all interesting sequences beginning at any point in time which is practically identical to the one in Fig 1 with the only exception that the minimum support thresholds used are iN 1 di instead of just The details have thus been omitted The algorithm does however require finding sequences starting at arbitrary point in time which are frequent in data and frequent according to the HMM Those tasks differ significantly from the case of mining interesting sequences beginning at t 0 Finding frequent sequences in data is a well researched problem many algorithms are available The description of those methods is beyond the scope of this paper see e g Laxman and Sastry 2006 or Han et al 2007 for an overview The modification to accommodate mining in more than one database sequence at a time is easily incorporated into those approaches We used a depth first version of the sequence oriented variant of the Apriori algorithm with an extra optimization that a list of locations at which a sequence was found is reused when counting support of its supersequences We will now focus on the more interesting problem of mining frequent sequences starting at any time point in HMMs As the development will be fairly long it has been placed in a separate section 6 Finding frequent sequences in HMMs Before we begin describing the mining algorithm we need to introduce another important concept related to HMMs namely that of backward probabilities Rabiner 1989 Welch 2003 Let s be a sequence of output symbols the backward probabilities of s are defined as s t i s i Pr u t s0 u t 1 s1 u t s 1 s s 1 z t qi 123 196 S Jaroszewicz where u t u t 1 u t s 1 is the sequence of symbols output by the HMM at times t t 1 t s 1 respectively s0 s1 st 1 are symbols at respective positions in the sequence s and z t is the state the HMM is in at time t Intuitively this is the probability that starting at time t the model will emit the sequence s given that at time t it was in state qi As will soon become apparent those probabilities do not depend on t so the index can be dropped Denote s s 1 s 2 s n Similarly to forward probabilities backward probabilities can be efficiently updated using dynamic programming i 1 n os i Eio j 1 Pi j s j 4 The name of those quantities comes from the fact the probabilities are updated backwards beginning with the last emitted symbol Note also that as mentioned before the above formulas do not depend on the starting time t This property is crucial for computing supports of sequences since the same backward probabilities of a sequence can be reused to compute its support at any point in time More details on backward probabilities can be found in Rabiner 1989 or Welch 2003 We are now ready to discuss issues related to the main topic of this section To effectively mine frequent sequences in an HMM we need to satisfy two conditions monotonicity property of support must hold and an efficient way of computing supports must be devised These two points are addressed below Theorem 1 Let s be a sequence o an output symbol D d1 d N a database of sequences and HMM a Hidden Markov Model Then suppHMM D s suppHMM D os Proof Notice first that Pr HMM t s Pr HMM t 1 os the probability of observing a sequence s at time t cannot be less than the probability of observing s at time t and observing the symbol o at time t 1 Using this fact and the definitions we have supp HMM D N s i 1 supp HMM di N di s s i 1 t 0 Pr HMM t s Pr HMM t 1 os N di s i 1 t 1 Pr HMM t s N di s i 1 t 1 N di s 1 i 1 t 0 Pr HMM t os suppHMM D os 123 Using interesting sequences to interactively build Hidden Markov Models 197 Notice that we are extending the sequence backwards since that is how the probabilities used in the algorithm are computed We will now address the issue of efficient computation of supports of sequences in an HMM Notice that the probability of observing a sequence s beginning at time t can be computed as Rabiner 1989 Welch 2003 Pr HMM t s t s where denotes vector transpose and t is the state probability distribution at time t Since the probabilities do not depend on t they need to be computed only once for each sequence and can then be reused at each time step Let us now rewrite the definition of support of a sequence in an HMM with respect to a database D using the above formula Denote N di s s i 1 t 0 t We now obtain the following equations suppHMM T s and suppHMM D s N i 1 T s t 0 Pr HMM t s T s t 0 t s T s t 0 t s suppHMM di s N di s i 1 t 0 t s s s 5 Notice that s depends only on the length of the sequence s and can thus be cached and computed only if a given sequence length has not been seen before Moreover a very efficient way to compute s will be presented below Equation 5 together with Theorem 1 motivate the frequent sequence mining algorithm presented in Fig 3 Computing s An obvious brute force way to compute s is to perform successive multiplications by the transition matrix to get values of t for all necessary values of t Time needed for computing s using this method can become significant for long sequences Indeed the time complexity is O n 2 T n iN 1 di where T maxi di is the time horizon and n the number of states in the model The first term comes from the computation of t vectors by matrix multiplications and the second is the time spent computing the actual sums A much faster approach is described below We will make use of the concept of diagonalizability van Loan and Golub 1996 Meyer 2001 A matrix P is diagonalizable if it can be written in the form P VLV 1 123 198 S Jaroszewicz Fig 3 An algorithm for mining sequences frequent in an HMM with respect to a database D where L is a diagonal matrix containing eigenvalues of P and V is a matrix whose columns are eigenvectors of P It is a well known and easy to prove fact that if the matrix P is diagonalizable then its powers can be computed using the following formula P t V L t V 1 Suppose that the transition matrix P is diagonalizable The equation N di s N di s N di s s i 1 t t 0 i 1 N di s i 1 t 0 0 P t 0 t 0 i 1 t 0 V L t V 1 0V L t V 1 shows that the problem can be reduced to computing another matrix L di s N L t Since the matrix L is diagonal so is the matrix L Moreover i 1 t 0 computing it can be performed separately for each element on the diagonal Let li denote the i th element on the diagonal of L and li the i th element on the diagonal of L We have N di s li i 1 t 0 li t N 1 li max 0 di s 1 i 1 1 li N i 1 max 0 di s 1 di s t N li i 1 t 0 if li 1 if li 1 if li 1 li 1 6 The equation is obtained by using formulas for partial sums of the geometric series The last sum is used for complex eigenvalues with module 1 The maxima are included to ensure that database sequences shorter than s are omitted from the sums Notice that since the transition matrix is the so called stochastic matrix the modules of all 123 Using interesting sequences to interactively build Hidden Markov Models 199 its eigenvalues are less than or equal to one Meyer 2001 so all possible cases are covered While it is easy to construct a non diagonalizable transition matrix transition matrices learnt from data were in our experiments always diagonalizable The justification is as follows The set of non diagonalizable matrices has measure zero so it is very unlikely for a random matrix not to be diagonalizable Uhlig 2001 It turns out that the transition matrices learned from real data are close enough to random to be almost always diagonalizable If however the transition matrix is not diagonalizable the implementation detects this and performs the calculations using brute force approach Sufficient conditions for diagonalizability of a matrix are that all its eigenvalues be distinct or that its eigenvectors be linearly independent van Loan and Golub 1996 Meyer 2001 Uhlig 2001 The computation of eigenvalues and the inversion of the V matrix can be performed once and reused for all s When the matrix is diagonalizable the time of computing s is O n 3 n N where the n 3 part n is the number of states in the HMM comes from matrix multiplication inversion and eigenvalue decomposition and the n N part comes from the computations performed in Eq 6 for each eigenvalue An exception is the case when P has complex eigenvalues with module 1 when the time complexity may become O n 3 n iN 1 di This case however seems to only occur for specially crafted transition matrices and we never encountered it in our experiments In any case the time complexity is still better than for the brute force approach as long as n T quite a reasonable assumption Notice that except the unlikely case of complex eigenvalues with module 1 the computation time needed to count the support of a given sequence does not depend on the length of sequences in the database As a consequence the algorithm for mining frequent sequences in an HMM will not depend on the length of sequences only on the number of states in the HMM and weakly on the number of sequences Sequences of arbitrary length can thus be mined with ease 7 Experimental evaluation mining web server logs We will first consider the method for mining interesting sequences starting at time t 0 The approach will be evaluated experimentally on web server log data Web log of the server of the National Institute of Telecommunications in Warsaw has been used Full data covered the period of about 1 year the first 100 000 events have been used for experiments The presented method has been applied to create a model of the behavior of visitors to the Institutes s webpage After starting with a simple initial model new internal states were added with the assumption that the new states represent underlying patterns of user behavior such as access to e mail account through the web interface or access a paper in the Institutes s journal The identification of the underlying behavior and model updating was done entirely by the user Recall that the user specifies the structure of the model that is the states possible transitions between those states and symbols which can be emitted in each state Actual values of all probabilities are found automatically using the Baum Welch algorithm 123 200 S Jaroszewicz 7 1 Data preprocessing Each record in a weblog contains a description of a single event such as a file being retrieved Several items such as date and time of the event the referenced file an error code etc are recorded The required data format is however a database of sequences each sequence corresponding to a single user session of pages visited by users Unfortunately the log does not contain explicit information about sessions and the data had to be sessionized in order to form training sequences There are several sessionizing methods Dunham 2003 Here the simplest approach has been used events originating from a single source which were 30 min apart were considered to belong to the same session Despite its simplicity the method worked very well Other methods e g involving a limit on total session time were problematic for example could not handle automated web crawlers sessions which were very long At the end of each training sequence an artificial _END_ symbol has been added such that an end of a session could be explicitly modelled Another choice that had to be made was the set of output symbols Since the server contains a very large number of available files assigning a separate symbol to every one of them would have introduced thousands of output symbols This would lead to enormous models adversely affecting understandability Also the analyst is typically more interested in the behavior of visitors at a higher level and cannot investigate each of the thousands of files separately On top of that probability estimation in case of rarely accessed files would be highly unreliable To solve this problem only the toplevel directory of each accessed file was used as an output symbol As the Institute s website s content is logically divided into subdirectories such as journal people etc such an approach gives a better overview of users behavior than specific files If finer level analysis is required it is probably better to build a separate model which covers only a subset of available pages in greater detail 7 2 An initial model As the author had no idea on how the initial model should look like an empty model given in Fig 4 was used The _all_ state can initially emit any of the symbols present in the log The quit state can emit only the artificial _END_ symbol The model corresponds to a user randomly navigating from page to page before ending the session As more states are added emitted symbols will be removed from the _all_ state This will allow for better identification of the internal state of the model based on the output symbol leading to better understandability as well as better and more efficient parameter estimation using the Baum Welch algorithm Fig 4 The initial HMM describing the behavior of webpage visitors 123 Using interesting sequences to interactively build Hidden Markov Models 201 7 3 Typical patterns of model updating We will now discuss two general patterns of model updates which will become useful while building the model Of course the list is by no means exhaustive there are very diverse possible underlying behaviors which can cause a pattern to become interesting identifying all of them is impossible However the following two patterns occur quite frequently as will be seen in the weblog model construction The first pattern occurs when a distribution of the number of successive visits to a given directory needs to be modelled This can be achieved by adding a chain of internal states each emitting only the symbol corresponding to that directory Nodes in the chain typically have edges to nodes outside the chain for example to the node denoting the end of a session Such a chain can exactly model the distribution of the number of visits not greater than its length The final state of a chain often has a self loop to model larger numbers of visits using the geometric distribution The actual transition probabilities will easily be found by the Baum Welch algorithm The chain pattern will be very frequently employed in the Web log data modelling below Sometimes several symbols are output repeatedly mixed with each other The actual order of the symbols is not important or interesting but their probability differs from model predictions resulting in patterns being output by the algorithm In such a case we can add a collection of fully connected hidden states each emitting one of the symbols After learning the weights the underlying behavior will be modelled well and patterns related to that group of output symbols will disappear allowing new possibly more interesting patterns to become visible This pattern will be employed below to model access to various elements of the main webpage such as images CSS stylesheets and JavaScript 7 4 Model construction We will now describe a few most interesting stages of model construction The first interesting sequences discovered were related to the Sophos antivirus program whose updates are available on the intranet The most interesting sequence was sophos sophos its probability in data was 11 48 while the initial model predicted it to be only 1 17 The second most interesting sequence was one in which the sophos directory has been accessed four times It is curious that every session contained either two or four or more accesses to this directory for over a year there has not been a single session where the directory would have been accessed only once or only three times The reason most probably lies in the internal behavior of the antivirus update software In order to better predict the probabilities of those sequences the model has been updated by adding new states shown in Fig 5 Each of the new states except _all_sink emits only the sophos symbol In addition that symbol has been removed from the list of symbols emitted by the _all_ state As the new states emit only the symbol sophos which has been removed from remaining initial states any session which begins by accessing the antivirus directory has to pass through the leftmost node in Fig 5 It is thus clear that this part of the HMM 123 202 S Jaroszewicz Fig 5 The fragment of the HMM describing accesses to Sophos antivirus updates models only sessions accessing this section of the website Of course the probability of starting in the _all_ state will now have decreased by about 0 117 that is by the probability of starting in the sophos2a state Looking further into the segment newly added to the HMM we see that there are in total five states emitting only the sophos symbol connected in a chain Notice that the first state can only transition to the second sophos state with probability 1 Therefore any session which begins in the sophos directory has to visit it at least twice From the second state in the chain we can either end the session moving to the quit state or visit the sophos directory twice more After that we can either get more data from the antivirus directory sophos_more state or start visiting other pages For that purpose the _all_sink state is used to model sessions in which after some initial sequence arbitrary pages can be visited Here it is used to model sessions where after downloading antivirus updates the visitor moves to another part of the website This state will be reused throughout the HMM to model final parts of other types of sessions too The above modification is a typical application of the chain of states pattern described in Sect 7 3 Notice that we don t have to set specific probabilities e g that of quitting after visiting the sophos directory exactly twice They will be established automatically by the Baum Welch algorithm Of course it is possible that a session occurs which does not match the chain of states exactly its probability will then be predicted to be zero This is not a problem if such sessions occur infrequently otherwise they will themselves become interesting patterns and the model will have to be updated to better accommodate them After the new states have been added the probabilities of related sequences were predicted accurately and new sequences became interesting The most interesting sequences with respect to the updated model were about the directory journal containing articles in PDF format published in the journal edited at the Institute Almost 2 of all sessions contained the sequence journal journal favicon ico which according to the model should appear extremely infrequently In addition the favicon ico file was absent and generated an error It turned out that the file favicon ico is the default location for a small icon appearing in the web browser next to webpage address On the Institute s website this file was however located at img favicon ico which was marked in the headers of all HTML files PDF files however did not contain this information which caused 123 Using interesting sequences to interactively build Hidden Markov Models 203 Fig 6 The fragment of the HMM describing visitors accessing journal articles the browser to try the default location and since the file was not there triggered an HTTP error It is interesting that very often the command to access PDF files has been issued twice in a row After inspecting the log it turned out that the transmission of the same file has often been restarted The author was not able to explain this effect To model sessions accessing journal pages several states have been added as shown in Fig 6 The idea here is quite similar to the model fragment shown in Fig 5 and won t be discussed in detail Again we can see the pattern of a chain of states each emitting only the journal symbol with branches leaving the states to model behavior such as finishing the session A large proportion about 10 of all sessions was initiated by automated web crawlers Such sessions can be easily recognized and modelled as they first access the robots txt file describing the site s policy towards such visitors A further 5 of all sessions were initiated by RSS news readers primarily Google reader The final model is shown in Fig 7 Several other states have been added to the model in the above fashion This includes states relating to web interface to e mail or pages related to conferences organized at the Institute An interesting part of the model corresponds to the main webpage It beings in a state denoted main which emits the symbol index html It is interesting to see that only about 6 7 of all traffic enters through the main page After the main state the pattern of a cluster of connected nodes has been used to model accesses to various elements of the main page such as images CSS stylesheets and JavaScript code This pattern was used as the actual order of visits to this types of elements was deemed uninteresting but had to be modelled in order to make other interesting sequences visible Despite a fairly large number of states the model is quite easy to understand Essentially the model consists of several parts which follow the chain pattern described in the case of the Sophos antivirus Each begins with a state emitting only one symbol corresponding to a single directory on the Web server then proceeds with some or none intermediate steps which model behavior dependent on the number of accesses to the directory All such chains share a common _all_sink state which models any remaining activity and a common quit state ending the session An exception is the part of the model related to the top level webpage which contains a more complicated 123 204 S Jaroszewicz Fig 7 The final HMM describing website visitors behavior loopback behavior involving images cascading stylesheets and JavaScript This exception shows that automatically adding chains of states based on interesting symbols is not a good overall strategy 123 Using interesting sequences to interactively build Hidden Markov Models 205 It should also be stressed that each state in the model has been explicitly added by the user and thus its meaning and purpose are clear and well defined Also the list of emitted symbols in each state is usually short significantly improving understandability This is typically not the case for automatically built models as will be shown in Sect 10 where interactive and automatic approaches are compared The final model in Fig 7 predicts the probability of all possible input sequences with accuracy better than 0 01 We can thus say that either a sequence is modelled well or it appears very infrequently and is thus of little importance to the overall picture of visitors behavior Let us briefly comment on the efficiency of the proposed approach Since the special case of sequences starting at t 0 is considered frequent sequence mining in data and in HMM is very fast The computation time is almost completely dominated by the Baum Welch algorithm The algorithm can be quite slow but we discovered that when the emission probability matrix E provides enough information about the internal state of the model given the output symbol the algorithm requires few iterations This is the case in the presented application where many states may emit only a single symbol The model shown in Fig 7 converged in just five iterations The computation took a few minutes on a dataset of 100 000 log events using a Python implementation A more thorough performance analysis is given for the more computationally demanding task of mining sequences starting at an arbitrary point in time 8 Experimental evaluation protein secondary structure In this section we present an application of the algorithm for finding interesting sequences starting at an arbitrary time point to analysis of protein secondary structure Let us first give a brief description of the problem Proteins are long sequences of amino acids When a protein molecule is synthesized in a living cell amino acids are added to it one by one The overall structure of the protein molecule does not physically remain linear but folds in a very complicated way The shape of the molecule determines its chemical properties so predicting the structure of the protein molecule based on the sequence of amino acids from which it is built is a practically important problem An easier subproblem is predicting the so called secondary structure that is not the shape of the whole molecule just the shape of small local fragments This is the problem we will look at in this section More biological background can be found in Hunter 1993 A dataset from the UCI repository containing several amino acid sequences annotated with secondary structure of protein they encode at each given location was used in the experiments Overall there are 20 amino acids denoted with uppercase letters Additionally there are three high level types of secondary structure helix strand and coil denoted by letters h e and c respectively Each sequence element contains both the amino acid and the type of secondary structure present at this location The problem now is to discover relationships between sequences of aminoacids and secondary structure of the protein they encode In this section the proposed method of mining interesting sequences beginning at arbitrary time points is used to address this task The problem has received significant 123 206 S Jaroszewicz attention in literature Qian and Sejnowski 1988 Bouchaffra and Tan 2006 Asai et al 1993 and we do not aim at building a competitive secondary structure prediction system Rather we want to demonstrate the presented approach on real data The set of output symbols has been chosen to have 60 elements Each output symbol consists of two letters the first denoting one of the 20 amino acids and the second the type of structure at this location The information on the amino acid and the structure is thus encoded jointly in the symbol Another solution could be to use HMMs with multiple output symbols per state The initial HMM contained only a single state which could emit all possible symbols and had an edge to itself with transition probability 1 The minimum interestingness threshold of 0 0001 was used The first five most interesting sequences are given in the Table 1 Due to large number of symbols interestingness values are not high but nevertheless it is possible to see clear patterns In all cases the probability predicted by the HMM was lower than data count thus the values of interestingness are marked as positive An interpretation is suggested for each pattern Looking at all the patterns the hypothesis is that sequences of amino acids A K L tend to produce the helix structure h and sequences of amino acids A G and S the coil structure c Note that A is present in the rules for both helix and coil structures This is not a contradiction and means that it is simply less likely to produce the third type of structure strand To validate the hypothesis we also looked at most interesting sequences of length three shown in Table 2 They confirm the above findings and suggest further that D and K might also be related to the coil structure In order to update the model to reflect those findings three new states have been added state h 1 which emits only symbols Ah Kh Lh and whose meaning is that a helix is being generated by a sequence of As Ks and Ls state c1 which emits only symbols Ac and Kc This state denotes the process of generating a coil while adding a sequence of amino acids A and K Table 1 The first five most interesting sequences discovered from the protein structure data Sequence Ah Ah Gc Sc Ac Ac Ah Lh Kh Ah Interestingness 0 0029 0 0027 0 0027 0 0025 0 0025 Proposed interpretation A sequence of As is likely to produce a helix A sequence of Gs and Ss is likely to produce a coil A sequence of As is likely to produce a coil A sequence of As and Ls is likely to produce a helix A sequence of Ks and As is likely to produce a helix Table 2 The most interesting sequences of length three discovered from the protein structure data Sequence Ac Ac Ac Kc Ac Kc Dc Gc Sc Ah Ah Kh Kh Ah Ah Interestingness 0 0010 0 0008 0 0008 0 0008 0 0006 123 Using interesting sequences to interactively build Hidden Markov Models Table 3 The most interesting sequences found in the protein structure data after the model has been updated Sequence Lc Pc Ve Te Vh Ah Gc Tc Eh Lh 207 Interestingness 0 0024 0 0023 0 0022 0 0021 0 0021 state c2 which emits only symbols Gc Sc and Dc This state represents the process of generating a coil by a sequence of amino acids other than those covered by the previous state Transitions between all states are possible transition and emission probabilities are computed using the Baum Welch algorithm The reason for splitting the coil generating state is to treat amino acids A and K specially as they can generate two types of secondary structure The mining procedure was repeated with the new model The new most interesting sequences are given in Table 3 The following modifications have been made to accommodate them state c3 has been added which emits only symbols Lc and Pc state e1 is created which emits only symbols Ve and Te and whose meaning is that a strand is being generated In addition Vh and Eh were added to the list of symbols emitted in state h 1 In the next iteration we omit the details due to its similarity to the previous steps symbols Vc and Tc were added to the emission list of state c2 the symbol Sh to the list of emitted symbols of h 1 and Se to the list for state e1 After the above iterations we looked at interesting sequences of length three and discovered for example that the sequence Ac Ac Ac occurs more frequently then expected This suggests that assigning a single state c1 to both Ac and Kc was too rough an approximation It should be split into two states since long sequences of As tend to produce a coil with a slightly higher probability As the purpose of this section is to demonstrate the methodology we finished the modelling process at this stage There is of course a problem of verifying the validity of the constructed model Luckily the UCI dataset comes with a description of an imperfect domain knowledge so the discoveries made in this section could be compared against it It turned out that the background knowledge identifies sequences of all amino acids except for S whose symbols are emitted by h 1 with formation of the helix structure Similarly V T and L emitted by e1 are associated with forming a strand This corroborates our findings Unfortunately the rules for coils are not explicit in the background knowledge We conjecture however that the discovered sequences are not accidental and correspond to true patterns responsible for coil formation Notice also that no special meaning has been given by the provided background knowledge to long sequences of As generating a coil so the sequence Ac Ac Ac can be considered a novel discovery in that context 123 208 S Jaroszewicz Fig 8 Efficiency of mining interesting sequences Computation times versus minimum interestingness threshold 9 Experimental evaluation performance We will now evaluate the performance of the proposed algorithms We will concentrate on mining sequences starting at an arbitrary point in time as this problem is much more challenging computationally To test performance a large database had to be used To this purpose we downloaded raw DNA data from the DNA Data Bank of Japan We used the TSA dataset for our experiments 1 which contains 3 226 raw DNA sequences totalling 2 730 222 symbols The algorithm was implemented in Python and tested on a 1 7 GhZ Pentium machine The HMM had 10 hidden states transition and emission probabilities were calculated using the Baum Welch algorithm We will separately report computation times for the whole interesting sequence mining procedure including the Baum Welch algorithm and mining frequent sequences in data and for mining frequent patterns in the HMM as this is one of the main contributions of the paper and takes just a small fraction of the total computation time The HMM mining times include both the time of finding frequent sequences in the HMM and of counting support of sequences which were frequent only in data Step 5 Fig 1 We begin by reporting times for the first 100 sequences 58 503 symbols with varying minimum interestingness threshold The relatively small size of data was chosen such that we can experiment with a broad range of thresholds The results are shown in Fig 8 As can be expected both times grow fast when the minimum interestingness becomes small but the computations remain possible even for very low thresholds It is also clear that mining in the HMM takes only a small fraction of the total time so improvements in efficiency are possible by e g using a better algorithm for mining frequent sequences in data As already mentioned this is a well researched area which is beyond the scope of this paper 1 Available by anonymous FTP from ftp ftp ddbj nig ac jp ddbj_database ddbj ddbjtsa seq gz Retrieved on March 19 2009 123 Using interesting sequences to interactively build Hidden Markov Models 209 Fig 9 Efficiency of mining interesting sequences Computation times versus the size of the database We now analyze the performance of the algorithm with respect to the size of the data By size we mean the total length of all sequences in D The results are shown in Fig 9 In order to vary the size of the database we simply included a certain number of sequences occurring first thus the irregular numbers for the sizes in the axis labels It can clearly be seen that the method works in reasonable time even for very long sequences It is also clear that mining patterns in the HMM is very quick compared to the remaining steps The right part of Fig 9 clearly shows that the time of mining sequences in the HMM indeed does not depend on the length of sequences in D even though the definition of support in HMM does involve the database This is the result of the method of computation of s by diagonalizing the transition matrix We now investigate the performance of mining frequent patterns in the HMM with respect to the number of hidden states in the model These experiments were performed with very low minimum support threshold of 0 0001 Their results are shown in Fig 10 It can be seen that thousands of hidden states can be handled probably beyond the scope of interactive model construction The figure also compares with the brute force approach based on explicit multiplication of probability vectors by the transition matrix An extra optimization has been Fig 10 Computation time for mining frequent sequences in the HMM versus the number of hidden states in the HMM Timing is shown for the proposed diagonalization based approach and the brute force approach 123 210 S Jaroszewicz used of caching probability vectors for each t It can be seen that the time needed to mine frequent sequences in the model using the brute force method is much longer and for larger but still practical numbers of states would have a noticeable contribution to the total pattern mining time Fig 9 10 Comparison with automatic HMM construction In this section a thorough experimental comparison with automatic HMM construction will be presented Several aspects of both types of models such as complexity prediction accuracy and understandability will be analyzed By automatic HMM construction we mean creating a model with a specified number of states and random parameters and applying the Baum Welch algorithm to the model In order to recover the structure transitions which are assigned small probabilities are assumed to be absent The same rule is applied to emission probabilities The Baum Welch algorithm is assumed to have converged when the maximum difference between any of the probabilities in the model between successive iterations was 0 001 The algorithm was allowed to run until convergence occurred there was no limit on the number of iterations 10 1 A small artificial example We will begin by showing a small artificial example which will illustrate several issues and differences between automatic and interactive construction of HMMs We will start with a small HMM shown in Fig 11 generate a sequence database using that model and try to reconstruct the original using automatic and interactive approaches Despite its simplicity the model will be sufficient to demonstrate several interesting properties of both methods The HMM has three states and two output symbols a and b Emission probabilities of the symbols in each state are shown inside the ellipse depicting the state Two of the Fig 11 A small artificial HMM used in learning examples 0 5 a 0 8 b 0 2 1 0 5 a 0 8 b 0 2 0 5 0 5 a 0 2 b 0 8 0 5 0 5 123 Using interesting sequences to interactively build Hidden Markov Models 211 Fig 12 Artificial example automatically built HMM states are more likely to output the symbol a and the remaining one the symbol b The overall structure is such that the a symbols are more likely to be emitted in sequences whose length is even The relationship is of course not perfect but the behavior is easily detected by looking at output frequencies For example the probability of the sequence aa being emitted by the model at any time point is about 0 38 while for ab and ba it is about 0 22 and for bb about 0 18 In the first experiment an attempt was made to relearn the HMM from Fig 11 based on data simulated from it The training data consisting of 100 sequences each 1 000 symbols long have been generated from the original model The data thus consists of one hundred thousand symbols which should be sufficient to learn all the probabilities well The learned model is given in Fig 12 The model has been shown as the matrices of its parameters not as a graph since there is little structure in the learned model and the graph would be too cluttered Probabilities in transition and emission matrices may not add up to one due to rounding One immediately notices that the original structure has not been uncovered Contrary to the original model all possible edges are present in the HMM Also the transition and initial probabilities are quite different The emission matrix shows some similarity to the original HMM with two states much more likely to emit an a but the probabilities themselves are significantly different On the other hand it turns out that the model fits the data very well One way to check this is to mine interesting sequences in the learned model with respect to the data it was trained on The most interesting sequence turned out to be baaab with interestingness of 0 0062 So the probabilities of all sequences in data and in the learned model differ by at most this relatively small value Another way is to compare the likelihoods of the original and relearned models given the training data The likelihood of the original model was 0 123 212 S Jaroszewicz true underlying process external knowledge in the approach proposed in this paper human intelligence and understanding of the analyzed domain must in general be included in the learning process Another problem is that of convergence the Baum Welch algorithm is only guaranteed to converge to a local minimum and the convergence can be quite slow It took 153 iterations to learn the three state model described above which is quite long compared to other models analyzed in this section Problems arising from the local optimality of solutions will be discussed below Since in real situations the underlying model is not known it is difficult to decide how well the model fits the data just based on its likelihood Interesting patterns present a viable alternative here as the value of interestingness is easy to interpret Let us now try to discover the structure using the interactive procedure proposed in this paper We begin with a model shown in Fig 13a The model has two states each emitting one of the symbols all transitions between states are possible After setting the parameters to random initial values and applying the Baum Welch algorithm model weights are obtained We now find interesting sequences in the training data with respect to the model of which five most interesting ones are shown in Table 4 Signed values of interestingness are shown to facilitate interpretation The meaning of discovered patterns is not immediately clear but after a careful examination comparing the first and fourth pattern shows that the predicted probability of the sequence aa is too low and that of the sequence a too high This suggests that modelling sequences with even number of as needs special attention Other patterns confirm this as patterns with an even number of as have their probability predicted to be too low and patterns with an odd number of as too high All two element sequences mined from the model in Fig 13a had very low interestingness Fig 13 Models used in the interactive HMM construction procedure a a b b Table 4 Five most interesting sequences found in data generated from the model in Fig 11 with respect to the model in Fig 13a Sequence baab aab aaa bab baa I Pr D PrHMM 0 013 0 010 0 010 0 010 0 010 Pr D 0 064 0 149 0 230 0 071 0 149 PrHMM 0 051 0 139 0 240 0 081 0 139 123 Using interesting sequences to interactively build Hidden Markov Models 213 An obvious way to fix the initial model is shown in Fig 13b Additional two states have been added in order to explicitly model the cases when a occurs an even number times The interestingness of the most interesting pattern was 0 0014 comparable to the change in the values of modelled parameters used as the stopping criterion for Baum Welch algorithm It is thus reasonable to assume that the model explains the data sufficiently well While this HMM has more states than the original one it has a much more understandable structure Moreover each state emits only one symbol which is good for convergence of the Baum Welch algorithm only 35 iterations were necessary The likelihood of the model given training data was 0 123 214 Fig 14 Automatically built model with state output symbols fixed S Jaroszewicz a b a a to uncover the true structure especially since there may be many possible structures but in any case the nature of the incorrectly modelled aspect of the data something causes pairs of as to occur more frequently one after another in the example above is much easier to see from the patterns Also the patterns may give the user a hint on how to set the initial emission probabilities thus helping an automatic algorithm to discover better model parameters The method also provides an indicator of the quality of the model often more practical than the value of model s likelihood 10 2 Web log and protein secondary structure examples continued Let us now compare the final interactively built model for the Web log data given in Fig 7 with models built automatically An arbitrary number of 20 hidden states has been picked for the HMM and the Baum Welch algorithm has been used to compute the transition and emission probabilities Figure 15 shows the resulting HMM retaining only edges corresponding to transition probabilities 0 01 Figure 16 displays the same automaton with all edges corresponding to transition probabilities 0 001 Each node contains the symbols emitted by the corresponding state for clarity only symbols with emission probabilities 0 01 are shown Note that this is in contrast with user built models where state labels were assigned by the user based on each state s intended meaning Looking at Fig 15 it can be seen that the model s structure is not a good description of users behavior In the upper part of the picture there is a connected group of nodes related to the web based e mail interface but they are not connected to the rest of the graph Moreover nodes related to e mail are also present in the large cluster of nodes below The Sophos antivirus nodes are present and the discovered patterns could be inferred from their transition probabilities albeit with some effort Other discovered patterns are not clearly visible and it would be very hard to infer them from transition probabilities Adding more edges to the picture as seen in Fig 16 does not help On the contrary it makes the automaton practically incomprehensible It should also be noted that the Baum Welch algorithm on the hand built model converged much faster than in the case of an automatically built model The automatic case usually required about a 100 iterations This is another this time real life example of the above described phenomenon of negative influence of ambiguity in emitted symbols on the convergence and efficiency of the Baum Welch algorithm 123 Using interesting sequences to interactively build Hidden Markov Models 215 Fig 15 Automatically build HMM for the weblog example Only edges corresponding to probabilities 0 01 are shown We will now give a more principled comparison of the complexity and quality of automatically and interactively built models To this end we need to define precise measures of model quality and complexity It is tempting to measure the quality of a model using the value of its likelihood given the data This approach however turns out not to be suitable for the problem at hand The reason is that in the interactively built models several nodes are left unconnected corresponding to zero transition probabilities If any such transition occurs in the data model s likelihood will be simply zero We have thus assessed model s quality based on its prediction accuracy i e how well it predicts the next symbol in a sequence based on symbols preceding it More formally model s prediction accuracy on a given database is defined as d D Pr HMM so Pr HMM s d D Acc D HMM so is a prefix of d d 1 that is the average of the probabilities of each symbol in data conditional on the sequence preceding it If the HMM predicts a zero probability for s an arbitrary choice of 0 is made for the predicted conditional probability that is the next symbol is assumed to be predicted incorrectly 123 216 S Jaroszewicz Fig 16 Automatically build HMM for the weblog example Edges corresponding to probabilities 0 001 are shown Model s complexity is defined simply as the number of nonzero parameters in the model that is the total number of nonzero initial transition and emission probabilities 0 0 i j Pi j 0 i j Ei j 0 Compl HMM i i Of course the original automatically built model is likely to have very high complexity In order to make the comparison meaningful we must be able to control the complexity of automatically built models To that end we use a postprocessing step in which a given percentage of lowest probabilities in the model are set to zero The final interactively built model in Fig 7 has 36 states so for the current experiment an automatically built model with 36 states has been used for comparison As in the previous cases the automatically built model has been learned using the BaumWelch algorithm starting with randomly chosen parameter values In order to help assess the models future accuracy the data has been split into training and test sets The training set was used to build the model and the test set to assess its accuracy Figure 17 shows the complexity vs accuracy tradeoff for automatically built model for the weblog data The final interactively built model from Fig 7 is marked on the 123 Using interesting sequences to interactively build Hidden Markov Models 217 Fig 17 The tradeoff between complexity and accuracy of the automatically built HMM for the Web log mining example The black dot shows the complexity and accuracy of the interactively built model Fig 18 The tradeoff between complexity and accuracy of the automatically built HMM for the protein secondary structure example The black dot shows the complexity and accuracy of the interactively built model figure with a black dot An analogous experiment has been conducted for the protein secondary Structure example the results are shown in Fig 18 In the automatically built models many probabilities turned out to be close to zero so even a tiny probability threshold significantly reduces model complexity with little effect on accuracy For this reason the figures show complexity only up to 500 and 200 respectively giving a more detailed picture in the interesting region Low overall accuracy for the protein example is the result of a large number of symbols 60 which makes prediction difficult Interactively built models have overall worse accuracy than complete automatically built ones In principle nothing prevents the user from continuing with interactive updates to the model until the accuracy increases further However reaching the accuracy of full automatically built models would be quite laborious and would probably defy the purpose of interactive modelling since very small divergences between the model and data would be hard to explain and the main appeal of the proposed method lies in the fact that all elements are included in the model for a well understood reason Automatically built HMMs do not possess this advantage It can also be seen that for a given complexity interactively built models were more accurate This is especially true in the case of protein data For the Web log 123 218 S Jaroszewicz data the difference is less visible but interactive models still performed better It can also be seen that the difference between interactively and automatically built HMMs is slightly larger on the test set which suggests that human explanations generalize better Notice also that low complexity does not mean understandability Recall Fig 15 The HMM shown in this figure has complexity of only 139 yet as discussed above it conveys very little information about the internal structure of the process 11 Conclusions An approach to interactive construction of global HMMs based on local patterns has been presented Two cases most important from the practical point of view have been discussed mining interesting sequences starting at the beginning of database sequences and mining interesting sequences starting at arbitrary location have been addressed Experiments have shown that the proposed methodology is capable of producing highly understandable yet accurate models In the experiments interactively built HMMs achieved accuracy equal to or better than automatic models of comparable complexity Moreover interactively built models were much more understandable because they were built by a human and every new state was added with a clear intention of its function Additionally human added model states had a limited number of emitted symbols and different states emitted disjoint sets of symbols which further improved model s clarity In contrast automatically built models were hard to interpret typically states emitted large numbers of symbols and their sets of emitted symbols overlapped Experimental evidence has been given that automatic HMM weight learning often fails to discover the internal structure of even very simple models This turns out to be inevitable as it is possible for several HMMs to generate the exact same stochastic process In view of this requiring an analyst to do the actual updating while being guided by the interesting patterns seems to be the correct approach if models with meaningful internal structure are required A disadvantage of the approach is that the process requires manual labor and that considerable effort may be required to understand the causes of interesting patterns However since automatic procedures cannot in general be guaranteed to discover meaningful internal structure such an approach seems indeed unavoidable if information is to be gleaned on the underlying behavior of the process which is generating the data Efficiency of the proposed approach is very good in the presented experiments even very large datasets have been successfully analyzed This is especially true for the step of the proposed algorithms in which frequent sequences in HMMs are mined The proposed approach based on diagonalization of the transition matrix makes the computation time of this step negligible The simple brute force approach based on repeated matrix multiplications has been shown to perform much worse and to have running time which may have a tangible contribution to the running time of the whole interesting pattern discovery process 123 Using interesting sequences to interactively build Hidden Markov Models 219 It should also be noted that parameter learning using the Baum Welch algorithm is much easier for interactively constructed models The convergence is much faster tens instead of hundreds of iterations and there is much less of a chance of ending up in a local minimum This has been conjectured and confirmed experimentally to result from the fact that output symbols in interactively built models depend on the internal state in a less ambiguous fashion Moreover it is sometimes possible to use the interesting patterns to set some of the initial model parameters e g assign initial emission probabilities to states such that a local minimum can be avoided References Agrawal R Imielinski T Swami A 1993 Mining association rules between sets of items in large databases In ACM SIGMOD conference on management of data pp 123 220 S Jaroszewicz Low Kam C Mas A Teisseire M 2009 Mining for unexpected sequential patterns given a Markov model http www math univ montp2 fr mas lmt_siam09 pdf Meyer C 2001 Matrix analysis and applied linear algebra book and solutions manual SIAM Philadelphia Prum B Rodolphe F de Turckheim E 1995 Finding words with unexpected frequencies in DNA sequences J R Stat Soc Ser B 57 123 Reproduced with permission of the copyright owner Further reproduction prohibited without permission 