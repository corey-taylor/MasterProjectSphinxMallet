Using Open Source Automatic Speech Recognition for Conceptual Knowledge Monitoring Ruben Lagatiea Fridolin Wildb Patrick De Causmaeckera Piet Desmeta Peter Scottb KULeuven Kulak Etienne Sabbelaan 52 Kortrijk 8500 Belgium The Open University Walton Hall Milton Keynes MK7 6AA United Kingdom Abstract Useful in many applications text mining techniques offer the possibility to analyze texts and discover their semantic meaning Unfortunately most of these methods only work on textual data but they are equally useful when dealing with spoken conversations In this contribution we show how an open source and freely available Automatic Speech Recognition engine can be used out of the box as a preprocessing step for keyword spotting by first transcribing spoken conversations Can lower quality transcriptions still prove useful for keyword extraction a b 1 Introduction Online video conferences provide an excellent opportunity for students to interact with each other and work together on a project being it to improve their language skills writing a paper or designing a software application One of the downsides of this interaction however is that it is still very hard to provide users with relevant feedback Teachers using these tools often replay all recorded meetings and manually grade and provide feedback obviously a time consuming task Moreover in this case immediate feedback would be more useful than delayed feedback Playback of these online events is also important and to assist the user in finding the relevant parts in the vast amount of digital content available to them we need to provide and automatically construct a catalog showing how they are related to each other For both of these tasks techniques exist that provide this functionality such as keyword extraction text categorization text clustering latent semantic analysis and many more which can be summarized as text mining techniques Unfortunately most of these techniques only work on textual data An obvious solution is to introduce a preprocessing step to transcribe the spoken text Transcribing speaker and domain independent free speech however is still a daunting task and although many recent advances have improved recognition accuracy it is still far from perfect Luckily for many language processing algorithms an imperfect transcript may be sufficient A good example is keyword spotting Even with a word error rate WER as high as 0 6 the chance of not recognizing any of three subsequent words is only 0 216 WER3 As a result we might not need a stateof the art commercial Automatic Speech Recognition ASR system The objective of this study is therefore to evaluate whether freely available and possibly open source software is mature enough to be used out of the box for keyword spotting It is important that we test this theory on real life preexisting recordings to evaluate the robustness of these systems against low quality data It is well known that while ASR works well in a controlled environment it is still unable to produce a high quality transcription of conversational speech 1 In Lagatie et al 2 we showed how keywords generated by this application can be used as feedback for students and as a grading tool for teachers for online spoken conversations by visualizing them as conceptual diagrams In this contribution however we focus on the more technical aspects of this endeavor 2 Related work Back in 1998 Ehsani and Knodt 3 investigated the applicability of ASR in Computer Assisted Language Learning CALL They stated not only that speech technology is an essential component of CALL but that it is in fact ready to be deployed successfully in second language education provided that the current limitations of the technology are understood and systems are designed in ways that work around these limitations They focus however on the use of ASR in very confined situations where the vocabulary and grammar can be easily predicted In our case the conversations are completely open there is no right or wrong answer We are not dealing with evaluating vocabulary or grammar use but the conceptual knowledge of the students Detecting conceptual knowledge means being able to detect the key concepts and their relations In recent years a number of different techniques have been proposed and implemented for automated essay scoring An overview is given by Rudner et al 4 Hearst 5 Valenti et al 6 and Dikli 7 These techniques all require textual data so in this paper we are investigating if and how ASR can be used as a preprocessing step for this Algorithms exists specifically developed to perform keyword spotting Most of these algorithms look for a few predefined keywords for instance when doing topic spotting Some contributions therefore construct a finite state grammar that contains the keywords to be detected and include a garbage state for all utterances that were not confidently recognized as part of the grammar 8 Others do not limit their vocabulary and use Large Vocabulary Continuous Speech Recognition LVCSR but look for keywords in the lattice afterwards 9 When the possible keywords are not limited and performance is less important we believe the best method is to simply transcribe the text and then use a keyword extraction algorithm on this text afterwards A similar approach was used by Roy and Subramaniam 10 to construct a domain model for call centers They showed that even from noisy transcriptions useful information can be extracted They however used a state of the art ASR system and their taxonomy generation is more than simply finding keywords So the question still remains is an open source ASR system sufficient for keyword spotting 3 Technologies We will be using three different technologies online conferencing speech recognition and keyword extraction It is important we test our theory on real life data so we will be using recordings made by the webbased conferencing tool Flashmeeting developed at the Open University These recordings can then be transcribed using speech recognition There are a number of possible open source ASR engines available From these generated transcriptions we can then extract keywords describing the topic of the conversation Again a number of possible algorithms can be used The main criteria here is robustness because errors in the transcriptions are inevitable The results are then passed back to Flashmeeting to be visualized In the remainder of this section we will give a short overview of the technologies we will use in this paper 3 1 Flashmeeting Flashmeeting is an academic research project aimed at understanding the nature of online events and helping users to meet and work more effectively 1 It records all meetings for research purposes and allows users to replay past public meetings Flashmeeting is often used for group discussions by students working on a joint project such as a paper an essay or a software application Flashmeeting is also auto moderated only one speaker is able to broadcast at a time Each speaker is recorded in a separate file and some metadata is kept in a database such as real name timestamp or duration 1 http flashmeeting open ac uk Next to broadcasting video and audio users can also send text messages using the chat This is often used whenever a participant wants to add something to the conversation but does not want to interrupt the current speaker It has also proven to be very useful for sending hyperlinks and files Using the techniques described here Flashmeeting is now able to present students and teachers a conceptual graph showing relationships between meetings based on common keywords In Lagatie et al 1 we showed how these graphs can serve as automatically generated additional feedback for students and help teachers in evaluating online spoken conversations 3 2 Open Source Automatic Speech Recognition There are three popular open source automatic speech recognition engines HTK Hidden Markov Model Toolkit CMU Sphinx and Julius HTK has many state of the art ASR features such as vocal tract length normalization VTLN heteroscedastic linear discriminant analysis HLDA and discriminative training with maximum mutual information MMI and minimum phone error MPE criteria 11 The Sphinx Toolkit released by Carnegie Mellon University CMU contains less advanced features but in contrast to HTK Sphinx focuses on speed and was one of the first ASR systems to offer support for speaker independent Large Vocabulary Continuous Speech Recognition LVCSR 12 There are currently four versions The latest Sphinx 4 is although less efficient than Sphinx 3 more flexible and it is therefore easier to develop and maintain applications with 13 Julius is a high performance two pass LVCSR decoder for researchers and developers 14 Julius focuses on three factors performance modularity and availability Unfortunately it is at the time of writing only distributed with Japanese models VoxForge offers an English acoustic model for Julius but they have yet to create a dialog manager to transcribe speech2 When comparing these engines there is no real winner Accuracy and performance are very similar especially for LVCSR As a result the choice is often based on the available models environmental factors the programmers preference and the development time For all these reasons and because Sphinx is easier to configure we opted to use Sphinx 4 for our test In section 4 we elaborate on the use and setup of this system 3 3 Keyword extraction We use the term keyword here as a synonym of descriptor It is contextual metadata a word or phrase that describes the topic of a text In our case any word can be a keyword except stop words such as the and a There are again a number of available techniques to extract keywords from a text Zhang et al 15 defines 4 categories simple statistical approaches linguistic approaches machine learning approaches and hybrid approaches A similar technique is keyword suggestion or recommender systems Instead of simply extracting keywords from a given text it suggest related concepts which might also be interesting for the user A survey of such systems is given by Adomavicius and Tuzhilin 16 Zemanta3 is a good example of such a keyword suggestion engine In Bosnic et al 17 Zemanta is categorized as a semantic entity extraction service The authors compared Zemanta with The Yahoo Term Extraction web service for recommending learning objects Zemanta uses semantic relationships to find related keywords As a result it is more robust than a simple term extraction engine Which natural language processing NLP techniques Zemanta uses is unknown Being a recommender system Zemanta not only finds words in a text it also suggests keywords which might even be missing from the test In our case for example the words Dublin Core were never correctly recognized in one of our meetings but because words like metadata LOM and application profile were the engine suggested Dublin Core as one of the keywords 2 3 http www voxforge org home downloads http www zemanta com 4 Keyword Spotting We perform keyword spotting in two separate steps We first transcribe the meetings and then extract the keywords that are their likeliest descriptors 4 1 Transcribing As already mentioned one benefit of keyword spotting is that there is no need for a perfect recognition like in dictation applications In our case precision is more important than recall so we can ignore words that have a low confidence score If it is a keyword chances are that it is pronounced multiple times by different persons in the same conversation increasing the chance of being correctly recognized at least once Writing an application using the Sphinx library that produces transcripts has proven to be relatively easy The API is well documented and the example applications are a good starting point In Flashmeeting each speaker is recorded individually so we also transcribe each segment individually 4 2 Keyword suggestion From these individual transcripts relevant keywords are then extracted by running them through Zemanta Doing this for each segment separately gives us more keywords which we can then filter using what is known as semantic co occurrence filtering 18 By doing so keywords that are not related to the other keywords are deleted because they are most likely wrong or not really relevant Extracting keywords per segment can also be useful if we want to compare individuals to suggest possible conversation partners It also enables us to give an overview of the interests of a single person and even show its evolution over time Zemanta offers a Java library to communicate with their web service making it easy to integrate it in our Java application which already uses the Sphinx library The output of this step is a list of keywords for each segment and their relevance scores 5 Sphinx Configuration The recognizer of Sphinx 4 comprises three components the linguist the decoder and the frontend They are all summarized in Walker et al 13 Because we have constructed our own model we will give a short overview of which configuration we used in our setup 5 1 Linguist Sphinx offers multiple implementations of the linguist component depending on the task at hand One of them is the LexTreeLinguist which is appropriate for large vocabulary recognition tasks that use large n gram language models 13 Using LVCSR in an ASR system requires a number of linguistic resources First a dictionary is needed with all words to be recognized and their pronunciation Second the system requires two models a so called acoustic model and a language model 5 1 1 Dictionary The domain and vocabulary of the recorded meetings vary and they contain many words that are not present in the English dictionary As a result the dictionary needs to be extended with these words Unfortunately we do not have enough transcripts to extract the words that have actually been said but there is an extensive amount of data available from the text chat The recorded meetings are not necessarily English spoken so foreign languages should be removed from the text chat as well For this we use a classifier trained on part of the Leipzig Corpora Collection implemented using the LingPipe library To increase accuracy we discard sentences with less than 70 characters Some filtering is also necessary to remove emotional abbreviations e g LOL ROFL and emoticons Chat data also contains a lot of typos so each word occurring infrequently less than 7 times is checked against the default dictionary If it is present there we know it is not a typo and can be added to our dictionary After all this filtering we retained 65 204 sentences Containing 9 790 words of which 1 232 were absent from the default dictionary CMU dict4 In our case we don t need a perfect transcription so we opted not to extend the default dictionary but simply use the dictionary we constructed This means that some words will have a low acoustic score because the correct word does not exist in the dictionary nor does one that sounds alike These words will be filtered out before keyword extraction A limited vocabulary increases speed and many of the words in the default dictionary are rarely used anyway 5 1 2 Acoustic model The acoustic model of Sphinx is implemented as a phonetic Hidden Markov Model HMM 12 Such a model describes each phone as a graph with probabilities assigned to the transitions This model thus assigns sounds to each phone used in the dictionary It is typically trained with a corpus of audio recordings and their text transcriptions Flashmeeting has been in use since 2003 At that time a tradeoff was made between quality and network load and as a result all audio has been encoded at 11kHz frequency It is known that recognition performance degrades severely when the input was recorded at a different sampling frequency than the speech signals the system was trained on 19 Unfortunately we do not have enough transcribed audio to train our own 11kHz model and we were unable to find an open source one so we have decided to downsample our input files to 8kHz and use the 8kHz Wall Street Journal WSJ acoustic model This approach was also used in 20 The WSJ model is implemented in Sphinx as a TiedStateAcousticModel 5 1 3 Language model The language model we use is a LargeTrigramModel which offers support for true n gram models An n gram describes the probability of a sequence of n words Using this we can more confidently select which word is more probable to have been pronounced Note that the trigram model of sphinx contains unigrams bigrams and trigrams The default language model just as the dictionary is inappropriate The bigram Dublin core would for instance have a very low probability or even be missing from the language model As a result we had to train a new language model again using the data available from the text chat A script was written to filter the corpus file as described above construct the vocabulary and a trigram and then convert the model to the correct binary format required by CMU Sphinx Training the model and constructing the dictionary was done using the CMU Statistical Language Modeling SLM Toolkit 21 5 2 Frontend The frontend is in charge of transforming an input signal to a sequence of features We use the default settings here so we won t deal with these settings in detail 5 3 Decoder The decoder forms a bridge between the frontend and the linguist It uses features provided by the frontend in conjunction with the searchgraph from the linguist to generate result hypotheses The main component of the decoder is the searchmanager which implements a certain search algorithm to find features In our case we opted to use the WordPruningBreadthFirstSearchManager which performs a frame synchronous Viterbi search as explained in 22 4 http www speech cs cmu edu cgi bin cmudict 6 Fine tuning It is often said that tuning Sphinx is somewhat of a black art5 There over a dozen variables that can be changed often related to each other Most of these allow developers to prioritize speed or accuracy Two of these parameters are highly dependent on the language model The first is the weight of the language model score compared to the acoustic score when calculating the total score The second is the weight of unigrams compared to bi and trigrams in the language model As a preliminary study we will limit fine tuning to these two parameters but want to automate the process to a certain extent In future work we will take more parameters into account and use more data The resulting accuracy is highly dependent on the length of the samples the topic the speakers the audio quality and many more factors To counter this effect we took a random set of speakers for a random set of meetings We made sure the segments contained an equal distribution of audio quality length and speaker gender In total we used 42 minutes with an average of 7 minutes and four speakers per meeting These conversations where then manually transcribed To fine tune the two parameters we decided not to use the Word Error Rate WER as is used often in evaluating dictation systems When fine tuning it is not so important to us whether or not the transcription is correct we just want meaningful keywords For this reason we compare the list of keywords constructed for our manual transcription to the keywords found for the transcription provided by the system We use the same keyword suggestion algorithm for both transcriptions which means that if the automatic transcription is perfect the same keywords will be found Using the F1 score we can compare both sets of keywords Our goal for this study is then to set the language and unigram weight so that the F1 score is as high as possible There is no clear correlation between the weights and the resulting score A quick test showed that changing a weight resulted in unpredictable changes to the accuracy To find the optimal setting we could use a Random Search RS method However these algorithms can take a long time to converge and automatically transcribing a meeting takes about the same time as its duration so 42 minutes We therefore decided to perform a preliminary test and try to visualize how the F1 is influenced by the two values In figure 1 a we plotted the F1 score for all integer language weights between 0 and 15 From this we concluded that the highest language weight was reached between 8 and 9 so in figure 1 b we plotted the F1 score for all language weights between 8 and 9 with increments of 0 1 The best result is reached when the language weight is set to 8 5 but the unigram weight is still left undetermined all we can say is that it should be between 0 2 and 0 8 To better determine this value we looked at the average f score of the keywords found for each speaker segment separately instead of those for each complete meeting Optimizing this would result in a better handling of individual users and shorter audio fragments The results are depicted in figure 2 With a language weight of 8 5 the best results are achieved when using a unigram weight of 0 4 or 0 5 a Figure 1 b 5 http www speech cs cmu edu sphinx models Figure 2 7 Evaluation In the fine tuning process which can be seen as a preliminary effectiveness evaluation we achieved an average F1 score of 87 5 The evaluation of our application could now be done the same way we finetuned the system simply take a random set of meetings manually transcribe them an see what f score is achieved The downside of this is that although the f score in this case depicts the quality of the transcriptions it says little about the quality of the keywords The found keywords may be different but therefore not necessarily bad If a number of words are not correctly recognized other keywords might be suggested but they can be just as useful to the user So in our evaluation we would like the usefulness of the keywords to the user be the quality indicator Furthermore we want to know which keywords are redundant or missing and how we can configure the system do deal with these problems Obviously there is no formula to calculate this value automatically so we are planning a user study where we will present keywords to the participants of a meeting and ask them specific questions to evaluate these keywords The results and setup of this user study do not fit into this more technical paper and will therefore be described in future work 8 Conclusions In this contribution we have shown how two freely available tools such as the open source ASR system Sphinx and the online semantic entity extraction service Zemanta can be combined to perform keyword spotting for online spoken conversations Preliminary results show that after some fine tuning an average F1 score of 87 5 is achieved In future work we will further fine tune and evaluate this system We are currently working on putting this topic spotting application into the backend of Flashmeeting so that we can collect more evaluation data about how users like the keywords At the moment we process the information on a separate server at night to minimize interference with the online conversations A future approach could be to do this analysis in real time which would allow for more interaction References 1 K Myers and S Singh A Boosting Approach to Topic Spotting on Subdialogues In proceedings of ICML 00 17th International Conference on Machine Learning 2000 2 R Lagatie F Wild P De Causmaecker P Scott Exposing Knowledge in Speech Monitoring Conceptual Development in Spoken Conversation IST Africa 2010 Conference Proceedings 2010 3 F Ehsani and E Knodt Speech technology in computer aided language learning Strengths and limitations of a new CALL paradigm Language Learning and Technology 2 1 45 60 1998 4 L Rudner P Gagne E C on Assessment and Evaluation An overview of three approaches to scoring written essays by computer ERIC Clearinghouse on Assessment and Evaluation 2001 5 M Hearst The debate on automated essay grading Intelligent Systems and their Applications IEEE 15 5 22 37 2002 6 S Valenti F Neri and A Cucchiarelli An overview of current research on automated essay grading Journal of Information Technology Education 2 319330 3 118 2003 7 S Dikli An overview of automated scoring of essays Journal of Technology Learning and Assessment 5 1 2006 12 2006 8 I 