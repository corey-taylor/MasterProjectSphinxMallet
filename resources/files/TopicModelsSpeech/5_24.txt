USEFUL TRANSCRIPTIONS OF WEBCAST LECTURES Cosmin Munteanu A thesis submitted in conformity with the requirements for the degree of Doctor of Philosophy Department of Computer Science University of Toronto Copyright c 2009 by Cosmin Munteanu Abstract Webcasts are an emerging technology enabled by the expanding availability and capacity of the World Wide Web This has led to an increase in the number of lectures and academic presentations being broadcast over the Internet Ideally repositories of such webcasts would be used in the same manner as libraries users could search for retrieve or browse through textual information However one major obstacle prevents webcast archives from becoming the digital equivalent of traditional libraries information is mainly transmitted and stored in spoken form Despite voice being currently present in all webcasts users do not benefit from it beyond simple playback My goal has been to exploit this information rich resource and improve webcast users experience in browsing and searching for specific information I achieve this by combining research in Human Computer Interaction and Automatic Speech Recognition that would ultimately see text transcripts of lectures being integrated into webcast archives In this dissertation I show that the usefulness of automatically generated transcripts of webcast lectures can be improved by speech recognition techniques specifically addressed at increasing the accuracy of webcast transcriptions and the development of an interactive collaborative interface iii iv that facilitates users contributions to machine generated transcripts I first investigate the user needs for transcription accuracy in webcast archives and show that users performance and transcript quality perception is affected by the Word Error Rate WER A WER equal to or less than 25 is acceptable for use in webcast archives As current Automatic Speech Recognition ASR systems can only deliver in realistic lecture conditions WERs of around 45 50 I propose and evaluate a webcast system extension that engages users to collaborate in a wiki manner on editing imperfect ASR transcripts My research on ASR focuses on reducing the WER for lectures by making use of available external knowledge sources such as documents on the World Wide Web and lecture slides to better model the conversational and the topic specific styles of lectures I show that this approach results in relative WER reductions of 11 Further ASR improvements are proposed that combine the research on language modelling with aspects of collaborative transcript editing Extracting information about the most frequent ASR errors from user edited partial transcripts and attempting to correct such errors when they occur in the remaining transcripts can lead to an additional 10 to 18 relative reduction in lecture WER Contents List of Figures List of Tables 1 Introduction 1 1 1 2 1 3 1 4 1 5 Overview Motivation Statement of Thesis Contributions Structure of the Dissertation 1 5 1 1 5 2 1 5 3 1 5 4 Research Design Outline of the Proposed Research xiii xix 1 2 3 6 7 8 8 9 Outline of the Surveyed Related Work 10 Peer Reviewed Publications 11 13 2 Webcasting and Automatic Speech Recognition 2 1 Lecture Webcasting 14 2 1 1 2 1 2 An Overview of Webcasting 15 Research Challenges 17 v vi 2 1 3 CONTENTS Integrating Text with Webcast Media 20 2 2 Automatic Speech Recognition for Webcast Lectures 21 2 2 1 An Overview of the Automatic Speech Recognition Process 21 2 2 2 2 2 3 Improving Automatic Speech Recognition 24 Research Challenges 26 3 The Acceptable Word Error Rate of Machine Generated Webcast Transcripts 29 3 1 Related Work 33 3 2 Research Questions and Hypotheses 36 3 3 Methods 37 3 3 1 3 3 2 3 3 3 3 3 4 3 3 5 3 3 6 3 3 7 3 3 8 3 3 9 Overview 37 Independent Variables 37 Task 38 Measures and Instruments 38 System 42 Acoustic Models 45 Language Models 45 Lexicon 46 Recordings 47 3 3 10 Recognition 47 3 3 11 Experimental Design 48 3 3 12 Participants 49 3 3 13 Procedures 49 3 3 14 Data Analysis 50 CONTENTS 3 4 vii Results Task Performance 51 3 4 1 3 4 2 Performance Quality Hypothesis 51 Performance Quality Hypothesis Breakdown by Quiz Question Type 54 3 4 3 Performance Quality Hypothesis Breakdown by Demographic Information 56 3 5 Results User Perception 60 3 5 1 3 6 3 7 Experience Quality Hypothesis 61 Limitations and Generalizations 67 Summary and Discussion 68 4 Improving Automatic Speech Recognition for Webcast Lectures 4 1 Existing Research on Adapting and Building Language Models for Dedicated Domains 73 4 2 4 3 Previous Work on Automatic Speech Recognition for Lectures Web Based Language Modelling for Automatic Lecture Transcription 82 4 3 1 4 3 2 4 3 3 4 4 General Algorithm 83 Corpora Adjustment 84 LM and ASR Scope Alternatives 85 78 71 Empirical Evaluation 87 4 4 1 4 4 2 4 4 3 4 4 4 Test Data 88 Web Based Lecture Models 89 Baseline Models 89 Results WER Reduction 91 viii 4 4 5 CONTENTS Results Precision Recall of Keywords 93 4 5 Limitations and Generalizations 95 4 6 Summary and Discussion 97 5 Wiki editing of Webcast Transcripts 99 5 1 Related Research 101 5 2 Enhancing Webcasts with Transcripts 103 5 3 Managing Imperfect Transcripts 103 5 3 1 Features of the Transcript Edit Tool 105 5 4 A Field Study 106 5 4 1 Research Objectives 106 5 5 Methods 107 5 5 1 5 5 2 5 5 3 5 5 4 System 108 Task and Procedures 109 Participants 110 Instruments and Measures 110 5 6 Results 115 5 6 1 5 6 2 5 6 3 5 6 4 Task completion 115 User Experience 117 Users Involvement and Motivation 122 General User Feedback 123 5 7 Interface Re design and Re evaluation 124 5 7 1 5 7 2 5 7 3 Assessment of Current Design 124 Extended Editing Mode 125 Evaluation of the Re designed System 127 5 8 Limitations and Generalizations 131 CONTENTS 5 9 ix Summary and Discussion 131 Speech Recognition for Webcast Lectures 133 6 Automatic Learning from Wiki enabled Transcript Corrections 6 1 6 2 6 3 Related Work 136 Transformation Based Learning 140 Minimally Trained Transformation Based Learning for Webcast Transcription 142 6 3 1 6 3 2 6 3 3 6 3 4 6 4 TBL Algorithm for ASR Output Correction 143 Rule Discovery for Lecture Transcripts 143 Scoring Function for the TBL rules 145 Rule Application 150 Empirical Evaluation 151 6 4 1 6 4 2 6 4 3 6 4 4 6 4 5 Evaluation Data 151 Language Models 153 Training and Test Data Partitioning 154 Scoring Functions 155 Results 156 6 5 6 6 Limitations and Generalizations 159 Summary and Discussion 164 167 7 Contributions Conclusions and Future Work 7 1 Contributions How Good is Good Enough and What to Do When It Isn t 168 7 1 1 ASR how good is good enough 168 x 7 1 2 CONTENTS What to do when ASR is not good Bibliography A Abbreviations B Glossary of Technical Terms 181 203 205 CONTENTS C Instruments Used in Experiments xi 209 xii CONTENTS List of Figures 2 1 2 2 2 3 2 4 The ePresence system displaying an archived lecture webcast 16 The display pane of the MIT lecture browser 19 The speech recognition process 21 An overview of the automatic speech recognition process with the different parts of the process grouped under three levels the recognition level the evaluation side and the resource level 24 3 1 3 2 3 3 The transcript enhanced ePresence system 30 The histogram of the relative quiz scores for WER 25 57 The histogram of the relative quiz scores for WER 25 only for subjects that haven t used the system or a similar one before 58 4 1 The algorithm for using the lecture slides and the World Wide Web to build corpora on which lecture specific LMs are trained 84 4 2 Examples of slide bullets used as web queries apparently un related examples are from the These same lecture Conceptual Design of the third year Computer Science undergraduate course The Design of Interactive Computational Media 85 xiii xiv LIST OF FIGURES 4 3 Examples of documents retrieved from the World Wide Web relevant to two of the queries in Figure 4 2 86 4 4 Average WER scores across all lectures for the baseline model built on general purpose conversational texts SWB the interpolation based optimization SWB KEYW and the best web based LMs LECT 93 4 5 Average Precision and Recall scores for keyword detection across all lectures when using the baseline model built on general purpose conversational optimization texts SWB the and interpolation based SWB KEYW the best web based LMs LECT 94 5 1 Our transcript enhanced ePresence system displaying a screen capture of the system with transcripts of 45 WER 102 5 2 Wiki like editing of imperfect transcripts 104 5 3 The percentage of edited transcript lines and relative WER reduction for each of the 21 lectures after all transcripts were corrected 114 5 4 The extended editing mode allowing for full control of the audio playback and for editing of consecutive transcript lines 126 LIST OF FIGURES 6 1 The TBL algorithm for correcting ASR output xv Transformation rules are learned from the alignment of manually transcribed text T with automatically generated transcripts TASR of training data ranked according to a scoring function S and applied to the ASR output TASR of test data 144 6 2 The discovery of transformation rules as part of the TBL algorithm described in Figure 6 1 146 6 3 6 4 6 5 7 1 An example of the rule discovery mechanism from Figure 6 2 147 An example of rule scoring and selection 149 An example of rule application 150 The ASR based search interface of the MIT lecture browser 176 C 1 Consent Form page 1 for the experiment in Chapter 3 210 C 2 Consent Form page 2 for the experiment in Chapter 3 211 C 3 Consent Form page 1 for the main study in Chapter 5 212 C 4 Consent Form page 2 for the main study in Chapter 5 213 C 5 Consent Form page 1 for the study following the interface re design in Chapter 5 214 C 6 Consent Form page 2 for the study following the interface re design in Chapter 5 215 C 7 The introductory quiz page 1 administered before the start of the experiment in Chapter 3 216 C 8 The introductory quiz page 2 administered before the start of the experiment in Chapter 3 217 xvi LIST OF FIGURES C 9 The introductory quiz page 3 administered before the start of the experiment in Chapter 3 218 C 10 An example of the quiz administered during the experiment in Chapter 3 Four quizzes with questions selected from the introductory quiz were used in a latin square setup as described in Section 3 3 219 C 11 The questionnaire administered after a session with a level of WER of either 0 25 or 45 from the experiment in Chapter 3 220 C 12 The questionnaire administered after a session with a level of WER of NT no transcripts from the experiment in Chapter 3 221 C 13 The questionnaire page 1 administered at the end of the experiment in Chapter 3 222 C 14 The questionnaire page 2 administered at the end of the experiment in Chapter 3 223 C 15 The questionnaire page 3 administered at the end of the experiment in Chapter 3 224 C 16 The questionnaire page 4 administered at the end of the experiment in Chapter 3 225 C 17 The web based questionnaire page 1 administered at the end of the field study in Chapter 5 226 C 18 The web based questionnaire page 2 administered at the end of the field study in Chapter 5 227 C 19 The web based questionnaire page 3 administered at the end of the field study in Chapter 5 228 LIST OF FIGURES C 20 The web based questionnaire page 4 administered at the xvii end of the field study in Chapter 5 229 xviii LIST OF FIGURES List of Tables 3 1 The variable used to control the training overfitting of the lecture language models 43 3 2 The training overfitting variables values for the target WERs of 25 and 45 44 3 3 Mean relative quiz scores for each level of WER and tests of significance over all levels of WER 52 3 4 Trend analyses over the ordinal values of WER and over all values of WER 53 3 5 Multiple comparisons between WER NT and each of the ordinal levels of WER 53 3 6 Mean relative NotOnSlide scores for each level of WER and tests of significance over all levels of WER 55 3 7 Trend analyses over the ordinal values of WER and over all values of WER for NotOnSlide scores 55 3 8 Multiple comparisons between WER NT and each of the ordinal levels of WER for NotOnSlide scores 55 3 9 Mean relative scores for each level of WER across novice users and tests of significance and trends over all levels of WER 56 xix xx LIST OF TABLES 3 10 Mean relative perception of difficulty and confidence in performance levels for each level of WER and tests of significance over all levels of WER Lower values mean increased confidence choice 1 on questionnaire indicated being very confident and perception of an easier task choice 1 indicated a very easy task 61 3 11 Trend analyses over the ordinal values of WER and over all values of WER for perception and confidence levels 62 3 12 Multiple comparisons between WER NT and each of the ordinal levels of WER for perception and confidence levels 63 3 13 Mean relative perception of helpfulness levels for each level of WER and tests of significance and trends over all levels of WER Lower values mean increase helpfulness choice 1 on questionnaire indicated transcripts helped solved the quiz faster better 65 4 1 Web based lecture LM the WERs corresponding to the training options described in Section 4 3 3 90 4 2 The WERs corresponding to the best web based lecture models LECT compared to the baseline model built on general purpose conversational texts SWB to the baseline model built using manually extracted keywords defining the lecture topic KEYW and to the baseline obtained through interpolation SWB KEYW 91 LIST OF TABLES 4 3 Precision and Recall scores for keyword detection when using the best training options as indicated in Table 4 1 for the web based models compared to the baseline model built on general purpose conversational texts SWB to the baseline model built using manually extracted keywords defining the lecture topic KEYW and to the baseline obtained through interpolation SWB KEYW 5 1 5 2 5 3 5 4 xxi 92 User acceptance of the transcript enhanced webcast system 116 Users attitudes toward imperfect transcripts 117 Users attitudes toward wiki editing 119 Users perception of the indirect benefits of wiki editing of transcripts 120 5 5 Users confidence in using the system as a relation of transcript quality 121 5 6 Users attitudes toward wiki editing with the re designed system 129 6 1 6 2 The evaluation data 152 The WER values for instructor R for which the ASR output using the WSJ 5K language model is corrected by TBL rules that are scored by the approximation functions XER and XER NoS as baselines and by the proposed globally scoped non heuristic scoring function SW ER 160 xxii LIST OF TABLES 6 3 The WER values for instructor R for which the ASR output using the WEB LECT language model is corrected by TBL rules that are scored by the approximation functions XER and XER NoS as baselines and by the proposed globally scoped non heuristic scoring function SW ER The average WER when using the SW ER function and the training parameter RT 2 is 40 161 6 4 The WER values for instructor G for which the ASR output using the ICSISWB language model is corrected by TBL rules that are scored by the approximation functions XER and XER NoS as baselines and by the proposed globally scoped non heuristic scoring function SW ER 162 6 5 The WER values for instructor K for which the ASR output using the WSJ 5K language model is corrected by TBL rules that are scored by the approximation functions XER and XER NoS as baselines and by the proposed globally scoped non heuristic scoring function SW ER 163 Chapter 1 Introduction Humankind has long relied on written text to share knowledge from handwritten letters to books and to printed mass media The advent of affordable broadband Internet and personal and portable computing devices is contributing to dramatic changes in the way people exchange information and store knowledge Universities colleges and other public institutions are not excluded from these changes Nowadays more lectures presentations and talks are being made available online In order to provide the same access to information as written materials online media must be accompanied by textual transcripts Two major Computer Science research areas are directly concerned with improving access to information in this context Automatic Speech Recognition ASR which focuses on automatically producing better quality text transcripts and Human Computer Interaction HCI which is dedicated to better and more naturally facilitating the information transfer between users and machines There is little evidence however of combined efforts in these two major 1 2 CHAPTER 1 INTRODUCTION research areas One finds instead rather disparate attempts to address the challenge of improving the usability of webcast archives from either one The research proposed in this dissertation brings together the ASR and HCI areas in order to achieve the goal of providing improved access to webcast lectures and presentations This chapter presents an overview of the ASR and HCI areas outlines the challenges common to these two areas introduces the motivation for this work and establishes the contributions and the statement of a thesis 1 1 Overview Automatic Speech Recognition ASR is one of the oldest areas of research that could now be called Natural Language Processing NLP Despite the progress recorded over the past half century that led to today s current commercial ASR systems the research opportunities in this area have not yet been exhausted Current state of the art research systems still operate under serious restrictions Furui 2005a Glass et al 2007 such as the need for individual training on each user restricted applicability to a dedicated domain such as travel reservations or even both for critical applications such as devices controlled through voice commands ASR systems that can transcribe large vocabulary speaker independent noisy environment continuous speech as is required for lectures and presentations are still a research goal for the future Deng and Huang 2004 Another well established research area is that of Human Computer Interaction HCI and similar to ASR numerous research opportunities Useful Transcriptions of Webcast Lectures 1 2 MOTIVATION 3 exist here Often such opportunities of merging research from two different domains appear due to advances in both areas One particular example is the domain of natural language based dialog systems Bernsen et al 1998 a research area that emerged more than 20 years ago and one in which HCI and ASR NLP research must work together Webcasts are one of the emerging technologies associated with the expansion of the World Wide Web and this is one area that could certainly benefit from research in both HCI and ASR NLP Currently there is little evidence of HCI and ASR research working together on improving webcast systems Despite voice being currently present in nearly all webcasts it can rarely be used beyond playback My goal is to exploit this information rich resource and thereby improve the usefulness of webcast archives 1 2 Motivation The recent increase in the availability and affordability of broadband Internet connections has led to an increase in the use of Internet broadcasting Ritter 2004 For example major media corporations offer newscasts and universities deliver lectures through the Internet Most such webcast media are stored after being delivered live and can be accessed by users through interactive systems such as ePresence http epresence tv A detailed review of webcast systems can be found in Baecker 2003 In contrast to archives of text documents video and audio archives pose some challenges to their users Useful Transcriptions of Webcast Lectures 4 archives given a text query and CHAPTER 1 INTRODUCTION example a user must listen to or watch a long recording in order to locate a specific passage instead of quickly skimming through the content of a text document looking for visual landmarks and textual cues This represents an important hurdle in making webcast archives the digital equivalent from a user s perspective of libraries Various methods propose improved access to speech recordings by manipulating the audio playback Arons 1997 Sawhney and Schmandt 2000 or by delivering better keyword based indexing and searching through the audio stream Chelba et al 2007 Hori et al 2007 as well as to webcast archives through a table of contents Baecker et al 2003 Toms et al 2005 although such methods have certain limitations User studies Dufour et al 2005 suggest however that transcripts are a much needed tool for carrying out complex tasks that require browsing comprehension and information seeking from webcast archives even in the presence of other tools such as table of contents or search capabilities Unfortunately there are several challenges to obtaining transcripts of spoken documents Manual transcription is an expensive process Replacing the transcript with a manually produced set of keywords is also not a Useful Transcriptions of Webcast Lectures 1 2 MOTIVATION 5 because annotating natural language based resources is in general susceptible to inter annotator disagreements diverse speakers with particular speech styles and various accents including non native ones and large vocabularies determined by the large pool of topics In perfect conditions anechoic room slow speaking rate limited vocabulary ASR system previously trained on the same speaker state of the art systems can achieve a Word Error Rate WER1 of less than 3 For less restricted domains with good acoustic conditions such as broadcast news the state of the art WER is about 20 25 Gauvain et al 2002 When acoustic conditions degrade such as in lectures or conference talks WER can increase to 40 45 Leeuwis et al 2003 Park et al 2005 Hsu and Glass 2006 We must therefore first establish what a satisfactory quality for archive transcripts is in order to improve the overall webcast archives user experience Equally importantly since ASR techniques that achieve close to 0 WER will likely not be available in the near future Whittaker and Hirschberg 2003 more studies are needed to understand users expectations from transcripts and to explore how imperfect transcripts should 1 WER is defined as the edit distance in words between the correct sentence and the output sentence from the ASR system Useful Transcriptions of Webcast Lectures 6 CHAPTER 1 INTRODUCTION be integrated into a highly interactive webcast system Therefore three directions of research present themselves for supporting the delivery of useful webcast lecture transcripts 1 studying how humans deal with and what they expect from error laden transcripts particularly in the context of webcast archives 2 improving ASR for lectures mainly by adapting language models or the corpora used in building language models to dedicated domains and 3 finding HCI based solutions for improving the usefulness of transcripts 1 3 Statement of Thesis In this dissertation I show that users access to and interaction with information rich media such as archives of webcast lectures can be improved through an integrated inter disciplinary approach that combines research in Automatic Speech Recognition and Human Computer Interaction In particular I claim and demonstrate that the usefulness of archived webcast lectures as well as users experience when interacting with these is improved by enhancing webcast systems with automatically generated transcripts of Word Error Rate of 25 or less I also show that significant transcript quality improvements toward the acceptable Word Error Rate are achieved by integrating speech recognition techniques specifically addressed at increasing the accuracy of webcast transcriptions with the development of an interactive collaborative interface that facilitates users editing of machine generated transcripts Useful Transcriptions of Webcast Lectures 1 4 CONTRIBUTIONS 7 1 4 Contributions As part of a larger research effort in the webcasting community to make archives of webcast lectures and presentations the digital equivalent of traditional libraries my research goals focus on providing useful transcriptions of lectures and presentations One particular research direction in aid of this goal is enhancing webcasts with automatically generated textual transcripts The quality of automatically generated transcripts should also be a focus of any effort directed towards improving access to information rich webcasts This dissertation s contributions are a reflection of these two inter dependent directions A study of how humans deal with error laden transcripts and what they expect from them I have determined the acceptable speech recognition quality for which transcripts of webcast lectures become useful and improve the usability2 of archived webcasts as well as discovered how users performance and experience is influenced by the quality of transcripts Improvement of Automatic Speech Recognition for lectures Several solutions are proposed to reduce the WER for both lectures and academic presentations by ways of domain oriented language modelling corpora HCI based solutions for 2 and automatically building webcast specific improving the usefulness of A definition of these terms is given in Section 3 3 4 Useful Transcriptions of Webcast Lectures 8 transcript enhanced webcasts CHAPTER 1 INTRODUCTION In order to further improve the quality of webcast transcripts a collaborative and interactive tool that supports user editing of transcripts is proposed and evaluated In addition to directly benefitting from the corrections made by users solutions are proposed that exploit the user edits to learn ASR correction rules that further improve transcript quality 1 5 Structure of the Dissertation Each research contribution is introduced in a separate chapter Chapter 2 introduces and outlines the topics addressed in the dissertation by surveying related work In addition existing research relevant to each topic is presented in the first section of the corresponding chapter 1 5 1 Research Design The first step toward proving the thesis stated in Section 1 3 was to identify the acceptable threshold of quality for automatically generated transcripts of webcast lectures when used in an information mining and question answering task as well as assessing the influence of transcripts quality on webcast users experience After determining this threshold I have investigated ASR methods that improve the quality of lecture transcripts through topic specific language modelling using lecture slides and relevant documents from the World Wide Web I have also proposed HCI based solutions for improving the quality of such transcripts by developing and evaluating an interactive tool that allows users of webcast to collaboratively correct lecture transcripts Useful Transcriptions of Webcast Lectures 1 5 STRUCTURE OF THE DISSERTATION 9 I then show that the transcript corrections enabled by such collaborative tool can be used to learn patterns of errors that serve as a mechanism of further improving the accuracy of lecture specific ASR systems 1 5 2 Outline of the Proposed Research The dissertation is structured as follows Chapter 2 Webcasting and Automatic Speech Recognition This introductory chapter presents an overview of webcasting the problem of enhancing the usability and usefulness of webcasts through transcripts and defines the problem of automatically obtaining transcripts for lectures and presentations outlining the challenges thereof Chapter 3 The Acceptable Webcast Word Error Rate of Machine Generated Transcripts The research introduced in this chapter focuses on determining the acceptable speech recognition quality that enables the usefulness of webcasts and enhances users experience by adding transcripts Chapter 4 Improving Automatic Speech Recognition for Webcast Lectures As part of my research on improving the ASR for lecture transcription several novel techniques are proposed here that address the problems of automatically building domain specific language models and improving the transcription of lecture specific speech Useful Transcriptions of Webcast Lectures 10 CHAPTER 1 INTRODUCTION Chapter 5 Wiki editing of Webcast Transcripts In order to further improve the usefulness of transcripts for webcasts an interactive and collaborative tool is introduced that allows users to correct errors in transcripts while watching an archive of a webcast This tool is evaluated through several in situ studies and refinements to the design are carried out Chapter 6 Automatic Speech Recognition for Webcast Lectures Learning from Wiki enabled Transcript Corrections In this chapter the research on improving ASR is integrated with the HCI aspects of my research by exploiting edits carried out by users through the collaborative editing tool to improve ASR even further rules extracted from the user edited transcripts of the first few minutes of each lecture are used to learn the most frequent ASR errors and adapt the system to correct them automatically Chapter 7 Contributions Conclusions and Future Work The final chapter contains further discussion of these contributions as well as a presentation of future research directions stemming from this work 1 5 3 Outline of the Surveyed Related Work The existing research relevant to each topic is presented as follows Chapter 2 Webcasting and Automatic Speech Recognition webcasting webcasting in education human factors in using webcast interfaces speech recognition Useful Transcriptions of Webcast Lectures 1 5 STRUCTURE OF THE DISSERTATION Chapter 3 The Acceptable Word Error Rate users 11 of dealing Machine Generated Webcast Transcripts with imperfect transcripts Chapter 4 Improving Automatic Speech Recognition for Webcast Lectures ASR for lectures and academic presentations language model adaptation to dedicated domains Chapter 5 Wiki editing of Webcast Transcripts computer suported collaborative work used to compensate for limitations in Artificial Inteligence systems Chapter 6 Automatic Speech Recognition for Webcast Lectures Learning from Wiki enabled from Transcript Corrections bootstrapping ASR manual transcripts corrections transformation based learning in NLP and ASR 1 5 4 Peer Reviewed Publications The contributions of this dissertation has been presented at several peer reviewed scientific conferences Munteanu et al 2006a 2006b 2006c Chapter 3 Webcasting and Automatic Speech Recognition Munteanu et al 2007 Chapter 4 Improving Automatic Speech Recognition for Webcast Lectures Munteanu et al 2006d 2008a Chapter 5 Wiki editing of Webcast Transcripts Useful Transcriptions of Webcast Lectures 12 CHAPTER 1 INTRODUCTION of the research in Munteanu et al 2008b a general overview Chapters 3 4 and 5 and summarized in Section 7 1 Useful Transcriptions of Webcast Lectures Chapter 2 Webcasting and Automatic Speech Recognition Internet broadcasting webcasting is becoming an increasingly popular method of delivering lectures and academic presentations over the Internet Baecker et al 2003 mainly sustained by the increased availability and affordability of broadband Internet connections For example major media corporations offer newscasts and universities deliver lectures over the Internet At the same time more of these media are being archived and accessed by users through interactive systems While interactive media such as webcasts of lectures and presentations has become increasingly ubiquitous current interfaces to online media archives do not provide the same experience in accessing information as humans are accustomed to when using traditional information repositories like libraries This is in part due to the absence of transcripts of the digital media s audio channel Spoken Language Processing and in 13 14 CHAPTER 2 WEBCASTING AND AUTOMATIC SPEECH RECOGNITION particular Automatic Speech Recognition is one of the research areas focused on developing algorithms and technologies dedicated to better and more naturally facilitating access to information stored in spoken form In this chapter I will present an overview of webcasting and the problem of enhancing the usefulness of webcasts through transcripts Section 2 1 I will then define the problem and outline the challenges of automatically obtaining transcripts of lectures and presentations Section 2 2 2 1 Lecture Webcasting Currently webcasting is a common medium found in numerous Internet based applications ranging from entertainment to news Beside the technological advancements facilitating this its widespread adoption can also be attributed to the fact that in most instances users were already familiar with similar unidirectional broadcasting media such as television In contrast educational environments for which lecture webcasting applications are developed are significantly more interactive and richer in information exchanges Although lecture webcasting is still a research area in itself there are several webcasting system exclusively dedicated to lectures Among those University of California s BIBS Rowe et al 2001 and University of Toronto s ePresence Baecker et al 2003 are some of the early webcasting systems that were extensively tested in academic environments the BIBS system is supposedly the first lecture webcast system having streamed the first on demand replay of a seminar recording over the Internet in 1995 More Useful Transcriptions of Webcast Lectures 2 1 LECTURE WEBCASTING 15 recently other systems became available such as the MIT Lecture Browser Glass et al 2007 or the Microsoft Research LecCasting System Zhang et al 2008 A comprehensive overview of available lecture dedicated webcasting technology can be found in Deal 2007 2 1 1 An Overview of Webcasting Traditionally webcasts differ from other broadcasts in that the transmission medium is the Internet a review of webcasting can be found in Baecker 2003 More recently however with the emergence of research on making webcasting more interactive the delivery of richer media is increasingly seen as a mean to enhance the participation of webcast users and to make knowledge sharing more accessible A typical webcast archive1 system allows users to access audio video recordings which are synchronized with other information media such as slides and often with higher level content structure such as tables of contents and time progress indicators As a framework for my research the ePresence webcasting system http epresence tv is used which is part of an ongoing research project that has the goal of making webcasting highly interactive more engaging and more accessible and webcast archives more useful and usable Baecker et al 2003 As illustrated in Figure 2 1 the ePresence system gives users full control of the archive through the display of slides used in lectures and video recording through interaction with a table of contents 1 Throughout this dissertation the terms webcast archive or archives are used to denote a collection repository of interactive recordings of webcasts Useful Transcriptions of Webcast Lectures 16 CHAPTER 2 WEBCASTING AND AUTOMATIC SPEECH RECOGNITION Figure 2 1 The ePresence system displaying an archived lecture webcast Useful Transcriptions of Webcast Lectures 2 1 LECTURE WEBCASTING 17 headings and the titles of the slides and through the timeline an interactive clickable fine grained time progress indicator 2 1 2 Research Challenges While development work for lecture webcasts is currently focused mainly on facilitating remote viewing either live or offline research efforts are beginning to be dedicated to improving and enhancing users experience One important aspect being studied is that of webcast systems enabling users the same experience as attending in a lecture in person Birnholtz et al 2008 However since classrooms are complex environments with rich interactions webcasts do not provide a real substitute for most aspects of the classroom experience Recent research showed that students do not perceive lecture webcasts as replacing the classroom experience Brotherton and Abowd 2004 Chandra 2007 but instead complement it This finding can in part be explained by some of the shortcomings of current webcast systems such as the lack of adequate support for real time interaction Brotherton and Abowd 2004 or a diminished sense of awareness among remote lecture participants Birnholtz 2006 Despite the current challenges faced by lecture webcast systems the use of webcasts for lectures is not only positively accepted by students some studies show improvement in students activities such as lecture reviewing Brotherton and Abowd 2004 Furthermore it was reported that students academic performance also increased when using webcast archives of pre recorded lectures Day and Foley 2006 As illustrated in the review by Deal 2007 several other studies show contradictory findings Useful Transcriptions of Webcast Lectures 18 CHAPTER 2 WEBCASTING AND AUTOMATIC SPEECH RECOGNITION For example students indicated they consider watching a webcast a better academic experience than going to classes although the same number of student mentioned that they would attend the classes even if webcasts are available The findings of the various studies on webcast lectures mentioned here suggest that such systems will not replace traditional educational environments but rather complement and enhance them As it is pointed out in Day and Foley 2006 webcasts can be used to deliver the main static content of a lecture allowing students to benefit from more interaction with their instructor during regular classrooms However for such changes to be effective the webcast system must allow and facilitate efficient access to the information delivered during lectures One research area that is recently receiving increased attention is that of improving access to lectures Although the main focus of this is on facilitating access for students with disabilities it is to be expected that everyone will benefit from improved access to and interaction with lectures content a phenomenon known as curb cuts effects Hesse 1995 As webcasts can be used to complement educational environments instead of aiming at replacing physical classrooms an interdisciplinary approach is needed for improving students experience when interacting with webcast lectures from delivering better information access tools to further analyzing the social aspects of web based interactions in educational environments Several other research areas where information access is of critical importance have turned to Natural Language Processing and Automatic Speech Recognition in particular to provide solutions Useful Transcriptions of Webcast Lectures However such 2 1 LECTURE WEBCASTING 19 research has been rather limited in its application to lecture webcasts Beside the work presented in this thesis only a small number of other lecture systems such as MIT s Lecture Browser Glass et al 2005 or Tokyo Institute of Technology s Julius system Kawahara 2004 Furui 2005a look at ASR as a solution for facilitating access to information Unfortunately these system do not integrate the ASR based advancements with extensive HCI research although the MIT Lecture Browser http web sls csail mit edu lectures offers a web based interface for displaying transcripts synchronized with video recordings of lectures as illustrated in Figure 2 2 Enabling speech recognition for lecture webcasts offers the opportunity to improve access through a range of text based tools ranging from full transcripts of a lecture to summaries and to search and indexing However advances in ASR research for lectures must be complemented by similar progress in all other critical aspects related to lecture webcasting such as Human Computer Interaction HCI or Computer Supported Collaborative Work CSCW Therefore this dissertation will focus on an interdisciplinary approach to enhancing access to webcast lectures by combining research in HCI CSCW and ASR Useful Transcriptions of Webcast Lectures Figure 2 2 The display pane of the MIT lecture browser 20 CHAPTER 2 WEBCASTING AND AUTOMATIC SPEECH RECOGNITION 2 1 3 Integrating Text with Webcast Media As more media become available on line and more information is shared through webcasting one of the striking differences from traditional forms of knowledge sharing is that newer media are often not accompanied by text Although there are several proposals for audio based techniques that improve access to recordings Arons 1997 Sawhney and Schmandt 2000 user studies Dufour et al 2005 suggest that transcripts are a much needed tool for carrying out complex tasks that require information seeking from archives of webcast media Moreover providing access to users with hearing impairments Wald and Basson 2003 only makes for an even stronger case in favour of offering text transcripts along with audio video media in archives of webcasts Obtaining transcriptions for online media would also improve the way human users search for organize and retrieve specific information from large collections Hauptmann et al 2003 As mentioned in Section 1 2 given the increasing size of information rich media archives automatic speech recognition is generally seen as a feasible solution for generating text transcripts of such archives The following section presents an introduction to the problem of automatic speech recognition and the challenges in using ASR to automatically transcribe archives of webcast media such as lectures and presentations Useful Transcriptions of Webcast Lectures 2 2 AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES 21 2 2 Automatic Speech Recognition for Webcast Lectures 2 2 1 An Overview of the Automatic Speech Recognition Process Speech recognition is a pattern matching process Schroeder 1999 Statistical pattern matching techniques based on dynamic programming and Hidden Markov Models together with the mathematical theory of digital signals and spectral analysis are the foundation of speech processing A more practical definition is introduced in Jelinek 1997 along with a schematic description Figure 2 3 A speech recognizer is a device that automatically transcribes speech into text Figure 2 3 The speech recognition process As seen in Figure 2 3 the speech recognition process can be divided into two stages Acoustic Processing and Linguistic Decoding The Acoustic Processing stage transforms the electrical signal coming from a microphone capturing an utterance into acoustic symbols such as phonemes or other Useful Transcriptions of Webcast Lectures 22 CHAPTER 2 WEBCASTING AND AUTOMATIC SPEECH RECOGNITION The Linguistic Decoding finer grained acoustic units sub phonemes stage then processes these symbols and outputs a set of word hypotheses representing with a certain degree of probability the text of the utterance While acoustic processing is an important part of the speech recognition process this is not the main focus of the research presented here Instead this section is focused on Linguistic Decoding and follows the introduction from Jelinek 1997 Complete descriptions of the speech recognition process and of all methods involved can be found in several books such as Rabiner and Juang 1993 Becchetti and Ricotti 1999 De Mori 1998 Manning and W This represents a string of words W w1 w2 wn that are the most probable to have been spoken given the observed sequence of acoustic symbols A Using Bayes formula the probability of a sequence of words given an observed sequence of acoustic symbols can be written as P W P A W P A P W A Useful Transcriptions of Webcast Lectures 2 2 AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES 23 Since A is fixed there is no other acoustic data the output of the speech recognizer can be expressed as argmax P W P A W W W Since P A W cannot be computed beforehand due to the extremely large number of possible pairings between all words in the vocabulary and all acoustic symbols this probability is computed during the speech recognition process For this an acoustic model pronunciation model is employed This would model the way a specific word is pronounced Acoustic models are trained on transcribed speech i e aligned sequences of acoustic symbols and linguistic units words in this particular case Usually acoustic models are built using Hidden Markov Models HMMs Rabiner 1989 Rabiner and Juang 1986 Acoustic modeling alone cannot solve the speech recognition problem A language model is also needed P W represents the probability of uttering a specific word sequence W More specifically the language model gives the probability of uttering a specific word wn after a sequences of words w1 wn 1 was uttered n gram models Language models are trained on various types of corpora from transcribed speech to collection of texts from the World Wide Web Since many of the domains where ASR systems are deployed are characterized by a certain topic vocabulary or speaking style significant research efforts are spent on finding appropriate ways to build corpora and to train language models that will better reflect that particular domain Useful Transcriptions of Webcast Lectures 24 CHAPTER 2 WEBCASTING AND AUTOMATIC SPEECH RECOGNITION 2 2 2 Improving Automatic Speech Recognition From a user point of view as well as from a research perspective the speech recognition process can be viewed as structured on three levels what can be done on the recognition level on the evaluation side and on the resource level It is not the purpose of this document to give a complete introduction to speech recognition Therefore I will only present a top level view of the recognition process structured on these three levels in Figure 2 4 by extending Figure 2 3 from Section 2 2 1 Figure 2 4 An overview of the automatic speech recognition process with the different parts of the process grouped under three levels the recognition level the evaluation side and the resource level The Recognition Level This level refers to the direct manipulation of the multi modal input channel For presentations three modalities can be identified speech signal needs to be processed in order to extract the feature vectors used in recognition Several particularities of the presentation speech Useful Transcriptions of Webcast Lectures 2 2 AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES 25 may need special attention such as the acoustic characteristics noise level reverberation level etc of the presentation room A task particularly important for lecture presentation speech recognition is the segmentation of the speech signal based on topic speaker events etc interaction if possible the movement and action of the pointing device used by the speaker should be captured supporting text includes any text if available that has been used in the presentation such as the content of the slides preferably synchronized with the speech signal This supplemental data can complement the speech signal during the recognition process In my research I will mainly focus on exploiting the supporting text and improve the linguistic decoding phase of the recognition process The Resource Level The resources available for speech recognition are the most critical aspect of the recognition process Several kinds of corpora are needed natural language texts speech signal collections and presentation texts While unannotated corpora are not a scarce resource especially with the increased amount of information available through the internet it is annotated corpora much more expensive to be produced that are needed for developing better speech recognition tools Through training these resources lead to the development of the language and acoustic models used in recognition A significant part of the research on improving speech recognition in particular Useful Transcriptions of Webcast Lectures 26 CHAPTER 2 WEBCASTING AND AUTOMATIC SPEECH RECOGNITION for automatic lecture transcription including some of the focus of this thesis is dedicated to building corpora and models specific to a particular domain The Evaluation Level The evaluation level consists of the word hypotheses proposed by the speech recognizer At this level accuracy evaluation can enhance the output by adding confidence labels to the word hypotheses The confidence labels can be used as a general measure for evaluating the speech recognition or can be used for further improvements to the recognition process As part of the specific research on lecture and presentation transcription my research is focused on providing measures for assessing the quality of an ASR system that would better reflect the fact that automatically generated lecture and presentation transcripts are used by humans 2 2 3 Research Challenges The most commonly used measure for the quality of a speech recognition system is word error rate WER the percent of incorrectly recognized words in a spoken utterance 2 In ideal conditions an ASR system can produce a WER close to 0 typically 1 3 In order to achieve this level of accuracy the recording must take place in an anechoic room the speaker should speak at a very slow rate e g text reading with pauses between sentences and the ASR system must be trained on previous recordings of the same 2 WER is defined as the edit distance percentage of substituted deleted and inserted words between the correct sentence and the output sentence from the ASR system Huang and Hon 2001 Useful Transcriptions of Webcast Lectures 2 2 AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES 27 speaker collected in similar conditions Several practical applications can deal well with noisy conditions but they usually carry with them significant restrictions such as a vocabulary limited to numbers or address book names for voice dialing State of the art speech recognizers dedicated to large vocabulary continuous speech LVCS such as broadcast news transcription systems can achieve WERs around 20 The conditions for webcast recordings are in stark contrast with these ideal conditions The archives consist of diverse speakers with particular speech styles and various accents including non native various acoustical conditions regular lecture or meeting rooms and the vocabulary is extremely large determined by the large pool of topics the archives unsuitable for training an ASR system This makes The recognition task for ePresence archives can be classified as LVCS speaker independent noisy background ASR As mentioned previously the state of the art WER for LVCS ASR broadcast news transcription is about 20 and for conference talk ASR described throughout Section 4 2 is about 40 As shown in Weintraub et al 1996 recognition accuracy could be degraded by a factor as large as 1 5 for each external condition that becomes non optimal for speech recognition such as moving from reading to conversation as well as from conversation to spontaneous speech Therefore it should be expected that automatically generated webcast transcriptions will have a lower WER than those of conference talks given the much more hostile both acoustically linguistically and pragmatically environments of ePresence recordings lecture open meetings etc Due to the adverse conditions characterizing lecture speech typical WER Useful Transcriptions of Webcast Lectures 28 CHAPTER 2 WEBCASTING AND AUTOMATIC SPEECH RECOGNITION for lecture speech can reach rates as high as 50 when general purpose ASR systems are used Park et al 2005 Munteanu et al 2007 While transcription accuracies in the range of 3 Glass et al 2007 report lecture WERs as low as 17 when transcripts for 29 hours of previous lectures by the same instructor are available Useful Transcriptions of Webcast Lectures Chapter 3 The Acceptable Word Error Rate of Machine Generated Webcast Transcripts Despite efforts to improve the quality of ASR systems current ASR systems do not perform satisfactorily in domains such as transcribing lectures or conference presentations This is caused by poor acoustic conditions diverse speakers with particular speech styles and various accents including non native and large vocabularies determined by the large pool of topics In perfect conditions anechoic room slow speaking rate limited vocabulary ASR system previously trained on the same speaker state of the art systems can achieve a Word Error Rate WER 1 of less than 3 1 For less While WER might not always be an adequate measure of transcript quality Wang and Chelba 2003 it is widely used due to practical considerations Thus it was also our choice as a measure of ASR accuracy 29 30 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS restricted domains with good acoustic conditions such as broadcast news the state of the art WER is about 20 25 Gauvain et al 2002 When acoustic conditions degrade such as in lectures or conference talks WER can increase to 40 45 Leeuwis et al 2003 although some reports suggest a 20 30 WER for lectures given in more artificial and better controlled conditions Rogina and Schaaf 2002 Kato et al 2000 Figure 3 1 The transcript enhanced ePresence system Useful Transcriptions of Webcast Lectures 31 In this research manually and semi automatically generated transcripts were introduced into webcast archives and are investigating the influence of WER on the usability and usefulness of these archives Three research questions were asked Useful Transcriptions of Webcast Lectures 32 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS time synchronized with the video by boldfacing the current line of the transcript thus emulating a closed captioning system while fully displaying the transcript of the segment of lecture for the current slide The line breaks do not represent ends of sentences but rather correspond to pauses longer than 200ms 2 To further enhance the user s control over the lecture users can re synchronize the playback of the video by clicking on a line in the transcript In this chapter I will survey the existing research on how humans deal with imperfect transcripts and how to improve their interaction with online repositories of multimedia Section 3 1 I will then formulate several research questions and hypotheses related to the usefulness of ASR transcripts for webcast archives and the relation between transcript quality and users experience Section 3 2 introduce the design and setup of a human experiment user study3 for testing the hypotheses Section 3 3 and present the results of this experiment Sections 3 4 and 3 5 2 Since the lecture recordings employed in all the evaluations described in this dissertation were transcribed by several transcribers an arbitrary 200ms but easier to visually detect pause metric was chosen in order to ensure inter transcriber consistency Such a metric is preferable to automatic silence detection algorithms such as that of Placeway et al 1997 because the latter requires manual fine tuning to maintain consistency across speakers 3 Whenever used throughout this dissertation the term user study denotes an experiment with human subjects through which some hypotheses were tested Useful Transcriptions of Webcast Lectures 3 1 RELATED WORK 33 3 1 Related Work As more media archives become available research is starting to emerge on users strategies for navigating through such information rich repositories Studies on how archived webcasts are used Dufour et al 2005 and on the effectiveness of navigational tools for webcast archives Toms et al 2005 provide clues as to how users access information in webcasts Transcripts seem much needed to aid navigating through a webcast Dufour et al 2005 or accessing information in spoken media Wald and Basson 2003 Research is therefore needed to establish what is a satisfactory quality for archive transcripts and to develop better ASR systems that deliver transcripts with lower WERs Equally important since ASR techniques that achieve close to 0 WER are not likely to be available in the near future Whittaker and Hirschberg 2003 more studies are needed to understand users expectations from transcripts and to explore how imperfect transcripts should be integrated into a highly interactive webcast system Transcribing lecture presentation speech is a research topic still in its infancy The challenges met by the task of recognizing open domain speaker independent large vocabulary continuous and noisy speech are very hard to overcome While a significant amount of research effort have been spent on improving speech recognition for lectures and presentations Kato et al 2000 Rogina and Schaaf 2002 Leeuwis et al 2003 Park et al 2005 the quality of the transcripts typically WERs of 30 40 at most 20 in particular conditions is still below that for other domains such as broadcast news transcriptions For certain automated applications where transcripts obtained through Useful Transcriptions of Webcast Lectures 34 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS ASR are used by a machine e g travel reservation systems such as ATIS Ward and Issar 1995 a lower WER might not affect the system s performance as long as keywords are recognized accurately However when transcripts are to be used directly by humans the overall quality of the text could be more critical Unfortunately the research that investigates how humans deal with error ridden transcriptions and which accuracy rates can be deemed acceptable is scarce One of the few existing studies of users needs with respect to the ASR accuracy sought to assess users perception of improvements in recognition accuracy Van Buskirk and LaLomia 1995 The result of a Wizard of Oz simulation showed that humans are only able to perceive differences in WER greater than 5 10 when asked to directly rate the quality of transcriptions A previous study on handwriting recognition systems by the same authors LaLomia 1994 showed that users expectations of accuracy vary with how critical the domain of the application is participants were less willing to accept higher error rates for documents to be sent to their boss than for documents of personal use This shows that while users perception of transcript quality is very subjective it is also coarse grained Unfortunately this research does not measure the perception of the recognition accuracy for scenarios in which users perform specific information oriented tasks A study based on recognition accuracy that assessed human ability to use transcripts is presented in Stark et al 2000 Users performed summarizations and relevance judgements of audio materials from the HUB news corpus using transcripts of various WERs obtained by different state of the art ASR systems As expected the better the transcript Useful Transcriptions of Webcast Lectures 3 1 RELATED WORK 35 accuracy the better users performed on several measures such as time to solution solution quality amount of audio played and probability of abandoning the transcripts This study served as a motivation for the SCANMail system Whittaker et al 2002 a voicemail user interface that offers synchronized browsing skimming through a voicemail message and its automatically generated transcription While the SCANMail study revealed that users spent less time performing their tasks when they could browse through speech and text simultaneously their performances were lower for keywords that were not properly transcribed Also subjects were sometimes misled in their tasks by transcription errors assuming that transcripts accurately reflected the content of the voice message Another finding was that users were looking in the voice messages mostly for critical information such as phone numbers or names and that phone numbers especially needed to be recognized accurately It is to be expected that users performance when faced with an errorful transcript in the context of a speech browsing interface can be improved by providing additional information mining tools Indeed it is shown in Whittaker and Hirschberg 2003 in a similar context as Stark et al 2000 the retrieval of spoken news documents that when users are using a search tool to retrieve documents matching their query providing visual information extracted from transcripts about their search results can be more effective than displaying only the errorful transcript of the news story Thus appropriate choices for the design of multimedia browsing tool can offset some of the shortcomings of having imperfect transcripts Useful Transcriptions of Webcast Lectures 36 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS Unfortunately while these studies provide valuable insights into the users handling of errorful transcripts they do not study the relation between performance and WER nor do they provide insights into what level of WER is acceptable for a transcript to be included in a browsing interface Furthermore research is also needed to investigate how users compensate for errors in transcripts 3 2 The Research Questions and Hypotheses purpose of this research was to assess the usefulness of automatically generated transcripts for webcast archives and their influence on the usability of webcasts For this I have studied how partially correct transcripts would affect user performance and user perception of the system and thus would suggest an appropriate level of WER in which to aspire Specifically this research is proposing to test Performance Quality Hypothesis User performance will decrease with increased WER It is expected that users performance to be influenced by the accuracy of the transcripts the performance should increase as the quality of transcripts improves It is expected that transcripts become useful users perform tasks at the same level as or better than without transcripts when the WER of the transcripts is 25 or less Experience Quality Hypothesis The quality of the user s experience will decrease with increased WER It is expected that users perception4 of their experience in completing tasks is influenced by 4 User perception was not measured as a single value but represented instead as a series of indicators each with its own result as discussed in the next section Useful Transcriptions of Webcast Lectures 3 3 METHODS 37 the accuracy of the transcripts positive experiences should increase as the quality of the transcripts improves It is expected that users appreciate transcripts as a feature of the system when the WER of the transcripts is 25 or less 3 3 3 3 1 Methods Overview In order to test these hypotheses a within subjects study was designed in which participants were exposed to multiple levels of WER in their interaction in a typical webcast use 3 3 2 Independent Variables The independent variable in this study was the WER The WER of a transcript was computed as the average WERs of the sentences transcript lines of length at least 3 words 5 We assessed the effect of the WER at four levels WER 5 Most 1 and 2 word lines were just breathing noises or repetitions Useful Transcriptions of Webcast Lectures 38 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS acoustics and diverse speakers WER 3 3 3 Task Each participant completed a quiz consisting of five questions for each webcast viewed Each webcast was on a different lecture The 38 minute lectures came from a a third year undergraduate course The Design of Interactive Computational Media Participants were required to complete each quiz in 12 minutes which forced them to finish the quiz without listening to the entire lecture Of the five quiz questions at least one had the answer on the slide and at least two did not have the answers on a slide and were obscured by the errors in the transcripts The quizzes contained only factual questions specific to each of the lectures and answers were typically very short e g Who developed PICTIVE 3 3 4 Measures and Instruments As stated in the introduction to Section 3 2 the purpose of the research presented in this chapter was two fold first to assess the usefulness of automatically generated transcripts for webcast archives and how this usefulness is affected by transcription quality and second to investigate how transcripts of various levels of quality influence the usability of webcast archives Although usefulness and usability are closely related and sometimes it is difficult to clearly divide them Landauer 1996 defining these terms is needed in order to justify the choice of measures and instruments used to Useful Transcriptions of Webcast Lectures 3 3 METHODS 39 verify the hypotheses formulated in Section 3 2 Usefulness is a term that is rarely introduced formally in the HCI literature however it is often used to describe the degree to which a system performs the functions it was designed to perform It is sometimes referred to as utility defined as the question of whether the functionality of the system in principle ca do what is needed Grudin 1992 Usability is a key concept in HCI research and several authors proposed various definitions most of them having the interaction between users and system at their core the question of how well users can use a system s functionality Grudin 1992 or the quality of a system with respect to ease of learning ease of use and user satisfaction Rosson and Carroll 2002 Therefore to compare the effect of each level of WER from the perspective of these two important HCI concepts two types of data were collected Task performance data and User perception data The instruments used to collect these data are included in Appendix C Through task performance data we will assess the relation between WER and the usefulness of transcripts for archives of webcast lectures The analysis of user perception data will provide information related to users subjective experience when browsing and mining for information in archives of webcast lectures and through several of the indicators described in the following paragraphs to the influence that transcripts and their quality has on various aspects of the usability of webcast archives Useful Transcriptions of Webcast Lectures 40 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS Task Performance Data This was assessed by the accuracy of responses to a quiz on the presentation Each five question quiz had a maximum value of 10 points with two points for each correctly answered question The questions were designed in such a way that answers were unambiguous Therefore no partial points were awarded except for answers that were half complete which received one point Typically half complete answers were those for which participants found a partially correct answer caused by speech recognition errors in the transcript but did not verify its accuracy by listening to the corresponding audio stream In order to eliminate the effects that differences in difficulty among lectures may have on quiz scores the scores referred from this moment on as raw quiz scores were averaged across participants for each lecture and relative quiz scores defined as the difference between the raw quiz score and the lecture average were used For the four lectures used in the experiments the raw quiz averages were 4 10 5 62 6 18 and 6 67 Therefore relative quiz scores could range from 6 67 to 5 90 For example quiz scores on the most difficult lecture of 4 10 average could have ranged from 4 10 a raw score of 0 to 5 90 a raw score of 10 User Perception Data User perception was assessed using a series of indicators derived from two instruments a post quiz questionnaire completed after each quiz that assessed user perception of the task at a specific level of WER and a final post session questionnaire which reflected the influence of WER on Useful Transcriptions of Webcast Lectures 3 3 METHODS 41 users experience These instruments consisted of multiple choice questions and or indicated agreement disagreement with various statements user perception indicators include Perception of task difficulty Participants rated the difficulty of each quiz relative to a preliminary quiz Confidence in performance Participants assessed the correctness The of their answers to the quiz by choosing one of All correct Mostly correct Some correct Mostly wrong All wrong choices for the question I think my answers on the quiz were Perception of speech recognition errors Participants indicated their degree of agreement disagreement with two statements The errors in the transcript didn t stop me from solving the quiz and I was bothered by the errors in the transcript These statements were included only for tasks in which transcripts were present Another statement assessing directly their perception of errors I haven t noticed significant differences in the quality of the transcripts for different lectures was presented on the post session questionnaire Perception of usefulness of transcripts Participants indicated agreement disagreement with statements referring to transcripts as being helpful in solving the quiz better and in solving the quiz faster while on the post session questionnaire they indicated their agreement with the statement I would rather have transcripts with some errors than not have transcripts at all Perception of usability of transcript enhanced webcasts Participants through the post session questionnaire indicated which Useful Transcriptions of Webcast Lectures 42 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS We also feature they used to compensate for errors in the transcripts assessed the usability of making transcripts clickable both to compensate for errors in the transcript and as a general browsing feature Confidence in using the entire system Participants indicated the context in which they would choose to use the transcript enhanced ePresence webcast system The contexts ranged from very critical to less critical Prepare for an examination instead of going to classes Prepare for an examination in addition to going to classes Prepare for an assignment and Make up for a missed class For each context participants could choose Yes No or Only if transcripts have no errors The user perception data consist of ordinal and discrete values representing either choices on a rating scale or agreement disagreement with various statements In order to eliminate the differences in the lectures difficulty the post quiz raw data were translated into relative values in the same manner as the quiz scores Data collected from the post session questionnaires were not adjusted since these questionnaires addressed users overall experience with the enhanced ePresence system 3 3 5 System A Wizard of Oz like study was conducted in order to determine these relations as this simulation method is one of the most appropriate for studying the natural language based human computer interaction Bernsen et al 1998 Life et al 1996 Although Wizard of Oz s drawback resides in the need for a skilled human wizard this method is preferred instead of prototyping since the cost of building a full featured natural language Useful Transcriptions of Webcast Lectures 3 3 METHODS Variable Size in sentences of lecture corpus Modified lecture sentence lengths Number of added HUB 4 sentences Modified HUB 4 sentence lengths Values 20 50 100 200 all 1 5 7 original 0 650 all 1 5 7 original 43 Table 3 1 The variable used to control the training overfitting of the lecture language models prototype is often prohibitive However the proposed simulation method provides the convenience of Wizard of Oz setups while behaving like a true prototype system with no on line wizard intervention As the goal is to evaluate the user performance at four pre determined levels of WER see Section 3 3 1 for the rationale of choosing these levels it was important to maintain a realistic scenario for the Wizard of Oz simulation as it is recommended for studying natural language based human computer interaction 6 WERs that are usually reported in the literature for current broadcast news 25 and lecture speech 45 ASR systems Future work will take in consideration finer grained levels of WER Useful Transcriptions of Webcast Lectures 44 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS Lecture Number of sentences in lecture Variables values for WER Size in sentences of lecture corpus Modified lecture sentence lengths Number of added HUB 4 sentences Modified HUB 4 sentence lengths 0 100 25 1 2 3 4 1280 928 811 972 45 25 45 25 45 25 45 20 200 20 100 20 50 20 original 5 original 5 original 5 original 5 650 0 650 0 650 0 650 1 1 1 1 Table 3 2 The training overfitting variables values for the target WERs of 25 and 45 Useful Transcriptions of Webcast Lectures 3 3 METHODS 45 particular to segments of lectures containing a variable number of sentences The following sections describe the design and setup of a WER controlled ASR system as well as details about the audio material used in testing 3 3 6 Acoustic Models The acoustic model AM that is part of the SONIC toolkit was used in our experiment The decision tree state clustered HMMs model is built on 30 hours of data from 283 speakers from the WSJ0 and WSJ1 subsets of the 1992 development set of the Wall Street Journal WSJ Dictation Corpus LDC 1994 The WSJ Dictation Corpus is a collection of microphone recordings 1 channel 16 bit 16KHz sampling rate of WSJ news texts read by journalists not necessarily with experience in dictation Both for the AMs and for the recognition process the acoustic vectors were represented using SONIC s default7 Perceptual Minimum Variance Distortionless Response PMVDR cepstral coefficients with a 39 dimensional feature vector 12 PMVDR parameters computed over 10ms audio frames and 20ms Hamming windows 3 3 7 Language Models Several alternatives exist for simulating the errors produced by ASR systems from a simple but unrealistic solution of replacing words in manual transcripts to complex algorithms that mimic the mismatches caused by both acoustic and language models such as in Schatzmann et al 2007 Since the focus of the ASR research presented in this dissertation was on 7 Overall we were pleased with SONIC s out of the box features pertaining to the acoustic part of the ASR process Useful Transcriptions of Webcast Lectures 46 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS improving LMs for lecture transcription all the experiments conducted as part of this research used speaker independent acoustic models As such for the experiment described in this Chapter ASR errors were simulated by controlling the LM training process In order to have a greater control of the overfitting process the training sentences were mixed with the transcripts of the 1997 LDC Broadcast News HUB 4 Corpus Stern 1997 Evaluation Set Although tri gram8 LMs were built on the training corpora further variability was introduced in the training process by altering the length of the training sentences this was achieved by concatenating all sentences in the corpus and then splitting them in new sentences of equal length A summary of the variables used to control the training overfitting process is presented in Table 3 1 The tri gram LMs were built in ARPA format using the CMU Cambridge Statistical Language Modeling toolkit Clarkson and Rosenfeld 1997 and converted to the SONIC binary format 3 3 8 Lexicon The pronounciation dictionary was build to cover all words found in the manual transcription thus there were no out of vocabulary items Individual lexicons were built for each segment of the lecture corpus on which LMs were trained The CMU Pronouncing Dictionary v 0 6 CMU 1998 was used to extract the pronounciations for the lecture words For technical words 8 A description of tri gram language models is given in Annex B Glossary of Technical Terms Useful Transcriptions of Webcast Lectures 3 3 METHODS 47 not in the dictionary9 the SONIC s sspell lexicon access tool was used to generate pronounciations using letter to sound predictions from a decision tree which we trained on the entire CMU Pronouncing Dictionary 3 3 9 The Recordings used for our study were collected in a large recordings amphitheatre style lecture hall 200 seating capacity using the AKG C420 head mounted directional microphone The lecturer is male early 60s and a native speaker of English The recordings were not intrusive and no alterations to the lecture environment or proceeding were made The 1 channel recordings were digitized using the TASCAM US 122 audio interface as uncompressed audio files with 16KHz sampling rate and 16 bit samples 3 3 10 Recognition The recognition was performed on each set of sentences using the language model that was trained on data consisting of or containing the same set For an individual lecture a set of models that produced the desired average WER was chosen such that all models in that particular set were trained using the same values for the training variables presented in Table 3 1 The variables values for the target WERs of 25 and 45 are outlined in Table 9 Only few technical words were not found in the CMU Pronouncing Dictionary Also as it will be shown in Chapter 4 if lectures are accompanied by slides the LMs can be easily adapted to include such words As such the realism of this simulation is not likely to be affected by the lack of OOV items Useful Transcriptions of Webcast Lectures 48 3 2 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS The SONIC decoder performs recognition in two passes The first pass decoding uses the specified AMs and LMs After the first pass is complete an unsupervised Maximum Likelihood Linear Regression MLLR of the AM is performed using the output of the first pass ASR hypotheses are labeled with confidence scores The second pass uses the MLLR adapted AM Since the recognition is performed in two passes each of them producing its own hypotheses we also considered using as one of the WER controlling variables the pass from which we selected the ASR output However as mentioned in Pellom 2001 SONIC s MLLR adaptation in the second pass usually produces an output of a slightly lower WER Thus we found that the output of the first pass was always a better choice for our purpose Besides allowing for a greater control of the WER variable the method used to generate lecture transcripts ensured that users were exposed to transcripts generated by a real ASR system Transcripts with these levels of WER as well as no transcript were integrated into an existing webcasting system that additionally provided the following components video of the presentation slides table of contents and timeline This setup allowed us to design an ecologically valid experiment as in a Wizard of Oz simulation without the need for the on line intervention of a human wizard 3 3 11 Experimental Design A repeated measures within subjects design was followed each participant completed four quizzes one for each level of the independent variable Each quiz was administered on a different lecture Useful Transcriptions of Webcast Lectures 3 3 METHODS 49 A Latin square design of size four was chosen to randomize the order in which participants were exposed to the four levels of the independent variable Kirk 1995 For the 48 participants 12 squares were used The squares were designed such that each level of the independent variable was matched with one of the four lectures an equal number of times and such that each of the four lectures appeared in every position in the sequence given to the participants 3 3 12 Participants The study was conducted using 48 students 26 female and 22 male at the undergraduate level recruited from various disciplines 3 3 13 Procedures Participants first completed a preliminary quiz that consisted of the questions from all four quizzes used in the experiment as well as filler questions to eliminate the potential for confounding effects that might have been caused by a previous exposure to the course lectures used in the study Only two participants correctly answered questions on the preliminary quiz and only one question each One of them answered the same question correctly on the quiz during the Useful Transcriptions of Webcast Lectures 50 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS data Next each of the four quizzes and the corresponding lectures were presented to participants Upon completion of each quiz participants were assigned a very brief post quiz questionnaire to assess user perception Breaks were permitted between quizzes After all quizzes were completed a post session questionnaire collected additional comparative user perception data and demographic information 3 3 14 Data Analysis The most suitable statistical test for within subjects design with various confounding effects and multiple level independent variables is the repeated measures ANOVA Howell 1999 which can be carried out in SPSS through the General Linear Model Repeated Measures procedure SPSS 2005 All tests were run using a significance level of 05 as the size of the null hypothesis rejection region For the ANOVA procedure the independent variable WER was used with its four levels WER 0 WER 25 WER 45 and WER NT Although we tested the data for normalcy a non parametric distribution free test Friedman s Rank Test for Correlated Samples Howell 1999 was also run and 2 scores were computed in order to confirm the validity of the F scores obtained through ANOVA Beside the tests for statistical significance simple descriptive statistics are also presented for each level of the WER variable Useful Transcriptions of Webcast Lectures 3 4 RESULTS TASK PERFORMANCE 51 3 4 Results Task Performance Synopsis Transcripts of WER 0 led to best Task performance followed in decreasing order by WER 25 WER NT and WER 45 The Performance Quality Hypothesis was tested through the ANOVA procedure Also a trend analysis was performed in order to estimate the nature of the relation between the scores corresponding to each level of WER As indicated in Howell 1997 and Howell 2002 for independent variables with ordinal values such as the WER variable trend analysis is more meaningful than multiple pairwise ANOVAs among levels of the independent variable in revealing the kind of relationship that exists between the independent and the dependent variable As the WER variable has a mixture of ordinal 0 25 and 45 error rates and categorical pairwise ANOVA comparisons were performed between WER NT and each of WER 0 WER 25 and WER 45 to confirm the findings of the trend analysis 3 4 1 Performance Quality Hypothesis Synopsis Users performance is indeed influenced by WER Also WER 25 is above the WER threshold for achieving the same performance as no Useful Transcriptions of Webcast Lectures 52 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS WER 0 25 NT 45 Mean Score 0 907 0 071 0 373 0 605 ANOVA Friedman F 3 141 7 264 p 001 2 F 3 18 325 p 001 Table 3 3 Mean relative quiz scores for each level of WER and tests of significance over all levels of WER transcripts However the increase in performance does not diminish as quality improves Instead the trend analysis detailed below suggests that performance varies linearly with the transcript s quality The ANOVA using all levels of WER shows a significant relation between quiz scores and quality of transcript Table 3 3 The results are also confirmed by the distribution free test Friedman s Rank test The trend analysis carried out on the ordinal values of WER shows a significant linear relation Table 3 4 WER 0 having the highest score and WER 45 the lowest In order to assess whether WER 25 leads to a better performance than having no transcripts we consider WER NT as an ordinal value being equivalent to a transcript of unknown WER The trend analysis Table 3 4 reveals that the quiz scores for WER NT fall between those for WER 45 and those for WER 25 Table 3 3 shows the average scores for each value of WER the relation still being best approximated as linear A set of multiple pairwise comparisons Table 3 5 was also carried out between the categorical value of WER WER NT and each of the Useful Transcriptions of Webcast Lectures 3 4 RESULTS TASK PERFORMANCE 53 WER 0 25 45 0 25 NT 45 Trend Linear F 1 47 20 133 p 001 Linear F 1 47 23 477 p 001 Table 3 4 Trend analyses over the ordinal values of WER and over all values of WER WER 0 25 45 Comparison with WER NT F 1 47 18 498 p 001 F 1 47 1 428 p 238 F 1 47 405 p 527 Table 3 5 Multiple comparisons between WER NT and each of the ordinal levels of WER Useful Transcriptions of Webcast Lectures 54 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS ordinal levels WER 0 WER 25 and WER 45 While the difference in means can be attributed to the quality of transcripts for WER 0 when compared to WER NT the differences between WER 25 and WER NT and between WER 45 and WER NT are not significant This confirms the trend analysis findings that the performance for WER NT is close to both WER 25 and WER 45 falling in between them and significantly lower than the performance for WER 0 3 4 2 Performance Quality Hypothesis Breakdown by Quiz Question Type A similar analysis was also performed on a OnSlide NotOnSlide breakdown of quiz scores Some of the quiz questions typically 1 or 2 out of 5 for each quiz could be answered without listening to the lecture the answer was found on the slides Therefore a separate analysis was carried out for quiz scores that summed up only the questions with answers on slides OnSlide scores and independently for those that required a thorough listening of the lecture or reading of transcripts in order to answer the questions NotOnSlide scores When considering only the OnSlide scores there are no significant effects of having different values for the WER variable However the differences in WER values have a significant effect on the NotOnSlide scores Table 3 6 The trend analysis Table 3 7 clearly shows a linear relation between WER values 0 25 and 45 and quiz NotOnSlide scores Interestingly the scores Table 3 6 for WER NT are now marginally lower than those for WER 45 Indeed the trend analysis over all levels of WER Table 3 7 indicates a linear Useful Transcriptions of Webcast Lectures 3 4 RESULTS TASK PERFORMANCE 55 WER 0 25 45 NT Mean Score 0 287 0 024 0 121 0 140 ANOVA Friedman F 3 141 8 473 p 001 2 F 3 18 175 p 001 Table 3 6 Mean relative NotOnSlide scores for each level of WER and tests of significance over all levels of WER WER 0 25 45 0 25 NT 45 Trend Linear F 1 47 18 139 p 001 Linear F 1 47 29 293 p 001 Quadratic F 1 47 4 010 p 051 Table 3 7 Trend analyses over the ordinal values of WER and over all values of WER for NotOnSlide scores WER 0 25 45 Comparison with WER NT F 1 47 27 996 p 001 F 1 47 1 447 p 235 F 1 47 034 p 855 Table 3 8 Multiple comparisons between WER NT and each of the ordinal levels of WER for NotOnSlide scores Useful Transcriptions of Webcast Lectures 56 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS WER 0 25 NT 45 0 635 Mean Score 0 812 0 261 0 438 ANOVA Friedman Trend F 3 117 5 567 p 001 2 F 3 12 570 p 006 Linear F 1 39 18 207 p 001 Table 3 9 Mean relative scores for each level of WER across novice users and tests of significance and trends over all levels of WER relation between WER values and scores with a slight quadratic component explained by the close means for WER NT and WER 45 However in terms of post hoc analysis between WER NT and the ordinal levels of WER Table 3 8 the effects of WER 25 and WER 45 are as expected still not significantly different than those of WER NT The analysis of NotOnSlide scores indicates that the WER threshold for which transcripts yield better performance than having no transcripts lies in the upper part of the interval 3 4 3 Performance Quality Hypothesis Breakdown by Demographic Information While ANOVA tests and trend analyses allow inferences about the data collected through the experiments in many cases it is also important to take Useful Transcriptions of Webcast Lectures 3 4 RESULTS TASK PERFORMANCE 57 Figure 3 2 The histogram of the relative quiz scores for WER 25 Useful Transcriptions of Webcast Lectures 58 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS Figure 3 3 The histogram of the relative quiz scores for WER 25 only for subjects that haven t used the system or a similar one before Useful Transcriptions of Webcast Lectures 3 4 RESULTS TASK PERFORMANCE 59 a closer look at simple descriptive statistics Howell 1999 In particular for our experiment examining the histograms of quiz scores for each level of WER leads to some interesting observations Although the distributions of quiz scores for each value of WER can be approximated as normal the histogram for WER 25 Figure 3 2 shows an almost bi modal distribution with scores between 3 and 2 occurring 14 times while scores between 1 and 2 occur 12 times In order to determine the cause for having scores distributed around 2 poles for WER 25 we looked at the demographic information collected through the post session questionnaire The demographic information consists of familiarity with webcast systems e g using the system or a similar one before estimated number of hours of daily internet usage interaction with media content on internet field and year of study enrollment in the course where the recordings of lectures come from The histograms for each demographic factor were analyzed and only the used a similar system before factor 8 subjects out of 48 produced a change in the shape of the histogram Figure 3 3 shows the histogram for WER 25 with these 8 subjects removed The distributions for the other levels of WER are not affected by this factor The same analyses that were carried out for testing the Performance Quality Hypothesis were also performed using the 40 subjects that never used a similar system before novice users While there is still a significant effect of the WER variable on quiz scores Table 3 9 and a linear trend can also be observed among all levels of WER the mean scores for WER 25 for novice users are higher than for WER 25 across Useful Transcriptions of Webcast Lectures 60 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS all participants while scores for WER 0 and WER 45 remain relatively unchanged A possible explanation of this is that similar webcast systems as well as previous versions of the ePresence system do not include any transcripts in the interface thus participants that used such systems were not accustomed to handling transcripts While perfect transcripts WER 0 equally helped such users and novice users and poor quality transcripts WER 45 lowered the performance for experienced users and novice users alike transcripts that are useful but not perfect WER 25 required participants to employ strategies to compensate for errors in transcripts that might be more easily to be developed by novice users than by more experienced users as novice users have no prior expectations about the system 3 5 Results User Perception Synopsis Transcripts of WER 0 led to best user experience followed in decreasing order by WER 25 WER NT and WER 45 As previously mentioned the user perception data were collected through post quiz questionnaires post quiz perception data and through the post session questionnaire post session user perception data The post quiz data were analyzed through Repeated Measure ANOVAs F scores in the same manner as the relative quiz scores Trend analyses were also carried out as well as multiple comparisons between WER NT and each of the ordinal levels of WER Since the post session user perception data were collected at the end of the Useful Transcriptions of Webcast Lectures 3 5 RESULTS USER PERCEPTION WER Confidence in perform Mean level ANOVA 0 25 NT 45 61 0 220 0 026 0 009 0 238 F 3 141 5 369 p 001 0 339 0 151 0 245 0 245 F 3 141 6 201 p 001 Percep task Mean level difficulty Table 3 10 ANOVA Mean relative perception of difficulty and confidence in performance levels for each level of WER and tests of significance over all levels of WER Lower values mean increased confidence choice 1 on questionnaire indicated being very confident and perception of an easier task choice 1 indicated a very easy task study and refer to users overall experience and as such are not influenced by the independent variables no tests of statistical significance needed to be performed Instead simple descriptive statistics are used to present this post experiment analysis of user perception 3 5 1 Experience Quality Hypothesis Synopsis Users experience is indeed influenced by WER Also WER 25 is above the WER threshold at which users welcome transcripts as a feature of the system However the increase in user experience does not always slow down as quality improves We found that some user perception data perception of task difficulty and confidence in performance exhibit only a linear relation with WER while other data perception of transcripts usefulness and perception of errors in transcripts show an increase in user Useful Transcriptions of Webcast Lectures 62 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS WER Confidence in perform Percep task difficulty 0 25 45 0 25 NT 45 F 1 47 12 006 p 001 0 25 45 F 1 47 10 735 p 001 0 25 NT 45 F 1 47 12 857 p 001 Table 3 11 Trend analyses over the ordinal values of WER and over all values of WER for perception and confidence levels experience as quality improves but seem to level off at lower values of WER Perception of task difficulty and confidence in performance These were the user perception data collected at all levels of WER The ANOVA shows that WER affects users experience both for perception of task difficulty and for confidence in performance Table 3 10 The increase in users experience over the ordinal values of WER is linear both for confidence in performance and perception of difficulty Table 3 11 For confidence in performance multiple comparisons pairwise ANOVAs between WER NT and each ordinal level of WER show a significant effect Table 3 12 between WER NT and WER 0 and between WER NT and WER 45 but not between WER NT and WER 25 Indeed the mean relative level of confidence for WER NT is very close to that for WER 25 which is confirmed by the trend analysis of all levels of WER This is still a linear relation since the confidences for WER 25 and WER NT are very close when compared to those for WER 0 and for Useful Transcriptions of Webcast Lectures 3 5 RESULTS USER PERCEPTION WER Confidence in perform 0 25 45 Percep task difficulty 0 25 45 Comparison with WER NT F 1 47 4 399 p 041 F 1 47 0 126 p 724 F 1 47 4 642 p 036 F 1 47 12 242 p 001 F 1 47 4 797 p 034 F 1 47 0 000 p 1 00 63 Table 3 12 Multiple comparisons between WER NT and each of the ordinal levels of WER for perception and confidence levels WER 45 For perception of difficulty pairwise comparisons Table 3 12 reveal significant effects between WER NT and WER 0 and between WER NT and WER 25 but not between WER NT and WER 45 Perceived difficulty levels for WER NT and WER 45 are equal thus the trend analysis over all values still shows a linear relation Therefore it can be concluded that WER 25 is at least equal or even better in improving users experience as having no transcripts Perception of speech recognition errors Participants indicated their agreement disagreement with two statements that appeared on post quiz questionnaires that were administered only after quizzes where transcripts were present The errors in the transcript didn t stop me from solving the quiz and I was bothered by the errors in the transcript For both questions the level of agreement was significantly Useful Transcriptions of Webcast Lectures 64 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS influenced by WER F 2 94 7 060 p 001 and F 2 94 12 212 p 001 respectively A trend analysis shows that the relation between WER and participants perception of error rates is linear for both questions F 1 47 12 746 p 001 and F 1 47 20 699 p 001 respectively with users being more aware of the errors in transcripts of higher WER The perception of errors in transcripts was also assessed through the post session questionnaire Participants indicated their agreement with the statement I haven t noticed significant differences in the quality of the transcripts for different lectures 64 of subjects disagreed or strongly disagreed with the statement while only 19 of subjects agreed or strongly agreed with it This further shows that participants were fully aware of the differences in transcripts quality levels across sessions Perception of transcripts helpfulness Participants indicated their agreement with two statements from post quiz questionnaires administered only when transcripts were present Transcripts helped me solve the quiz faster and Transcripts helped me solve the quiz better For both questions the level of agreement was significantly influenced by WER However the trend analysis shows Table 3 13 both a linear and a quadratic component of the relation between perception of transcripts helpfulness and WER transcripts of WER 0 being perceived as more helpful than those of WER 25 which in turn are more helpful than those of WER 45 The quadratic component is explained by users perception of helpfulness for WER 25 being closer to that for WER 0 than to the perception for WER 45 Table 3 13 Useful Transcriptions of Webcast Lectures 3 5 RESULTS USER PERCEPTION WER Solved faster Mean Score ANOVA Trend linear quadratic Solved better Mean Score ANOVA Trend linear quadratic 0 25 45 0 627 65 0 345 0 282 F 2 94 20 164 p 001 F 1 47 24 790 p 001 F 1 47 11 594 p 001 0 283 0 240 0 523 F 2 94 14 721 p 001 F 1 47 18 884 p 001 F 1 47 8 051 p 006 Table 3 13 Mean relative perception of helpfulness levels for each level of WER and tests of significance and trends over all levels of WER Lower values mean increase helpfulness choice 1 on questionnaire indicated transcripts helped solved the quiz faster better Besides the statements from the post quiz questionnaires users perception of usefulness was also assessed through one question on the post session questionnaire by indicating their agreement with the statement I would rather have transcripts with some errors than not having transcripts at all 91 of subjects indicated agreed or strongly agreed their preference for having access to transcripts even if their quality is not perfect This further demonstrates that 25 error rate is acceptable from the users perspective Useful Transcriptions of Webcast Lectures 66 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS Perception of usability of transcript enhanced webcasts The post session questionnaire also asked participants to indicate which features they used to compensate for the errors in the transcripts by indicating their agreement with statements such as When transcripts seemed to be full of errors I used X to find the answer with X being each of slides audio playback table of contents and timeline Slides audio playback and table of contents were equally favoured by users about 65 of subjects agreed or strongly agreed with each as navigational tools useful in compensating for speech recognition errors in transcripts timeline was used by only 18 Of these the table of contents was the highest rated 31 as the first choice strongly agreed that it helped compensate for transcripts errors followed by the audio playback 23 Participants also indicated over 80 agreed or strongly agreed that being able to play individual lines from transcripts both made the tasks easier to accomplish and was useful when transcripts had errors Participants were also asked on the post session questionnaire to rate all features of the system from an overall usefulness perspective About 95 of subjects rated all features except the timeline as useful or very useful the timeline was rated as useful or very useful only by 59 of the subjects The table of contents was rated the highest for first choice only very useful for 79 of the subjects followed by transcripts 62 This analysis leads to the conclusion that appropriate navigational tools improve users experience when errorful transcripts are present Useful Transcriptions of Webcast Lectures 3 6 LIMITATIONS AND GENERALIZATIONS Confidence in using the system 67 Users overall confidence in using the system was also assessed with respect to the importance of the application where the system is to be used When asked if they would use the system to prepare for an examination instead of going to classes 33 of respondents chose no while 37 of them indicated only if transcripts have no errors Unsurprisingly their confidence changed when asked if they would use the system to prepare for an examination in addition to going to classes 75 opted for an unconditional yes With respect to less critical tasks preparing for an assignment 72 indicated they would use the system while 21 conditioned it by having transcripts with no errors Meanwhile using a system to make up for a missed class would not demand accurate transcripts 93 would use the system for such a task only 4 conditioning it by having perfect transcripts This shows that transcripts quality is more critical in some applications than in others 3 6 Limitations and Generalizations The experiment described in this chapter was carried out under an information mining scenario in which students used a webcast system to answer quiz questions about the content of lectures The main limitation of this task is in the types of information sources that are integrated into the webcast archives specifically the existence of a structured table of contents and of the slides used in the presentation Additional limitations may have been caused by the information mining aspects of the task that was carried out as well as by the choice of Useful Transcriptions of Webcast Lectures 68 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS time constrained factoid quizzes as a metric of user performance Similar research such as that of Whittaker et al 1999 and of Whittaker and Hirschberg 2003 explore the trade off between accuracy and time to completion as a measure of user performance when dealing with imperfect transcripts under tasks that also include comprehension e g writing summaries While our choice of the task was based not only on its typicality for educational activities it was also based on practical reasons comprehension tasks have more confounding variables and are more difficult to assess quantitatively as in the case of judging summaries Penn and Zhu 2008 Within the scope of information mining tasks other measures besides quiz scores can be employed such as the time to completion used by Whittaker et al 1999 or the rate of correct answers per second used by Lonsdale et al 2006 such measures however are not significantly different than the scores of time limited quizzes Under similar conditions it is plausible that the findings of this research especially the fact that WER is correlated with users performance and experience can be generalized to other information mining and comprehension tasks as well as presentation environments although the quality threshold is likely to be lower for more information critical talks such as possibly some business presentations 3 7 Summary and Discussion One of the major drawbacks for the users of audio video archives such as those of webcast lectures and presentations is the difficulty Useful Transcriptions of Webcast Lectures 3 7 SUMMARY AND DISCUSSION 69 in performing operations typically associated with archived text such as scanning and browsing While manual transcription is a very expensive and time consuming task speech recognition systems can provide an alternative solution However for lecture and presentation speech the poor accuracy of automatically generated transcripts makes their use questionable In this research users expectations for transcription accuracy in webcast archives was investigated and how the quality of the transcripts affects the usability and usefulness of the archives was measured Also the investigation looked at what other navigational tools table of contents slides etc users employ to compensate for errors in the transcripts For this an ecologically valid experiment was designed where 48 subjects used a fully featured webcast browsing tool while answering quizzes based on archives of webcast lectures The analysis of the task performance data revealed that users performance correlates with the accuracy of speech recognition For transcripts with a word error rate equal to or less than 25 users task performance was better than that of using no transcripts Word error rate also influenced users experience as shown by the analysis of the user perception data Error rates of 25 led to users experience above that achieved when using no transcripts When exposed to transcripts with WER of 45 both task performance and user experience were worse in most cases10 than if no transcripts had been provided 10 The usefulness threshold for transcript quality is in the proximity of 45 WER when the information users are searching for is found only in the audio channel and not present in any accompanying sources such as the slides Useful Transcriptions of Webcast Lectures 70 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS As current lecture dedicated ASR systems deliver in realistic conditions WERs of 45 or even greater more research is needed in order to reduce this to the acceptable level of 25 as determined by the study presented here As such in Chapter 4 a solution for improving lecture ASR is proposed that takes steps toward reaching this acceptable transcript quality threshold Useful Transcriptions of Webcast Lectures Chapter 4 Improving Automatic Speech Recognition for Webcast Lectures Automatically transcribing lecture speech is one of the most challenging areas of ASR research One direction of research where significant efforts are being spent is the improvement of Language Models LMs for lecture transcription The main goal of this direction is finding appropriate methods of modelling the dual nature of lecture speech it is characterized as large vocabulary continuous speech speaker independent and as topic and domain specific Usually for lecture transcription tasks only small amounts of training data are available compared with for example broadcast news transcription Typically solutions for this type of problem were sought by building separate LMs targeting the reduction of WER independently for each of the traits specific to lecture speech while the recognition was performed using an 71 72 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES interpolated LM Rosenfeld 2000 One of the disadvantages of previous approaches is the more complicated process of determining and acquiring the most appropriate corpora for building two or more separate models Other shortcomings include the need to fine tune the interpolation or hypothesis space combination parameters and the difficulties in automatically or manually extracting reliable topic specific keywords from additional knowledge sources e g lecture slides The approach proposed in this chapter eliminates the need for multiple models yet achieves similar or better WER reductions Our method exploits the slides used in the lecture or presentation to be transcribed by using the entire content of the slides as web search queries it retrieves web corpora that can be used to directly train a single LM suitable for both the conversational and the topic specific styles of lectures In this chapter I will first present a survey of the research on language model adaptation as a solution to improve ASR for dedicated domains Section 4 1 followed by a review of existing work on lecture ASR and related domains Section 4 2 I will then introduce a technique for improving the ASR process for lectures and presentations and discuss several web retrieval and training alternatives Section 4 3 Before concluding this chapter I will also compare the best solution with current interpolation based LM adaptation methods Section 4 4 Useful Transcriptions of Webcast Lectures 4 1 EXISTING RESEARCH ON ADAPTING AND BUILDING LANGUAGE MODELS FOR DEDICATED DOMAINS 73 4 1 Existing Research on Adapting and Building Language Models for Dedicated Domains It is an undeniable fact that automatic speech recognition systems are improving constantly nowadays some reaching accuracies close to 100 in user dependent limited vocabulary dedicated applications One of the reasons for such improvements resides in the increased availability of resources e g extremely large amounts of recordings for such applications that support the development of accurate language and acoustic models Unfortunately the availability of such resources is limited in the large vocabulary continuous speech lecture domain and applications targeting new domains face some of the challenges existing systems had to cope with in the past As a result increasingly significant research has been directed toward porting existing systems to new domains mainly by adapting their LMs without having to pay the high price of collecting the required amount of data As mentioned in Chapter 3 the WER of current systems is still higher than the minimum level for which transcripts are accepted by humans During recent years several approaches were proposed to improve the ASR systems used in lecture transcription Although some significant improvements can be achieved through acoustic model adaptation if manual transcripts of the same lecturer are available Park et al 2005 Useful Transcriptions of Webcast Lectures 74 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES Kato et al 2000 Of potential interest to the task of lecture presentation speech recognition is the domain of broadcast news transcription These two domains share some important aspects such as speaker independence large vocabulary less than optimal acoustic conditions for lectures caused by the lecture room and for broadcast news caused by external disturbances such as background music for studio broadcasts or background noises for outside broadcasts or the occurrence of speaker interruptions One of the more recent attempts to port broadcast news recognition systems is presented in Giuliani and Federico 2001 Bertoldi et al 2002 In this research a broadcast news transcription system for Italian is ported to the domain of spontaneous dialogue systems The acoustic model HMMs is adapted through Maximum Likelihood Linear Regression MLLR The language model is adapted through recursive interpolation Federico and Bertoldi 2004 by combining trigrams from the source model with trigrams computed on relatively small texts from the target domain Another adaptation example is provided by Ariki et al 2003 in which the target is a recognition system for sports broadcast speech The language model was initially built on a collection of documents gathered from the World Wide Web by selecting documents related to a particular sport A smaller adaptation corpus was also built using manual transcription of sport event recordings The final language model was built by interpolating the initial and adaptation language models a technique that increased the accuracy on two test sets by 16 8 and 11 1 respectively no details are given on the baseline WER Useful Transcriptions of Webcast Lectures 4 1 EXISTING RESEARCH ON ADAPTING AND BUILDING LANGUAGE MODELS FOR DEDICATED DOMAINS 75 Adapting existing models to the task of lecture presentation speech recognition was also the solution chosen in Spontaneous Speech Corpus and Processing Technology Kawahara et al 2001 a large project aimed at the development inter alia of a corpus of spontaneous speech consisting of lectures and oral presentations Broadcast news is not the only source for building language models for adaptation to particular domains As presented in Bacchiani and Roark 2004 metadata can be used to condition the language model in contrast to other approaches where the language model is conditioned by external knowledge sources such as the topic Metadata refers to any type of knowledge external to the speech signal for example the record of a customer calling a company s customer service The authors use caller IDs as metadata for their experiments with the SSNIFR corpus voice mail recordings at a customer care centre They report up to 27 WER for speech recognition on the SSNIFR corpus using metadata adapted models compared to 30 WER using models trained on the SSNIFR corpus When adapting language models to a new domain a variety of other sources of information can be used For example the World Wide Web is employed as a source for building topic specific corpora and language models in several topic dependent recognition tasks such as call routing applications by extracting relevant semantic information from web data as in Hakkani Useful Transcriptions of Webcast Lectures 76 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES building web queries of keyword centered N grams extracted from existing transcriptions as in Sarikaya et al 2005 However such approaches require the availability of either manual transcripts costly or impractical or of an automatic keyword extraction procedure not necessarily guaranteed to yield the most relevant keywords to a specific topic A further example of using relevant textual information to build topic specific LMs is provided independently by Seymore and Rosenfeld 1997 and Bigi and De Mori 2004 where the output from a first pass of an ASR system through the speech document is used as the main source of information for LM adaptation Seymore and Rosenfeld 1997 used the ASR output to detect a set of topics from a pool of 5000 topics that describe segments of the speech document to be transcribed Each detected topic is then used to extract that topic related part of the training corpus and build language models dedicated to the detected topic Such topic based adaptation reduces the WER from 40 to 35 ASR output can also be used to reduce the Out Of Vocabulary OOV rate which is the focus of the work presented in Bigi and De Mori 2004 The authors employ Information Retrieval IR techniques based on the Kullback Leibler Distance a measure of divergence between probability distributions to obtain relevant documents based on the hypothesized transcription Such documents are then used mostly to build better vocabularies for a second ASR pass While such technique can be exploited in LM adaptation the main benefits are a 28 reduction of the OOV rate The OOV rate is also the focus of Schwarm et al 2004 as new words needed by the target domain to which the language model is adapted even Useful Transcriptions of Webcast Lectures 4 1 EXISTING RESEARCH ON ADAPTING AND BUILDING LANGUAGE MODELS FOR DEDICATED DOMAINS 77 if added to the new vocabulary cannot always be integrated into the new language models This is mainly caused by a lack of sufficient training data from the new domain which typically leads to poor WER for vocabulary items that are specific to the target domain In particular the authors are concerned with such words in the context of automatic meeting transcription For this a variety of information sources are used ranging from e mails sent to the research groups which are the target of the meeting to papers related to the topic of the meeting Using linear interpolation of a general purpose conversational language model with a domain specific model a 9 overall reduction in WER and a 61 reduction in WER for new vocabulary items can be achieved Underlying problems related to LM adaptation are also addressed in Useful Transcriptions of Webcast Lectures The authors 78 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES report a WER of between 15 5 and 12 5 although no baseline WER is mentioned 4 2 Previous Work on Automatic Speech Recognition for Lectures Transcribing lecture presentation speech is a research topic still in its infancy The challenges met by the task of recognizing open domain speaker independent large vocabulary continuous and noisy speech are very hard to overcome While a significant amount of research effort has been spent on improving speech recognition for lectures and presentations the quality of the transcripts typically WERs of 30 40 at most 20 in particular conditions is still below that of other domains such as broadcast news transcriptions An important effort directed at lecture transcription has been carried out by several Japanese research groups as part of the larger project The Corpus of Spontaneous Japanese The approach proposed in the context of this project Kato et al 2000 was to train the language model on corpora consisting of lectures and presentation transcriptions 837 000 words were collected for this purpose with a vocabulary of 32 000 unique tokens The language model is a tri gram model trained on this collection The model is topic independent all words related to a specific lecture topic having been removed It is assumed that the remaining words especially those which frequently appear across lectures are related to various acts of the presentation ranging from filler words such as hesitations to commands for a Useful Transcriptions of Webcast Lectures 4 2 PREVIOUS WORK ON AUTOMATIC SPEECH RECOGNITION FOR LECTURES 79 voice operated projector The language model is then adapted to a specific topic for recognition of individual lectures The adaptation is performed through linear interpolation of the general model with a topic dependent model trained on a collection of technical papers indicated by the instructor of each lecture as bibliographical and background material The reported improvement ranges from 18 to 26 WER for an artificial evaluation with 3 speakers each giving a 10 minute technical presentation Another approach also relying on presentation slides to adapt the language model was followed by Rogina and Schaaf 2002 wherein all content words extracted from the slides are ranked according to their 1 A brief definition of more formal and detailed description can be found in Manning and Useful Transcriptions of Webcast Lectures 80 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES algorithm and achieves alignment error rates of 27 31 the authors did not compare this to using manually generated transcripts Significant progress has been recorded lately in improving the recognition of continuous large vocabulary speaker independent speech such as broadcast news mostly due to the increased availability of large annotated corpora However for the domain of lecture presentation transcription such necessary resources are very scarce For this reason E Leeuwis and colleagues 2003 used the TED corpus combined with the American English newspaper Wall Street Journal dictation corpus The TED Translanguage English Database corpus released by ELRA and LDC in 2002 is a collection of 188 manually transcribed recordings of talks given in English at the Eurospeech conference in 1993 Both the acoustic and language models were adapted The acoustic model was adapted from the Wall Street Journal on the TED corpus using Maximum Likelihood Linear Regression MLLR Leggetter and Woodland 1995 The language model was adapted through recursive interpolation and non linear smoothing from a 15 million word corpus of scientific papers and a 300 000 word corpus of conversational speech transcripts to the 55 000 word TED training corpus The language model was then adapted to a specific speaker by using various parts of the paper presented Word error rates were reported as 44 2 adaptation using the title 43 9 using the abstract and 43 8 using the entire text of the paper using Probabilistic Latent Semantic Analysis Hofmann 1999 to perform the adaptation Using the entire text and a mixture model obtained through interpolation at the level of relative frequencies achieved a 39 2 error rate Useful Transcriptions of Webcast Lectures 4 2 PREVIOUS WORK ON AUTOMATIC SPEECH RECOGNITION FOR LECTURES 81 As webcast of lectures are becoming increasingly common new research is emerging on automatic transcription of lectures Such a project was recently started by the Spoken Language Systems Group at MIT Glass et al 2004 Park et al 2005 Hsu and Glass 2006 The goals of this project are to improve upon the ASR process for lectures mainly for the purpose of document indexing As part of this project a lecture browsing interface Cyphers et al 2005 and illustrated in Figure 2 2 from Chapter 2 was developed that assists users in querying a lecture for information This interface is not a multimedia visualization tool instead being quite similar to the story retrieval tool of Whittaker and Hirschberg 2003 The ASR part of the project is focused on reducing the OOV rate and the WER The OOV rate is reduced by finding the appropriate adaptation Useful Transcriptions of Webcast Lectures 82 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES a more significant role WER with AM only adaptation on the same corpus is 41 2 This can be explained by the large amount of material from the same lectures used in the AM adaptation which unfortunately is not always a practical solution 4 3 Web Based Language Modelling for Automatic Lecture Transcription Significant research efforts are dedicated to the improvement of LMs for lecture transcription the main goal being finding appropriate methods of modelling the dual nature of lecture speech it is characterized as large vocabulary continuous speech speaker independent and as topic and domain specific Typically solutions for this problem were sought by building separate LMs targeting the reduction of WER independently for each trait while the recognition was performed using either interpolated LMs or separate models followed by a combination of the resulting hypothesis spaces One of the disadvantages of previous approaches is the more complicated process of determining and acquiring the most appropriate corpora for building two or more separate models such as in Kato et al 2000 or Leeuwis et al 2003 Other shortcomings include the need to fine tune the interpolation or hypothesis space combination parameters as well as the difficulties in automatically or manually extracting reliable topic specific keywords from additional knowledge sources e g lecture slides as in Rogina and Schaaf 2002 In contrast the approach proposed in this Useful Transcriptions of Webcast Lectures 4 3 WEB BASED LANGUAGE MODELLING FOR AUTOMATIC LECTURE TRANSCRIPTION 83 Chapter eliminates the need for multiple models and for keyword extraction procedures yet achieves similar or better WER reductions It exploits the slides used in the lecture or presentation to be transcribed by using the entire content of the slides as web search queries it retrieves web corpora that can be used to directly train a single LM suitable for both the conversational and the topic specific styles of lectures This section presents the implementation details of the proposed method discuss several web retrieval and training alternatives and compare the best solution with current interpolation based LM adaptation methods 4 3 1 General Algorithm The fundamental principle of the proposed web based language modelling method consists in treating the entire content of the lecture presentation slides as the source of external knowledge used in building the dedicated LMs As it will be shown in this Section this approach eliminates the need to build both topic dependent and general purpose LMs Figure 4 1 describes the algorithm used to collect the web based corpora used in training lecture specific LMs It assumes every lecture is accompanied by slides which are mostly organized in bullet form one idea constitutes a line on the slide however every line on the slides is treated as a separate web query even if part of a larger text Figure 4 2 shows several examples of typical queries based on lines from slides There is no pre processing of the slides each web query being an exact copy of a slide line Not all lines consist exclusively of topic specific keywords since many slide bullets do not contain any keywords while some lines are artifacts of the slide conversion process Useful Transcriptions of Webcast Lectures 84 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES which ensures that the corpora retrieved using such queries appropriately matches both the topic specific and conversation style of a lecture Figure 4 3 illustrates the differences in the type of documents that are retrieved for different queries for every slide Si in a lecture for every line Li j on slide Si define Ti j as the text of line Li j run web search with query Q Ti j retrieve most relevant first N documents in PDF format convert retrieved documents to text corpora Ci j Figure 4 1 The algorithm for using the lecture slides and the World Wide Web to build corpora on which lecture specific LMs are trained 4 3 2 Corpora Adjustment Several parameters can be adjusted both during corpora retrieval and language modelling Number of documents to be retrieved for each slide line N in Figure 4 1 which will be the main factor influencing the size of the final corpus Percentage of non dictionary words permitted corpus filtering For each retrieved document Ci j sentences or lines in Ci j for which Useful Transcriptions of Webcast Lectures 4 3 WEB BASED LANGUAGE MODELLING FOR AUTOMATIC LECTURE TRANSCRIPTION 85 Figure 4 2 Examples of slide bullets used as web queries These apparently un related examples are from the same lecture Conceptual Design of the third year Computer Science undergraduate course The Design of Interactive Computational Media the number of words not in dictionary exceeds a desired threshold are removed from the corpus This is useful for preserving the integrity of the LM with respect to the pronunciation dictionary 4 3 3 LM and ASR Scope Alternatives Once all corpora Ci j are collected and filtered LMs can be built using the entire collection or a slide specific selection Three alternatives are proposed one LM for the entire lecture one LM for each slide and one LM for each cluster of slides For the latter two the slides must be time indexed by recording the time of each change of slides One LM per lecture in order to obtain a single model M for the entire lecture all collected corpora will be joined in a single corpus C i j Ci j on which M will be trained One LM per slide for every slide Si of a lecture a corresponding LM Mi Useful Transcriptions of Webcast Lectures 86 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES Figure 4 3 Examples of documents retrieved from the World Wide Web relevant to two of the queries in Figure 4 2 Useful Transcriptions of Webcast Lectures 4 4 EMPIRICAL EVALUATION will be trained for every Ci 87 Ci j During the recognition process j a separate model Mi will be used for the audio segment of the lecture corresponding to the time span of slide Si One LM per cluster of slides assuming slides are numbered chronologically2 for each slide Si a cluster of slides of range r is defined as S i r k Sk i r k i r The related corpora are k also clustered in the same manner C i r where Ck j Ck i r k i r Ck j Thus individual LMs M i r are separately trained on corpora clusters C i r and subsequently used during recognition for the audio segments associated with the time span of S i r 4 4 Empirical Evaluation An extensive evaluation of the approach proposed in Section 4 3 1 was carried out in which several combinations of corpora and LM scope parameters were tested as the proposed method was not refined during the evaluation no developmental iteration was performed The evaluation was carried out using the SONIC toolkit Pellom 2001 We used the acoustic model that is part of the toolkit3 built on 30 hours of data from 283 speakers from the WSJ0 and WSJ1 subsets of the 1992 development set of the Wall Street Journal WSJ Dictation Corpus LDC 1994 2 If the same slide is displayed more than once during a lecture e g it s re visited by the lecturer the multiple occurrences of that slide are treated as separate slides and numbered accordingly 3 The AM related parameters were the same as those described in detail in Section 3 3 Useful Transcriptions of Webcast Lectures 88 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES For all the LMs used web based as well as baseline models a pronunciation dictionary was custom built to include all words appearing in the corpus on which the LM was trained The pronunciations were extracted from the 5K word WSJ dictionary included with the SONIC toolkit and from the 100K word CMU pronunciation dictionary CMU 1998 For all models one non dictionary word was allowed per line of corpus only for lines longer than four words using the CMU CAM Language Modelling Toolkit Clarkson and Rosenfeld 1997 with a training vocabulary size of 40K words the out of vocabulary rate was low for all 4 4 1 Test Data The test data consist of four lectures of approximately 50 minutes each recorded in different weeks of the same course The recordings used for our evaluation were collected in a large amphitheatre style lecture hall 200 seating capacity using the AKG C420 head mounted directional microphone English The lecturer is male early 60s and a native speaker of The recordings were not intrusive and no alterations to the lecture environment or proceeding were made The 1 channel recordings were digitized using the TASCAM US 122 audio interface as uncompressed audio files with 16KHz sampling rate and 16 bit samples The audio recordings were manually segmented at pauses longer than 200ms Useful Transcriptions of Webcast Lectures 4 4 EMPIRICAL EVALUATION 89 4 4 2 Web Based Lecture Models For each of the four lectures all LM training options described in Section 4 3 3 were considered with a range r 1 for the cluster option In terms of the number of retrieved documents Section 4 3 2 for each LM training option three values were allowed for N 10 20 and 30 documents per bullet the actual number was in some cases slightly lower than N due to web retrieval and PDF conversion errors The Google APIs http code google com were used for returning the URLs of the web documents relevant to each query the search was limited to documents in English 4 4 3 Baseline Models The transcripts of the Switchboard SWB corpus Godfrey et al 1992 were used for training the baseline model SWB LM The SWB corpus is a large collection of about 2500 scripted telephone conversations between approximately 500 English native speakers making it a suitable choice among general purpose LMs for the conversational style of lectures as also suggested in Park et al 2005 In order to compare the proposed web based modelling with the typical approach of using interpolation based optimizations for domain specific ASR two more baseline LMs were built for each of the four lectures a topic specific model and one built through the typical approach of adapting a general purpose model to a specific topic For these a set of keywords relevant to each lecture was manually extracted4 from the slides by the 4 While several automatic both supervised and unsupervised keyword extraction algorithms exist Turney 2000 they do not produce entirely accurate results a Useful Transcriptions of Webcast Lectures 90 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES teaching assistant associated with the course A query was constructed with the selected keywords and 200 relevant web documents were retrieved on which a language model KEYW LM was trained Finally the KEYW LM was statically interpolated with an interpolation weight 0 5 with the SWB LM to generate the third baseline LM Each of the baseline LMs was accompanied by a lexicon constructed in the manner described at the beginning of this section LM scope Docs per slide bullet 10 Lecture 20 30 10 Slide 20 30 10 Cluster 20 30 1 42 34 41 71 42 03 51 02 48 37 47 38 46 94 46 17 45 21 Lecture 2 41 38 40 70 41 01 51 25 49 06 47 63 46 99 46 24 46 31 3 44 19 44 18 43 73 50 63 49 79 49 04 49 02 49 43 48 36 4 48 35 47 56 46 95 57 29 55 26 54 60 54 41 52 89 52 65 Table 4 1 Web based lecture LM the WERs corresponding to the training options described in Section 4 3 3 human based assessment of such algorithms producing relevance scores in between 60 and 80 As such the evaluation of LMs was restricted to using manually extracted keywords in order to avoid confounding this evaluation with the keyword extraction algorithm Furthermore keyword selection is regarded by other disciplines such as Information Studies e g Gross and Taylor 2005 as a subjective and task dependent process that cannot always be quantitatively compared to automatic methods Useful Transcriptions of Webcast Lectures 4 4 EMPIRICAL EVALUATION Lecture LM SWB KEYW SWB KEYW LECT 1 2 3 4 91 47 26 48 08 48 71 50 48 44 04 45 39 46 39 50 39 41 11 43 43 42 64 46 39 41 71 40 70 43 73 46 95 Table 4 2 The WERs corresponding to the best web based lecture models LECT compared to the baseline model built on general purpose conversational texts SWB to the baseline model built using manually extracted keywords defining the lecture topic KEYW and to the baseline obtained through interpolation SWB KEYW 4 4 4 Results WER Reduction Table 4 1 presents the WER for each of the four lectures on ASR runs using the LMs described in this section The lowest WER among the web based LMs is achieved for training over the corpus relevant to the entire lecture where the number of documents retrieved for each slide bullet ranges from 20 to 30 Comparatively as described in Table 4 2 and illustrated in Figure 4 4 the baseline model SWB LM yields WERs higher on average by relatively 11 while the average difference between the web based LMs and the best model trained with manual supervision SWB KEYW is less than 1 Useful Transcriptions of Webcast Lectures 92 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES LM scope Docs per slide bullet 1 Lecture 2 3 Precision 4 Lecture Lecture SWB KEYW 20 30 93 41 92 79 91 39 93 01 93 99 91 64 91 33 87 22 89 88 90 60 92 02 93 01 90 20 91 26 92 78 87 70 87 90 83 24 85 71 85 52 SWB KEYW Recall Lecture Lecture SWB KEYW SWB KEYW 20 30 81 04 80 26 57 92 79 48 77 14 81 94 81 44 51 55 75 52 74 48 71 49 71 49 57 02 69 01 69 01 67 72 68 99 48 73 64 56 59 81 Table 4 3 Precision and Recall scores for keyword detection when using the best training options as indicated in Table 4 1 for the web based models compared to the baseline model built on general purpose conversational texts SWB to the baseline model built using manually extracted keywords defining the lecture topic KEYW and to the baseline obtained through interpolation SWB KEYW Useful Transcriptions of Webcast Lectures 4 4 EMPIRICAL EVALUATION 93 Figure 4 4 Average WER scores across all lectures for the baseline model built on general purpose conversational texts SWB the interpolation based optimization SWB KEYW and the best web based LMs LECT 4 4 5 Results Precision Recall of Keywords Text transcripts are often used in automatic information retrieval tasks e g using queries to search a webcast lecture repository for a particular topic However one of the challenges associated with such retrievals is the accuracy of keyword transcription during the ASR process For this the proposed web based modelling was also evaluated for lectures through the Precision and Recall5 of the transcribed keywords measured against the manual transcripts Since no general agreement exists on how to automatically identify such keywords a manually generated list of keywords was used an approach similar to that taken in Park et al 2005 where 5 A definition of Precision and Recall is given in Appendix B Glossary of Technical Terms Useful Transcriptions of Webcast Lectures 94 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES Figure 4 5 Average Precision and Recall scores for keyword detection across all lectures when using the baseline model built on general purpose conversational texts SWB the interpolation based optimization SWB KEYW and the best web based LMs LECT Useful Transcriptions of Webcast Lectures 4 5 LIMITATIONS AND GENERALIZATIONS 95 the course textbook s index was used For each lecture the list was set to an arbitrary length equal to 1 of the number of words in the manual transcript of each lecture and words on the list were selected by the teaching assistant associated with the course from all words appearing on slides Table 4 3 compares the Precision and Recall scores of the two best web based models with that of the baseline and the keyword based models As can be observed and also illustrated in Figure 4 5 Precision scores are higher for the web based models than for the Switchboard models and similar to those for the keyword interpolated Switchboard model while Recall scores are higher for web based models even than those for the manually supervised SWB KEYW model 4 5 Limitations and Generalizations The solution proposed in this chapter relies on the textual content of lecture slides and related documents retrieved from the World Wide Web to improve the accuracy of lecture ASR systems Therefore it is expected that its applicability will be limited6 to classes that are accompanied by slides containing most of the information in textual natural language form Furthermore it is expected that the lecture content concepts examples etc delivered by the instructor will be contained within the content summarized by the slides although as shown by the evaluation Section 4 4 4 the order in which the content is delivered does not need to be the same as that of the 6 Section 7 2 1 discusses various proposals for future work that include enlarging the applicability of the proposed solution to other domains and conditions Useful Transcriptions of Webcast Lectures 96 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES slides The extremely large number and diversity of documents that can be retrieved from the World Wide Web suggest that relevant material can always be found for any lecture However there might be lectures on more obscure topics for which less relevant training data is retrieved In such cases it is expected that the proposed web based LM solution will still produce adequate conversational models as mentioned in this Chapter the non topic parts of the lecture slides are responsible for the conversational aspect of the LM while the Recall of topic specific keywords will be lower 7 Future work should explore combining the proposed web based lecture LM with Information Extraction methods such as in Sarikaya et al 2005 to filter the retrieved web corpora for more topic relevant documents when the lecture topic is less popular Despite the limitations mentioned above the proposed LM solution can be generalized to other domains For example in contrast to the approaches for improving lecture ASR that will be introduced in Chapters 5 and 6 the web based lecture LM can be applied to tasks requiring a faster recording to transcript time This can be as low as the real time factor of the ASR system employed if lecture slides are available before the lecture is recorded thus allowing for the web corpus to be retrieved and the LM trained on it Furthermore the proposed LM solution could serve as the basis of LM methods that can be generalized to other domains For example if a 7 As suggested earlier in this Chapter the high Recall values for the topic keywords of the web based lecture LM is due to the straightforward slide to query conversion resulting in many web queries containing only topic keywords Useful Transcriptions of Webcast Lectures 4 6 SUMMARY AND DISCUSSION 97 lecture or presentation does not have well structured slides the web based LM approach can be enhanced with topic modelling to identify areas on slides or on other presentation materials that are promising candidates for serving as web queries 4 6 Summary and Discussion A novel algorithm for corpora building and language modelling aimed at improving the accuracy of automatic lecture transcription was introduced in this chapter The proposed approach uses the entire content of the slides presented in a lecture to build a language model that captures both the conversational style and the topic specific content of that particular lecture This leads to a reduction in WER of up to 11 relative to baseline algorithms using general purpose language models as shown through an empirical evaluation carried out over several recordings of university lectures In contrast to existing optimizations that rely on the interpolation of very large general purpose LMs with smaller topic specific models the proposed approach eliminates the need for multiple models By using the entire content of the presented slides as web queries only a single web based corpus needs to be collected on which a language model is trained The WER reductions are similar or better than those of existing methods based on interpolating general purpose with topic specific models as well as improved Precision and Recall scores for keyword identification One of the drawbacks of interpolation based optimizations is selecting the keywords that define the topic of the recording to be transcribed Useful Transcriptions of Webcast Lectures 98 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES This is needed to build an accurate topic specific LM however manual intervention or fine tuning of an automated algorithm is usually required to accurately define such keywords Beside allowing for an entirely automated lecture specific modelling the approach proposed in this chapter does not rely on identifying keywords for each lecture topic Therefore this unsupervised method is suitable for integration into webcast archive systems such as ePresence where the process of publishing a recording must be fully automated in order to maintain its cost effectiveness Despite the improvements to the ASR performance for lecture transcriptions brought by the research presented here there still exists a significant gap between the desirable and actual WER the lecture specific Web based modelling leads to average WERs of 43 below the usefulness threshold of 45 for some tasks as discussed in Section 3 4 2 but above the desired threshold of 25 determined for all tasks described in Chapter 3 As such Chapter 5 presents an HCI in particular user collaboration based solution aimed at reducing and in some cases even eliminating this gap Useful Transcriptions of Webcast Lectures Chapter 5 Wiki editing of Webcast Transcripts As manually transcribing the increasing amount of available on line recordings is an expensive solution such task would ideally be accomplished by an Automatic Speech Recognition ASR system However due to adverse acoustic and linguistic characteristics large vocabulary speaker independent continuous speech imperfect recording conditions currently available ASR systems do not perform satisfactorily in domains such as lectures or conference presentations As it was shown in Chapter 3 The Acceptable Word Error Rate of Machine Generated Webcast Transcripts and in Munteanu et al 2006a when using a fully featured webcast browsing tool users task performance and perception of difficulty was better than using no transcripts at all only for transcripts with Word Error Rates WERs equal to or less than 25 It was also shown that for most browsing scenarios users prefer having 99 100 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS transcripts even if their quality is less than optimal Unfortunately most recognition systems achieve WERs of about 40 45 in the acoustically and linguistically challenging context of lecture recordings Leeuwis et al 2003 Munteanu et al 2007 Moreover it is expected that such systems will not reach perfect or near perfect accuracy in the near future Whittaker and Hirschberg 2003 In order to achieve useful transcripts of archived webcast lectures in this chapter I am proposing and evaluating an alternative tool to reduce current Word Error Rate WER levels of 40 45 to the desired 25 or better For this I have developed an interactive tool that extends the ePresence webcast system s functionality by facilitating in a wiki manner the collaboration between users mainly students attending such lectures in correcting the ASR produced errors in the lecture transcripts The editing tool is seamlessly integrated into the regular archive viewing mode of our webcast system allowing users to make corrections on the fly while viewing an archived webcast This tool was evaluated in a field study showing that it provides a feasible solution for improving the quality of webcast lecture transcripts In this chapter I will first briefly survey other research efforts directed at improving the quality of ASR transcripts for lectures as well as examples of research where human collaboration was employed to correct or compensate for various computational shortcomings Section 5 1 I show how transcripts can be integrated into webcast archives Section 5 2 and present the design and implementation of the wiki editing tool Section 5 3 I then describe the evaluation of the editing tool through a field study Sections 5 4 and 5 5 followed by an analysis of the collected data Section 5 6 and the re design Useful Transcriptions of Webcast Lectures 5 1 RELATED RESEARCH 101 and re evaluation of the tool based on the findings of the field study Section 5 7 5 1 Related Research The most commonly used measure for the quality of a speech recognition system is the word error rate WER lecture speech typical WER for lecture speech can reach rates as high as 50 when general purpose ASR systems are used Park et al 2005 Munteanu et al 2007 Despite significant research efforts Leeuwis et al 2003 Munteanu et al 2007 WERs are still greater than 40 under unsupervised un controlled training conditions with further reductions of WER only achieved by strictly controlling the ASR training conditions Furui 2005b Useful Transcriptions of Webcast Lectures 102 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS exists that address the cost of this approach for reducing the WER of transcripts Human intervention however has been successfully used to correct ASR errors such as in the voicemail transcript editing system introduced by Burke et al 2006 which proposes solutions to assist users in correcting transcripts when input capabilities are restricted such as in mobile environments In areas collaboration various other scientific computer supported has emerged as an alternative to single user intervention For example it was shown in two separate studies von Ahn and Dabbish 2004 Volkmer et al 2005 that the task of indexing and labelling a large collection of images for query based carried out retrieval using can be web based Our transcript enhanced collaboration Collaboration has Figure 5 1 also been successfully applied ePresence system displaying a screen to various other tasks from capture of the system with transcripts controlling a mechanical robot of 45 WER over the Internet Goldberg et al 2000 to open source software development Crowston et al 2004 and to geographic information mapping Li and Coleman 2002 Recent years have Useful Transcriptions of Webcast Lectures 5 2 ENHANCING WEBCASTS WITH TRANSCRIPTS 103 also witnessed an increase in online collaborative writing mainly in the form of wikis from large scale encyclopedias http wikipedia org to classroom projects Forte and Bruckman 2006 5 2 Enhancing Webcasts with Transcripts For this research the transcript enhanced version illustrated in Figure 5 1 of the ePresence webcast system was employed As described in Chapter 3 the ePresence system allows users to control the playback and interaction with a video archive as well as to access the slides used in the recorded presentation or lecture The interactive clickable transcripts are synchronized with the playback emulating a closed captioned system while fully displaying the transcript of the segment of lecture for the current slide complete description of this system is given in the introductory part of Chapter 3 and detailed in Figure 3 1 on page 30 5 3 Managing Imperfect Transcripts Current ASR systems deliver transcripts of webcast lectures and presentations of 40 45 WER while the necessary WER threshold is 25 as shown in Munteanu et al 2006a The collaborative editing tool that we developed for our webcast interface allows users to correct and edit the transcripts It extends the basic functionality of the system without burdening the user at the same time During regular playback of a webcast archive users can right click Useful Transcriptions of Webcast Lectures 104 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS on any transcript line not necessarily the one currently being played back and an edit box Figure 5 2 is displayed allowing users to make corrections to the selected line 1 This line becomes highlighted in red which potentially differentiates it from the current line which is bold faced Besides colour highlighting the edit box is popped up on the screen about two transcript lines above the selected line to maintain a visual connection with the transcript context Figure 5 2 Wiki like editing of imperfect transcripts To avoid editing conflicts a server side locking mechanism prevents users from simultaneously editing the same line When trying to edit a locked line users are informed that the line is being edited by a different user and that 1 While many systems that allow user correction of ASR transcripts employ text shading as a method of conveying information about possible errors as in Burke et al 2006 such methods rely mainly on ASR own confidence scores and are not always reliable Since other studies e g Vertanen and Kristensson 2008 suggest that ASR confidence visualization does not always help users As such no ASR confidence information was included in the proposed system Useful Transcriptions of Webcast Lectures 5 3 MANAGING IMPERFECT TRANSCRIPTS 105 a browser refresh might be needed to update the transcript webcasts need accurate time synchronization between all components so regularly checking for transcript updates is not possible This on the fly editing mode has the advantage of being light weight on the 5 3 1 Features of the Transcript Edit Tool Edit area users can freely make corrections to the transcript line displayed in the edit box Suggestion drop down when right clicking on words in the edit box a list of possible replacement words is displayed These are choices under consideration by the ASR system during the recognition process and extracted from the word lattices produced by the ASR Useful Transcriptions of Webcast Lectures 106 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS deletions insertions and substitutions Also editing access can be restricted to certain users up to the level of transcripts corresponding to certain slides which is useful for defining a collaboration model of students lecture transcript editing 5 4 A Field Study A field study was carried out to assess the feasibility of the proposed tool and to obtain further insight into how users accept and manage lectures with imperfect transcripts In this section we describe the setup of this in situ evaluation while the following section will present the results and recommendations arising from the field study 5 4 1 Research Objectives As we propose a collaborative tool that engages users to edit and correct imperfect transcripts of webcast lectures we devised and conducted a field study whose main objectives were to assess The feasibility of the interface for wiki editing of webcast transcripts as a solution for completing the task of improving the quality of computer generated transcripts User experience when using the transcript enhanced webcast system and the editing tool encompassing several components users acceptance of such interface transcript quality s influence on user experience the attitude towards using it in a real lecture setting and other Useful Transcriptions of Webcast Lectures 5 5 METHODS 107 indirect benefits gained by users such as better lecture comprehension Moreover users confidence in using the system was also measured 2 Appropriate motivational schemes for increasing users students involvement and effectiveness in wiki editing of webcast lecture transcripts General user feedback on the interface design elements that can be improved in order to maximize the benefits of transcript editing such as to improve the amount of edits that can be done in a short period of time 5 5 Methods The field study was carried out in the context of a real classroom over a 13 week semester consisting of 21 lectures each approximately one hour long The lectures are part of the same course third year Computer Science course Although the recordings took place in classroom all measures were taken such that the recording did not interfere with the regular lecture proceeding 26 students were enrolled in the course while typical classroom attendance was approximately 15 students every week The lecture recordings were made available online using our webcast system within a day of the lecture date 2 Since our previous laboratory based experiment Munteanu et al 2006a also investigated users acceptance of machine generated transcripts as an enhancement of webcast systems a similar assessment was conducted for the present field study in a real life setting Useful Transcriptions of Webcast Lectures 108 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS 5 5 1 System Our webcast interface is entirely web based and the recordings can be accessed through most browsers Internet Explorer and Mozilla Firefox on several platforms Windows Linux Mac OS without installing additional software beside the Real Player plugin which is freely available for a variety of browsers and platforms For every recorded lecture the webcast system gives users full control of the archive mainly through the display of the slides used in lectures and the video recording through interaction with the TOC at the left of the screen which contains chapter headings and the title of the slides and through the timeline an interactive clickable fine grained time progress indicator Textual transcripts of the recording are displayed below the slide The lines of text are time synchronized with the video by boldfacing the current line of the transcript thus emulating a closed captioning system while fully displaying the transcript of the segment of lecture for the current slide The line breaks do not represent ends of sentences but rather correspond to pauses longer than 200ms To further enhance the user s control over the lecture users can re synchronize the playback of the video by clicking on a line in the transcript The transcripts were editable as described in Section 5 3 Transcripts were obtained using the SONIC ASR toolkit3 Pellom 2001 The lecturer is male late 30s native but accented speaker of English Due to the high speaking rate accent and speaking style the WER was between 50 and 60 3 The AM and LM used for this task were those described in detail in Section 3 3 Useful Transcriptions of Webcast Lectures 5 5 METHODS 109 5 5 2 Task and Procedures Participants were provided access to the web based interface for accessing webcast archives No restrictions were imposed on how the participants watched the recorded lectures They were encouraged to make use of these as an additional course material and as thus it was linked from the course website With respect to editing the lecture transcripts users were also left the choice of which lectures and which parts of lecture to correct Since the purpose of the study was to investigate the use of the wiki editing tool in a real situation no further requirements beside watching lectures and editing transcripts were formulated during the course of the in situ evaluation All participants were required to complete a questionnaire and brief interview after the course was completed The first 9 of the total of 21 lectures were freely accessible to all students in the class while the rest were available only to the participants of the field study Transcript editing was restricted to users registered as participants in the study The remaining 12 lectures could be accessed unlocked by participants through a credit gaining scheme for each user the number of words edited was recorded as credits that can be exchanged for access to the restricted lectures In a previously run pilot study we have determined that students correct an average of 300 erroneous words per hour In agreement with the course lecturer4 the required amount of participants involvement was set to 4 hours during the entire semester beside normally watching the lecture recording As such the amount of credits needed to unlock access to one lecture 4 The authors were not affiliated with the course in which the study took place Useful Transcriptions of Webcast Lectures 110 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS was set to 80 words thus allowing full access to all lectures in exchange of 4 hours of transcript editing As an additional incentive for editing the transcripts students received a small course grade bonus and a modest financial compensation according to the amount of transcript corrections significant only for those dedicating more than 4 hours 5 5 3 Participants The study was conducted with 15 participants all third year Computer Science undergraduate students enrolled in the course that was recorded participation in the study was not compulsory Two participants had previous experience with the webcast system as it is used without transcripts for other courses However due to the intuitiveness of the webcast interface controls no training was required for the other participants beside a brief explanation of the system s web based controls Moreover all participants indicated they are familiar with various forms of Internet based media 5 5 4 Instruments and Measures In order to answer the research questions that motivated this field study four types of data were collected task completion data user experience data involvement and motivation data and general user feedback The instruments used to collect these data are included in Appendix C Useful Transcriptions of Webcast Lectures 5 5 METHODS Task completion 111 One of the objectives of this field study is to assess the feasibility of the wiki editing tool as a solution for improving the quality of the lecture transcripts As such we have collected data indicating what percentage of the lecture transcripts were corrected by users As it will be shown in Section 5 6 1 the task completion is assessed through the percentage of edited transcript lines sentences as well as through the relative WER reductions a commonly used measure of ASR accuracy User experience A post study questionnaire was used as the instrument for collecting user experience data The questionnaire consisted of multiple choice questions and indicated agreement disagreement with various statements The user experience was assessed through a series of indicators User acceptance This indicator measured students willingness to use the transcript enhanced webcast system through statements such as Being able to access lectures through the webcast system helps me better review the course material I would like to see the system used for more classes I didn t need the lecture archives the slides and examples on the prof s page are enough and I only need to review parts of lectures occasionally I don t need transcripts for that Transcript quality s influence on user experience was assessed through the answers to several statements I would rather use this system without transcripts Having transcripts for every lecture means I don t have Useful Transcriptions of Webcast Lectures 112 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS to attend classes anymore I think the quality of the transcripts was good enough for what I needed Attitudes toward wiki editing Measuring task completion rates as the percentage of corrected words provides an objective assessment of the feasibility of the wiki editing solution However a subjective evaluation from the users perspective is also needed For this we have collected data related to users attitudes toward the editing tool mainly focused on determining if students perceive the editing tool as a useful addition to the webcast system and if they are willing to use it Several statements on the questionnaire were used for this such as Being able to correct errors in the transcripts really improved access to the course material I think I also benefited from other users editing of the transcripts I would gladly help the class by editing transcripts for lectures using the webcast system I would have rather payed to access perfect transcripts than do my part of the editing and I rather go to class and take notes than edit transcripts Perception of indirect benefits Beside determining the attitudes toward using the wiki editing tool we queried students about how they perceived the educational benefits of the editing tool mainly motivated by the hypothesis that more exposure to the lecture material would be beneficial For this we have asked questions such as I don t think that my editing of the transcripts helped me better prepare for the course When editing the transcripts I payed more attention to the lecture and I think editing the transcripts helped me better understand the course material Useful Transcriptions of Webcast Lectures 5 5 METHODS 113 Confidence in using the system Similar to our previous study Munteanu et al 2006a and to compare differences between the use of transcripts in an artificial experiment with that of a real lecture setting participants indicated the context in which they would choose to use the transcript enhanced webcast system The contexts ranged from very critical to less critical Prepare for an examination instead of going to classes Prepare for an examination in addition to going to classes Prepare for an assignment and Make up for a missed class For each context participants could choose Yes No or Only if transcripts have no errors In addition to these a new choice was added to the possible answers Only if everyone is helping correct the transcripts Involvement and motivation In order to determine if the wiki editing tool is an appropriate solution for correcting the automatic transcription errors it is important to investigate how much time students are willing to dedicate for improving the lecture transcripts and how to better motivate them Multiple choice questions on the study completion questionnaire were used to collect data about the number of hours spent weekly for transcript editing the amount of time willing to spent and students estimate of the amount of time others would be willing to spend Ideally students would voluntarily edit the transcripts for the benefit of the entire class However this might not be a realistic expectation for smaller size classrooms Therefore we have also investigated possible Useful Transcriptions of Webcast Lectures 114 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS motivational schemes Beside the edit for access scheme employed during the field study we have also asked students to indicate their preference for others such as cost increases making transcript editing part of course requirements and edit for access combined with course bonus marks for editing more than the required minimum Preference for each of the scheme was indicated by choosing one of Fair Maybe or Not a fair deal options General user feedback We have invited users to also provide free form feedback as answers to an interview like questionnaire suggesting as possible dimensions features of the webcast system features of the editing tool positive negative impressions of the entire system and general comments Figure 5 3 The percentage of edited transcript lines and relative WER reduction for each of the 21 lectures after all transcripts were corrected Useful Transcriptions of Webcast Lectures 5 6 RESULTS 115 5 6 5 6 1 Results Task completion The improvements in transcript quality through collaborative editing are measured at sentence level through the percentage of corrected transcript lines but also in terms of relative WER reductions As Figure 5 3 shows most lectures had a significant number of sentences corrected for example 16 of the 21 lectures had more than 75 of transcript lines corrected On average 84 of all transcript lines were edited This resulted in an average relative WER reduction of 53 In order to facilitate users editing of transcripts no restrictions5 were imposed with respect to what users were allowed to type in the edit box resulting in inconsistencies6 between the 12 participants such as abbreviations formulas proper names even spelling errors that do not influence the text 5 The lack of restrictions was mainly motivated by the quest to understand the pattern of edits carried out by users however as explained in Section 5 7 1 users preferred to re transcribe entire lines 6 There were no spurious edits as all transcripts corrections were logged under a numeric user ID that was uniquely assigned to each participant However future versions of this system will include a spell checker and other vocabulary based editing restrictions in order to ensure consistency across users contributions Useful Transcriptions of Webcast Lectures 116 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS However while the 25 threshold was computed over uniformly distributed imperfect sentences the wiki editing of lecture transcripts creates a slightly uneven distribution of corrected sentences as illustrated in Figure 5 3 by the non linear variations in the percentage of corrected lines and WER reductions Strongly Q1 Q2 Q3 Q4 Agree 25 63 64 0 0 Agree Neutral 58 33 27 27 33 33 8 33 16 67 9 09 8 33 16 67 Strongly Disagree Disagree 0 0 50 66 67 0 0 8 33 8 33 Q1 Being able to access lectures through the webcast system helps me better review the course material Q2 I would like to see the system used for more classes Q3 I only need to review parts of lectures occasionally I don t need transcripts for that Q4 I didn t need the lecture archives the slides and examples on the prof s page are enough Table 5 1 User acceptance of the transcript enhanced webcast system Useful Transcriptions of Webcast Lectures 5 6 RESULTS 117 5 6 2 User Experience As previously mentioned user experience data was collected through a questionnaire administered at the end of the semester Students indicated their level of agreement with several statements related to the indicators described in Section 5 5 4 User acceptance Participants indicated their agreement with four questions relevant to their willingness to use the transcript enhanced webcast system As Table 5 1 shows most users consider it a necessary addition to traditional lecture preparation materials Strongly Q1 Q2 Q3 Agree 0 0 0 Agree Neutral 16 67 16 67 33 33 25 16 67 25 Strongly Disagree Disagree 33 33 41 67 33 33 25 25 8 33 Q1 I would rather use this system without transcripts Q2 Having transcripts for every lecture means I don t have to attend classes anymore Q3 I think the quality of the transcripts was good enough for what I needed Table 5 2 Users attitudes toward imperfect transcripts Useful Transcriptions of Webcast Lectures 118 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS The influence of transcript quality Table 5 2 shows users responses to questions assessing their acceptance of imperfect transcripts While users indicate they prefer having transcripts significant disagreement with Q1 these are shown to not provide the same experience as attending the lecture disagreement with Q2 the quality of the transcripts continuing to pose challenges Q3 It should be noted however that responses to Q3 could also be the result of users being exposed to transcripts of varying level of correctness as access to lecture archives as well as transcript editing was not uniformly distributed over the entire semester Attitudes toward wiki editing One of the areas that was the main focus of this field study is the users attitude toward wiki editing as a viable solution for improving the quality of the transcripts As illustrated in Table 5 3 students manifested a positive attitude toward the system not only as a transcript correction tool but as an enhancement of the classroom experience as well Perception of indirect benefits One of the hypotheses that motivated the field study was that of user editing of transcripts not only as a solution for improving the accuracy of the transcripts but as having the added benefit of providing students with more exposure to the lecture materials Indeed as Table 5 4 indicates most users perceive this as a benefit by disagreeing with Q1 and agreeing with Q2 although such benefits do not seem to stem from increased attention to the lecture slight preference for disagreement with Q3 Useful Transcriptions of Webcast Lectures 5 6 RESULTS 119 Strongly Q1 Q2 Q3 Q4 Q5 Agree 8 33 25 8 33 0 8 33 Agree Neutral 58 33 50 83 33 0 25 33 33 8 33 8 33 16 67 16 67 Strongly Disagree Disagree 0 16 67 0 33 33 50 0 0 0 50 0 Q1 Being able to correct errors in the transcripts really improved access to the course material Q2 I think I also benefited from other users editing of the transcripts Q3 I would gladly help the class by editing transcripts for lectures using the webcast system Q4 I would have rather payed to access perfect transcripts than do my part of the editing Q5 I rather go to class and take notes than edit transcripts Table 5 3 Users attitudes toward wiki editing Useful Transcriptions of Webcast Lectures 120 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS Strongly Q1 Q2 Q2 Agree 8 33 16 67 0 Agree Neutral 16 67 41 67 16 67 16 67 25 41 67 Strongly Disagree Disagree 41 67 0 25 16 67 16 67 16 67 Q1 I don t think that my editing of the transcripts helped me better prepare for the course Q2 I think editing the transcripts helped me better understand the course material Q3 When editing the transcripts I payed more attention to the lecture Table 5 4 Users perception of the indirect benefits of wiki editing of transcripts Useful Transcriptions of Webcast Lectures 5 6 RESULTS 121 Only if Yes transcripts have no errors Q1 Q2 25 75 16 67 0 8 33 8 33 Only if everyone is helping correct the transcripts 0 8 33 0 0 58 33 16 67 50 8 33 No Q3 41 67 Q4 83 33 Q1 Would you consider using the ePresence system to prepare for an examination instead of going to classes Q2 Would you consider using the ePresence system to prepare for an examination in addition to going to classes Q3 Would you consider using the ePresence system to prepare an assignment Q4 Would you consider using the ePresence system to make up for a missed class Table 5 5 Users confidence in using the system as a relation of transcript quality Useful Transcriptions of Webcast Lectures 122 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS Confidence in using the system Similar to our study that determined the minimum WER level for useful transcripts Munteanu et al 2006a we have assessed users overall confidence in using the system with respect to the application where the system is to be used The responses presented in Table 5 5 are consistent with the findings of our previous 5 6 3 Users Involvement and Motivation Out of the total fifteen participants two chose to contribute more than the required 4 hours of transcript editing The combined contributions of these two participants accounted for approximately 75 of the transcript corrections Ten more participants contributed the full required amount of editing thus gaining access to the entire archive of lectures Three participants contributed less than the required amount and did not respond to the questionnaires Users level of involvement was also assessed through the final questionnaire Participants indicated that they are willing to spend an average of approximately 50 minutes a week for transcript editing two users indicating 2 hours while other two indicating only 15 minutes However when asked about other students willingness to edit transcripts most of the Useful Transcriptions of Webcast Lectures 5 6 RESULTS 123 twelve responses estimated 15 minutes four responses or no time also four responses 7 Participants were also asked to estimate how much time they spent weekly editing transcript during the field 5 6 4 General User Feedback Given the particularities of this field study mainly evaluating a completely new concept wiki editing of imperfect transcripts in a real setting students attending lectures during an entire semester participants feedback is one of the most important aspects of the data collection Beside various comments related to the web interface such as video player plugin most of users feedback was focused on the editing tool and on the integration of transcripts While the shortcomings of the editing tool and the measures taken 7 A plausibility argument on the cost effectiveness of ASR improvements through wiki enabled user corrections is given in Section 7 2 7 Useful Transcriptions of Webcast Lectures 124 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS to address them are discussed in the following section the integration of transcripts into the webcast system generated several suggestions from users Both our previous study Munteanu et al 2006a and the analysis of the data collected during this field study show that transcripts are regarded as a necessary addition to the webcast system even if they are of lower quality However the participants indicated that the sheer amount of transcripts make them difficult to be skimmed through This suggest a more compact representation either as a summary or another high level representation of the lecture content may be more appropriate especially for lectures that are accompanied by information rich slides or discussing topics such as a programming language that are easily accessible elsewhere 5 7 5 7 1 Interface Re design and Re evaluation Assessment of Current Design In terms of the wiki editing the feedback provided by participants highlighted the need for a more versatile editing tool Due to the initial recording segmentation process by pauses of 200ms as described in Section 5 2 and given the fast speaking rate of the lecturer some lines of transcript were spanning longer audio segments 15 20 seconds This resulted in difficulties when correcting the transcripts and as such users suggested better playback control for the editing interface The second significant suggestion involved users approach to correcting transcripts Both through the open form responses and through specific questions on the final questionnaire it emerged that our initial model of Useful Transcriptions of Webcast Lectures 5 7 INTERFACE RE DESIGN AND RE EVALUATION 125 editing on the fly is mainly applicable to occasional corrections of the transcripts However in the context of the large scale editing needed to correct entire lectures an editing tool that incorporates transcript navigation is needed 7 of the 12 participants indicated they prefer to consecutively edit several transcript lines without switching to lecture watching Therefore a second editing tool was integrated with the interface that incorporates multi line transcript editing with slide navigation and better playback control 5 7 2 Extended Editing Mode The on the fly editing tool is activated by right clicking on transcript lines during regular webcast viewing and does not obscure the webcast interface while active being implemented as a small pop up window In contrast the extended editing tool replaces the webcast viewing mode when activated as it is designed for longer editing tasks For this users can click on the Edit this slide button from the slide navigation panel on the right of the webcast interface The extended editing mode Figure 5 4 allows full editing of a transcript line similarly to the editing box In addition the extended mode provides enhanced features for Transcript navigation users can navigate through all transcript lines corresponding to the current slide Enhanced audio playback the audio segment associated with the line of text being edited is controlled through the visually intuitive playback panel of the Real Player plugin RealNetworks 2004 Useful Transcriptions of Webcast Lectures Users can 126 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS Figure 5 4 The extended editing mode allowing for full control of the audio playback and for editing of consecutive transcript lines play pause stop the audio segment fast forward through it or use the slider for more accurate positioning This allows for better visual synchronization between the length of text and the audio segment as requested by participants after the first field study Enhanced editing the text area is no longer limited to a single row facilitating editing of longer transcript lines Users are also provided with an Undo options The Save button will instantly update the changes in the original transcripts stored on the server and since multiple lines of text can be edited in the extended mode will also advance to the next transcript line Return to archive exits the extended editing mode and returns the user to the webcast viewing mode Locking mechanism similar to the pop up editing tool a server side locking mechanism prevents simultaneous corrections to the same line For the Useful Transcriptions of Webcast Lectures 5 7 INTERFACE RE DESIGN AND RE EVALUATION 127 extended editing mode the lock is engaged only when the users click inside the text edit area avoiding unnecessary locks when users do not edit the current line When advancing to a locked line the edit buttons Save and Undo and text area are gray shaded and inactive Clicking on the text area of a previously locked line or of a line that became locked after the user advanced to it activates a prompt informing the user that the line is currently edited by a different user 5 7 3 Evaluation of the Re designed System The re designed webcast system enhanced with the extended editing tool was deployed for use by students in a different motivational scheme was modified the incentives to correct the transcript were limited to course bonus marks instead of both marks and financial compensation As this follow up study took place during the Summer term when students typically work full time and enroll in at most one course and due to the weaker incentives enrollment in the user study was lower than in the initial study Moreover the overall interest in viewing the lecture archives was significantly lower in part explained by the very informative lecture slides and the comprehensive reading package Five students from a class of 30 participated in the follow up study and four of them completed the final questionnaires Useful Transcriptions of Webcast Lectures 128 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS Since the focus of this re evaluation was to test the changes in the design of the transcript editing mode we will discuss here the main differences in collected user experience data in motivation data and in general user feedback User Experience With respect to the wiki editing of imperfect webcast transcript data collected from two indicators of user experience was analyzed attitudes toward wiki editing and perception of indirect benefits In terms of attitudes toward wiki editing the four participants in the follow up study responded mostly as in the initial study as illustrated in Table 5 6 Compared to the initial data described in Table 5 3 a slight shift toward positive attitudes was observed for Q2 and Q3 no Disagree responses for Q2 and only Agree responses for Q3 However the responses for Q1 were divided one of the four participants indicating disagreement with Q1 and the rest indicating agreement a difference from the initial study that can be explained by the fewer overall corrections of the lecture transcripts Differences were also observed for Q48 and Q5 where most responses were centered around the neutral answers a possible consequence of the part time nature of summer courses consisting mainly of full time working students The analysis of perception of indirect benefits data showed more divided responses than in the initial study Two of the four participants indicated they saw an overall learning benefit from transcript editing This difference can also be attributed to the higher availability of lecture preparation 8 One of the participants did not answer Q4 Useful Transcriptions of Webcast Lectures 5 7 INTERFACE RE DESIGN AND RE EVALUATION 129 Strongly Q1 Q2 Q3 Q4 Q5 Agree 0 25 0 0 0 Agree Neutral 75 50 100 0 25 0 25 0 66 67 50 Strongly Disagree Disagree 25 0 0 33 33 25 0 0 0 0 0 Q1 Being able to correct errors in the transcripts really improved access to the course material Q2 I think I also benefited from other users editing of the transcripts Q3 I would gladly help the class by editing transcripts for lectures using the webcast system Q4 I would have rather payed to access perfect transcripts than do my part of the editing Q5 I rather go to class and take notes than edit transcripts Table 5 6 Users attitudes toward wiki editing with the re designed system Useful Transcriptions of Webcast Lectures 130 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS materials than during the initial field study Motivation Although we expected students preference for motivational scheme to be significantly different in the follow up study due to the different course setting participants indicated the same preference 75 for using the credit earned for transcript editing to gain access to lectures Moreover all respondents supported the same scheme that also allows receiving of course bonus marks for additional editing General User Feedback Our analysis of participants free form feedback and of their responses to questions of preference for editing modes indicated a positive response to the introduction of the extended editing tool Compared to the initial study the same preference for editing larger ranges of transcript was manifested three of the four participants indicated a strong preference for this while one response indicated a preference for a mixed mode One participant explicitly commended it for allowing fast transcript corrections No other negative comments were collected regarding the editing modes suggesting that providing both alternatives for editing on the fly and extended is the appropriate solution for facilitating transcript editing Useful Transcriptions of Webcast Lectures 5 8 LIMITATIONS AND GENERALIZATIONS 131 5 8 Limitations and Generalizations The research presented here and the findings of the field studies are mostly applicable within the scope of university lectures and rely on the availability of specific incentives to elicit users contributions Some contributions of this research can be generalized to other environments such as large and popular online repositories of information media where either explicit motivation e g gaining access to recordings receiving financial compensations etc or intrinsic motivation e g users altruism is present However due to the high importance of the motivation for the success of the proposed solution more investigation is needed to determine the applicability of these findings to other domains and under different assumptions 5 9 Summary and Discussion The usefulness and usability of webcast archives of lectures and academic presentations can be significantly improved by the integration of text transcripts Unfortunately manual transcription can often be logistically or cost ineffective At the same time ASR systems for lectures yield error rates of 40 45 under realistic conditions below the 25 threshold of usefulness as determined in Munteanu et al 2006a and presented in Chapter 3 As a solution to bridging the WER gap we have developed a collaborative tool that extends the basic functionality of a transcript enhanced webcast system by engaging users to collaborate in transcript editing and correction for webcast lectures and presentations We have evaluated this tool through a field study carried out in the context of a real classroom and have shown Useful Transcriptions of Webcast Lectures 132 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS that this is a feasible solution for alleviating the ASR errors of webcast lecture transcripts The editing tool was evaluated iteratively by integrating it with the other educational resources available to the students of two Computer Science courses We have analyzed not only the improvements in transcript quality brought by the editing tool but also looked at how students are making use of transcripts in general and of the wiki editing tool and what is their attitude toward such enhancements of webcast systems We have found that wiki editing is well received by webcast users and that students are willing to contribute to the improvement of lecture transcripts Our study revealed that access to transcript editing must be facilitated by providing both the option of on the fly editing and that of mass editing Since providing various academic or even financial incentives for transcript editing may not always be feasible particularly outside the classroom environment there might be cases where the solution described in this chapter will lead to only a part of the transcripts being corrected In Chapter 6 I will propose to combine the HCI and ASR based efforts to improve lecture transcript quality by introducing a method that exploits users corrections of the early parts of a lecture to further improve the ASR system Useful Transcriptions of Webcast Lectures Chapter 6 Automatic Speech Recognition for Webcast Lectures Learning from Wiki enabled Transcript Corrections Improving access to archives of webcast lectures is a task that by its very nature requires research efforts common to both Automatic Speech Recognition ASR and Human Computer Interaction HCI One of the main challenges to integrating text transcripts into archives of webcast lectures is the poor performance of ASR systems when transcribing lectures This is in part caused by the mismatch between the language used in a lecture and the predictive language models employed by the ASR system The solution proposed in Chapter 4 and in Munteanu et al 2007 addresses this issue through an information retrieval technique that exploits lecture 133 134 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS slides by automatically mining the World Wide Web for documents related to the presentation topic and using these to build a better matching language model Despite the improvements to ASR performance on lecture transcriptions 11 relative WER reduction brought by the proposed lecture specific language modelling there still exists a significant gap between the actual WER and the established quality threshold of 25 presented in Chapter 3 and in Munteanu et al 2006a To and in Munteanu et al 2008a facilitates the editing of ASR generated transcripts and under certain conditions e g proper student motivation leads to entirely corrected transcripts One of the research questions that remains open is determining the appropriate motivational method for eliciting the necessary user participation that leads to satisfactory correction of webcast transcripts in more general scenarios particularly outside the classroom environment However research evidence suggests that further WER reductions are possible in subsequent lectures when manual transcripts of large enough number of earlier lectures in a series are used to re train an ASR system Glass et al 2007 For this the corrected transcripts can be employed to address one of the main problems that ASR systems face in large vocabulary and unconstrained domains such as lectures the lack of previously collected data on the same topic and from Useful Transcriptions of Webcast Lectures 135 the same speaker Our previous work on improving the quality of lecture transcripts found that users of web based lecture viewing systems can successfully contribute to the reduction of WER Although it is possible to entirely correct the transcripts for a lecture this usually requires additional incentives We have observed Chapter 5 Munteanu et al 2008a however that manually transcribing the first 10 to 15 minutes of each lecture is a goal that can be easily achieved In this chapter we will look at exploiting this finding to further improve the quality of lecture ASR systems A solution for improving the quality of automatically generated webcast lecture transcriptions is introduced in this chapter In contrast to the approach of Glass et al 2007 that uses the manual transcripts of the lectures from the first half of a semester long course the proposed solution makes use of a minimal amount of manual transcripts corresponding to as little as the first ten minutes of each hour long lecture These transcripts are obtained through user collaboration enabled by the wiki editing system described in Chapter 5 The WER reductions exhibited by the empirical evaluation demonstrate that wiki editing of imperfect webcast transcripts not only directly improves the quality of the transcripts but can be successfully used to further improve the ASR output despite the significantly small size of the user corrected transcripts In this chapter I will review the existing work on improving ASR for lectures by making use of existing manual transcripts Section 6 1 and present a brief introduction Section 6 2 to Transformation Based Learning TBL a method used in various Natural Language Processing tasks to Useful Transcriptions of Webcast Lectures 136 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS correct the output of a stochastic model I will then introduce a TBL based solution for improving ASR transcripts for lectures Section 6 3 followed by a description of an empirical evaluation of the proposed solution and an analysis of its results Section 6 4 6 1 Related Work Several ASR research directions are focused on improving the quality of lecture transcripts from better acoustic modelling Park et al 2005 Useful Transcriptions of Webcast Lectures 6 1 RELATED WORK 137 lectures instead of completely transcribing the entire course In contrast if a university aims to improve access to its potentially large collection of online lectures by offering transcripts for each lecture the costs of manually producing such transcripts would be prohibitive Therefore as argued in Hazen 2006 any ASR improvements that rely on manual transcripts need to offer a balance between the cost of producing those transcripts and the amount of improvement i e WER reductions Since manual transcripts are a costly resource several research efforts have looked at more efficiently exploiting limited amounts of them One such approach is active learning where the goal is to select or generate a subset of the available data for which the manual transcripts would be the best candidate for ASR adaptation or training Riccardi and Hakkani Tur 2005 Huo and Li 2007 Usually the goal of this research is to reduce the size of the training data without an increase in example by treating the ASR output as being in a foreign language and trying to translate that into the target language through the noisy channel method used in Machine Translation Ringger and Allen 1996 This reduced WER from 41 to 35 on a corpus of train dispatch dialogues In other examples post ASR improvements are achieved by combining either the Useful Transcriptions of Webcast Lectures 138 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS transcripts or the word lattices from which transcripts are extracted of two complementary ASR systems a technique first perfected by NIST s ROVER system Fiscus 1997 with a 12 relative WER reduction and subsequently widely employed throughout the ASR community Most of the ASR research surveyed here and in particular for difficult tasks such as lectures requires large quantities of manually annotated training data Only a few approaches e g Munteanu et al 2007 try to solve the problem in a realistic scenario such as in lectures for which collecting extensive training data is not practical In this Chapter an enhancement of Transformation Based Learning detailed in Section 6 2 is proposed for automatically correcting ASR transcripts Our approach uses minimal training data that can be easily obtained such as through the wiki editing system proposed in Munteanu et al 2006d 2008a and described in Chapter 5 Transformation Based Learning TBL has been successfully applied to other Natural Language Processing NLP tasks such as Part of Speech tagging Brill 1992 In recent years TBL has received increasing attention in the ASR community being used for example to improve the word lattices from which the transcripts are selected Mangu and Padmanabhan 2001 resulting in a relative WER reduction of 5 over test data from a general purpose conversational corpus SWITCHBOARD using a language model that is trained over a diverse combination of broadcast news and telephone conversation transcripts Unfortunately such approaches are not suitable for cases in which the particular combination of acoustic and language models leads to word lattices that have a limited number of paths through them Useful Transcriptions of Webcast Lectures 6 1 RELATED WORK 139 as anecdotally evidenced during our evaluation of the web based lecture modeling described in Chapter 4 An alternative is to directly correct the ASR transcripts through n gram word level editing rules such as in Peters and Drexel 2004 which reports a 9 6 relative WER reduction on a professional dictation corpus with a 35 initial WER What is shown in this chapter is that a true WER calculation is so valuable that a manual transcription of only about 10 minutes of a one hour lecture is necessary to learn the TBL rules and that this smaller amount of transcribed data in turn makes the true WER calculation computationally feasible With this combination we achieve a greater average relative error reduction 12 9 than that reported by Peters and Drexel 2004 on their dictation corpus and a relative reduction over three times greater than that of our reimplementation of their heuristics on our lecture data 3 6 This is on top of the average 11 RER from language model adaptation on the same data We also achieve this reduction from TBL without the obligatory round of development set parameter tuning required by their heuristics and in a manner that is robust to perplexity Less is more Section 6 2 briefly introduces Transformation Based Learning TBL a method used in various Natural Language Processing tasks to correct the output of a stochastic model and then introduces a TBL based solution for improving ASR transcripts for lectures Section 6 3 describes the proposed TBL approach to lecture transcript correction while Section 6 4 presents the setup of the experimental evaluation and analyses its results Useful Transcriptions of Webcast Lectures 140 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS 6 2 Transformation Based Learning Historically research in Natural Language Processing can be divided into two major approaches statistical driven and rule based with statistical approaches being more recently predominant Roche and Schabes 1995 However research has emerged that successfully combined the two approaches such as Brill s Part of Speech POS tagger Brill 1992 Brill s tagger introduced the concept of Transformation Based Learning TBL The fundamental principle of TBL is to employ a set of rules to correct the output of a stochastic model In contrast to traditional rule based approaches where rules are manually developed TBL rules are automatically learned from training data The training data consist of sample output from the stochastic model aligned with the correct instances For example in Brill s tagger the system assigns POSs to words in a text which are later corrected by TBL rules These rules are learned from manually tagged sentences that are aligned with the same sentences tagged by the system Typically rules take the form of context dependent transformations for example change the tag from verb to noun if one of the two preceding words is tagged as a determiner Brill 1992 An important aspect of TBL is the rule scoring and ranking While the evidence from the training material can suggest a certain transformation rule there is no guarantee that such a rule will indeed correct and improve an NLP system s output In order to ensure that rules will not be used to falsely transform correct output a scoring function is used to rank rules From all the rules learned during training only those scoring higher than a certain threshold are retained For a particular task the scoring function Useful Transcriptions of Webcast Lectures 6 2 TRANSFORMATION BASED LEARNING ideally reflects an objective quality function 141 Since Brill s tagger was first introduced the TBL approach has been used for other NLP applications One recent example is the system described by Peters and Drexel 2004 in which TBL rules are employed to directly correct ASR output a graphical illustration of TBL used for ASR corrections is presented in Figure 6 1 in Section 6 3 In this case the rules consist of word level transformations that correct sequences of words n grams One of the main challenges is the heavy computational requirements of the rule scoring function Roche and Schabes 1995 Ngai and Florian 2001 This is particularly prevalent in applications of TBL to ASR corrections where large training corpora are needed Therefore while the objective function for improving the ASR transcript is the WER reduction the use of this for scoring the TBL rules is prohibitive For example Peters and Drexel 2004 had access to a very large amount of manually transcribed data so large in fact that the computation of true WER in the TBL rule selection loop was computationally infeasible and so they used a set of faster heuristics instead In Section 6 3 a solution for correcting ASR transcripts for lectures is presented that makes use of minimal amounts of training data for short one time recordings and as such can employ the WER as the rule scoring function Useful Transcriptions of Webcast Lectures 142 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS 6 3 Minimally Trained Transformation Based Learning for Webcast Transcription The underlying concept behind transformation based learning is the use of a scoring function to rank rules for correcting the output of a predictive model For ASR systems such rules are typically applied to either the compact search space from which the transcript is generated i e word lattices or directly to the first best path through this space the ASR transcript itself In both cases a large training data size is required to robustly learn transformation rules This requirement is more critical for ASR where the large vocabulary can lead to potentially many more errors than for example POS tagging where the set of tags is quite limited As such TBL applications to ASR traditionally employed heuristic approximations of transcript quality improvements in order to score rules While the objective function for improving the ASR transcript is WER reduction the use of this for scoring TBL rules can be computationally prohibitive over large data sets Peters and Drexel 2004 address this problem by using an heuristic approximation to WER instead and it appears that their approximation is indeed adequate when large amounts of training data are available Our approach stands at the opposite side of this trade off restrict the amount of training data to a bare minimum so that true WER can be used in the rule scoring function As it happens the minimum amount of data is so small that we can automatically develop highly domain specific language models for single 1 hour lectures We show here that the rules selected by this function lead to a significant WER reduction for individual Useful Transcriptions of Webcast Lectures 6 3 MINIMALLY TRAINED TRANSFORMATION BASED LEARNING FOR WEBCAST TRANSCRIPTION 143 lectures even if a little less than the first ten minutes of the lecture are manually transcribed This combination of domain specificity with true WER leads to the superior performance of the present method at least in the lecture domain we have not experimented with a dictation corpus Another alternative would be to change the scope over which TBL rules are ranked and evaluated but it is well known that globally scoped ranking over the entire training set at once is so useful to TBL based approaches that this is not a feasible option one must either choose an heuristic approach such as that of Peters and Drexel 2004 or reduce the amount of training data to learn sufficiently robust rules 6 3 1 TBL Algorithm for ASR Output Correction As our proposed TBL adaptation operates directly on ASR transcripts we employ an adaptation of the specific algorithm proposed by Peters and Drexel 2004 which is schematically represented in Figure 6 1 This in turn was adapted from the general purpose algorithm introduced in Brill 1992 6 3 2 Rule Discovery for Lecture Transcripts The TBL rules are represented as contextual replacement rules to be applied to ASR transcripts The rules are learned by performing a word level alignment between corresponding utterances in the manual and ASR transcripts of training data and extracting the mismatched word sequences anchored by matching words that serve as contexts The rule discovery algorithm outlined in Figure 6 2 is applied to every instance of Useful Transcriptions of Webcast Lectures 144 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS Figure 6 1 The TBL algorithm for correcting ASR output Transformation rules are learned from the alignment of manually transcribed text T with automatically generated transcripts TASR of training data ranked according to a scoring function S and applied to the ASR output TASR of test data Useful Transcriptions of Webcast Lectures 6 3 MINIMALLY TRAINED TRANSFORMATION BASED LEARNING FOR WEBCAST TRANSCRIPTION 145 mismatching word sequences between the utterance level aligned manual and ASR transcripts For every matching sequence of words a set of transformation contextual replacement rules is generated The set contains the original matching sequence and its replacement part by itself and together with three alternatives where it is surrounded by the left right or both anchor context words In addition all possible splices of the matching sequence and the surrounding context words are also considered 1 Rules are represented as replacement expressions in a sed like syntax e g every match of the left side of the expression in the automatically generated transcript is to be replaced with the correct instance on the right side Rules that would result in arbitrary insertions of single words e g w1 are discarded An example of a rule learned from transcripts is presented in Figure 6 3 6 3 3 Scoring Function for the TBL rules As described in Section 6 2 the scoring function that ranks rules is the main component of any TBL algorithm The goal of this function is to select rules that when applied to unseen test data will lead to a reduction in the error rate Typically for speech recognition tasks the goal is the reduction of Word Error Rate WER Ideally TBL rules would be scored by a function that directly correlates with WER However for large vocabulary 1 The splicing preserves the original order of the word level utterance output of a typical dynamic programming implementation of the edit distance algorithm Gusfield 1997 For this word insertion and deletion operations are treated as insertions of blanks in either the reference manual or ASR transcript Useful Transcriptions of Webcast Lectures 146 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS for every sequence of words c0 w1 wn c1 in the ASR output that is deemed to be aligned with a corresponding sequence c0 w1 wm c1 in the manual transcript add the following contextual replacements to the set of discovered rules c0 w 1 w n c1 c0 w 1 w m c1 c0 w 1 w n c0 w 1 w m w 1 w n c1 w 1 w m c1 w1 wn w1 wm for each i such that 1 i min n m add the following contextual replacements to the set of discovered rules c0 w 1 w i c0 w 1 w i c1 w i w n c1 w i w m c1 w 1 w i w 1 w i c1 wi wn wi wm Figure 6 2 The discovery of transformation rules as part of the TBL algorithm described in Figure 6 1 Useful Transcriptions of Webcast Lectures 6 3 MINIMALLY TRAINED TRANSFORMATION BASED LEARNING FOR WEBCAST TRANSCRIPTION 147 Utterance align ASR output and correct transcripts ASR Correct the okay one and you come and get your seats ok why don t you come and get your seats Insert sentence delimiters to serve as possible anchors for the rules ASR Correct s the okay one and you come and get your seats s s ok why don t you come and get your seats s Extract the mismatching sequence enclosed by matching anchors ASR Correct s the okay one and you s ok why don t you Output all rules for replacing the incorrect ASR sequence with the correct text using the entire sequence a or splices b with or without surrounding anchors a a a a b b b b b b b b the okay one and the okay one and you s the okay one and s the okay one and you the okay s the okay one and one and you the okay one s the okay one and and you ok why don t ok why don t you s ok why don t s ok why don t you ok s ok why don t why don t you ok why s ok why don t don t you Figure 6 3 An example of the rule discovery mechanism from Figure 6 2 Useful Transcriptions of Webcast Lectures 148 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS ASR such as broadcast news transcriptions especially for tasks covering multiple speakers and several sub topics the size of the training corpus must be large in order to avoid selecting rules that are scoring high only locally Thus various heuristic approximations are employed for which rules will be assigned scores that can be globally optimized in a computationally feasible manner Furthermore a development corpus is needed on which several parameters of the heuristic approximation are fine tuned As outlined in both the introduction to this Chapter and Section 6 2 for the task of transcribing lectures the availability of training data is very limited Furthermore it is often impractical to manually transcribe large parts of lectures in order to establish a development corpus particularly when the goal is the transcription of a single one hour lecture given by a one time speaker such as the case of invited presentations However as demonstrated in Chapter 5 and in Munteanu et al 2008a manual transcripts of the first 10 15 minutes in a lecture can be easily obtained Assuming a relatively small size for the available training data a TBL scoring function that directly correlates with WER can be conducted globally over the entire training set In keeping with TBL tradition however rule selection itself is still greedily approximated Our scoring function is formally described in Equation 6 1 and an example of the rule scoring process is given in Figure 6 4 SW ER r TASR T W ER TASR T W ER r TASR T where r TASR is the result of applying rule r on text TASR is more formally introduced in Section 6 3 4 6 1 Useful Transcriptions of Webcast Lectures 6 3 MINIMALLY TRAINED TRANSFORMATION BASED LEARNING FOR WEBCAST TRANSCRIPTION 149 Figure 6 4 An example of rule scoring and selection As outlined in Figure 6 1 rules that occur in the training sample more often than an established threshold will be ranked according to the scoring function The ranking process is iterative in each iteration the highest scoring rule rbest is selected In subsequent iterations the training data TASR will be replaced with the result of applying the selected rule on it TASR rbest TASR and the remaining rules will be scored on the modified training text This ensures that the scoring and ranking of remaining rules takes into account the changes brought by the application of the currently selected rule to the training transcript The iterations stop when the scoring function reaches zero none of the remaining rules improves the objective function WER on the training data Useful Transcriptions of Webcast Lectures 150 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS 6 3 4 Rule Application During the training phase the best scoring rule in each iteration is selected as described in Section 6 3 3 On testing unseen data rules are applied to ASR transcripts in the same order in which they were selected As shown in Figure 6 2 and discussed in Section 6 3 2 we have chosen a sed like representation for the replacement rules Therefore rule application as exemplified in Figure 6 5 is a straightforward process for each rule r w1 wn w1 wm the new transcript r TASR is obtained by replacing all instances of the n gram w1 wn appearing in the ASR transcript TASR with the n gram w1 wm ASR the okay let s start Insert sentence delimiters ASR s the okay let s start s Identify a rule that matches the original ASR text s the okay s ok Apply the rule replace the incorrect ASR sequence with the correct text ASR corrected s ok let s start s Remove sentence delimiters ASR corrected ok let s start Figure 6 5 An example of rule application Useful Transcriptions of Webcast Lectures 6 4 EMPIRICAL EVALUATION 151 6 4 Empirical Evaluation The proposed TBL based approach to ASR transcript correction was evaluated over several lecture recordings Several combinations of TBL parameters were tested As the proposed method was not refined during the evaluation and since one of the goals of our proposed approach is to eliminate the need for developmental data sets the available data were partitioned only into training and test sets 2 The empirical evaluation was conducted using the SONIC toolkit Pellom 2001 We used the acoustic model that is part of the toolkit built on 30 hours of data from 283 speakers from the WSJ0 and WSJ1 subsets of the 1992 development set of the Wall Street Journal WSJ Dictation Corpus LDC 1994 6 4 1 Evaluation Data The evaluation data consist of a total of eleven lectures of approximately 50 minutes each recorded in three separate courses each taught by a different instructor For each course the recordings were performed in The recordings were collected in a different weeks of the same course large amphitheatre style lecture hall 200 seats using the AKG C420 head mounted directional microphone The recordings were not intrusive no alterations to the lecture environment or proceedings were made The mono recordings were digitized using the TASCAM US 122 interface as 2 With the exception of one hour of lecture recording used as a testbed during code development and debugging Useful Transcriptions of Webcast Lectures 152 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS uncompressed audio files with a 16KHz sampling rate and 16 bit samples The audio recordings were segmented at pauses longer than 200ms manually for one instructor and automatically3 for the other two The evaluation data are described in Table 6 1 Four evaluations tasks were carried Evaluation task name Instructor Gender Age Segmentation Number of lectures Lecture topic R 1 R Male R 2 G 1 G Male Mid 40s automatic 3 Software design ICSISWB K 1 K Female Early 40s automatic 4 Unix programming WSJ 5K Early 60s manual 4 Interactive media design Language model WSJ 5K WEB LECT Table 6 1 The evaluation data 3 The automatic segmentation was performed using an implementation of the silence detection algorithm described in Placeway et al 1997 manually fine tuned for every instructor in order to detect all pauses longer than 200ms while allowing a maximum of 20 seconds in between pauses 4 The AM related parameters were the same as those described in detail in Section 3 3 Useful Transcriptions of Webcast Lectures 6 4 EMPIRICAL EVALUATION 153 6 4 2 Language Models The four evaluations were carried out using the language models mentioned in Table 6 1 either custom built for a particular topic or the baseline models included in the SONIC toolkit as follows WSJ 5K is the baseline model of the SONIC toolkit It is a 5K word model built using the same corpus as the base acoustic model included in the toolkit 30 hours of data from 283 speakers from the WSJ0 and WSJ1 subsets of the 1992 development set of the Wall Street Journal WSJ Dictation Corpus ICSISWB is a 40K word model created through the interpolation of LMs built on the entire transcripts of the ICSI Meeting corpus and the Switchboard corpus The ICSI Meeting corpus consists of recordings of university based multi speaker research meetings totaling about 72 hours from 75 meetings Janin et al 2003 The Switchboard SWB corpus Godfrey et al 1992 is a large collection of about 2500 scripted telephone conversations between approximately 500 English native speakers suitable for the conversational style of lectures as also suggested in Park et al 2005 WEB LECT is a LM built for each particular lecture using information retrieval techniques that exploit the lecture slides to automatically mine the World Wide Web for documents related to the presented topic and uses these to build a language model that better matches the lecture topic 5 A pronunciation dictionary was custom built to 5 A complete description of this LM solution can be found in Munteanu et al 2007 Useful Transcriptions of Webcast Lectures 154 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS include all words appearing in the corpus on which the LM was trained The pronunciations were extracted from existing initial dictionaries the 5K word WSJ dictionary included with the SONIC toolkit and the 100K word CMU pronunciation dictionary CMU 1998 Only one non dictionary word per line of corpus was allowed for lines longer than four LMs were trained using the CMU CAM Language Modelling Toolkit Clarkson and Rosenfeld 1997 with a training vocabulary size of 40K words 6 4 3 Training and Test Data Partitioning As previously mentioned in the survey of related work Section 6 1 most approaches to ASR improvement that operate on the transcripts themselves such as TBL based corrections need both a training and a development corpus Furthermore these are often up to five times larger than the test corpus on which the methods are evaluated and both the test and the training set must be subsets of the same cohesive corpus While such requirements can be favourably met for larger scale applications e g broadcast news transcription they are rarely satisfied for lecture transcription Despite the challenges facing automatic lecture transcription even in the more realistic scenario of transcribing a single lecture one rather important and in Chapter 4 Useful Transcriptions of Webcast Lectures 6 4 EMPIRICAL EVALUATION 155 condition is met it can be assumed that a one hour lecture given by the same instructor will exhibit a strong cohesion both in topic and in speaking style between its parts Therefore in contrast to typical TBL solutions we have decided to evaluate our TBL based approach by partitioning each 50 minute lecture into a training and a test set where the training set is smaller than the test set As mentioned in the introductory part of this chapter in a real life scenario a feasible solution exists to obtain manual transcripts for the first 10 to 15 minutes of a lecture As such the evaluation was carried out with two values for the training size the first fifth T S 20 and the first third T S 33 part of the lecture being manually transcribed 6 4 4 Scoring Functions Three scoring functions were assessed through the experimental evaluation the proposed TBL scoring function as well as two baseline functions XER is the baseline used in this evaluation Since our TBL solution is an extension of the solution proposed in Peters and Drexel 2004 this baseline is a simple implementation of the heuristic approximation based TBL approach introduced there The scoring function for this baseline is the expected error reduction XER Useful Transcriptions of Webcast Lectures 156 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS text being unnecessarily corrected by a XER NoS An analysis of the rules selected by both TBL implementations revealed that using the XER approximation leads to several single word rules being selected such as rules removing all instances of frequent stop words such as the and for or pronouns such as he Therefore an empirical improvement XER NoS of the baseline was implemented that beside pruning rules below the RT threshold omits such single word rules from being selected SW ER is the proposed TBL scoring function formally introduced in Equation 6 1 This function directly correlates with WER and scores rules gloally over the entire training set as described in Section 6 3 3 6 4 5 Results Beside the training size parameter during all evaluation sessions a second parameter was also considered the rule pruning threshold RT As described in Section 6 3 1 from all the rules learned during the rule discovery step only those that occur more often than the threshold are scored and ranked This parameter can be set as low as 1 consider all rules or 2 consider all rules that occur at least twice over the training set For larger scale tasks this threshold serves as a pruning mechanism in order to reduce the computational burden of scoring several thousand rules especially when the scoring function is not a simple approximation of the objective Useful Transcriptions of Webcast Lectures 6 4 EMPIRICAL EVALUATION 157 function However a larger threshold could potentially lead to discrediting low frequency but high scoring rules Therefore due to the small size of the training data for lecture TBL the lowest threshold was set to RT 2 When a development set is available several values for the RT parameter can be tested and the optimal one chosen for the evaluation task In our case a development set is not available As such we have tested two more values for the rule pruning threshold RT 5 and RT 10 Tables 6 2 6 3 and 6 4 present the evaluation result for instructors R and G The transcripts were obtained through ASR runs using three different language models The TBL implementation with our scoring function SW ER brings relative WER reductions ranging from 10 5 to 18 0 with an average of 12 9 These WER reductions are greater than those produced by the XER baseline approach It is not possible to provide confidence intervals since the proposed method does not tune parameters from sampled data which we regard as a very positive quality for such a method to have Our speculative experimentation with several values for T S and RT however leads us to conclude that this method is significantly less sensitive to variations in both the training size T S and the rule pruning threshold RT than earlier work making it suitable for application to tasks with limited training Useful Transcriptions of Webcast Lectures 158 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS TBL follows upon benefits words with higher information content more It is possible that the favour observed in TBL with SW ER towards lower information content is a bias produced by the preceding round of language model adaptation but regardless it provides a much needed complementary effect This can be observed in Tables 6 2 and 6 3 in which TBL produces nearly the same relative error reductions in either table for any lecture The empirical improvement XER NoS of the baseline function XER slightly improves the performance of the approximation based TBL for some values of the RT and T S parameters as outlined in Tables 6 2 6 3 and 6 4 However it still does not consistently match the WER reductions of the globally scoped scoring function While the evaluation presented in Table 6 3 is carried out over the same lectures as those in Table 6 2 it is worth noting that the LM employed here WEB LECT is a lecture specific model as described in Section 6 4 2 and in Munteanu et al 2007 Despite this being already optimized for each task the TBL with SW ER scoring is able to bring further WER reductions resulting in average WERs of 40 over the four lectures from instructor R Although the empirical evaluation shows positive improvements in transcript quality through TBL in particular when using the SW ER scoring function an exception is illustrated in Table 6 5 The recordings for this evaluation were collected from a course on Unix programming and lectures were highly interactive Instructor K used numerous examples of C or Shell code many of them being developed and tested in class While the keywords from a programming language can be easily added to the ASR lexicon the pronunciation of such abbreviated forms especially for Shell Useful Transcriptions of Webcast Lectures 6 5 LIMITATIONS AND GENERALIZATIONS 159 programming and of mostly all variable and custom function names proved to be a significant difficulty for the ASR system This combined with a high speaking rate and often inconsistently truncated words led to few TBL rules occurring even above the lowest RT 2 threshold despite many TBL rules being initially discovered Anecdotal evidence collected from users of the transcript correction system showed they often paraphrased the speaker due to a lack of guidelines for how to deal with such difficulties in transcribing the recordings As previously mentioned one of the drawbacks of global TBL rule scoring is the heavy computational burden However the empirical evaluation conducted here showed an average learning time of one hour per one hour lecture reaching at most three hours6 for a threshold of 2 when training over transcripts for one third of a lecture Therefore it can be concluded that despite being computationally more intensive than a heuristic approximation a TBL system using a globally scoped WER correlated scoring function is not only a better performing but also a feasible solution to improving the quality of lecture transcripts when manual transcripts for the beginning of each lecture are present 6 5 Limitations and Generalizations The TBL scoring function proposed in this Chapter is mostly applicable to the improvement of single e g one hour lectures delivered by one 6 It should be noted that in order to preserve compatibility with other software tools the code developed for this evaluation was not optimized for speed It is expected that a dedicated implementation would result in even lower run times Useful Transcriptions of Webcast Lectures 160 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS Instructor R Language model WSJ 5K Lecture TS Initial WER RT 10 XER RT 5 RT 2 RT 10 XER NoS RT 5 RT 2 RT 10 SWER RT 5 RT 2 20 50 48 49 97 50 01 49 87 47 25 49 03 52 21 45 18 44 82 44 04 1 33 50 93 49 82 50 07 51 75 46 82 48 78 53 47 44 58 43 82 43 99 20 51 31 49 27 49 99 49 52 49 98 47 37 49 31 49 06 46 73 45 81 2 33 51 90 49 77 51 13 51 13 48 72 51 25 52 29 45 97 45 52 45 16 20 50 28 46 85 48 39 47 13 48 44 47 84 50 85 46 49 45 64 44 35 3 33 49 23 48 08 47 37 47 31 45 21 44 07 49 41 45 30 43 18 41 49 20 54 39 52 17 50 91 52 70 51 37 49 54 50 63 49 60 47 79 46 89 4 33 54 04 50 58 49 62 50 56 49 73 48 97 51 81 47 95 46 74 44 28 Table 6 2 The WER values for instructor R for which the ASR output using the WSJ 5K language model is corrected by TBL rules that are scored by the approximation functions XER and XER NoS as baselines and by the proposed globally scoped non heuristic scoring function SW ER Useful Transcriptions of Webcast Lectures 6 5 LIMITATIONS AND GENERALIZATIONS 161 Instructor R Language model WEB LECT Lecture TS Initial WER TBL with 1 20 45 54 42 91 43 45 43 26 43 51 44 96 46 72 41 98 40 97 40 67 33 20 2 33 43 87 43 81 44 37 44 66 41 98 40 52 45 87 40 75 39 08 38 07 20 46 69 46 78 46 90 43 77 44 66 44 66 40 44 44 66 44 66 40 00 3 33 47 14 45 35 42 12 45 12 46 59 41 74 44 32 45 27 40 84 40 08 20 49 78 46 92 47 34 61 54 47 24 47 23 61 84 47 24 45 27 43 31 4 33 49 38 49 65 46 04 60 40 46 30 44 35 64 40 45 85 42 39 41 52 45 85 43 36 43 90 43 81 45 46 42 97 42 98 48 16 41 44 40 56 42 44 42 65 44 19 42 11 40 01 44 79 42 11 38 85 RT 10 RT 5 RT 2 RT 10 RT 5 RT 2 RT 10 RT 5 RT 2 XER scoring TBL with XER NoS scoring TBL with SWER scoring 40 47 38 00 Table 6 3 The WER values for instructor R for which the ASR output using the WEB LECT language model is corrected by TBL rules that are scored by the approximation functions XER and XER NoS as baselines and by the proposed globally scoped non heuristic scoring function SW ER The average WER when using the SW ER function and the training parameter RT 2 is 40 Useful Transcriptions of Webcast Lectures 162 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS Instructor G Language model ICSISWB Lecture TS Initial WER TBL with 1 20 50 93 46 63 48 34 54 05 49 54 49 54 59 00 46 63 46 63 44 48 33 50 75 49 38 49 75 56 84 49 38 49 31 59 28 46 53 45 60 44 30 20 54 10 49 93 49 32 52 01 54 10 56 70 57 61 49 80 47 75 47 46 2 33 53 93 48 61 48 81 49 11 53 93 55 50 55 03 48 44 47 23 47 02 20 48 79 49 52 49 58 50 37 48 79 48 51 50 41 45 83 44 76 43 60 3 33 49 35 50 43 49 26 51 66 48 24 48 42 52 67 45 42 44 44 44 13 RT 10 RT 5 RT 2 RT 10 RT 5 RT 2 RT 10 RT 5 RT 2 XER scoring TBL with XER NoS scoring TBL with SWER scoring Table 6 4 The WER values for instructor G for which the ASR output using the ICSISWB language model is corrected by TBL rules that are scored by the approximation functions XER and XER NoS as baselines and by the proposed globally scoped non heuristic scoring function SW ER Useful Transcriptions of Webcast Lectures 6 5 LIMITATIONS AND GENERALIZATIONS 163 Instructor K Language model WSJ 5K Lecture TS Initial WER TBL with 1 20 33 20 2 33 20 3 33 20 4 33 44 31 44 06 46 12 45 80 51 10 51 19 53 92 54 89 RT 10 44 31 44 06 46 12 46 55 51 10 51 19 53 92 54 89 RT 5 RT 2 44 31 44 87 46 82 47 47 51 10 51 19 53 96 55 56 47 46 55 21 50 54 51 01 52 60 54 93 57 48 60 46 XER scoring TBL with RT 10 44 31 44 06 46 12 46 55 51 10 51 19 53 92 54 89 RT 5 RT 2 44 31 44 87 46 82 47 47 51 10 51 19 53 96 55 56 46 43 54 41 50 54 51 01 53 01 55 02 57 47 60 02 XER NoS scoring TBL with RT 10 44 31 44 06 46 12 45 80 51 10 51 19 53 92 54 89 RT 5 RT 2 44 31 44 05 46 11 45 88 51 10 51 19 53 92 54 89 44 34 44 07 46 03 45 89 50 96 50 93 54 01 55 16 SWER scoring Table 6 5 The WER values for instructor K for which the ASR output using the WSJ 5K language model is corrected by TBL rules that are scored by the approximation functions XER and XER NoS as baselines and by the proposed globally scoped non heuristic scoring function SW ER Useful Transcriptions of Webcast Lectures 164 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS instructor The ASR system used to transcribe such lectures must have pronunciation dictionaries that cover topic specific keywords Since the experimental evaluation showed reductions in WER when using this scoring function over eleven hours of recordings from two different instructors it can be expected that the results of this research are generalizable to most lectures for which the manual transcripts of the first ten minutes of the lecture are available However as pointed out by the results on one particular instructor lecturer K the proposed method and in fact any other TBL solution will not produce significant WER reductions for lectures characterized by the frequent usage of numerous technical terms for which pronunciations are not available such as the name of variables or functions in C or Shell script A visual inspection of the rules selected by the proposed scoring function reveals that most of these rules repair ASR errors pertaining to a particular instructor s speaking style and to the genre of lecture speech instead of topic specific errors As such the applicability of this research can be extended to other domains where the transcripts of the first 10 or 15 minutes of an approximately one hour recording are available provided that these introductory parts of the recording are representative of the speaker s speaking style that such recordings are from a single speaker and that they belong to a genre characterized by a limited number of speaking styles 6 6 Summary and Discussion One of the challenges to reducing the WER of ASR transcriptions of lecture recordings is the lack of manual transcripts on which to train various ASR Useful Transcriptions of Webcast Lectures 6 6 SUMMARY AND DISCUSSION 165 improvements In particular for one hour lectures given by different lecturers such as for example invited presentations it is often impractical to manually transcribe parts of the lecture that would be useful as training or development data However transcripts for the first 10 minutes of a particular lecture can easily be obtained In this chapter a solution is presented that improves the quality of ASR transcripts for Useful Transcriptions of Webcast Lectures 166 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS threshold practically scoring all rules that occur at least twice with limited impact on the computational burden of learning the transformation rules The ASR improvement proposed in this chapter can be combined with other lecture specific optimizations resulting in average WERs of 40 over the lectures where such combinations were evaluated Specific WERs on these lectures ranged from 38 to no more than 43 This brings the lecture WER closer to the levels for which transcripts become a useful addition to webcast archives under certain tasks as mentioned in Section 3 4 2 It shows that combining HCI and ASR efforts is a promising path toward delivering useful lecture transcripts of WERs below 25 Useful Transcriptions of Webcast Lectures Chapter 7 Contributions Conclusions and Future Work Improving access to archives of webcast lectures is a task that by its very nature requires research efforts drawn from several disciplines such as Human Computer Interaction Natural Language Processing in particular Automatic Speech Recognition and Computer Supported Collaborative Work In this dissertation I have measured the threshold of quality for integrating transcripts into webcast archives and proposed solutions for improving the usefulness of automatically generated transcripts of webcast lectures and academic presentations I have shown that such improvements are possible through just such an interdisciplinary approach integrating novel speech recognition techniques with the development of an interactive collaborative interface Having determined the acceptable quality of automatically generated transcripts to be included in webcast lectures archives I showed that 167 168 CHAPTER 7 CONTRIBUTIONS CONCLUSIONS AND FUTURE WORK users performance and transcript quality perception are affected by the Word Error Rate With this precisely determined goal in mind for ASR performance two solutions were proposed that aimed to bring the quality of ASR transcripts to these desired levels The two methods independently address aspects of usability and transcript quality for lecture webcasts from the perspective of HCI and ASR respectively The ASR specific improvement makes use of an information retrieval technique that exploits lecture slides by automatically mining the World Wide Web for documents related to the presented topic and then uses these to build a better fitting language model for lecture transcription The HCI and CSCW based solution consists of a collaborative tool that extends a webcast system s functionality by allowing users to edit and correct the webcast transcripts in a wiki like manner Lastly a joint HCI and ASR method was introduced in that further improves the quality of webcast lecture transcripts by making use of the transcript corrections collected through the wiki editing tool to learn a set of transformation rules that improves the performance of the ASR system 7 1 Contributions How Good is Good Enough and What to Do When It Isn t 7 1 1 ASR how good is good enough Through an extensive user study I have investigated the user needs for transcription accuracy in webcast archives For this a within subjects study was designed in which 48 participants were exposed to multiple levels of Useful Transcriptions of Webcast Lectures 7 1 CONTRIBUTIONS HOW GOOD IS GOOD ENOUGH AND WHAT TO DO WHEN IT ISN T 169 WER in their interaction in a typical webcast use scenario students writing a quiz based on a lecture The results showed that users performance and transcript quality perception is linearly affected by WER with transcripts of WER equal to or less than 25 being useful and accepted by users of webcast archives This was determined by assessing users performance in a question answering task their perception of transcript quality as well as users confidence in their performance and their perceived level of task difficulty The study also revealed that for most browsing scenarios users prefer having transcripts even if the quality of those transcripts is less than optimal 7 1 2 What to do when ASR is not good Spoken Language Processing and in particular Automatic Speech Recognition have long focused on methods and technologies that would ultimately mimic human performance when transcribing speech to text In this vein I have worked on improving the accuracy of ASR systems for lectures by building statistical predictive language models specific to the topic and genre presented One of the common challenges when transcribing lectures is the mismatch between the language used in a lecture and the predictive language models employed by the ASR system In my dissertation I have proposed a solution that addresses this issue through an information retrieval technique that exploits lecture slides by automatically mining the World Wide Web for documents related to the presented topic and using these to build a better fitting language model Such an approach overcomes Useful Transcriptions of Webcast Lectures 170 CHAPTER 7 CONTRIBUTIONS CONCLUSIONS AND FUTURE WORK the need to manually fine tune the process of interpolating two models a large topic independent one and a smaller topic specific model and avoids relying on often imprecise algorithms or manual procedures for extracting keywords needed to build topic specific models The solution proposed in this dissertation achieves a relative WER reduction of up to 11 7 1 3 What to do when ASR is not good Despite improvements to the ASR performance for lecture transcriptions there still exists a significant gap between the desirable and actual WER To reduce and possibly eliminate this gap I have developed and evaluated an iterative design of a collaborative tool that in a wiki like manner allows users of webcast archives to edit and correct the transcripts on the fly while viewing an archived webcast The editing tool was evaluated iteratively by integrating it with other educational resources available to the students in two Computer Science courses The field studies showed that this is a feasible solution for alleviating the errors of ASR webcast lecture transcripts when fully engaged and properly motivated students completely corrected the transcripts for almost all lectures in the course I have also found that wiki editing is well received by webcast users and that students see the ability to correct transcripts as an enhancement of the classroom experience and are willing to contribute to the improvement of lecture transcripts Useful Transcriptions of Webcast Lectures 7 1 CONTRIBUTIONS HOW GOOD IS GOOD ENOUGH AND WHAT TO DO WHEN IT ISN T 171 7 1 4 What to do when ASR is not good Although engaging users to collaboratively correct the ASR transcripts of webcasts can lead to WERs of close to zero it is not always feasible to rely exclusively on this approach for improving the quality of webcast transcripts However in most cases such as webcasts of lectures transcripts for the first 10 minutes of an hour long presentation can be easily obtained As such in my dissertation I have proposed a joint HCI and ASR based approach in which partially corrected transcripts can be employed to address one of the main problems that ASR systems face in large vocabulary and unconstrained domains such as lectures the lack of previously collected data on the same topic and from the same speaker The proposed method employs users transcript corrections to learn word level transformation based rules that attempt to replace parts of the ASR transcript with possible corrections This leads to WER reductions of between 10 and 18 relative to initial values with an average of close to 13 even on transcripts obtained by using language models already optimized for the lecture topic Such results show that wiki editing of imperfect webcast transcripts not only directly improves the quality of the transcripts but can be successfully used to further improve the ASR output By pursuing this approach the user is no longer be a simple recipient of an ASR system s output but rather an active contributor to the system s internal operation Useful Transcriptions of Webcast Lectures 172 CHAPTER 7 CONTRIBUTIONS CONCLUSIONS AND FUTURE WORK 7 2 Future Work As traditional forms of sharing and preserving information are continuously being replaced by online multimedia it becomes increasingly important to provide users with efficient and usable information access tools Future work should continue on the interdisciplinary path of advancing Human Computer Interaction and Spoken Language Processing research to improve the usability and usefulness of information rich media in particular within educational environments 7 2 1 Enlarging the Scope of Current Research As mentioned in the Limitations and Generalizations Sections 3 6 4 5 5 8 and 6 5 of this dissertation the research presented here was conducted within the scope of university lectures in which the instructors follow very detailed slides While certain aspects can be generalized as discussed in the aforementioned Sections one direction that future research should explore is the applicability of this research to completely different settings for example measuring the acceptable transcript quality of presentations in information critical domains such as business meetings or investigating topic specific language modelling approaches for lectures and presentations that do not use slides or that make use of other visual aids such as interactive examples Useful Transcriptions of Webcast Lectures 7 2 FUTURE WORK 173 7 2 2 User Motivated Measure of Transcript Quality One of the first steps toward the goal of improving users experience when accessing media archives is the development of a user motivated measure of transcript quality WER is the de facto standard for evaluating Automatic Speech Recognition but it is exclusively based on a blind comparison with a reference text This bears no resemblance to how imperfect ASR output is used by humans where various aspects of text readability become relevant as shown by Jones et al 2003 Some researchers have started to question the adequacy of WER in language understanding tasks such as automatic knowledge extraction from ASR transcripts Wang and Chelba 2003 Unfortunately no similar research was conducted to our knowledge on tasks involving humans As such one of the goals for future research is to formulate a scientific measure of the usability of transcripts This putative computationally feasible measure would be based on text features that correlate with human indicators of success in reading and comprehension tasks particularly in the context of using transcripts of information rich webcasts such as those of lectures and presentations 7 2 3 Refining the Acceptable Quality of ASR Transcripts The study presented in Chapter 3 is limited to one specific task quiz answering under strict time constraints and to an undergraduate level student population most of whom used such a system for the first time While the findings of this research can be generalized as indicated by the Useful Transcriptions of Webcast Lectures 174 CHAPTER 7 CONTRIBUTIONS CONCLUSIONS AND FUTURE WORK post session questionnaire to various academic activities such as making up for a missed class or preparing an assignment future work must extend this study to a broader pool of participants such as corporate webcast users for which accuracy might be more critical and to more diverse tasks and conditions such as presentations for which no slides are available With respect to more accurately determining the relation between transcript quality and usefulness of webcasts experiments should also be carried out with finer grained levels of WER e g in increments of 5 around the threshold of 25 determined in Chapter 3 The trend analysis of the data collected in the experiments described in Chapter 3 showed that there is a statistically significant effect on users performance and experience caused by the different experimental conditions in the order WER 0 WER 25 no transcript and WER 45 with the exception of situations where the information users searched for was not present on lecture slides in which case there was an ordinal inversion of the no transcript and the WER 45 conditions In terms of pairwise comparisons between the immediate values of WER 25 no transcript and 45 not all measures indicated a statistically significant categorical difference As such it might not be possible to determine an accurate gold standard for the quality of webcast transcripts experiments with finer grained levels of WER however could reveal with statistical significance an interval in which such a true WER usefulness threshold lies Finally this research must be extended to include any user motivated text quality measures as proposed in Section 7 2 2 Useful Transcriptions of Webcast Lectures 7 2 FUTURE WORK 175 7 2 4 Higher Level Text Based Representations of Lectures and Presentations ASR systems are not likely to improve significantly in the near future and thus transcripts of webcasts may not reach the same usefulness levels as perfect manually generated transcripts As such although several experimental findings including those presented in this dissertation highly commend the importance of transcripts for webcast systems a higher level text based representation but more comprehensive than a table of contents of lecture or presentation content is also needed Future work should look at enhancing current webcast interfaces with other more compact textual representations of lectures and presentations such as summaries hierarchical tables of contents or slide annotations 7 2 5 ASR Based Search Tools for Webcast Archives Existing literature as well as the research evidence presented in this dissertation indicate that transcripts are a much needed tool in improving users browsing and information mining through webcast archives However certain tasks would benefit from the availability of a reliable search function for example retrieving a recording from a large collection of webcasts based on a user specified topic description i e document retrieval or locating a point in the archive of a lecture webcast when an instructor mentioned a particular concept i e utterance retrieval These operations are not possible without searching through and indexing the audio channel of the archived webcasts There is an increased interest in the ASR community Useful Transcriptions of Webcast Lectures 176 CHAPTER 7 CONTRIBUTIONS CONCLUSIONS AND FUTURE WORK in improving the spoken document utterance retrieval and indexing in particular on performing these operations without the need for accurate transcripts Saraclar and Sproat 2004 Hori et al 2007 and within the context of large scale Internet based repositories of audio recordings Zhou et al 2006 Beside the challenges of improving the speech searching and indexing algorithms this area presents some interesting opportunities for future HCI research as well while many solutions were proposed and successfully adopted for the problem of displaying in a usable manner results of queries over text collections searching through repositories of and within information rich mixed media presentations is still an open research problem the MIT lecture browser being one of the few examples of webcast systems that allows users to interact with ASR based search results as illustrated in Figure 7 1 Figure 7 1 The ASR based search interface of the MIT lecture browser Useful Transcriptions of Webcast Lectures 7 2 FUTURE WORK 177 7 2 6 Further ASR Improvements for Webcast Lecture Transcription Since one of the typical sources of recognition errors is large vocabulary and language model size future work should look at improving the web based retrieval of relevant corpora introduced in Chapter 4 Corpus and text similarity measures could be considered that select from the retrieved web corpus only those documents that better maximize the match between the retrieved corpus and the conversational style of a lecture and of a particular lecturer Some simple lexical measures of corpus similarity1 were already employed with moderate but promising success such as assessing the effect on the perplexity of a language model brought by the removal of documents from the training corpus for a broadcast news transcription task Klakow 2000 Given the less scripted nature of lectures it is expected that instructors particular speaking styles will be reflected more in their lecture speech As such the suitability of syntactic similarity measures used in authorship attribution especially for short texts as in Hirst and Feiguina 2007 should also be investigated As was shown in Chapter 6 transformation based TBL rules that correct the ASR transcripts and reduce the WER by up to 18 can be learned on manual transcripts corresponding to as little as 10 minutes of a one hour lecture Future research should consider other methods of improving ASR systems that make use of only small amounts of training data Among these methods are TBL approaches that operate at the word lattice level or at the 1 An extended survey of corpus similarity measures can be found in Kilgarriff and Rose 1998 Useful Transcriptions of Webcast Lectures 178 CHAPTER 7 CONTRIBUTIONS CONCLUSIONS AND FUTURE WORK phonetic transcription level 7 2 7 Maximizing the Trade off between User Editing and ASR Improvements The method proposed in Chapter 6 improves the ASR for a lecture by exploiting the manual transcripts of the first 10 minutes A possible direction of future research is the exploration of various trade offs between the amount of manual transcripts used and the reductions in WER This is particularly relevant in the context of keeping the users in the ASR editing loop e g combining user edits with TBL in a cycle converging to lower WER values Previous research such as that of Glass et al 2007 shows that manually transcribing one half of the recordings in a course series can through a combination of AM and LM adaptation reduce the WER on the remaining lectures below 20 While such a low WER value is certainly desirable the financial proposition of manually transcribing lectures might not warrant it For example as a plausibility argument if a typical base cost of a course is 20 000 10 000 in instructor fees and another 10 000 in University overhead and considering a standard student work pay rate of 12 50 hour transcribing 20 hours of lectures in a 40 hour semester would increase the entire cost of the course by 12 5 or 2500 However if the users are kept in the ASR improvement loop e g the ASR system improves constantly as users correct the transcripts this cost can be reduced further For this future work should focus on developing better editing interfaces that would facilitate faster correction of ASR errors by webcast users Assuming that the threshold at which correcting errors is faster than re transcribing is also at Useful Transcriptions of Webcast Lectures 7 2 FUTURE WORK 179 25 WER the mark up costs of providing useful transcriptions for webcast lectures can be reduced to below 10 of the total course costs Furthermore if the course is to be taught again by the same instructor in subsequent years the initial WER will potentially be lower if for example the previous speaker adapted AMs are used As such the number of wiki enabled user corrections required for useful transcripts can be even lower 7 2 8 Other Collaborative Approaches to Webcast Usability Improvement Another research question that remains open is that of determining the appropriate motivations for subjects to participate in collaborative transcript correction Combining lecture access limitations with academic and financial incentives yielded sufficiently many contributions from students even in a smaller sized class Weaker incentives coupled with comprehensive lecture materials slides readings resulted in both significantly fewer contributions and reduced interest in archived lecture viewing Evaluating the wiki editing concept in a larger sized class with reduced availability of course materials should also be considered The ePresence system is used not only for webcast lectures but for other genres of presentations which are archived and available through the ePresence tv media portal Many of these archives are accessed by several thousand viewers Such communities have already overcome the issue of reaching a critical mass of contributors and they typically offer intrinsic motivations to contribute Kuznetsov 2006 Future development efforts should be dedicated to integrating the wiki editing interface into the Useful Transcriptions of Webcast Lectures 180 CHAPTER 7 CONTRIBUTIONS CONCLUSIONS AND FUTURE WORK ePresence portal Besides investigating the use of automatically generated textual projections of lectures such as summaries future work should expand the scope of collaborative solutions to encompass all text based representations of webcast archives In the lecture domain for example course materials such as class handouts are a common textual resource that accompanies lectures and students typically enhance its content through handwritten notes annotations taken during lectures Natural language processing can be combined with collaborative approaches to generate handout annotations 7 3 In Conclusion In this dissertation I have shown that an interdisciplinary approach combining research in Automatic Speech Recognition and Human Computer Interaction can improve the access to and the interaction with online archives of webcast lectures and academic presentations For this I have proved that integrating automatically generated transcripts of Word Error Rate of 25 or less into archives of webcast lectures enhances the usability of archives and results in improved usefulness of archived webcast lectures I have also shown that significant transcript quality improvements toward the acceptable Word Error Rate can be achieved achieved if speech recognition techniques specifically addressed at increasing the accuracy of webcast transcriptions are integrated with the development of an interactive collaborative interface that facilitates users editing of machine generated transcripts Useful Transcriptions of Webcast Lectures Bibliography Ariki et al 2003 Y Ariki T Shigemori T Kaneko J Ogata and M Fujimoto Live speech recognition in sports games by adaptation of acoustic model and language model In Proceedings of Eurospeech pages skimming recorded speech Interaction 4 1 ACM Transactions on Computer Human Bacchiani and Roark 2004 M Bacchiani and B Roark conditional language modeling Meta data In Proceedings of the International Conference on Acoustics Speech and Signal Processing Montreal Canada May 2004 Baecker et al 2003 R Baecker G Moore D Keating and A Zijdemans Reinventing the lecture Webcasting made interactive In Proceedings of HCI International volume 1 pages 182 BIBLIOGRAPHY archives In Proceedings of CASCON pages Recognition Theory and C Implementation John Wiley Sons New York 1999 augmentation and language model adaptation using singular value decomposition In Pattern Recognition Letters volume 25 pages distributed groups In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems pages Useful Transcriptions of Webcast Lectures BIBLIOGRAPHY Birnholtz 2006 J Birnholtz Back to school 183 Design principles for improving webcast interactivity from face to face classroom observation In Proceedings of the ACM SIGCHI Conference on Designing Interactive Proceedings of the Third Conference on Applied Natural Language Processing pages ACM Transactions on Computer Human Interaction 11 2 Useful Transcriptions of Webcast Lectures 184 BIBLIOGRAPHY Chelba et al 2007 C Chelba J Silva and A Acero Soft indexing of speech content for search in spoken documents Computer Speech and Language 21 3 http www speech cs cmu edu 1998 Crowston et al 2004 K Crowston H Annabi J Howson and C Masango Effective work practices for software engineering Free libre open source software development In Proceedings of the ACM workshop on Interdisciplinary software engineering for automatic transcription and browsing of audio visual presentations Technical report MIT CSAIL Research Abstract 2005 Useful Transcriptions of Webcast Lectures BIBLIOGRAPHY 185 factors in computing systems pages Useful Transcriptions of Webcast Lectures Spoken Dialogues with Computers 186 BIBLIOGRAPHY IEEE Workshop on Automatic Speech Recognition and In Proceedings of the Seventh International ISCA INTERSPEECH Conference 2006 Furui 2005a S Furui Recent progress in corpus based spontaneous speech recognition IEICE Transactions on Information and Systems E88 D 3 Recent progress in corpus based spontaneous speech recognition IEICE Transactions on Information and Systems 3 88 Useful Transcriptions of Webcast Lectures BIBLIOGRAPHY Gauvain et al 2002 J L Gauvain L Lamel and G Adda LIMSI broadcast news transcription system 37 187 The Speech Communications Giuliani and Federico 2001 D Giuliani and M Federico Unsupervised language and acoustic model adaptation for cross domain portability In Proceedings of the ISCA ITR Workshop on Adaptation Methods for Speech Recognition Sophia Antipolis France August 2001 Glass et al 2004 T J Glass J R amd Hazen L Hetherington and C Wang Analysis and processing of lecture audio data Preliminary investigations In Proceedings of the HLT NAACL Workshop on Interdisciplinary Approaches to Speech Indexing and Retrieval pages lecture processing project In Proceedings of the Tenth ISCA European Conference on Speech Communication and Useful Transcriptions of Webcast Lectures 188 BIBLIOGRAPHY Godfrey et al 1992 J J Godfrey E C Holliman and J McDaniel SWITCHBOARD Telephone speech corpus for research and development In Proceedings of the IEEE Conference on Acoustics Speech and Signal development context Interacting with Computers 4 2 Bootstrapping language models for spoken dialog systems from the World Wide Web In Proc IEEE Conf on Acoustics Speech and Signal Useful Transcriptions of Webcast Lectures BIBLIOGRAPHY 189 Informedia at TRECVID 2003 Analyzing and searching broadcast news video In Proceedings of VIDEO TREC 2003 The Twelfth Text Retrieval Conference Gaithersburg Maryland USA November 2003 Hazen 2006 T J Hazen Automatic alignment and error correction of human generated transcripts for long speech recordings In Proceedings of the Ninth International ISCA Conference on Spoken Language Useful Transcriptions of Webcast Lectures 190 BIBLIOGRAPHY Howell 1997 D C Howell Statistical Methods for Psychology Duxbury Press USA 1997 Howell 1999 D C Howell Fundamental Statistics for the Behavioural Sciences Duxbury Press USA 1999 Howell 2002 D C Howell Multiple comparisons with repeated measures http www uvm edu dhowell 2002 Hsu and Glass 2006 B J Hsu and J Glass Style topic language model adaptation using HMM LDA In Proc ACL Conf on Empirical Methods in Natural Language Useful Transcriptions of Webcast Lectures Spoken Belgium BIBLIOGRAPHY 191 Jones et al 2003 D A Jones F Wolf E Gibson E Williams E Fedorenko D A Reynolds and M Zissman readability of automatic speech to text transcripts Measuring the In Proceedings of the Eight European Conference on Speech Communication and transcription of lecture speech using topic independent language modeling In ICSLP volume 1 pages onautomatic Speech Recognition and Understanding 2001 Kawahara 2004 T Kawahara Spoken language processing for audio In Processing of the archives of lectures and panel discussions International Conference on Informatics Research for Development of Knowledge Society Behavioural Sciences Brooks Cole Publishing 1995 Useful Transcriptions of Webcast Lectures 192 Klakow 2000 D Klakow BIBLIOGRAPHY Selecting articles from the language model training corpus In Proc IEEE Conf on Acoustics Speech and Signal DARPA CSR The Linguistic Data Consortium LDC94S13 1994 Leeuwis et al 2003 E Leeuwis M Federico and M Cettolo Language modeling and transcription of the TED corpus lectures In Proceedings of International Conference on Acoustics Speech and Signal Proceedings of the Joint International Symposium and Exhibition on Geospatial Theory Processing and Applications pages Useful Transcriptions of Webcast Lectures BIBLIOGRAPHY 193 Life et al 1996 A Life I Salter J N Temem F Bernard S Rosset S Bennacef and L Lamel Data collection for the MASK kiosk Woz vs prototype system In Proceedings of the International Conference on Speech and Language Processing pages Useful Transcriptions of Webcast Lectures 194 BIBLIOGRAPHY Munteanu et al 2006c C Munteanu G Penn R Baecker and Y Zhang Automatic speech recognition for webcasts How good is good enough and what to do when it isn t In Proceedings of the Eight International Conference on Multimodal Interfaces ICMI pages for usefulness usability transcript enhanced webcasts In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems pages Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Useful Transcriptions of Webcast Lectures BIBLIOGRAPHY Nanjo and Kawahara 2003 H Nanjo and T Kawahara 195 Unsupervised language model adaptation for lecture speech recognition In Proc ISCA IEEE Workshop on Spontaneous Speech Processing and learning in the fast lane In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational processing of audio lectures for information retrieval Vocabulary selection and language modeling In Proceedings of the IEEE Conference on Acoustics Speech and Signal evaluation baselines for speech summarization In Proceedings of ACL 08 HLT pages Useful Transcriptions of Webcast Lectures 196 BIBLIOGRAPHY Peters and Drexel 2004 J Peters and C Drexel Transformation based error correction for speech to text systems In Proc ISCA Conf on Spoken Language http www realnetworks com support education production html Riccardi and Hakkani Tur 2005 G Riccardi and D Hakkani Tur Active learning Theory and applications to automatic speech recognition IEEE Transactions on Speech and Audio Processing 13 4 Useful Transcriptions of Webcast Lectures BIBLIOGRAPHY 197 Ringger and Allen 1996 E K Ringger and J F Allen Error correction via a post processor for continuous speech recognition In Proc IEEE Conf on Acoustics Speech and Signal Technical report Wainhouse Research Whitepapers 2004 Roche and Schabes 1995 E Roche and Y Schabes part of speech tagging with finite state transducers Linguistics 21 2 International Conference on Multimodal where do we go from here 88 8 Human Computer Interaction Morgan Kaufmann 2002 Rowe et al 2001 L A Rowe D Harley P Pletcher and S Lawrence Bibs A lecture webcasting system Technical report Berkeley Multimedia Research Center University of California Berkeley USA June 2001 Useful Transcriptions of Webcast Lectures 198 BIBLIOGRAPHY Saraclar and Sproat 2004 M Saraclar and R Sproat Lattice based search for spoken utterance retrieval In Proceedings of the HLT NAACL Workshop on Interdisciplinary Approaches to Speech Indexing and Retrieval pages language model development using external resources for new spoken dialog domains In Proc IEEE Conf on Acoustics Speech and Signal 7 3 Useful Transcriptions of Webcast Lectures BIBLIOGRAPHY 199 Seymore and Rosenfeld 1997 K Seymore and R Rosenfeld Large scale topic detection and language model adaptation Technical Report CMU CS 97 152 School of Computer Science Carnegie Mellon University 1997 SPSS 2005 SPSS Spss 13 0 http www spss com 2005 Stark et al 2000 L Stark S Whittaker and J Hirschberg Asr satisficing The effects of asr accuracy on speech retrieval In Proceedings of International Conference on Spoken Language Processing 2000 Stern 1997 R Stern Specification of the 1996 hub 4 broadcast news evaluation In Proceedings of the DARPA Speech Recognition Workshop Chantilly Virginia USA February 1997 Toms et al 2005 E G Toms C Dufour J Lewis and R M Baecker Assessing tools for use with webcasts In Proceedings of the Joint Conference on Digital inter annotator agreement In Proceedings of the SENSEVAL Workshop Useful Transcriptions of Webcast Lectures 200 BIBLIOGRAPHY Evaluating Word Sense Disambiguation Programs Herstmonceux Castle Sussex England September 1998 Vertanen and Kristensson 2008 K Vertanen and P O Kristensson On the benefits of confidence visualization in speech recognition In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems pages recognition in university classrooms In Proc of the International ACM SIGCAPH Conference on Assistive Technologies pages Useful Transcriptions of Webcast Lectures BIBLIOGRAPHY 201 Weintraub et al 1996 M Weintraub K Taussig K Hunicke Smith and A Snodgrass Effect of speaking style on LVCSR performance In Proceedings of the International Conference on Spoken Language Processing pages or listen Discovering effective techniques for accessing speech data In Proceedings of the Human Computer Interaction Conference pages interfaces to support retrieval from speech archives In Proceedings of the Twenty Second ACM SIGIR Conference on Research and Development in Information Retrieval pages Useful Transcriptions of Webcast Lectures 202 recognition BIBLIOGRAPHY In Proc IEEE Conf on Acoustics Speech and Signal spoken document retrieval for the internet Lattice indexing for large scale web search architectures In Proceedings of the HLT NAACL Workshop on Interdisciplinary Approaches to Speech Indexing and Retrieval pages Useful Transcriptions of Webcast Lectures Appendix A Abbreviations AM Acoustic Model ANOVA ANalysis Of VAriance ASR Automatic Speech Recognition CSCW Computer Supported Cooperative Collaborative Work HCI Human Computer Interaction HMM Hidden Markov Model LM Language Model LVCS Large Vocabulary Continuous Speech MLLR Maximum Likelihood Linear Regression NLP Natural Language Processing OOV Out Of Vocabulary rate 203 204 APPENDIX A ABBREVIATIONS PMVDR Perceptual Minimum Variance Distortionless Response POS Part Of Speech SVD Singular Value Decomposition TOC Table Of Contents TBL Transformation Based Learning WER Word Error Rate Useful Transcriptions of Webcast Lectures Appendix B Glossary of Technical Terms Acoustic Model a statistical model of how specific words are 206 APPENDIX B GLOSSARY OF TECHNICAL TERMS Woodland 1995 N gram Model see Language Model Out Of Vocabulary the percent of vocabulary items in the actual utterance not appearing in the language model Perceptual Minimum Variance Distortionless Response an algorithm for extracting acoustic features from the speech signal For more details see Yapanel and Dharanipragada 2003 Precision Given a number of keywords S transcribed by an ASR system the precision with respect to these keywords is N S where N is the number of keywords correctly transcribed by the system Succintly precision true positives true positives f alse positives few false positives e g in the context of a keyword based search there will be less search results that actually do not contain the searched keyword Pronunciation dictionary sometimes referred to as lexicon contains all possible phonetic transcription of the words that an ASR system is expected to encounter Recall Given a total number of keywords T identified in the manual transcript the recall of the ASR system with respect to these keywords is N T where N is the number of keywords correctly transcribed by the system Succintly recall true positives true positives f alse negatives score means few false negative e g in the context of a keyword based Useful Transcriptions of Webcast Lectures 207 search more of the transcript lines that actually contain the search keyword will be included in the search results Singular Value Decomposition a method for factorization of rectangular matrices In NLP it is often used for dimensionality reduction tasks such as latent semantic indexing Hofmann 1999 Supervised training typically refers to ASR training for which manually annotated data is provided Often during supervised training other forms of manual intervention are also present such as making informed decisions about training data selection or fine tuning parameters information retrieval measure useful for determining the terms that are salient to a particular document in a collection e g terms that appear often in that document while being infrequent or not appearing at all in other documents It can also be used to score the relevance of documents with respect to a query term Tri gram Model an n gram model with n 3 see Language Model Unsupervised training the opposite of supervised S D I L where S D I Useful Transcriptions of Webcast Lectures 208 APPENDIX B GLOSSARY OF TECHNICAL TERMS are the number of substitutions deletions and insertions needed to transform the output sentence into the correct sentence and L is the number of words in the correct sentence Useful Transcriptions of Webcast Lectures Appendix C Instruments Used in Experiments 209 210 APPENDIX C INSTRUMENTS USED IN EXPERIMENTS Figure C 1 Consent Form page 1 for the experiment in Chapter 3 Useful Transcriptions of Webcast Lectures 211 3 5 6 0 1 1 2 44 05 7 8 1 99999999999999999999999999999 99999999999999999999999999999 999999999999 8 9999999999999999999999999999 8 9999999999999999999999999 8 Figure C 2 Consent Form page 2 for the experiment in Chapter 3 Useful Transcriptions of Webcast Lectures 212 APPENDIX C INSTRUMENTS USED IN EXPERIMENTS http test kmdi utoronto ca cosmin Figure C 3 Consent Form page 1 for the main study in Chapter 5 Useful Transcriptions of Webcast Lectures 213 0 3 7 9 4 5 1 0 2 6 6 8 3 9 2 0 4 1 0 0 0 4 2 0 3 Figure C 4 Consent Form page 2 for the main study in Chapter 5 Useful Transcriptions of Webcast Lectures 214 APPENDIX C INSTRUMENTS USED IN EXPERIMENTS http test kmdi utoronto ca cosmin 1 1 0 Figure C 5 Consent Form page 1 for the study following the interface re design in Chapter 5 Useful Transcriptions of Webcast Lectures 215 0 3 7 9 4 5 1 0 2 6 6 8 3 9 2 0 4 1 0 0 0 4 2 0 3 Figure C 6 Consent Form page 2 for the study following the interface re design in Chapter 5 Useful Transcriptions of Webcast Lectures 216 APPENDIX C INSTRUMENTS USED IN EXPERIMENTS What is a metaphor What does a prototype give you a sense of Why does the summative evaluation has this name In what kind of respondend strategy are we asking people about themselves and their work What kind of design is grounded in cognition Where was the mobile device shown as an interface sketch example intended to be used How many hours are needed for a careful analysis of one hour of audio video recording Figure C 7 The introductory quiz page 1 administered before the start of the experiment in Chapter 3 Useful Transcriptions of Webcast Lectures 217 How many usability principles guidelines does Nielsen s heuristic have What other program beside KidPix is given as an example of mental models of painting Who developed Pictive Why is it difficult to get people to think aloud What is the name of the colleague from xerox park that did the airport video project What company has become the leading vendor of personal financial management software What drawing package was used to create some of the ePresence interface prototypes What user related concept is described as more in vogue now Figure C 8 The introductory quiz page 2 administered before the start of the experiment in Chapter 3 Useful Transcriptions of Webcast Lectures 218 APPENDIX C INSTRUMENTS USED IN EXPERIMENTS What lab had the slogan demo or die In participatory design the users participate on the design team as what What do you ask yourself in information design What do you actually do when doing user testing What does the HCI research suggests from an engineering point of view Figure C 9 The introductory quiz page 3 administered before the start of the experiment in Chapter 3 Useful Transcriptions of Webcast Lectures were used in a latin square setup as described in Section 3 3 Chapter 3 Four quizzes with questions selected from the introductory quiz Figure C 10 An example of the quiz administered during the experiment in What concept is given to us by the visual psychologists Useful Transcriptions of Webcast Lectures What are the most effective icons What is a relevant example of a good choice of imagery Where and when were the Gestalt principles first formulated What controls are a relevant example of the good use of both psychology principles and graphic design 219 220 APPENDIX C INSTRUMENTS USED IN EXPERIMENTS Figure C 11 The questionnaire administered after a session with a level of WER of either 0 25 or 45 from the experiment in Chapter 3 Useful Transcriptions of Webcast Lectures 221 Figure C 12 The questionnaire administered after a session with a level of WER of NT no transcripts from the experiment in Chapter 3 Useful Transcriptions of Webcast Lectures 222 APPENDIX C INSTRUMENTS USED IN EXPERIMENTS 1 0 121 3 4 2 5 5 5 1 0 2 2 4 Figure C 13 The questionnaire page 1 administered at the end of the experiment in Chapter 3 Useful Transcriptions of Webcast Lectures 223 Figure C 14 The questionnaire page 2 administered at the end of the experiment in Chapter 3 Useful Transcriptions of Webcast Lectures 224 APPENDIX C INSTRUMENTS USED IN EXPERIMENTS Figure C 15 The questionnaire page 3 administered at the end of the experiment in Chapter 3 Useful Transcriptions of Webcast Lectures 225 Figure C 16 The questionnaire page 4 administered at the end of the experiment in Chapter 3 Useful Transcriptions of Webcast Lectures 226 APPENDIX C INSTRUMENTS USED IN EXPERIMENTS 0 1 1 2 3 1 3 2 4 5 Figure C 17 The web based questionnaire page 1 administered at the end of the field study in Chapter 5 Useful Transcriptions of Webcast Lectures 227 Figure C 18 The web based questionnaire page 2 administered at the end of the field study in Chapter 5 Useful Transcriptions of Webcast Lectures 228 APPENDIX C INSTRUMENTS USED IN EXPERIMENTS 0 0 1 2 5 5 0 5 2 5 3 4 1 1 3 4 1 1 Figure C 19 The web based questionnaire page 3 administered at the end of the field study in Chapter 5 Useful Transcriptions of Webcast Lectures 229 Figure C 20 The web based questionnaire page 4 administered at the end of the field study in Chapter 5 Useful Transcriptions of Webcast Lectures 230 APPENDIX C INSTRUMENTS USED IN EXPERIMENTS Useful Transcriptions of Webcast Lectures 