Multi Pass ASR using Vocabulary Expansion Katsutoshi Ohtsuki Nobuaki Hiroshima Shoichi Matsunaga and Yoshihiko Hayashi NTT Cyber Space Laboratories NTT Corporation 1 1 Hikari no oka Yokosuka shi Kanagawa 239 0847 Japan ohtsuki katsutoshi lab ntt co jp vocabulary using morphological knowledge was also studied in 9 for Serbo Croatian and German broadcast news speech Our approach directly estimates relevant words to input speech based on the conceptual base that models word cooccurrence patterns 10 The conceptual base enables one to measure the distance between words in word co occurrence pattern space and direct estimation of relevant words can reduce OOV words more effectively than estimation of relevant words via relevant documents or sub word sequences An expanded vocabulary is built by adding the relevant words to a reference vocabulary and is used in the second recognition process Since the vocabulary expansion process just adds relevant words to a reference vocabulary the second recognition process runs just after the first recognition We refer to this approach of multiple recognition processes as MASSIVE Multi pass Automatic Speech recognition uSIng Vocabulary Expansion The rest of the paper is organized as follows Section 2 presents an overview of multi pass speech recognition using vocabulary expansion Section 3 describes vocabulary expansion based on the conceptual base Section 4 presents the evaluation of MASSIVE on broadcast news speech Section 5 concludes the paper Abstract Current automatic speech recognition ASR systems have to limit their vocabulary size depending on available memory size expected processing time and available text data for building a vocabulary and a language model Although the vocabularies of ASR systems are designed to achieve high coverage for the expected input data it cannot be avoided that input data includes out of vocabulary OOV words This is called the OOV problem We propose dynamic vocabulary expansion using a conceptual base and multi pass speech recognition using an expanded vocabulary Relevant words to content of input speech are extracted based on a speech recognition result obtained using a reference vocabulary An expanded vocabulary that includes fewer OOV words is built by adding the extracted words to the reference vocabulary The second recognition process is performed using the new vocabulary The experimental results for broadcast news speech show our method achieves a 30 reduction in OOV rate and improves speech recognition accuracy 1 Introduction Out of vocabulary OOV words that are not included in a recognition vocabulary not only cannot be recognized when they appear in input speech but also affect their surrounding words and cause them to be misrecognized Although the vocabularies of automatic speech recognition ASR systems are generally designed to cover as many expected words in input speech as possible vocabulary sizes are limited depending on available memory size expected latency of speech recognition processes and the quantity and variety of available training text data Therefore OOV problems cannot be avoided by current ASR technologies and more or fewer OOV words can be included in input speech In some kinds of speech recognition applications such as broadcast news indexing 1 2 and meeting speech transcription 3 4 newly appearing words and infrequent words specific to a certain topic which tend to be OOV are critical and therefore need to be recognized accurately Modeling OOV words using sub word units has been shown to have an effect on OOV detection and estimating subword sequences 5 6 For retrieving contents by keyword queries in indexing applications notations of OOV words especially names of persons places and products need to be obtained instead of sub word sequences Information retrieval IR techniques were applied to the OOV problem in 7 and 8 They dynamically adapted a vocabulary and a language model to the topic of input speech using relevant articles obtained from a database or the Web Dynamic adaptation of 2 Multi Pass Speech Recognition using Vocabulary Expansion The speech recognition process using vocabulary expansion is executed according to the following procedure 1 First run recognize input speech using a reference vocabulary 2 Extract relevant words estimate relevant words to input speech using the recognition result of the first run 3 Rebuild vocabulary build an expanded vocabulary by adding relevant words to the reference vocabulary 4 Second run recognize input speech using the expanded vocabulary Although this kind of procedure needs to run recognition processes for input speech at least twice and cannot be executed in real time some applications such as transcription or indexing of archived speech data does not have to obtain recognition results in real time In view of this multi pass recognition approaches combined with unsupervised adaptation techniques for improving recognition accuracy befit such applications It takes much fewer processes to add words to a vocabulary than to rebuild a language model In addition the second run can be executed much faster than the first run if the acoustic model does not change and acoustic likelihood calculation of Currently at Department of Computer and Information Sciences Nagasaki University Currently at Graduate School of Language and Culture Osaka University the first run can be used for the second run 3 Vocabulary Expansion using Conceptual Base Relevant words for vocabulary expansion are extracted using word conceptual vectors that model word co occurrence patterns Words that appear in a recognition result of the first run are clustered based on word conceptual vectors and words with similar vectors to the centroid vectors of the obtained clusters are extracted from a vocabulary database as relevant words 3 1 Conceptual Base A conceptual base is a database that consists of concept words and corresponding conceptual vectors To build a conceptual base first a word co occurrence matrix is created by collecting word content word co occurrence frequencies within one sentence in a training text corpus Each row on the matrix is a word co occurrence vector for a particular word Since the matrix is quite sparse even with a huge corpus it is transformed into a lower order matrix by singular value decomposition SVD The reduced matrix is composed of word conceptual vectors The word conceptual vector is a vector representation of a word co occurrence pattern and if a pair of words has similar conceptual vectors they tend to appear in the same sentence and are relevant to each other 3 2 Vocabulary Database As mentioned word conceptual vectors are derived based on statistical word co occurrence patterns so that they cannot be derived properly for infrequent words which are to be extracted in vocabulary expansion for reducing OOV words To assign conceptual vectors to all words regardless of their frequency we calculated smoothed conceptual vectors as follows 1 Obtain sentence conceptual vectors for each sentence that includes a target word by calculating centroid vectors of all concept words appearing in the sentence A sentence conceptual vector v s for a sentence s which includes N s concept words is calculated as where s j is an auxiliary function that returns 1 if a target word j is included in a sentence s and returns 0 otherwise The denominator of the right side of equation 2 is the number of sentences that includes word j The smoothed conceptual vector is based on word conceptual vectors of co occurrence words with a target word and can be derived even for infrequent words if they co occur with concept words in a sentence Regardless of the frequency of words the distance between words depends on the distance between word conceptual vectors of co occurred concept words for each word The vocabulary database consists of all the words appearing in training text data and their corresponding smoothed conceptual vectors and is used for vocabulary expansion 3 3 Relevant Word Extraction Relevant words to input speech are extracted from the vocabulary database based on the distance between input speech and the words in the vocabulary database To measure the distance a result of the first run for input speech is represented as conceptual vectors through the clustering process 3 3 1 Clustering All the concept words appearing in the result of the first run are clustered based on their word conceptual vectors using the centroid method The clusters are considered to consist of a lot of concept words representing the content of input speech and the clusters with small number of words are irrelevant to the content or include recognition error words As centroid vectors of large clusters are considered to represent the content of the input speech the relevant words are extracted based on them By ignoring small clusters negative effects of speech recognition errors are reduced Also by using centroid vectors of multiple large clusters input that includes multiple topics can be handled 3 3 2 Relevance Score vs 1 NS c k 1 Ns wk 1 The distance between the input speech and words in the vocabulary database is measured by using relevance score r j D between a word j and a document D calculated as follows where cwk is a word conceptual vector of concept word wk 2 Obtain a smoothed conceptual vector of the target word by calculating a centroid vector for the sentence conceptual vectors of all sentences that include the target word A smoothed conceptual vector g j for a target word j is calculated as r j D max k g j dk g j dk 4 gj s j v s s s j s 2 where j 1 J is a word in the vocabulary database g j is a smoothed conceptual vector of word j k 1 K is a cluster of concept words appearing in the results of the first run and d k is a centroid vector of cluster k By calculating the relevance score to the input document or the recognition results of the first run for all the words in the vocabulary database the words with high relevance scores are extracted as relevant words to input speech 3 4 Rebuilding Vocabulary for Second Run s j 1 if j s 0 otherwise 3 The extracted relevant words are added to a reference vocabulary to build an expanded vocabulary Adaptation techniques for n gram language models can be applied to assign n gram probabilities to the new words In case of class n gram language models existing probabilities can be shared for the new words Since n gram probabilities can be used without any changes in the case the second run can be executed just after the vocabulary expansion gram probability of the added words in the OOV word class was 0 01 when adding 100 words and was 0 001 when adding 1000 words 4 4 Experimental Results The experimental results for the 25k vocabulary are shown in Table 1 For the reference vocabulary REF we compared our proposed method using the smoothed conceptual vector SCV with the results of a relevant document retrieval approach using the Okapi similarity measure OKAPI 7 The proposed method reduced many more OOV words oov number of OOV oov OOV rate red OOV reduction rate than the conventional method for both number 100 or 1000 of additional words for each news story add Though the word error rate wer of speech recognition increased when 100 words were added using the conventional method the word error rates were improved when the vocabulary was expanded by the proposed method Table 2 shows the results for the 50k vocabulary and a similar trend to the 25k It is notable that the 25k vocabulary using vocabulary expansion yielded better speech recognition performance than the 50k reference vocabulary Table 1 Experimental results 25k vocabulary add REF OKAPI conventional SCV proposed 100 1000 100 1000 oov 1471 1440 1159 1206 1002 oov 2 10 2 06 1 66 1 72 1 43 red 2 1 21 2 18 0 31 9 wer 27 50 27 71 27 38 27 17 27 00 4 Evaluations We evaluated OOV reduction in proposed dynamic vocabulary expansion and ASR performance of MASSIVE using broadcast news speech data 4 1 Evaluation Data We used 30 Japanese broadcast news programs which were aired on December 2002 as evaluation data The programs which vary from 5 to 30 minutes in length include 265 news stories in total The number of utterances is 2 898 and the number of words is 69 068 in the evaluation data An evaluation was carried out for each news story 4 2 Speech Recognition A speech recognition engine called VoiceRex which is being developed at NTT was used for the speech recognition experiments The acoustic models were 3 state 12 mixture state tied triphone HMMs male female and gender independent trained using approximately 300 hours of speech 150 hours each for male and female models There were approximately 5 000 states for each model The beginning of each utterance was evaluated with 96 mixture GMMs each one representing one of the three acoustic models and the model used for recognition was selected automatically 2 The reference vocabularies and the trigram language models were trained using 450 thousand sentences 15 million words of broadcast news transcription and newspaper text collected before December 2002 The vocabulary sizes were 25 thousands 25k and 50 thousands 50k and include words appearing ten or more times and words appearing twice or more respectively The 25k vocabulary covered 99 18 of the training text and 97 90 of the evaluation data 2 10 OOV The 50k covered 99 87 and 98 98 1 02 OOV 4 3 Vocabulary Expansion The conceptual base was trained using one year 2002 of newspaper text approximately 100 thousand articles The rows of the word co occurrence matrix or concept words had 47 thousand frequent words in the training data and the columns had 1 thousand frequent words except the 50 most frequent The columns were compressed to 100 by SVD that is to say the word conceptual vectors had a hundred dimensions The vocabulary database consisted of 160 thousand words that were all the content words appearing in the training data and smoothed conceptual vectors as described in Section 3 2 were assigned to all of them The clustering started from the status of each word constituting a single cluster and stopped when the number of clusters was less than one fifth from the start The centroid vector of the single largest cluster was used for calculating the relevance score The top 100 and 1000 words in terms of relevance score were extracted and added to the reference vocabulary for each news story Class n gram probability of the OOV word class is distributed equally to the added words That is to say the uni Table 2 Experimental results 50k vocabulary add REF OKAPI conventional SCV proposed 100 1000 100 1000 oov 712 710 580 586 506 oov 1 02 1 01 0 83 0 84 0 72 red 0 3 18 5 17 7 28 9 wer 27 32 27 39 27 22 27 08 26 99 Table 3 shows the number of OOV words oov and the number of word errors word error with their reduction red when the 25k reference vocabulary REF was expanded using the proposed method The efficiency eff which is the ratio of the reduction in word error to the reduction in OOV words was quite high and the additional words contributed greatly to word error reduction Table 4 shows the same numbers for the 50k vocabulary In this case the efficiency was more than 100 That is to say more words came to be recognized than the obtained OOV words Tables 5 and 6 show the experimental results using the proposed method by breaking the news stories for evaluation down by their OOV rate with the 25k and 50k reference vocabularies For both OOV rates oov were reduced for the news stories in every range of OOV rate oov range The word error rates were also reduced for every range except the news stories with less than 1 OOV rate for the 25k vocabulary in Table 5 Table 3 Reduction of OOV and word errors 25k vocabulary oov REF 100 1000 1471 1206 1002 red 265 469 word error 18995 18767 18646 red 228 349 eff 86 0 74 4 ond run can be executed shortly after the first run The experimental results show that the proposed method can reduce the number of OOV words effectively and the obtained words contribute to reducing word error rate efficiently 6 Acknowledgements The authors would like to thank Hisashi Ohara Manager of the Speech Acoustics and Language Laboratory and Masahiko Hase Director of Cyber Space Laboratories for their support and encouragement of this work 7 References Table 4 Reduction of OOV and word errors 50k vocabulary oov REF 100 1000 712 586 506 red 126 206 word error 18867 18703 18643 red 164 224 eff 130 2 108 7 1 J Makhoul F Kubala T Leek D Liu L Nguyen R Schwartz and A Srivastava Speech and Language Technologies for Audio Indexing and Retrieval Proc of the IEEE Vol 88 No 8 pp 1338 1353 2000 2 K Ohtsuki K Bessho Y Matsuo S Matsunaga and Y Hayashi Automatic Indexing of Multimedia Content by Integration of Audio Spoken Language and Visual Information Proc of ASRU pp 601 606 2003 3 A Waibel T Schultz M Bett M Denecke R Malkin I Rogina R Stiefelhagen and J Yang SMaRT The Smart Meeting Room Task at ISL Proc of ICASSP pp 752 755 2003 4 N Morgan D Baron J Edwards D Ellis D Gelbart A Janin T Pfau E Shriberg and A Stolcke The Meeting Project at ICSI Proc of HLT pp 246 252 2001 5 I Bazzi and J Glass A Multi Class Approach for Modeling Out of Vocabulary Words Proc of ICSLP pp 1613 1616 2002 6 K Tanigaki H Yamamoto and Y Sagisaka A Hierarchical Language Model Incorporating Class Dependent Word Models for OOV Words Recognition Proc of ICSLP Vol III pp 123 126 2000 7 T Kemp and A Waibel Reducing the OOV Rate in Broadcast News Speech Recognition Proc of ICSLP pp 1839 1842 1998 8 H Yu T Tomokiyo Z Wang and A Waibel New Developments in Automatic Meeting Transcription Proc of ICSLP Vol IV pp 310 313 2000 9 P Geutner M Finke and P Scheytt Adaptive Vocabularies for Transcribing Multilingual Broadcast News Proc of ICASSP pp 925 928 1998 10 T Kato S Shimada M Kumamoto and K Matsuzawa Idea Deriving Information Retrieval System Proc of 1st NTCIR Workshop pp 187 193 1999 Table 5 Classification by OOV rate 25k vocabulary oov range story 1 1 2 2 3 3 4 4 5 5 89 57 46 31 15 27 oov REF 0 53 1 39 2 48 3 49 4 56 8 55 100 0 44 1 21 1 95 2 99 2 80 7 44 1000 REF 0 35 1 00 1 57 2 39 2 45 6 55 20 0 28 7 31 7 28 4 29 7 46 1 wer 100 20 2 28 6 31 1 27 6 27 4 45 6 1000 20 2 28 5 30 9 26 9 27 1 44 4 Table 6 Classification by OOV rate 50k vocabulary oov range story 1 1 2 2 3 3 4 4 5 5 142 53 30 20 5 15 oov REF 0 48 1 41 2 42 3 42 4 59 7 54 100 0 29 0 76 1 31 2 07 3 53 5 59 1000 REF 0 26 0 64 0 97 1 65 3 18 5 32 23 0 33 0 28 3 24 5 43 5 53 4 wer 100 22 9 32 9 27 5 23 7 43 3 53 2 1000 22 9 32 9 27 2 23 3 42 3 52 9 5 Conclusions This paper described a multi pass approach of speech recognition using dynamic vocabulary expansion based on the conceptual base and its experimental results for broadcast news speech Relevant words to input speech were extracted from the vocabulary database based on the relevance score that was calculated by using word conceptual vectors The extracted words were added to the reference vocabulary to build an expanded vocabulary that is used for the second recognition process The vocabulary expansion process is simple and ngram probabilities do not have to be changed Thus the sec 