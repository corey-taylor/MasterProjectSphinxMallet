INFORMATICA 2006 Vol 17 No 1 111 Cache based Statistical Language Models of English and Highly Inflected Lithuanian UNAS Department of Applied Informatics Vytautas Magnus University Vileikos 8 LT 44404 Kaunas Lithuania e mail airenas freemail lt g raskinis if vdu lt Received August 2005 Abstract This paper investigates a variety of statistical cache based language models built upon three corpora English Lithuanian and Lithuanian base forms The impact of the cache size type of the decay function including custom corpus derived functions and interpolation technique static vs dynamic on the perplexity of a language model is studied The best results are achieved by models consisting of 3 components standard 3 gram decaying cache 1 gram and decaying cache 2 gram that are joined together by means of linear interpolation using the technique of dynamic weight update Such a model led up to 36 and 43 perplexity improvement with respect to the 3 gram baseline for Lithuanian words and Lithuanian word base forms respectively The best language model of English led up to a 16 perplexity improvement This suggests that cache based modeling is of greater utility for the free word order highly inflected languages Key words language models n grams cache models dynamic interpolation perplexity reduction inflected language free word order language Lithuanian 1 Introduction Statistical language models LM have become key components for large vocabulary continuous speech recognition LVCSR systems These models provide prior probabilities that are used to rate hypothesized sentences and to disambiguate their acoustical similarities During the last few decades much experimental work has been done in the field of statistical language modeling covering widespread world languages such as English French and German The most popular modeling techniques developed for those languages are known as n grams Although n grams have shown a good performance they are far from optimal because of false word independency assumption Lithuanian language modeling has started since 2002 Lithuanian has free word order and is highly inflected i e new words are easily formed by inflectional affixation These properties of a language result in difficulties of statistical modeling known as huge vocabulary size model sparseness high perplexity and a high out of vocabulary OOV word rate The attempts to overcome the abovementioned difficulties of Lithuanian included word parsing into stems and endings Vai 112 A Vai 2004 In this paper we investigate an alternative cache based modeling To our knowledge the cache based modeling of highly inflected free word order languages has not been attempted The cache based models presented in this paper are interesting in two respects They are able to adapt dynamically to the text under investigation and they have the potential of catching dependencies spanning longer word sequences than n grams do The impact of the model architecture cache size type of the decay function including custom corpus derived functions and interpolation technique static vs dynamic on the perplexity of a language model is studied Cache language models of Lithuanian are compared to the corresponding English ones 2 Related Work Cache based n gram model for linguistic applications was first introduced by Kuhn and De Mori 1988 1990 It can be thought of as a usual n gram Markov model trained on a relatively short history of recent words of some particular word wi Let wi be the i th word of a text and let h wi K wi 1 denote the cache or history of wi where K is the size of the cache Let C h K be the number of words within h belonging to the chosen vocabulary V Let C wi h be the number of occurrences of a word wi within h Let C wi 1 wi h be the number of word pairs wi 1 wi within h Finally let I condition denote the indicator function taking the value 1 if condition is true and 0 otherwise Then the conditional probabilities of 1 gram and 2 gram cache language models can be estimated by formulas 1 and 2 respectively C wi h PH wi h C h PH 2 wi wi 1 h i 1 j i K I wi wj i 1 j i K I wj V i 2 C wi 1 wi h j i K I wi 1 wj wi i 2 C wi 1 h j i K I wi 1 wj 1 wj 1 2 Conditional probabilities of a 3 gram cache LM can be estimated in a similar way Jelinek et al 1991 showed that 2 gram and 3 gram cache outperformed 1 gram cache in terms of LM perplexity1 Rosenfeld 1996 and Goodman 2001 reported just minor improvements of 3 gram cache over the 2 gram cache Nevertheless 1 gram cache language models are often used because of the problem of LM sparseness arising due to the limited cache size K Clarkson and Robinson 1997 suggested an improvement to 1 and 2 based on the experimental evidence that the probability of a word reoccurrence in a text exponentially 1 Perplexity refers to how many different equally probable words a statistical LM expects to appear in average for a particular type of a context It is estimated on the test subset of the corpora Cache based Statistical Language Models of English and Highly Inflected Lithuanian 113 decays as the distance to that word increases Otherwise stated recent words wj have greater influence on probability distribution of the current word wi The influence diminishes as the distance i j increases The decay cache is used to model this phenomenon i 1 j i K I wi wj Pd H wi h 3 wj 1 4 where d x is the decay function that tends to zero as the distance x increases Two exponentially decaying functions are often used d x e bx and d x ae bx c The decay speed b as well as parameters a and c are chosen experimentally or estimated by approximating the function of word reoccurrence i e the actual histogram of distances between the two consecutive repetitions of the same word Because of a very limited cache size standalone cache models Here s are interpolation weights optimized on the validation corpus Sometimes conditional interpolation formula is used Goodman 2001 PW 3 H H 2 wi wi 2 wi 1 PW 3 H H 2 wi wi 2 wi 1 PW 3 H wi wi 2 wi 1 if wi 1 h 7 otherwise Besides the standard word 3 gram models cache models can be interpolated with class based skip sentence mixture models Goodman 2001 topic mixture models Kneser and Steinbiss 1993 Iyer and Ostendorf 1999 and trigger pair models Tillmann and Ney 1996 Models 2 The percentage of words wi of the test corpus such that C wi h 0 114 A Vai Dynamic weights may be adapted on a word by word basis by optimizing perplexity on the recent word history hD wi D wi 1 PW 3 H H 2 wi wi 2 wi 1 W 3 i hD PW 3 wi wi 2 wi 1 H i hD PH wi h H 2 i hD PH 2 wi wi 1 h 8 where W 3 i hD H i hD H 2 i hD 1 for all i and D represents the length of an empirically chosen word history The M i hD can be estimated by an expectation maximization algorithm see Kneser and Steinbiss 1993 Martin et al 1997 or Gotoh and Renals 1997 before estimating the combined probability estimate 8 Dynamic interpolation was previously introduced in topic mixture models of highly inflected Slovenian Maucec and Kacic 2001 and Finish Siivola et al 2001 Some other attempts to avoid using static interpolation weights include the definition of interpolation weights as the function of a cache size K Goodman 2001 and the use of distinct weights i for classes of topic specific and general purpose words Federico and Bertoldi 2001 Gotoh and Renals 1997 Martin et al 1997 Seymore et al 1998 There is no consensus about the efficiency of the cache based LM embedded in a speech recognition system Jelinek et al 1991 Rosenfeld 1996 Tillmann and Ney 1996 reported WER3 reduction while Clarkson 1999 and Goodman 2001 reported WER degradation due to the use of a cache based LM In all those cases the perplexity of the cache based LM was significantly better than the perplexity of a word 3 gram LM 3 Resources and Tools Our investigations were based on three corpora The main corpus was the Lithuanian text corpus compiled by the Center of Computational Linguistics at Vytautas Magnus University Marcinkevi cien e 2000 containing 84 202 576 word tokens henceforth LT corpus This corpus represented a great variety of genres and topics of the present day written Lithuanian It was used for the investigation of cache based language modeling phenomena of inflected Lithuanian Two auxiliary corpora were the corpus of Lithuanian base forms LTBF and The Sunday Times English corpus of the year 1995 EN The LTBF corpus was derived from the LT corpus by replacing each word with its base form4 Auxiliary corpora served for inflected non inflected and Lithuanian English comparison purposes All corpora were divided into training validation and test subsets constituting 98 1 and 1 of the original corpora respectively The same proportions of text genres were kept within all subsets We used some text clearing punctuation was removed numbers and out of vocabulary OOV words i e words found in the test subset but absent from the vocabulary V where replaced by tags num and oov respectively Error Rate is the standard measure of accuracy of a speech recognition system forms the infinitive for verbs the singular nominative case for nouns etc were obtained with the morphological lemmatizer of Lithuanian Zinkevi cius 2000 In case of morphological ambiguity the first base form out of the list of possible base forms was selected 4 Base 3 Word Cache based Statistical Language Models of English and Highly Inflected Lithuanian Table 1 Summary of corpora characteristics Word types vocabulary 1158k 84 202 k LTBF EN 371k 235k 40 525 k 409 k 400 k 91167 445 853 k 713 k 1996 42185 Word tokens Articles training validation testing Average words per article 115 Corpus LT Majority of our investigations were carried out using locally developed cache based language modeling tools Simple n grams were built using CMU Cambridge Statistical Language Modeling Toolkit Clarkson and Rosenfeld 1997 that was extended to handle vocabularies of more than 65k words 4 Experimental Results We have investigated cache based models in order of increasing complexity First the simple 1 gram cache 1 gram decaying cache and 1 gram decaying cache using dynamic weight adaptation were investigated Thereafter the best performing 1 gram cache models were complemented with the components of 2 gram cache 2 gram decaying cache and 2 gram decaying cache using dynamic weight adaptation Throughout all experiments cache based LMs were compared on the basis of perplexity and perplexity improvement with respect to the baseline The results are briefly summarized in the Table 2 More detailed description of our investigations is presented in the subsections that follow Table 2 Summary of cache based language modeling experiments Language model LT 1157k LTBF 371k Perplexity PW 3 3 gram baseline Kneser Ney 1027 21 451 27 259 46 EN 235k Perplexity improvement PW 3 H 1 gram cache PW 3 d H 1 gram decaying cache PW 3 d H dynamic weight adaptation PW 3 d H H 2 2 gram cache static weights PW 3 d H d H 2 2 gram decaying cache PW 3 d H d H 2 dynamic weight adaptation 24 72 28 20 29 62 33 51 33 32 36 21 30 64 34 24 35 94 39 55 40 13 43 03 12 24 13 49 13 71 15 69 16 02 16 20 116 A Vai Table 3 Perplexities and OOV rates of 3 gram language models obtained with Kneser Ney and Good Turing smoothing techniques Vocabulary size Perplexity of PW 3 Kneser Ney smoothing 1027 21 451 27 259 46 Good Turing smoothing 1117 42 478 68 276 76 1 73 1 15 0 31 Corpus OOV LT LTBF EN 1157k 371k 235k All experiments were carried out without cache flushing5 as we wanted to investigate the ability of LMs to adapt to the changes in text topics OOV handling was realized in the following way terms of type P oov h and P oov wi h were skipped but P wi oov h were included into perplexity calculations 4 1 Choice of the Baseline Language Model We have chosen the conventional word based 3 gram PW 3 wi wi 2 wi 1 including all singleton 3 grams and smoothed using Kneser Ney Kneser and Ney 1995 smoothing technique as our baseline model Kneser Ney smoothing systematically outperformed Katz backoff technique Jelinek 2001 coupled with Good Turing smoothing see Table 3 4 2 1 gram Cache based Models We have constructed a series of 1 gram cache based models PW 3 H for cache sizes K ranging from 50 to 1000 Perplexity improvement and cache hit for each K was measured The obtained results are summarized by Fig 1 and Fig 2 1 gram cache based model significantly improved perplexity with respect to baseline PW 3 by 12 EN 25 LT and 31 LTBF Perplexity improvement showed similar cache size dependency curves for both languages The best improvements were achieved at K 300 EN and LTBF and K 500 LT Cache hit estimates confirmed our intuition that Lithuanian words were less used by the cache because of a bigger inflected vocabulary Cache hit curve for LTBF was similar to that of EN but the perplexity improvement for EN was much lower 4 3 1 gram Decaying Cache based Models We have investigated 1 gram cache based models PW 3 d H with four types of decay functions 5 The term cache flushing means that all words are removed from the cache at the end of an article Cache based Statistical Language Models of English and Highly Inflected Lithuanian 117 Fig 1 Impact of the 1 gram cache size on the perplexity improvement Fig 2 Impact of the 1 gram cache size on the cache hit Exponential decay function Linear decay function6 Gamma like decay function7 Corpus derived decay function exp db x e bx linear da x max a x 0 9 10 11 dgamma x xa 1 e bx a b x dcorpus o N i x 1 I wi wi x Occ i x o i 1 12 Here N is the size of the corpus and Occ i x j i x 1 I wi wj The expression wi wi x Occ i x o is true if and only if wi wi x and there is exactly o occurrences of the same word in between wi and wi x Thus the functions dcorpus x and dcorpus x represent respectively the histograms of distances between 0 1 two consecutive and two next to consecutive repetitions of the same word8 linear decay function was included for comparison purposes only popular exponential decay function has maximum at the position one But this contradicts empirical data as identical words rarely follow one another Empirical evidence suggests that the probability of the word to reoccur grows from the start and then starts decaying after some position 8 In all decaying cache experiments we used a discrete array implementation for storing function values The maximum cache size K was truncated at K 1000 for speed up purposes The values of d K are relative small for K 1000 7 The 6 The 118 A Vai Optimum parameters for the decay functions 9 11 were found experimentally by optimizing perplexity on the validation data set Corpus derived decay functions were individually estimated on training subsets of LT LTBF and EN corpora Adding decay to 1 gram cache resulted in an improvement of about 3 5 for Lithuanian and 1 3 for English models The optimum cache size and decay speed were inversely related Thus slower decaying functions were used for the LT task as it had longer caches It is interesting to note that dexp 0 01 x was one of the best decay functions for EN corpus and actually had a decay speed different from the decay speed of the distribution of word reoccurrences dEN 0 x Fig 3a 3d Nevertheless taking into account the second reoccurrence of the word could be of some help for both languages Table 4 last line It is also interesting to note that Fig 3 Sample decay functions normalized by dividing by the maximum Table 4 Perplexity improvements obtained with various decay function types of 1 gram cache Perplexity of PW 3 d H LT 1157k None best cache size dlinear x 500 dexp x 0 015 dexp 0 01 x dexp 0 005 x dexp 0 0025 x dgamma 1 05 0 005 x dgamma 1 10 0 01 x dcorpus x 0 dcorpus x dcorpus x 0 1 773 31 500 757 52 756 83 746 70 742 13 750 90 743 22 746 23 737 56 737 49 LTBF 371k 313 00 300 305 20 302 84 299 23 299 15 305 06 299 88 299 25 296 07 296 14 EN 235k 227 71 300 225 90 225 79 224 60 224 99 227 40 225 19 224 47 225 17 224 46 Decay function d x Cache based Statistical Language Models of English and Highly Inflected Lithuanian 119 LT the distribution of word reoccurrences dEN 0 x and d0 x of English and Lithuanian appeared to be very similar This distribution seems to be a language independent parameter 4 4 2 gram Cache based Models We have constructed a series of 2 gram cache based models PW 3 d H H 2 7 for cache sizes K ranging from 50 to 50000 2 gram cache based model PW 3 d H H 2 signifi x dcorpus x cantly outperformed 1 gram model PW 3 d H having d x dcorpus 0 1 by 5 LT LTBF and 2 EN The optimum K value was about 30000 2000 and 500 words for LT LTBF and EN corpora respectively Important differences in optimum K values could be explained by the fact that the average article size is more than 40k words in LT and only 445 words in EN corpus Adding decay to the cache 2 gram improved LTBF and EN models but not the LT model Table 5 This can be explained by the fact that decay functions used truncated cache size of K 1000 much less than the optimum cache size for LT models Corpus derived decay functions analogous to 12 seem to be best suited for Lithuanian corpora and exponential decay works best for the English corpus An interesting fact is that 2 gram cache hit on LTBF and even on LT was larger than on EN texts see Fig 4 This can probably explain why 2 gram cache improves English LMs not as much as Lithuanian LMs 4 5 Dynamic Adaptation of Component Weights We have constructed a series of 1 gram and 2 gram cache based models of type PW 3 d H and PW 3 d H d H 2 8 for D ranging from 20 to 500 As it was expected dynamic weight adaptation outperformed static weight optimization The model Table 5 Perplexity improvements obtained with various decay function types of 2 gram cache Perplexity of PW 3 d H d2 H 2 LT 1157k None best cache size dlinear 1000 x dexp 0 015 x dexp 0 01 x dexp 0 005 x dexp 0 0025 x dgamma 1 05 0 005 x dgamma 1 10 0 01 x dcorpus x 0 dcorpus x dcorpus x 0 1 683 04 30000 687 06 688 71 687 01 685 48 686 10 685 55 686 85 685 11 684 97 LTBF 371k 272 81 2000 271 32 272 30 271 29 270 51 270 79 270 54 271 29 270 28 270 19 EN 235k 218 76 500 218 22 218 23 217 88 217 67 217 99 217 69 217 83 217 76 217 89 Decay function d x 120 A Vai Fig 4 Impact of the 2 gram cache size on the cache hit Fig 5 Impact of the size of interpolation optimization history D on the perplexity of PW 3 d H PW 3 d H d H 2 added about 3 LT LTBF and 0 2 EN of improvement Tiny improvement of LMs built over EN corpus was probably due to the shortness of EN articles The 2 gram cache component of EN models had its utility as well as its average weight reduced Thus weight adaptation procedure could bring little gain over static weights In contrary 2 gram cache component was extremely useful for some articles of LT and LTBF corpora In this case the dynamic weight adaptation boosted LM performance Though short interpolation optimization histories hD had the potential of better adaptation to the changes in article or text topic there were no perplexity improvements for short histories D 50 words because of small reliability of such short histories The optimum history size was found to be D 200 for both 1 gram and 2 gram models for all three corpora The perplexity grew slowly for D 200 see Fig 5 It is interesting to note that LMs using dynamic weight adaptation had different average component weights for texts belonging to different stylistic categories Table 6 For instance legal documents had average H 2 i hD 0 2 indicating repeated usage of word pair collocations Cache based Statistical Language Models of English and Highly Inflected Lithuanian Table 6 Average weights of PW 3 d H d H 2 components per text category of LT corpus Average weights of PW 3 d H d H 2 8 components W 3 i hD National newspapers Translated philosophy Legal documents 0 88 0 70 0 75 H i hD 0 11 0 23 0 05 H 2 i hD 0 01 0 07 0 20 121 Text category 4 6 Other Approaches Related Cache based Modeling Rosenfeld 1996 found that the reduction of the perplexity could be achieved by using the cache for rare words only Such cache usage appeared not to be useful to Lithuanian However we found that some perplexity reduction could be gained by omitting certain unpromising words of the validation corpus from the cache boosting the probabilities of remaining words The unpromising words were defined as those having average probability estimate given by PW 3 H lower than PW 3 i e words w having perf w less than some negative constant where NV al perf w i 1 log2 PW 3 H wi wi 2 wi 1 log2 PW 3 wi wi 2 wi 1 and the sum is over validation corpus This approach resulted in some though negligible improvement 5 Conclusions In this paper we described a number of experiments with the cache based LMs of Lithuanian and English Our work confirmed that significant reduction of perplexity 43 03 36 21 and 16 20 for LTBF LT and EN corpora respectively could be achieved by the use of the cache based modeling Improvements over the baseline are higher than twice for Lithuanian with respect to English English 3 gram baseline model performs relatively well and is hard to improve as English has a strict word order Simplistic claim that worse models can be better improved cannot explain this difference Actually we repeated the whole set of experiments by replacing Kneser Ney smoothed 3 grams with worse GoodTuring smoothed 3 grams Perplexity improvement obtained with those worse models was the same as with the better ones through the whole set of experiments This suggests that cache based modeling brings more benefits to the free word order languages by being capable of capturing some dependencies that lie besides the strict word order 122 A Vai Cache improved LMs of Lithuanian word base forms LTBF better than LMs of Lithuanian words LT This suggests that additional efficiency can be brought into language modeling of Lithuanian by methods that are able to cope with the highly inflected nature of Lithuanian The impact of different modeling techniques had similar tendencies in case of both languages Adding 1 gram cache component to the 3 gram model brought the greatest part of improvement Additional improvement was gained by adding a 2 gram cache component by selecting an appropriate decay function and by replacing static component interpolation weights with the procedure of dynamic weight update It was found that optimal decaying function differs from the distribution of the distances of the word reoccurrence in general However it is possible to construct better decay functions by analyzing longer relations for example distribution of the distance to the second reoccurrence It appeared that the best 1 gram cache size is independent of language i e it is the same for EN and LTBF tasks Experiments confirmed that longer cache size should be used in the 2 gram cache case These last findings should be regarded with care because of the differences in article size in Lithuanian and English corpora This research confirms that cache based modeling significantly improves LM perplexity Our next task is to integrate them into a speech recognition system ant to investigate their impact on a speech recognition accuracy References Clarkson P 1999 Adaptation of Statistical Language Models for Automatic Speech Recognition PhD thesis Cambridge University Engineering Department Cambridge Clarkson P and R Rosenfeld 1997 Statistical language modeling using the CMU Cambridge Toolkit In Proceedings of 5th European Conference on Speech Communication and Technology pp Cache based Statistical Language Models of English and Highly Inflected Lithuanian 123 Kuhn R 1988 Speech recognition and the frequency of recently used words a modified Markov model for natural language In Proceedings of 12th International Conference on Computational Linguistics pp 124 A Vai Statistiniai kalbos modeliai naudojantys trumpalaike atminti anglu ir lietuviu kalboms UNAS Siame straipsnyje aprasomi statistiniu kalbos modeliu naudojan ciu trumpalaike atminti tyrimai Modeliai ivertinami naudojant tris skirtingus tekstynus angliska lietuviska ir lietuviska pae nuo atminties grindiniu formu tekstyna Darbe pateikiama siu modeliu maisaties priklausomyb cios funkcijos tipo bei modeliu interpoliavimo 