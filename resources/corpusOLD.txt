1	a	A Framework for Integrating Heterogeneous Sporadic Knowledge Sources into Automatic Speech Recognition Stefan Ziegler Guillaume Gravier CNRS IRISA Campus de Beaulieu 35042 Rennes France firstname lastname irisa fr Abstract Heterogeneous knowledge sources that model speech only at certain time frames are difficult to incorporate into speech recognition given standard multimodal fusion techniques In this work we present a new framework for the integration of this sporadic knowledge into standard HMM based ASR In a first step each knowledge source is mapped onto a logarithmic score by using a sigmoid transfer function Theses scores are then combined with the standard acoustic models by weighted linear combination Speech recognition experiments with broad phonetic knowledge sources on a broadcast news transcription task show improved recognition results given knowledge that provides complementary information for the ASR system Index Terms multimodal fusion landmark driven ASR eventbased speech recognition 1 
Introduction Multimedia data in the form of broadcasts podcasts as well as audio visual content present difficult challenges for state ofthe art hidden Markov model HMM based automatic speech recognition ASR since ASR systems are still sensitive towards unseen speaking styles and changes in acoustic conditions To improve acoustic modeling of HMM based ASR many studies advocate the incorporation of complementary knowledge sources into standard ASR to achieve improved recognition accuracy or robustness Examples of such complementary knowledge sources are phonetic models that aim at exploiting different features and modeling techniques motivated by phonological studies to build reliable and sometimes highly specialized detectors for phonetic classes 1 2 3 4 Another example is audio visual ASR where if available the visual modality is added to the existing acoustic information to benefit from the fact that acoustically similar speech classes might correspond to very different visual counterparts visemes that are 
reliable to detect 5 While it has often been argued that it is desirable for each knowledge source to rely on individual features and modeling techniques the common architecture of state of the art ASR has become a bottleneck for seamlessly integrating heterogeneous knowledge into speech recognition Consequently external knowledge sources often rely on rather homogeneous standard modeling techniques like frame based Gaussian mixture models that are integrated with conventional feature or decision fusion techniques inside the given architecture of HMM based ASR In this paper we present a new framework for integrating heterogeneous sporadic knowledge sources into HMM based ASR with the term sporadic referring to the fact that each knowledge is only defined at certain time frames often referred to as events e g 1 6 or landmarks e g 7 8 Indeed many acoustic or visual cues for phonetic events or visemes are naturally modeled as a sequence of discrete events rather than continous values which makes their 
integration into ASR very difficult given common multimodal fusion techniques In our framework integration of theses knowledge sources into standard HMM based ASR is performed in two steps First we map each knowledge source onto a logarithmic score using a sigmoid transfer function This allows the integration of knowledge sources of different scaling that appear asynchronously and do model arbitrary phonemic classes In a second step the obtained scores are combined with the acoustic scores of standard HMM based ASR using weighted linear combination These modified acoustic scores are integrated into the Viterbi decoding of the first pass of a large vocabulary ASR system In audio visual ASR continuous visual knowledge is often integrated into ASR via feature fusion i e concatenating audio and visual features to train refined acoustic models 9 This approach is also used for the integration of a burst onset landmark detector in 2 Decision fusion at the frame level using GMMs and HMMs by weighted linear 
combination of loglikelihood scores is used for integration of phonetic information in 10 and for visual information in 11 Phonetic knowledge is also integrated into ASR during the rescoring step of multipass ASR 3 7 Landmark based phonetic models have been used inside alternative probabilistic ASR frameworks 12 and in 1 statistical post processing of sporadic phonetic landmarks resulted in improved detection accuracy In the following section we will present our framework in detail before presenting speech recognition experiments using broad phonetic knowledge sources The paper will conclude with an outlook on future work 2 Integration of sporadic knowledge into ASR Given a speech utterance with t frames we consider a sporadic knowledge source k to be a function xk t with xk t being defined only for nk frames Txk t1 tnk Each source is the result of an external system specialized in detecting a given set of phonemes Sk which is a subset of the complete set of phonemes including non speech symbols P with Sk P 
To integrate this knowledge into triphone based ASR systems the phonemes in Sk have to be mapped to the corresponding states Ik which is equally a subset of the complete search space I see Figure 2 While the range of xk t is arbitrary for each source k for example one source could provide a probability from 0 to 1 while another source might correspond to a score in the range from to or to 0 we assume a clear correlation between xk t and Sk Assuming positive correlation low values for xk t are supposed to signal poor confidence in 37 Proceedings of the First Workshop on Speech Language and Audio in Multimedia SLAM Marseille France August 22 23 2013 the presence of Sk at t while high values have a very low error rate with a more or less sharp transition in between To illustrate such external knowledge sources we use the example of integrating phonetic landmark detectors into HMMbased ASR Landmark detection usually consists of two steps see for example 13 First the system detects potential locations for speech 
events landmarks before acoustic cues in vicinity of these landmarks are evaluated to estimate the probability of one or several phonetic classes for each landmark For example vowels can be detected by local maxima in the first formant frequency and evaluation of additional features around this landmark can specify the type of vowel An additional detector might provide landmarks signaling the presence of plosives by detecting abrupt changes in the signal and studying several cues like voice onset time or energy of the burst around this point see for example 14 It is obvious while the detection of vowels and plosives can be highly specialized for each phonetic class both classes are only defined at very specific locations Txk Furthermore the landmarks for vowels and plosives will be attached with a confidence estimate xk t that cannot be compared with each other since each class uses different classification algorithms and features With x t not being defined for most t sporadic knowledge can avoid to model 
parts of speech with high uncertainty about the acoustic content which is a major advantage compared to HMM based acoustic modeling While heterogeneity i e the fact that the ranges of each xk t are very different from each other could be overcome by normalization the sporadic nature of knowledge sources makes common fusion at the feature or decision level not feasible any more since k knowledge sources cannot be mapped onto a k dimensional vector at each frame t see Figure 3 In the following we present a general framework for the integration of k knowledge sources into the Viterbi decoding of a HMM based ASR system Given k sources xk t two steps are necessary from raw knowledge to knowledge driven ASR First we map each source xk onto a log likelihood score log sk given a sigmoid transfer function which parameters are estimated using cross entropy as the objective function In the second step these knowledge sources are integrated into the ASR system using a weighted linear combination of the obtained scores 
log sk and the acoustic scores of the ASR system 2 1 Weighted linear combination of k knowledge sources Given k knowledge sources our goal is to modify the acoustic score s i t for state i I at frame t according to weighted linear combination of the log likelihoods of k knowledge sources log sk i t and the unweighted log likelihood of the acoustic model log sasr i t given the weights wk log s i t log sasr i t X k xk t t log sk xk log sk t t Figure 1 Mapping a sporadic knowledge source xk t onto log sk t 2 2 Mapping of detection functions onto knowledge scores Intuitively log sk t should maximize the scores added to the correct path i e the scores added to frames t where the correct phoneme actually is a member of Sk but minimize the error it will introduce into the system by enhancing the wrong path Therefore our mapping function should result in log sk t 0 for low values of xk t but grow according to the confidence that higher values of xk will correctly indicate Sk This desired behavior can be obtained by 
a sigmoid function with log sk t k t T xk 1 exp k xk t k 2 k determines the steepness of the slope of the sigmoid k shifts the sigmoid to its optimal working point and k is a scaling factor For example if a knowledge source k provides a very reliable knowledge above a certain score k k will be a high value reflecting the confidence in the correctness of log sk t and a high k changes the transfer function from a smooth transition to a step function like behavior Equation 2 maps noisy unreliable values onto values very close to zero and rounding those values to a limited precision results in log sk t 0 Since log sk t 0 for all t Txk log sk t is effectively a sparse vector and we refer to its non zero frames as Tsk To find the optimal k k and k we maximize the crossentropy cce t between log sk t and the correct solution yk t at each frame cce t yk t log pk t log 1 pk t 1 yk t 3 Nk 1 Nk 0 wk log sk i t 1 With log sk i t 0 and wk 0 each source k enhances states i Ik that are associated with the phonemes in set Sk 
see Figure 2 Evidently log sk i t 0 for all states i Ik and for all frames t for which the source k is not defined with t Txk All states i Ik share the same likelihood score log sk i t to which we will refer to as log sk t The next section describes how to map xk t onto log sk t for each source before we discuss determining wk yk t is a binary vector with yk t 1 if Sk is correct at frame t and yk t 0 if not yk t is derived from the forced alignment of the correct utterance using our baseline ASR system pk t reflects the probability that knowledge source k is present at frame t Since some knowledge sources might have a skewed distribution we normalize pk t by the number of frames Nk 1 that are in Txk for which yk t 1 and respectively Nk 0 for which yk t 0 38 Proceedings of the First Workshop on Speech Language and Audio in Multimedia SLAM Marseille France August 22 23 2013 i t log s i t log sasr i t wk log sk i t k alignment of the n th hypothesis contained in the n best output of the ASR system By maximizing 
the MMI the correct hypothesis will become more likely while at the same time the competing hypothesis that do not correspond to the correct path at frame t will become less likely In this work we use only the best hypothesis as a competing alternative to the correct path so that n 1 which turns the MMI criterion into corrective training see 15 The optimization problem consists then in finding the T weights wk that maximize cmmi t over all frames in T k Tsk X Fmmi w u u log s cmmi t 8 tT Figure 2 Integration of knowledge into the speech decoding Arrows correspond to the transition probabilities while the nodes represent the acoustic scores log s i t The modified computation of log s i t is displayed for one node highlighted in grey 3 Experiments The corpus used in the experiments corresponds to radio broadcast news in the French language from the ESTER2 campaign 16 The ESTER2 dataset contains broadcast shows with speech in studio environments RFI but also difficult tasks like debates Inter or speech with 
strong accents radio TVME and radio Africa 1 Since we need the correct hypothesis to generate the correct state sequences u t and the aligned nbest recognition hypothesis u n t we discard every sentence containing out of vocabulary words during training and testing During testing this allows us to assure that finding the correct path by modifying the acoustic scores during the decoding is not prevented by missing vocabulary Additionally we discard all telephone speech from the dataset The estimation of the parameters k k k and wk are conducted on the ESTER2 development set using only broadcasts shorter than 20 minutes while final speech recognition experiments are conducted on the full ESTER2 test set The speech recognizer used in this paper is a two pass system trained on the ESTER1 and ESTER2 training data The first pass uses word internal triphones with 32 Gaussians per state and a trigram language model The second pass relies on 4 grams and cross word triphone models In this paper we integrate knowledge 
only in the first pass of our ASR system to generate improved word graphs for rescoring 3 1 Phonetic knowledge sources and baseline ASR system In the experiments we use broad phonetic classes BPCs as knowledge sources obtained from the Gaussian mixture models of a Mel frequency cepstral coefficients based monophone GMM classifier We derive 6 detection functions xk t for the BPCs vowels nasals approximants fricatives plosives and a non speech class Each BPC at frame t is first scored with the maximum score among all phonemes of this BPC before we perform normalization at each frame t by taking the logarithmic sum of exponentials for each source k to obtain 6 continuous detection functions After smoothing we convert these 6 functions into k 6 sporadic knowledge sources xk t by simple picking the local maxima for each detection function see Figure 3 Since the monophone models were trained on the same training data like our acoustic models it is unlikely that they actually will provide complementary information 
to the ASR system To experiment with more informative knowledge sources we additionally create oracle knowledge by adding a bias to the correct BPC at each frame t before performing normalization We refer to this knowledge sources as BPC oracle bias with bias being the scalar added to the correct BPC While this knowledge does not represent homogeneous knowledge in the sense that it incorporates different modeling and training frameworks we discuss the influence of multiplicative and additive scaling of Given the log likelihood scores of two complementary classes sk and sk we use the softmax function to estimate pk t according to p k t exp log sk t exp log sk t exp log sk t 4 As a consequence of the facts that all knowledge sources might model only a subset of P and sporadic knowledge results in asynchronous landmarks there is no score log sk t estimating the absence of knowledge source k at frame t Consequently this anti score log sk t always equals 0 log sk t 0 t 5 The final optimization problem consists in 
finding the parameters k k and k that maximize cce t for all frames of the training data X Fce k k k k xk yk cce t 6 tTxk 2 3 Estimation of the combination weights While the optimized knowledge sources log sk t might achieve low error rates according to Equation 6 it has yet to be determined if this source represents complementary knowledge to the acoustic models of the ASR system Therefore we use discriminative training to determine the weight wk for each source k that adjusts the contribution of source k to the overall acoustic score according to Equation 1 Estimating the weights wk of a linear combination of loglikelihoods is a well studied problem and several discrimination criteria have been proposed in the literature 15 11 10 In this paper we use the frame based maximum mutual information MMI between correct alignment and n competing hypothesis according to X cmmi t log s u t t log exp log s un t 7 n u t is the state sequence obtained by force aligning the correct solution of an utterance while u n t 
corresponds to the 39 Proceedings of the First Workshop on Speech Language and Audio in Multimedia SLAM Marseille France August 22 23 2013 Table 1 Word error rates of 4 different broad phonetic knowledge sources and the baseline ASR system on the ESTER2 development and test set each xk t in section 3 5 3 2 Optimization Given k knowledge sources xk t we have to optimize two objective functions to obtain the parameters k k and k for each knowledge source individually and the weights wk jointly We use L BFGS B minimization implemented in pythons scipy library for both objective functions with the constraints k 0 and k 0 for Equation 6 and wk 0 for Equation 8 The gradients of the objective functions are in both cases calculated using the symbolic differentiation implemented in the Theano package 17 For both objective functions we could achieve fast convergence by carefully choosing initial values for both optimization problems The scaling factor k should be proportional to the variance of xk t while the median 
of xk t is a good starting point for k For Equation 8 we started with the same value wk for all knowledge sources k by choosing the uniform weight which maximized Equation 8 This lead to Equation 6 needing about 20 iterations to converge while Equation 8 converged already after very few iterations Though we maximized the weights wk globally instead of using gradient descent we did not observe problems concerning convergence or overfitting 3 3 Speech Recognition Experiments After optimizing the mapping from xk t to log sk t for all sources k and estimating the weights wk on the development set speech recognition experiments were performed for BPC 0 BPC oracle 2 BPC oracle 3 and BPC oracle 4 Table 1 shows the word error rates WER on the ESTER2 development and test set along with the WER of the baseline As expected BPC0 did not provide any new information for the ASR system and obtained wk 0 for all BPCs except for the non speech class Consequently this led to no improvement in WER For the oracle BPCs the WER 
decreases with increasing the bias of the knowledge source For all cases the improvement on the development set is higher than on the test set as often observed in discriminative training 3 4 Evaluation of knowledge sources Table 2 displays two criteria evaluating the quality of xk t and log sk t for the BPCs of three different experiments AU C area under the curve is a performance measurement derived from the ROC curve receiver operator characteristic and equal to the probability that a classifier will rank a randomly selected true BPC higher than a randomly selected false BPC We use the AU C to give an indication of the quality of the raw knowledge source xk t Additionally for every knowledge source k we calculate a misclassification cost MI related to the mutual information criterion MMI in Equation 7 by calculating the av vow 0 nas 0 app 0 plo 0 fr knowledge baseline BPC 0 BPC oracle 2 BPC oracle 3 BPC oracle 4 WER dev 28 0 28 0 27 7 27 4 26 8 WER test 31 8 31 8 31 6 31 3 31 0 0 wav 0 log sk score 0 t 
vow nas app plo fr Figure 3 Spectrogram of the French word Bonjour uttered at the beginning of a broadcast show followed by six sporadic broad phonetic knowledge sources xk t BPC oracle2 including non speech and the obtained log likelihoods log sk t at the bottom All xk t are normalized so that 0 represents the maximum value The correct sequence of BPCs is marked in grey erage score added at each frame Tsk with weighting every correct frame by 1 and every incorrect frame by 1 This results in a negative value if a knowledge source introduces more errors into the decoding than it enhances the correct path 1 X 2yk t 1 log sk t T tT M I k T 9 T corresponds to the cardinality of the frames T used to calculate M I k T Both measures are shown on all available frames Txk for AU C and Tsk for M I Additionally they are calculated only on those frames Tk where the correct BPC of the true alignment u t differs from the BPC in u n t Since the acoustic score of the standard ASR system is not modified see Equation 1 we 
expect an improvement of the WER only if a knowledge source is able to correctly enhance most of the frames that are not already correctly aligned in the best recognition hypothesis Indeed it can be seen that BPC0 while performing relatively well on all frames T has a below random AU C with AU C 0 5 for all BPCs except silence for T For those BPCs M I is negative which means these knowledge sources make it less likely for the decoder to find the correct path at frames T Consequently discriminative training resulted in wk 0 for all BPCs except silence to prevent the ASR system from degrading In general evaluating the errors of a knowledge source without taking the output of the speech recognizer into account might be misleading Only 40 Proceedings of the First Workshop on Speech Language and Audio in Multimedia SLAM Marseille France August 22 23 2013 BPCs BPC 0 BPC oracle AUC MI AUC MI AUC MI 2 BPC oracle 4 T Tk Tk Tk Tk Tk Tk Tk Tk Tk Tk Tk Tk 0 84 0 41 0 9 0 0 94 0 67 1 9 0 7 0 98 0 87 3 0 1 9 vow 0 90 0 43 
0 6 0 1 0 96 0 63 1 0 0 4 0 99 0 81 1 7 1 3 nas 0 95 0 37 2 1 0 5 0 98 0 59 3 3 0 5 0 99 0 80 4 6 2 1 plo 0 93 0 35 1 7 0 5 0 98 0 61 3 0 0 6 0 99 0 83 4 2 2 1 fri 0 96 0 34 2 5 0 8 0 99 0 53 3 1 0 2 0 99 0 73 3 6 0 8 app 0 83 0 46 0 8 0 1 0 93 0 70 1 9 0 6 0 98 0 88 3 2 2 0 knowledge baseline BPC oracle 2 vow nas pl BPC oracle 3 vow nas pl BPC oracle 4 vow nas pl WER dev 28 0 27 6 27 5 27 3 WER test 31 8 31 8 31 7 31 5 Table 3 Word error rates of 3 different broad phonetic knowledge sources using only the BPCs vowels nasals and plosives each time curate to be effective Obviously efforts have to be made to research on existing and new knowledge sources that provide sufficiently accurate landmarks Furthermore it is desirable to experiment with additional feature systems like distinctive features or visual features like visemes Objective functions While the sigmoid transfer function in connection with the cross entropy criterion in Equation 6 as well as the MMI criterion for discriminative training provided 
good results one might consider additional transfer functions and training criteria State dependent weights and context dependency One disadvantage of the presented approach is the fact that it does not include state or phoneme dependent weights wi k for Equation 1 Enhancing states that are not in Ik for a knowledge source k might help to reduce the error introduced into the decoding since this might take into account common phonetic confusions like it is the case for vowels and approximants Additionally the speech recognition system could be modified to accommodate for a weight wasr that scales log sasr i t in Equation 1 to improve the discriminative training criterion Given phonetic landmarks as employed in this paper the probability of a speech class Sk at t depends on the context i e its preceding and subsequent landmarks To address this context dependency landmarks xk t could be rescored by additional models that are trained on landmark sequences like it has been proposed in 1 Integration into multi 
pass ASR In the current implementation we only implemented knowledge driven ASR in the first pass of our speech recognizer To fully benefit from heterogeneous knowledge sources integration into rescoring steps of multi pass ASR systems is desirable Table 2 AU C for xk t and M I for log sk t given different knowledge sources and their broad phonetic classes silence and non speech vowels nasals plosives fricatives and approximants Tk corresponds either to Txk for AU C or Tsk for M I when knowledge sources xk t achieve above random AU C on T M I tends to turn positive and the source contributes to improving the WER as it is the case for BPC oracle 2 and BPC oracle 4 3 5 Heterogeneous knowledge The previous broad phonetic knowledge sources were obtained using homogeneous monophone GMM classifier and thus did not represent a collection of heterogeneous knowledge sources Assuming heterogeneous knowledge will change xk t into xk t by multiplicative and additive scaling with xk t ak xk t bk it is evident that given 
our proposed sigmoid transfer function this scaling can be reversed by estimating the corresponding k and k To avoid the problem of finding an individual initialization for k and k to optimize objective function 6 for each knowledge source we recommend to perform a simple normalization for example mean and variance normalization for each knowledge source xk t All of our experiments showed that given proper initialization for k and k log sk t and consequently M I k T was similar for different multiplicative and additive scaling factors One advantage of our presented framework is the fact that it is able to deal with selected knowledge sources that may not cover the complete set of phonemes P This allows to design individual detectors for each phonemic group Sk without forcing to model the whole set P Table 3 shows the same speech recognition experiments as in section 3 3 but with the reduced set of BPCs vowels nasals and plosives It can be seen that the WER increases compared to using the complete range of 
BPCs and the overall impact of the provided knowledge sources is reduced This is expected since the broader the external knowledge sources become the less impact they will have onto the speech decoding even if a knowledge source inserts only few errors into the decoding 5 Conclusions The presented framework focused on the integration of heterogeneous and sporadic knowledge sources into HMM based ASR It allows the use of individual training and detection algorithms for each knowledge source that can be developed independently from each other Furthermore it accounts for event or landmark based models of speech and does not require the re training of existing acoustic models We used a transfer function to map each knowledge source onto a logarithmic score before the obtained values were combined with the acoustic scores by weighted linear combination While the knowledge sources that improved the WER in this paper corresponded to oracle knowledge we conclude from our experiments that landmarks which achieve an 
above random detection performance on frames where the ASR system aligns the wrong path are likely to improve the recognition performance of HMM based ASR systems 4 Future Work Our presented framework showed promising results given different kinds of broad phonetic knowledge sources Before concluding the paper we want to point out several directions for future research Knowledge sources Our experiments showed while the integration of rather broad speech landmarks into HMM based ASR improves the recognition these landmarks need to be ac 41 Proceedings of the First Workshop on Speech Language and Audio in Multimedia SLAM Marseille France August 22 23 2013 6 References 1 A Jansen and P Niyogi Point process models for event based speech recognition Speech Communication vol 51 no 12 pp 42 Proceedings of the First Workshop on Speech Language and Audio in Multimedia SLAM Marseille France August 22 23 2013 
2	a	IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING VOL 20 NO 5 JULY 2012 1513 Topic Dependent Class Based n Gram Language Model Welly Naptali Masatoshi Tsuchiya and Seiichi Nakagawa Member IEEE Abstract A topic dependent class TDC based gram language model LM is a topic based LM that employs a semantic extraction method to reveal latent topic information extracted from noun noun relations A topic of a given word sequence is decided on the basis of most frequently occuring weighted noun classes in the context history through voting Our previous work W Naptali M Tsuchiya and S Seiichi Topic dependent language model with voting on noun history ACM Trans Asian Language Information Processing TALIP vol 9 no 2 pp I INTRODUCTION I T has been decades since a statistical gram dominated the usage of language models LMs in automatic speech recognition ASR systems A word based gram uses only Manuscript received June 13 2011 revised September 11 2011 and December 22 2011 accepted December 22 2011 Date of 
publication January 11 2012 date of current version March 14 2012 This work was supported in part by the Global COE Program Frontiers of Intelligent Sensing from the Ministry of Education Culture Sports Science and Technology Japan The associate editor coordinating the review of this manuscript and approving it for publication was Dr Gokhan Tur W Naptali is with the with the Academic Center for Computing and Media Studies Kyoto University Kyoto 606 8501 Japan e mail naptali ar media kyoto u ac jp M Tsuchiya is with the Department of Information and Multimedia Center Toyohashi University of Technology Toyohashi 441 8580 Japan e mail tsuchiya imc tut ac jp S Nakagawa is with the Department of Computer Science and Engineering Toyohashi University of Technology Toyohashi 441 8580 Japan e mail nakagawa slp cs tut ac jp Digital Object Identifier 10 1109 TASL 2012 2183870 word level information from the context history to predict the next current word Many researchers have tried to alleviate this problem by adding 
more information Unsupervised class based LMs such as Random Forest LM 35 Model M 18 have been shown to outperform the word based gram However humans try to recognize speech in the context of the topic of the speech Without knowing the topic they may not be able to understand the conversation Incorporating topic information into gram was shown to improve the LM performance either by using the simplest method building several topic specific gram LMs and interpolating it altogether 23 20 19 or by using a more advance method employing a semantic analysis 3 17 4 26 27 33 Previously we have proposed a novel topic based LM named a topic dependent class TDC 29 In terms of perplexity its performance is better than several state of the art baselines such as word based and or class based gram LMs a cache based LM an gram based topic dependent LM and a latent Dirichlet allocation LDA based topic dependent LM The model is based on the belief that noun relations contain latent topic information Hence a semantic 
extraction method is employed along with a clustering method to reveal and define topics based on nouns only Given a word sequence a fixed size window is used to observe noun occurrences in the context history to decide the topic through voting Finally the topic is integrated as a part of the word sequence in the gram model The process is illustrated by Fig 1 As the number of topics increases the TDC standalone model suffers from a shrinking training corpus 29 Therefore to achieve good resuls the TDC model needs to be interpolated with a word based gram as a general model Although we have solved it by introducing soft voting in the test phase the model can be still improved by performing soft voting in the training phase Soft voting in the test phase converts the TDC model into a TDC mixture model whereas soft voting in the training phase increases the number of word sequences in topics We also introduce soft clustering in the training and test phases Soft clustering makes topic definition more reliable 
because it permits a noun to belong to multiple topics A common approach to improve an LM is by interpolating it with another LM that captures a different property of the language 6 Chueh and Chien 11 10 constructed an LM that combined information regarding languages from two topics one of which was obtained from the gram history through the Dirichlet distribution 9 and the other from the outer context of gram history topic cache through the multinomial distribution Tam and Schultz 32 also present a topic caching approach via LDA To further improve our TDC model we in 1558 7916 31 1514 IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING VOL 20 NO 5 JULY 2012 Fig 2 LSA illustration on matrix decomposition and dimension reduction Fig 1 Topic dependent class based n gram language model diagram corporate a cache based LM through unigram scaling Unigram scaling dynamic marginal was first introduced by Kneser et al 24 as a special case of the Minimum Discriminant Estimation adaptation with unigram 
constraints Its aim is to obtain a new LM so that the probability distribution satisfies some constraints and minimizes its relative entropy Kullback Leibler distance against the baseline LM Because the TDC models capture topical words while the cache based gram models consider reoccurring words their combination should improve the performance of the LM To complete our evaluation of the TDC LM for ASR besides reporting its perplexity we also perform automatic speech recognition experiments because perplexity does not always agree with the word recognition accuracy 22 We provide a complete TDC analysis in terms of word error rate WER on rescoring task The evaluation on both perplexity and WER was performed using two language resources they are Wall Street Journal WSJ for English and Mainichi Shimbun newspaper for Japanese In this paper there are three key differences compared to our previous work First we improve TDCs by employing soft clustering and or soft voting techniques which solve data shrinking 
problems and make TDCs independent of the word based gram in the training and or test phases Second for further improvement we incorporate a cache based LM through unigram scaling because the TDC and cache based LM capture different properties of the language Finally we provide an evaluation in terms of the word error rate WER and an analysis of the automatic speech recognition ASR rescoring task The remainder of this paper is organized as follows In Section II we describe our proposed TDC LM with explanations for soft clustering and soft voting and discuss the incorporation of the cache based gram This is followed in Section III by the evaluation of perplexity in the English and Japanese corpora Section IV provides the evaluation of the WER in the English and Japanese systems Finally the paper ends with conclusions and a discussion of possible future work importance of nouns can also be found in automatic summarization 1 5 18 However it will depend on the application it did not work well for automatic 
sentence clustering for multi document summarization 16 Unlike words topics are unobservable The relation between nouns is a supporting factor to define topics Latent semantic analysis LSA 3 is employed to reveal these hidden relations to define topics LSA extracts semantic relations from a corpus and maps them into a semantic vector space Discrete indexed words are projected into the LSA space by applying singular value decomposition SVD to a matrix representing the corpus A noun document matrix is used to represent the training rows correspond to nouns and columns corpus in which to documents Let be a representation matrix of dimension The SVD decomposes into three other matrices and 1 Because the dimensionality of the sowhere lution is very large for computing resources and the original matrix is presumed to be noisy dimensions of LSA matrices and are set to be smaller than those of the original 2 and is the best least square fit approximation to where The resulting matrix corresponds with the rows of 
matrix and matrix corresponds with the columns of matrix see Fig 2 These LSA matrices are used to project words into the dimension LSA vector space We apply a term frequency inverse document frequency tfidf in each document weight to each noun 3 4 5 where is the total number of documents After applying SVD the resulting matrices and contain information regarding words and documents respectively Thus is used to project nouns into the LSA space According to the following equation each noun can be mapped into an dimensional vector space for 6 II TOPIC DEPENDENT CLASS BASED GRAM LM This model is based on the assumption that nouns in a sentence play an important role in the whole discourse and are the core of the underlying LM Chen 7 shows the importance of and is where is a projection matrix of dimension a discrete vector of the noun where the th element of the vector is set to 1 and all other elements are set to 0 For the discrete vector for word is instance if NAPTALI et al TDC BASED GRAM LM 1515 Fig 3 
Illustration of hard clustering and k best soft clustering Fig 4 Illustration of hard voting and l best soft voting 0 0 0 1 0 To simplify a continuous vector for word represented by the th row vector of so that each word a continuous vector for is has where serving 7 is the topic class obtained by obwords in outer contexts of the near gram especially nouns Formally can be written as 11 is the voting score for a given window size where defined as follows 12 where if otherwise 13 is a vector representing the noun any familiar Because clustering method can be applied to form semantically similar noun classes which in turn define these clusters as topics A Soft Clustering In the LSA space VQ 14 is applied to cluster these words into topics A VQ algorithm is iterated using the cosine similarity between nouns until the desired number of clusters topics is reached A code centroid word in a VQ codebook corresponds to a topic vector A confidence measure is defined as the distance between a word vector and its class 
can be calculated using centroid Thus in this case and its topic the same cosine similarity between the noun class for 8 is the word vector mapped into the LSA space of word where and is the centroid vector of the topic class for 9 where is the number of topics The score indicates how confident a noun has to be in the topic class The larger the score is the more typical a word is in the class Previously 29 based on the score we mapped each noun into only one topic class This is known as a hard clustering technique Mapping a word to only one topic is very strict and could be dangerous because in nature a word may belong to multiple topics To make this model more robust soft clustering is performed so that a noun may belong to multiple topics We use a fixed number based on the largest score to map each into classes Fig 3 noun B Soft Voting A TDC with window size probability of a word sequence by leads to an LM in which the is defined Note that and are defined only for nouns otherwise 0 is a assigned If there 
are no nouns inside the window is defined dummy topic class Equation 11 is known as a hard voting decision In other words we construct the topic dependent gram LM of types Deciding if a given word sequence belongs to a topic or not is also very strict and could hurt the performance of the LM Therefore instead of choosing the best topic we may choose best topics soft voting for a given word sequence Fig 4 Soft voting in the training phase increases the number of word sequences in topics and by extension virtually increases the size of the training corpus Whereas soft voting in the test phase converts the TDC model into the TDC mixture model and 10 becomes TDC 14 where by is the th topic of the best topics obtained 15 and to is a mixture weight for th topic calculated in according TDC 10 16 1516 IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING VOL 20 NO 5 JULY 2012 Fig 6 Backoff of the TDC model The TDC 3 gram backoff to the TDC 2 gram then to the TDC 1 gram and then to the word based 1 gram where the 
shaded area is a window size of m Fig 5 TDC illustration with soft clustering and soft voting where is a voting score defined by 17 Given a word sequence we look for the nouns inside the window according to the 17 then we map each of these nouns into best classes For number of nouns there are classes in total All of these classes are weighted by The weights for the same class is summed up at most there are unique classes and choose the best topics according to 15 The mixture of these topics was calculated according to the score calculated in the previous step by 16 Finally the TDC is formed by 14 The whole process is illustrated in Fig 5 C TDC Backoff In an gram LM when the model encounters unseen events gram In our it is usually backed off by the shorter model we follow a similar approach in handling unseen events We use the Katz backoff with an absolute discounting method is seen in the training If the sequence where is a count dataset or then TDC 18 close to the predicted word it contains more infor The 
idea is similar to mation than the distant word hierarchical backoff 39 and decaying cache based model 12 to topic class then incorporate we backed off the word the topic class to the voting for topic decision and discard the A preliminary experiment topic class of distant word was conducted and the result shows that eliminating the distant in the backoff process gives better perplexity word than eliminating the close word probability can be calculated from the The discounted training corpus by using the following equation 20 where is the observed frequency of a particular sequence and is the discounting coefficient factor We use the absolute discounting method to determine 21 is a constant value which usually equals to discount the same amount of probability as the 22 The history otherwise TDC 19 is the discounted probability and is the backoff where weight Note that the backoff method is performed not by elimbut by sliding the window from inating word to Finally if there is no i e then the TDC 1 gram will 
be backed off to the statistical word based 1 gram See Fig 6 for the TDC backoff illustration This approach was taken because of word on is a shorter history of This is done by releases the last sliding the window so that the window word and adding the word into the window Recording these types of events is comto become putationally expensive therefore because the window size is quite large a topic switch rarely occurs Thus it is assumed that such a word exchange does not affect the topic switch to a great extent or NAPTALI et al TDC BASED GRAM LM 1517 D Interpolation A common way to improve an LM is by combining two or more LMs that capture different properties of the language 6 Here we combine the TDC with a word based gram and a cache based LM 1 Word Based Gram The word based gram LM 21 is the most common LM currently used in ASR systems It is a simple and powerful method based on the assumption that the preceding words Given a current word depends only on word sequence the word based gram predicts the 
probability according to the following equation is unity We refer to these as the TDC CACHE NGRAM and TDC NGRAM CACHE models respectively The cache window size is equal to the TDC window size with the addition gram history which is not covered by the TDC s of the window as shown in 26 and 27 at the bottom of the page III EVALUATION OF PERPLEXITY To evaluate the proposed TDC LM for ASR we use a simple and widely used approach by calculating its perplexity as defined by 28 NGRAM 23 is a word history where We used a word based gram as the LM for capturing the local constraint through linear interpolation TDC NGRAM 24 where is a weight constant 2 Cache Based LM A cache based LM 25 is based on the notion that words appearing in a document will increase the probability of appearing again in the same document Given a history the unigram cache model is defined by the following equation CACHE 25 Although perplexity calculation does not always agree with the word recognition accuracy 22 it is the first approximation 
toward a better LM 28 A English Corpus 1 Setup Experimental data were taken from the WSJ corpus between 1987 and 1989 and were divided into training and test datasets The training dataset contains 36 754 891 words in 85 445 documents while the test dataset contains 336 096 words in 809 documents ARPA s official 20o nvp 20k most common WSJ words with non verbalized punctuation is used as the vocabulary and it gives an out of vocabulary OOV rate of 2 47 and 2 57 for the training and test datasets respectively By adding a beginning sentence symbol s an end sentence symbol s and an unknown symbol to map all OOV words the total vocabulary size becomes 19 982 words The TreeTagger toolkit1 is used to filter nouns from the vocabulary2 with the Penn Treebank tagset resulting 12 086 nouns The library used for SVD is SVDLIBC 3 Table I gives eight examples of noun topic classes taken from TDC model with 80 topics The table only shows ten words with the highest confidence measure the closest noun to the centroid 2 Soft 
Clustering and Soft Voting First we will examine the performance of each soft clustering or soft voting in the training or test phase given by Figs 1http www ims uni stuttgart de projekte corplex TreeTagger 2We used the tagger to label individual word in the vocabulary in the English experiments That is by assuming one word can only have one label However the tagger is applied to the whole corpus in the Japanese experiments It is shown that either method gives good performance 3http tedlab mit edu dr svdlibc denotes how many times occurs in the where history The TDC tries to model topical words and does not model reoccurring words very well Combining these two LMs should improve the performance of the model We ensure that the TDC model is able to increase the probability of reoccurring words by incorporating the cache based LM The cache based model is used to scale the unigram probability of the TDC LM There are two ways of combining LMs i e to scale the TDC before or after it is linearly interpolated with 
the word based gram Equation 26 shows how to calculate the probability of the before model while 27 shows that of the after model is the normalizawhere is the scaling factor tion factor that guarantees that the summation of probabilities TDC CACHE NGRAM 26 CACHE 27 TDC NGRAM 1518 IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING VOL 20 NO 5 JULY 2012 TABLE I SAMPLE LIST OF TOPIC WORDS OF SEVEN RANDOMLY SELECTED TOPICS FROM TDC MODEL Fig 7 Perplexity of the TDC 3 gram with soft clustering and soft voting Fig 9 Increasing number of parameters of the TDC when soft voting is performed in the training phase Fig 8 Perplexity of the TDC 3 gram with soft clustering and hard voting let an TDC imply that the TDC was performed with best soft clustering in the training phase best soft voting in the training phase best soft clustering in the test phase and best soft voting in the test phase Therefore a 1 1 1 1 TDC implies a TDC with hard clustering and hard voting on the training and test phases Fig 7 shows the 
result of the TDC standalone model with soft clustering and or soft voting in the training and or test phases If we analyze test phase experiments i e 1 1 1 TDC and 1 1 1 TDC increasing best soft clustering gradually decreases perplexity The improvement is not very significant therefore its perplexity is still worse than that of the word based 3 gram even with 5 best soft voting in the training phase i e 5 5 1 TDC Soft clustering might solve the unreliable topic Fig 10 Perplexity of the TDC 3 gram with soft clustering and or soft voting linear interpolated with the word based 3 gram Note that the word based 3 gram perplexity is 111 6 mapping but the shrinking training data problem remains On the other hand increasing best soft voting in the test phase significantly improves perplexity Because soft voting in the test phase solved the shrinking training data problem by predicting the event by using mixtures of the TDC with 4 best soft voting in the test phase the TDC standalone model outperforms the word based 
3 gram By analyzing training phase experiments i e 1 1 1 TDC and 1 1 1 TDC we found out that increasing best soft clustering is also increasing its perplexity Soft clustering in the training or test phase alone does not improve the performance of the TDC LM However if we apply it on both sides training NAPTALI et al TDC BASED GRAM LM 1519 and test phases it improves perplexity slightly as seen in results given by Fig 8 The 1 1 1 TDC experiment suggests that increasing best soft voting in the training phase improves perplexity Unlike soft voting in the test phase soft voting in the training phase increases the number of LM parameters because it maps an event to multiple topics so that the number of parameters increases linearly and becomes at most times larger Fig 9 shows the number of parameters for the TDC with soft voting in the training phase It is increasing the number of word sequences in each topic Thus it solves the shrinking training data problem However similar to the disadvantage of the Random 
Forest LM performing soft voting in the training phase will increase the memory required for calculations In Fig 8 we can also see that increasing both and of soft clustering and soft voting in the training phase significantly improves perplexity From the 1 1 1 TDC to the 5 5 1 perplexity is reduced by 20 4In our previous paper 29 we have shown that TDC LM performs slightly better than LDA ADAPT 26 27 on perplexity This LDA ADAPT has been shown to perform better than Blei s LDA 4 TABLE II PERPLEXITY OF TDC WITH THE BEST CONFIGURATION TDC model From this table the result suggests that and values do not necessarily show similarity between training and test phases Increasing the value compensates the shrinking data problem in the TDC model where each topic in the TDC model on average is trained only by a division of topic size of and values alleviates a whole corpus While increasing improper topic decision specially for the unseen event Having different optimal values indicates how severe the data sparseness 
problem is and how reliable the topic decision is 3 Incorporation of the Cache Based LM In this experiment we also used a fixed 80 topics and a window size of 320 Because there are three parameters to tune 26 and 27 namely the linear interpolation weight and a scaling factor we explored the model s behavior by fixing two parameters and varying the third Table III shows the performance of TDC CACHE NGRAM 26 and TDC NGRAM CACHE 27 with baselines of the first four lines Because a cache based LM is trained on the basis of only a limited word history the interpolation weight is usually very small A large scaling factor always gives the best perplexity The best perplexity achieved by TDC CACHE NGRAM is 87 0 That against the gives 22 0 relative improvement word based 3 gram and 9 6 relative improvement against the TDC without a cache based LM combination The TDC NGRAM CACHE model gives even more improvement with the best perplexity 84 9 It implies 23 9 relative imagainst the word based 3 gram and provement 11 8 
relative improvement against the TDC without a cache based LM The large improvements in both models support our statement that both LMs capture different aspects of the language A side by side comparison between the TDC CACHE NGRAM and the TDC NGRAM CACHE and It shows that is given by Fig 11 where the TDC NGRAM CACHE is better than TDC CACHE NGRAM Note that the TDC used in Table III is the one with hard clustering and hard voting Perplexity can be further improved by performing soft clustering and soft voting The TDC with soft clustering and soft voting is given in Table IV with the window 1520 IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING VOL 20 NO 5 JULY 2012 Fig 11 Perplexity comparison between TDC3CACHE NGRAM and TDC NGRAM 3 CACHE TABLE III PERPLEXITY OF TDC 3 CACHE NGRAM AND TDC NGRAM 3 CACHE TDC HARD CLUSTERING AND HARD VOTING 1 1 1 1 6 01 for training and test datasets respectively The baseline cache based LM was executed with an increasing window size from 20 to 640 and the best 
perplexity was achieved with a window size of 160 Because the Mainichi Shimbun corpus contains manually tagged topic information of 17 topics such as sports culture social science economic and entertainment we also conduct a topic dependent language model based on word based gram mixtures denoted as a known topic word based gram All models perplexities are given in Table V Of all the baseline methods it can be seen that our model gives the best perplexity of 53 3 for the cache based LM combination and 57 5 for the TDC with soft voting in the test phase resulting in relative improvements of approximately 25 7 and 19 8 against the word based 3 gram LM respectively We can see that soft clustering and soft voting in training and test phases reduce perplexity makes the TDC standalone model performs as better as the interpolated TDC LM We expect further improvements when we interpolate the soft clustering and soft voting TDC model with the cache based model As the number of grams parameter is increasing the 
calculation of perplexity will take time because it needs to be normalized However we can see the evaluation on WER in the following section IV EVALUATION OF WORD ERROR RATE IN ASR A English ASR 1 Setup Using the HTK toolkit 37 we trained acoustic models for American English by using 49 190 utterances from the WSJ corpus from 1987 to 1989 The resulting feature vector is 39 dimensional and comprises of 12 MFCCs plus the 0th ceptral and their first and second deviation coefficients are normalized using ceptral mean subtraction We used the CMU pronunciation dictionary 6 containing 39 phonemes without lexical stress HMMs are initialized on the basis of TIMIT phonetic transcriptions Cross word triphones are using tied state triphones based on the decision tree There are 16 Gaussians for the non silent state and 32 for the silent state For more details please refer to 34 A description of the LM and its training data used in this experiment is given in Section III A1 with 20 k vocabulary size Test data taken from 
the WSJ corpus for November 1992 contain 2729 words 170 utterances in eight documents with each document spoken by a different speaker The OOV rate is 1 83 Note that the size of this test data has been reduced from 330 to 170 utterances to provide a sufficiently long history words at the beginning of each document for the TDC and cache based LMs Additionally we provide development and data to determine TDC parameters which include for the TDC with soft clustering and soft voting on the test set and and for the TDC interpolation Development data were also taken from the WSJ corpus for November 1992 and contain 4010 words 251 utterances in 12 documents with an OOV rate of 3 24 Similar to test data the development data size has been reduced from 491 to 251 utterances The best perplexity values achieved for both datasets are given in Table VI In the table implies linear interpolation 6http www speech cs cmu edu cgi bin cmudict size of 320 For the TDC CACHE NGRAM perplexity is improved from 87 0 to 83 6 for the 
TDC NGRAM CACHE perplexity is improved from 84 9 to 83 9 These results suggest that the TDC CACHE NGRAM performs comparable to the TDC NGRAM CACHE if the TDC is performed with soft clustering and soft voting Thus in ASR experiments we only employ the TDC NGRAM CACHE model B Japanese Corpus Training data for Japanese taken from the Mainichi Shimbun Japanese newspaper corpus from 1991 to 1998 contain 207 215 663 words in 855 825 documents Test data taken from the Mainichi Shimbun for January 1999 contain 385 863 words in 1119 documents Normally Japanese text does not have spaces between words For this task we used the Mecab toolkit5 Yet Another Part of Speech and Morphological Analyzer and converted the corpus into basic units word part of speech The vocabulary size is 20 k words taken from among most frequent words With a beginning sentence symbol s an end sentence symbol s and an unknown symbol to map all OOV words the total vocabulary size is 20 001 words This gives OOV rates of 4 11 and 5http mecab 
sourceforge net NAPTALI et al TDC BASED GRAM LM 1521 TABLE IV PERPLEXITY OF TDC AND CACHE BASED COMBINATION WITH SOFT CLUSTERING AND SOFT VOTING TABLE V PERPLEXITY FOR THE MAINICHI SHIMBUN CORPUS TABLE VI TDC BASED PERPLEXITY ON ENGLISH DEVELOPMENT AND TEST CORPORA while implies unigram scaling with the LM in the previous line The TDC referred to in the Test column was executed using parameters optimized for development data whereas that in the Best Test column was optimized with test data as the reference From Table VI we can see that parameters tuned using development data are also stable for test data For the decoder we used the inhouse large vocabulary SPOken continuous speech recognition system SPOJUS Japanese Understanding System 15 By using the two pass decoder 1000 best hypotheses were generated The LM used in the first and second passes is a word based 3 gram yielding a WER 6 3 and 4 6 in the first and second passes respectively 7 In this experiment we observed the behavior of the TDC LM with 
respect to the WER by using test data First we investigated the improvement on the TDC standalone LM Then we analyzed 7The baseline WER given by the original test data 330 utterances was almost similar to that of reduced test data 170 utterances used for our experiment They were 6 4 and 4 7 for the first and second passes respectively the combination of the TDC a word based gram and a cachebased model with the topic number and window size set to 80 and 320 respectively 2 TDC Stand Alone Model Fig 12 shows the change in the WER as we increased the number of topics or the window size The number of topics varied as 20 40 80 and 160 while the window size varied as 20 40 80 160 and 320 For the TDC we fixed the window size to 80 while increasing the number of topics and similarly while increasing the window size we fixed the number of topics at 20 From Fig 12 we can see that the best WER was achieved with 80 topics and a window size of 320 in subsequent experiments these values were then used as the number of 
topics and window size respectively Fig 13 shows the changes in the WER as we vary and and in the test phase Increasing sometimes gives a better WER but at other times it makes it worse 4 best soft clustering in the test phase gives an absolute improvement of 0 1 in the WER Meanwhile increasing decreases the WER from 6 8 to 5 1 For the next experiment we performed soft clustering and soft voting in the training phase Results are shown in Table VII Increasing soft clustering and soft voting in the training phase to the 3 best gives a better WER than the word based 3 gram Increasing them further to the 5 best gives a comparable or better WER than the word based 3 gram with cache based unigram scaling Despite a worse perplexity as compared with the word based 3 gram 72 9 versus 62 5 the TDC 3 gram with the best configuration 3 3 4 10 yields a relative improvement in the WER against the baseline of 10 9 1522 IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING VOL 20 NO 5 JULY 2012 Fig 12 Stand alone 1 1 1 
1 TDC based WER with increasing number of topics or window size Fig 14 1 1 1 1 TDC NGRAM based WER with increasing the number of topics or window size Fig 13 Stand alone TDC based WER with increasing soft clustering or soft voting in the test phase on the English system TABLE VII TDC BASED WER WITH SOFT CLUSTERING AND SOFT VOTING IN THE TRAINING AND TEST PHASES Fig 15 1 1 1 1 TDC NGRAM CACHE based WER with increasing the number of topics or window size 3 TDC Interpolation Similar to TDC standalone model experiments we first increased the number of topics or window size and calculated the WER Figs 14 and 15 show these results In these figures the TDC 3 gram is denoted as the TDC the word based 3 gram as NGRAM and the cache based 1 gram as CACHE The TDC in this experiment was implemented using hard clustering and hard voting in the training and test phases Both interpolation models give a better WER than the baseline The best WER was achieved by interpolating the three LMs yielding a WER of 4 0 with 20 topics 
and a window size of 160 Then a combination of the word based 3 gram and the cachebased 1 gram was implemented using the TDC with the best number of topics and window size from the previous section i e 80 topics and a window size of 320 Using development data WERs for the TDC are given in Table VIII Compared with the TDC WER for test data the TDC WER for development data is only slightly better and performs similar to the word based 3 gram unigram scaled by the cache based 1 gram This might be resulted from the higher OOV rate as compared with test data The best WER achieved using parameters optimized on development data is 4 0 i e a 13 0 relative improvement against the baseline with a WER of 4 6 This improvement is statistically significant8 at the 0 88 level 2 tailed Note that there are many combinations of configuration for getting the same WER Table IX gives the ASR results for deletion insertion and substitution errors and also the correctness and accuracy of 8Statistical significance was investigated 
according to Strik et al 30 31 by using a combination of the Number of Errors per Sentence NES metric and the Wilcoxon Signed Rank WSR test NAPTALI et al TDC BASED GRAM LM 1523 TABLE VIII TDC BASED WER OF ENGLISH DEVELOPMENT AND TEST CORPORA TABLE IX TDC BASED ACCURACY OF THE ENGLISH TEST CORPUS TABLE X EXAMPLES OF IMPROVED UTTERANCES the TDC optimized for test data third column of Table VIII The first two lines give baseline results for the first and second passes respectively followed by rescoring results Rescoring using a word based 3 gram unigram scaled by a cache based 1 gram gives an absolute improvement of 0 2 over the baseline This improvement is worse than that of the TDC Combining a word based 3 gram improves the substitution error slightly Further combining the LM with the cache based unigram through unigram scaling yields a slight absolute improvement of 0 1 in the WER Overall the LM yields a relative improvement of 15 2 in the WER against the baseline Table X shows three examples of improved 
utterances The Ref shows the transcript of the input utterance The Base is the result given by the word based 3 gram the same result also given by the word based 3 gram unigram scaled by the cache based 1 gram The Prop is the result given by the best TDC 3 gram model For the first example note that focus is a noun and meyerman is an OOV word In the baseline focus was not recognized while the proposed method correctly recognized it For the second and third examples the TDC correctly recognize eroded and gains which are topical words in economics These results show the effectiveness of the TDC LM B Japanese ASR 1 Setup Acoustic models were trained using 27 992 utterances spoken by 175 male speakers from the Japanese News Article Sentence JNAS corpus The feature vector is 38 dimensional and comprises of 12 dimensional MFCCs their first and second deviation coefficients and the first and second deviations TABLE XI TDC BASED PERPLEXITY ON JAPANESE TEST CORPUS of log power The data were used to train 928 Japanese 
context dependent syllable HMMs CDHHMs Each continuous density HMM has five states of which four have pdfs of output probability Each pdf consists of four Gaussians with full covariance matrices For more details please refer to 38 Training data for the LM were taken from the Mainichi Shimbun corpus from 1991 to 1997 containing 144 804 277 words in 597 539 documents For this experiment we used the morphological analyzer ChaSen9 to convert the corpus into basic units word pronunciation part of speech The vocabulary size is 20 k words taken from words that occur most frequently With a beginning sentence symbol s an to end sentence symbol s and an unknown symbol map all OOV words the total vocabulary size is 20 001 words This gives an OOV rate of 5 04 for training The baseline word based 3 gram was constructed using the 9http chasen legacy sourceforge jp 1524 IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING VOL 20 NO 5 JULY 2012 TABLE XII ACCURACY OF THE JAPANESE ASR SYSTEM Japanese corpus used in the 
experiment is six times larger than that of English corpus The soft clustering and soft voting technique compensate the data sparseness or shrinking problem by clustering V CONCLUSION AND FUTURE WORKS A TDC is a topic dependent LM with unsupervised topic extraction employing semantic analysis and voting on nouns We demonstrated that a TDC with soft clustering and or soft voting in the training and or test phases improved performances Soft clustering solved the unreliable topic mapping while soft voting solved the shrinking data problem in the TDC Soft clustering in the TDC should be performed in both the training and test phases Soft voting yielded a larger improvement compared with soft clustering Soft voting performed in only one phase either the training or test phase also produced good results We also demonstrated that incorporating a cache based LM improved the TDC further The cache based LM helped the TDC capture an aspect of the language that was not covered such as increasing the probability of co 
occurring words The evaluation of perplexity showed that the TDC achieved a 25 1 relative reduction in perplexity for the English corpus and a 25 7 relative reduction for the Japanese corpus compared with the baseline Finally we showed that the TDC improved not only perplexity but also the WER The ASR experiment on rescoring showed that the best result for the English system achieves a 15 2 relative improvement and that for the Japanese system achieved a 24 3 relative improvement in the WER as compared with the baseline The only drawback of the TDC LM is that it causes an increase in the number of parameters when performing soft voting in the training phase In this study we used a fixed value of for soft clustering and for soft voting This may cause unimportant word sequences to increase while also increasing the memory requirement In future work we intend to investigate the use of a dynamic threshold based on a confidence measure to reduce the number of parameters in the language model without sacrificing 
performance The TDC backoff by sliding the window is a simple approach we took for unseen words We will apply more complicated approaches for the TDC backoff REFERENCES 1 R Barzilay and M Elhadad Using lexical chains for text summarization in Proc ACL Workshop Intell Scalable Text Summariz 1997 pp Fig 16 Stand alone TDC based WER with increasing the number of soft clustering or soft voting in the test phase on the Japanese system Test data comprises 100 utterances 1746 words from the Mainichi Shimbun read documents JNAS for the period NAPTALI et al TDC BASED GRAM LM 1525 5 C Bouras and V Tsogkas Improving text summarization using noun retrieval techniques in Knowl Based Intell Inf Eng Syst ser Lecture Notes in Computer Science Berlin Heidelberg Germany Springer 2008 vol 5178 pp 31 H Strik C Cucchiarini and J M Kessens Comparing the performance of two CSRS How to determine the significance level of the differences in Proc Eurospeech Aalborg Denmark 2001 vol 3 pp Masatoshi Tsuchiya received the B E M E and Dr 
in Informatics degrees from Kyoto University Kyoto Japan in 1998 2000 and 2007 respectively He joined the Computer Center Toyohashi University of Technology Toyohashi Japan which was reconstructed to the Information Media Center in 2005 as an Assistant Professor in 2004 His major interest in research is natural language processing Seiichi Nakagawa M 87 received the Dr Eng degree from Kyoto University Kyoto Japan in 1977 He joined the faculty of Kyoto University in 1976 as a Research Associate in the Department of Information Sciences From 1980 to 1983 he was an Assistant Professor from 1983 to 1990 he was an Associate Professor since 1990 he has been a Professor in the Department of Information and Computer Sciences Toyohashi University of Technology Toyohashi Japan From 1985 to 1986 he was a Visiting Scientist in the Department of Computer Sciences Carnegie Mellon University Pittsburgh PA Dr Nakagawa received the 1997 2001 Paper Award from the IEICE and the 1988 JC Bose Memorial Award from the Institution 
of Electronics Telecommunications Engineers His major interests in research include automatic speech recognition speech processing natural language processing human interface and artificial intelligence He is a Fellow of IPSJ and IEICE 
3	a	Author manuscript published in SLT Workshop on Spoken Language Technology United States 2012 TOWARDS A NEW SPEECH EVENT DETECTION APPROACH FOR LANDMARK BASED SPEECH RECOGNITION Stefan Ziegler Bogdan Ludusan Guillaume Gravier CNRS IRISA Campus de Beaulieu 35042 Rennes France ABSTRACT In this work we present a new approach for the classification and detection of speech units for the use in landmark or eventbased speech recognition systems We use segmentation to model any time variable speech unit by a fixed dimensional observation vector in order to train a committee of boosted decision stumps on labeled training data Given an unknown speech signal the presence of a desired speech unit is estimated by searching for each time frame the corresponding segment that provides the maximum classification score This approach improves the accuracy of a phoneme classification task by 1 7 compared to classification using HMMs Applying this approach to the detection of broad phonetic landmarks inside a landmark driven HMM 
based speech recognizer significantly improves speech recognition Index Terms speech event detection landmark driven ASR 1 INTRODUCTION In state of the art hidden Markov model based HMM automatic speech recognition ASR speech is modeled as a sequence of phone segments often referred to as the beadson a string model of speech 1 In contrast to that acousticphonetic or event based approaches to speech recognition model speech as a stream of asynchronous phonetic events which have to be further processed to obtain a higher level speech representation 2 3 4 5 Many of these approaches require the detection of phonetic events as time instances referred to as landmarks There are numerous studies concerning the detection of phonetic events in the speech signal which can be divided into two general groups The first group uses expert knowledge to derive detection rules from various signal representations e g 2 The second group uses a classification and detection approach where labeled data is converted into the desired 
phonetic feature representation and classifiers are trained to map acoustic observations onto phonetic speech This work was partially supported by the Agence National de la Recherche in the framework of the ASH project Furthermore we would like to thank Christian Raymond for advices concerning the use of his boosting software units e g 3 4 Employing these classifiers sequentially on the speech signal results in detection functions indicating the presence of the desired speech units These classifiers operate usually on a frame basis extracting a fixed dimensional observation vector for each frame eventually considering a small context window On the one hand this fixed observation space enables the use of non linear and non parametric classifiers like support vector machines 3 4 or multilayer perceptrons for classification which possess good discrimination abilities and can be trained efficiently On the other hand predicting speech units by observing a fixed size fraction of the speech signal does only partly 
model time variable speech units including phonemes and many articulatory feature units We are interested in the problem of extracting a fixeddimensional observation vector from time variable speech units to discriminate them using a classifier that requires a fixed number of observations To obtain this observation vector we use a maximum likelihood segmentation method to force each labeled speech unit into three spectrally homogeneous parts from which the observation vector is extracted In our work this observation vector is used to train speech units with boosted decision trees and to predict unknown segments during testing To obtain a detection function indicating the locations of a speech event by its local maxima we search for each frame the segment that maximizes the classification score at this frame This method is applied to a recently proposed landmark driven ASR framework that uses phonetic landmarks to guide the search in an HMM based ASR system 6 The paper is organized as follows First we present 
the details of our proposed method and review boosting with an ensemble of weak learners as well as landmark driven ASR Evaluation is carried out on a phoneme classification task and on landmark driven speech recognition experiments and we conclude with an outlook on future work 2 PROPOSED SYSTEM FOR SPEECH EVENT DETECTION In event based speech recognition phonetic speech units are often trained and classified at the frame level The classification scores of successively predicted frames correspond to a hal 00758424 version 1 28 Nov 2012 Fig 1 Diagram of the proposed method of using segmentation to extract a fixed dimensional observation vector from a labeled speech unit training or an unknown segment testing detection function that indicates the presence of the speech event usually by its magnitude Landmarks corresponding to single time instances marking the most salient points of speech events are then obtained by post processing the detection profiles varying from simple peak picking to processing with 
statistical models e g 3 To capture spectral transitions and temporal information usually a concatenation of parametrized speech frames create a fixed dimensional observation space for each frame for classification But since speech units are commonly modeled as time variable segments a fixed number of frames will either capture only fractions of the spectral information of the speech unit or contain misleading information about previous or following speech events If the detection function is obtained by inaccurate modeling of the desired speech event it is difficult to recover the loss in information by post processing noisy and erroneous detection profiles In the following we present a method that overcomes this shortcomings by extracting a fixed dimensional observation vector x from a given time variable sequence of parametrized speech Y y1 yt section 2 1 corresponding to any desired speech unit We are able to keep a fixed dimensional observation space since we reduce every speech segment to three 
subsegments similar to a three state HMM This observation vector enables the use of any desired classification method while generalizing all time variable speech units In our case we apply a committee of boosted decision stumps to train a classier F x section 2 2 F x is then used to generate a detection function dk dk 1 dk t for k desired speech units with the profile of dk indicating the presence of this speech unit in the unknown signal For each k th detection function a set of lk time instances Lk t1 tlk is extracted representing the exact locations of the speech event associated with speech unit k section 2 3 2 1 Maximum likelihood segmentation applied to speech units In the following we apply the maximum likelihood speech segmentation approach described in 7 to the segmentation of speech units Given the parametrized frame based representation of a labeled speech unit Y y1 yn for example a sequence of Mel frequency cepstral coefficients MFCCs vectors where n corresponds to the length in frames of the 
speech unit we aim at finding the segment borders b2 and b3 which segment the unit into i 3 segments yb1 yb2 1 yb2 yb3 1 and yb3 yn with b1 1 The optimal segment borders are considered to be the borders minimizing the intra segment distortion of each segment As proposed in 7 we measure the intra segment distortion as the accumulation of distances from each frame inside the segment to the segment 3 bi 1 1 b2 b3 arg min b2 b3 i 1 n bi yn hal 00758424 version 1 28 Nov 2012 1 This corresponds to a shortest path problem which can be solved for each segment using dynamic programming The observation vector x for each speech unit is obtained by first concatenating the centroids of the obtained segments T Fig 2 Simplified example illustrating the extraction of a detection function dk from a given set of segments Each segment is displayed with its prediction score The maximum scores at each frame t are mapped onto dk t to train a classifier Hj x on each partition The final classification consists in averaging the 
output scores of the full committee of classifiers a segment and x s e to the observation vector extracted from this segment The minimum number of frames in a segment is i 3 The obtained detection function indicates the positions of the k th speech event in the speech signal by its local maxima Since we put most effort into the accurate modeling of the speech units we expect clear indications of the speech events without requiring sophisticated post processing The following simple peak picking algorithm can be applied to a system of discrete speech units like phonemes or broad phonetic classes BPCs Given the detection functions for k speech units we extract landmarks in three steps 1 We first collect the time instances corresponding to local maxima for each detection function dk and associate each local maximum t with its magnitude dk t Since a local maximum corresponds to a segment of at least three frames the central frame t of a segment is always selected as the time instance t of the local maximum 2 If 
several units k share the same local maximum t only the landmark with the highest magnitude dk t is kept at t 3 Some of the remaining landmarks might correspond to a local maximum at a very low magnitude Therefore at each landmark all units k with a score dk t bigger than the landmark magnitude are also activated at that time instance Afterwards each set Lk t1 tlk contains the final collection of lk discrete time instances signaling the presence of the k th speech event If a local maximum t of speech unit k already corresponds to the highest score dk t at that time frame step 3 will have no effect If not this step will merge several speech units into broader units effectively signaling the possible presence of several units at the same time 3 LANDMARK DRIVEN HMM BASED ASR We use the presented speech event detection approach to extract broad phonetic landmarks for a recently proposed landmark driven HMM based ASR framework 6 In this framework broad phonetic landmarks are used to guide the Viterbi decoding of 
the first pass of a HMM based speech recognizer which we briefly summarize in the following The Viterbi algorithm searches the best path reaching state j t by computing a score S j t max S i t 1 log aij log p y t j R j t i hal 00758424 version 1 28 Nov 2012 F x J j 1 Hj x J 2 2 3 From classification to detection As stated in the introduction of section 2 F x is used to obtain a detection function dk dk 1 dk t If F x is trained to predict single frames yt of parametrized speech dk t directly corresponds to the prediction score of class k at frame t Since our classifier provides segment and not framebased prediction scores we provide a new method to generate detection profiles from a collection of predicted segments Intuitively in order to obtain dk t one has to search for the segment containing t that maximizes the likelihood of the given speech unit at frame t Given a collection of overlapping segments with each segment corresponding to a hypothetical speech unit we can directly compare the score of variable 
length segments using our proposed observationvector A collection of segments does not correspond to a graph so that single segments might not have a connection to preceding or subsequent segments see simplified example in Figure 2 With this work being a preliminary study we do not consider the task of finding a collection of suitable prior segments which is desirable to reduce the computational costs for generating dk t Instead we use an exhaustive collection of segments by predicting the observations extracted from all possible segments inside the speech signal up to a maximum segment length of 300ms Using fast classification like boosting and an effective implementation for the solution of equation 1 this stays computationally feasible The value of the detection function dk t at t is calculated as follows dk t max Fk x s e s t e e s i 1 3 Fk x is the score of the k th detection function given the observation x s and e correspond to the first and last frame of 4 where log aij is the transition probability 
and log p yt j corresponds to the acoustic likelihood R j t enhances paths through states j that are compatible with phonetic landmarks by R j t j to triphones checking the compatibility is trivial since each state can be directly linked to one BPC j t is a binary indicator that becomes 1 if state j is compatible with the landmark at time t and 0 if there is no landmark at all Rmax limits the influence of landmarks onto the overall score and has to be determined experimentally 4 EXPERIMENTS Our experiments focus on two objectives First we are interested in evaluating whether our proposed approach can capture the acoustic information of speech units and compare the classification performance to existing approaches Second the proposed approach will be used to extract broad phonetic landmarks to guide the search of an HMM ASR system as described in section 3 4 1 Experimental setup The speech corpus used in the experiments corresponds to radio broadcast news in the French language from the ESTER2 broadcast 
transcription evaluation campaign 10 Our training set consists of over 200 hours of speech the development set of 3 hours and the test set of 4 5 hours from 4 broadcast shows radio Africa1 radio Inter radio RFI radio TVME The test set is divided into J 12 non overlapping partitions for a committee of 12 classifiers Classification experiments are run on the development set while speech recognition is carried out on the test set Speech labels are obtained by forced alignment The speech recognizer used for recognition experiments is a two pass system with the first pass generating a word graph and the second pass rescoring the previously obtained graph using more complex models We use the tool bonzaiboost1 for the training of the ensemble of weak classifiers 4 2 Phoneme classification While landmark detection is primarily used for the detection of phonetic events our proposed method is designed to learn any time variable speech unit This includes phonetically motivated units as well as phonemes We decided to do 
classification experiments on a phoneme classification task since this is a challenging task considering the fine acoustic distinctions between phonemes Our French phoneme alphabet consists of 40 phonemes including 5 non speech events HMM monophone models serve as the baseline phoneme classifier because of their time warping ability which makes them perfectly suitable for discriminating time varying speech units The models employed correspond to monophone 3state left to right HMMs with 64 diagonal covariance Gaussian components per state HMMs are trained on the full 1 developed by Christian http bonzaiboost gforge inria fr Fig 4 Proposed observation vector for the phoneme classification experiments On the left the observation vector extracted for each phoneme as proposed in this paper classifier 2 3 4 and 6 On the right the observation vector as a concatenation of subsequent speech frames at the center of each phoneme classifier 1 trained on 1 12 of training set classifier boosting concatenated frames depth 
2 boosting proposed depth 1 boosting proposed depth 2 boosting proposed depth 3 trained on full training set classifier HMM 64 gaussians committee proposed J 12 depth 2 hal 00758424 version 1 28 Nov 2012 1 2 3 4 accuracy 55 7 57 8 63 0 62 8 5 6 accuracy 65 9 67 6 Table 1 Classifiers of the phoneme classification task The classifier is described by its classification method e g boosting and observation vector e g proposed Raymond available at training set using the same speech parametrization as the boosted ensembles which are 39 dimensional MFCC vectors composed of 13 MFCC coefficients with first and second order derivations To compare the abilities of the classifiers to capture the acoustic properties of each phoneme we evaluate on a pure classification task i e all observation vectors were extracted using the known phoneme borders obtained by forced alignment either for training or predicting the associated phoneme HMM classification was performed by force aligning each model to the known phoneme borders 
and comparing the obtained likelihoods First we trained four classifiers on only one partition i e 1 12 th of the training data for comparison before creating the full committee of 12 classifiers The first classifier uses the three concatenated frames at the temporal center of each phoneme as observation vector x for training and testing see Figure 4 Training and predicting only the center of each phoneme is supposed to minimize errors due to coarticulation effects We compare this approach to our proposed method which we run using weak learners of different depth classi hal 00758424 version 1 28 Nov 2012 Fig 3 Detection profiles of the four broad phonetic units vowels glides plosives voiced and fricatives voiced for the french word aujourd hui uttered during a broadcast news show and its corresponding broad phonetic annotation On the left every frame of the detection function was predicted using classifier 1 see Table 1 On the right the proposed method was used to generate the detection score at each frame 
using classifier 3 While vowels plosive and fricative are well indicated for both classifiers the detection function for classifier 1 is more noisy especially for the vowels Glides seem to be more clearly represented on the right side which can be due to the fact that glides correspond to slow articulatory movements which can be difficult to capture by concatenated frames fier 2 3 and 4 For all experiments the number of boosting rounds was limited to 3000 Table 1 displays the classification accuracy on the development set as the percentage of correctly classified phonemes Using the proposed observation vector for boosting increases the performance by 7 3 using decision trees with depth 2 compared to training on concatenated frames While boosting on one partition of the training set does not outperform the accuracy of HMMs applying the committee of 12 boosted weak ensembles improves by 1 7 compared to HMMs 4 3 Landmark driven ASR To use our phoneme classifier for broad phonetic landmark detection we simply 
derived a collection of detection functions dj for j 7 BPCs vowels nasals glides fricatives voiced and unvoiced and plosives voiced and unvoiced by scoring every frame dj t with the score dk t of the phoneme k classifier 3 6 574k 608k 20 2 18 0 12 0 8 8 20 20 number of landmarks phoneme errors missed phonemes AVG BPC size phonemes 1 1 352k 67 7 4 6 13 Table 2 Statistics of extracted landmarks on the whole development set for three classifiers AVG BPC size corresponds to the average number of phonemes provided at each landmark that has the maximum score among all phonemes of this BPC at frame t We extracted landmarks as proposed in section 2 3 for classifiers 3 and 6 from Table 1 For classifier 1 we equally extracted landmarks but using the detection function obtained by predicting each individual frame Figure 3 compares de broadcast Inter RFI TVME Africa1 WER baseline 18 7 17 6 24 2 31 5 WER landmark driven 18 6 17 3 23 8 30 8 Table 3 Speech recognition performance driven by broad phonetic landmarks 
Landmarks are extracted using classifier 6 tection profiles obtained by predicting single frames using classifier 1 and by employing our proposed method classifier 3 Table 2 contains information about the landmark accuracy A phoneme error corresponds to a phoneme with at least one landmark that misclassifies this phoneme As one can expect directly extracting local maxima from frame based detection functions leads to more than twice as much landmarks compared to the proposed method because of the noisy detection profile see also Figure 3 resulting in many incorrect landmarks It should be noted that this is partly due to our landmark extraction algorithm which is designed to avoid any post processing like smoothing or heuristic peak picking algorithms The high average number of active phonemes at each landmark is due to many landmarks that consist of several merged BPCs For the landmark driven ASR experiments according to section 3 we used the landmarks obtained with classifier 6 First the development set was 
used to tune Rmax see equation 4 The optimal Rmax was then employed for the landmark driven decoding on the test set The results in Table 3 show an improvement for all 4 broadcast shows tested compared to the baseline which does not include landmarks The improvement in word error rate WER varies from 0 1 radio Inter to 0 7 radio Africa1 The overall WER of the test set was 23 1 compared to a 23 5 baseline and a Wilcoxon signed rank showed it to be significant at the 5 level Compared to the WER obtained by the phonetically guided decoding presented in 6 where BPCs were trained on selected frames and decoding was guided by predicting the BPC of every frame no broadcast show of the test set was degraded by the use of broad phonetic information 5 CONCLUSIONS In this work we presented a new method for the detection of variable length speech units We used segmentation to obtain a fixed dimensional observation vector for each speech unit to train a committee of boosted decision stumps for speech unit classification 
To detect speech units in an unknown signal we searched for each time frame the corresponding segment that provides the maximum classification score for the desired speech unit This approach improved the accuracy of a phoneme classification task compared to HMM phoneme classification as well as the WER of a hybrid HMM based landmark driven ASR framework compared to its HMMbased baseline With the promising results we were able to obtain by employing our proposed framework there are several directions for future research First the proposed fixed dimensional observation space could be refined by considering different segmentation and attribute extraction strategies Second the step from classification to detection by evaluating all possible segments should be replaced by an efficient search for a suitable collection of prior segments While boosting decision stumps performed well on the presented classification task applying other classification techniques might further improve speech unit classification and 
detection 6 REFERENCES 1 M Ostendorf Moving beyond the beads on a string model of speech in Proc of ASRU 1999 1999 pp hal 00758424 version 1 28 Nov 2012 
4	a	Proceedings of the Twenty Fifth International Florida Artificial Intelligence Research Society Conference Snackbot The Process to Engage in Human Robot Conversation Dekita Moon1 Paul Rybski2 Cheryl Swanier1 and Chutima Boonthum Denecke3 1 Fort Valley State University Fort Valley GA 31030 USA 2 Carnegie Mellon University Pittsburgh PA 15213 USA 3 Hampton University Hampton VA 23668 USA dekitamoon yahoo com prybski cs cmu edu swanierc fvsu edu chutima boonthum gmail com Abstract While delivering snacks Snackbot s need to actively engage in conversation with the customers and other individuals provides an approach for verbal interaction This paper addresses the verbal human robot interaction between humans and robots using a speech recognizer named Sphinx 4 Sphinx 4 written entirely in Java is capable of recognizing predetermined words and sentences Thereby allowing robots to actively engage in conversations using spoken language dialog must be manipulated by a research assistant listening in on the 
conversation between Snackbot and its customer Introduction In order to better serve customers during face to face encounters verbal communication should be utilized during any transaction Therefore the need for Snackbot see Figure 1 Lee et al 2009 to communicate with humans requires implementing a way for a robot to recognize words using a speech recognizer Speech recognition technologies allow computers equipped with microphones to interpret human speech e g for transcription or as a control method As of 2001 developers recognized the need to implement speech technology capturing continuous speech with a large vocabulary at normal pace with an accuracy of 98 Kumar 2008 There is a huge commercial market waiting for this technology to mature although there are few appliances using speech recognition as a method for communication and control Images SI Inc n d Currently Snackbot s speech is manipulated through the use of a dialog tree This dialog tree can be used to easily create and manipulate nodes and 
transitions according to the previous interactions with customers However this Figure 1 Snackbot CMU snackbot org In this work we aimed to develop a more autonomous way for Snackbot to acknowledge the responses from customers versus the research assistant taking on the task Methods and Materials Before using a speech recognizer you must be knowledgeable of what might be spoken between the customer and the robot Speech recognizers like those used on a daily basis recognize speech that is predetermined by the developer such as automated phone systems for obtaining optimal performance Likewise being familiar with how the customer and Snackbot interact with each other allows the developer to train the application to look for certain inputs spoken With 527 Snackbot initiating conversation with the customer it is fairly easy to recommend what Snackbot will speak Snackbot s dialog legend is a script in which it speaks to initiate conversation with its customers The dialog legend encompasses many subjects that are 
categorized by the topic of conversation The topics used by Snackbot include greeting weather gaining feedback from customers and the offering of a snack However Snackbot deals with many customers that are different Therefore customer s responses may vary significantly but are very dependent upon the predetermined dialog of Snackbot For example if Snackbot enters a room and asks for someone by name there are several preconceived and data supported responses that might be made Snackbot will recognize these responses and reply To become familiarized with the responses that will be received Snackbot s customers were recorded These recordings were put in writing to be analyzed and categorized relevant to Snackbot s dialog legend The categorization of the responses made by customers makes it easier to program Snackbot to look for certain phrases at specific times and respond accurately Once the sentences used by customers were categorized a training data set was made A training set is a text file listing all 
possible input spoken by customers as well as the possibilities that could have been said For example if a one customer states I want a cookie it is important to list the potential alternatives to other snacks used in this sentence e g I want a banana I want an orange etc Though tedious forming the training set is one of the most important steps in using a speech recognition tool Figure 2 Li 2005 shows a block diagram of two stage processing in speech recognition feature extraction and classifier or speech recognizer After the training set is made Sphinx 4 tools can be used to make a language model and plug in the appropriate data With these tools a training set can be used to create an N gram According to Jurafsky and Martin 2006 an N gram is a probabilistic model which predicts the next word from the previous N 1 words clicking compile knowledge base Once compiled the tool will create three documents including an N gram language model sentences and dictionary to be used In order to use these documents 
Sphinx 4 should be downloaded from sourceforge net Sphinx 4 also requires JAVA SE 6 Development Kit JDK Ant 1 6 0 and Subversion svn Links to this necessary software are provided on the official website of Sphinx 4 The JDK tool makes your code able to compile The Apache Ant is a command line tool that is used to drive processes described in build files as targets and extension points dependent upon each other The Subversion tool is recommended but is not required allowing you to interact with the svn tree An Integrated Drive Electronics IDE is also needed to build jar files used to compile and run Sphinx 4 There are several IDE tools that can be used but I choose to use Eclipse IDE for Java Developers Sphinx 4 lists several instructions on setting up your IDE adding source folders jar files and build xml ant files from Sphinx 4 to your project It is important to note Sphinx 4 also includes demos that includes its own language model or grammar class Manifest and xml files These files can be manipulated to 
cater to the needs of your project The LiveConnectDemo in particular provides an example of how to embed ECMAScript logic into JSGF grammars that will cause objects in your application to be activated when a JSGF RuleParse is processed by the edu cmu sphinx tools tags ObjectTagsParser If the demo is run and the speaker utters I want a pizza with mushroom and onions Sphinx will recognize the order and perform the action tag designated with the utterance resulting in the textual feedback of pizza with mushroom and onions It is very important to make sure all of the words in the language model are included in the acoustic model The acoustic model is a dictionary used to pronounce words within the language model using Alphabet symbols Most projects will use the cmudict 0 6d file located in the WSJ 8gau 13dCep 16k 40mel 130Hz 6800Hz jar Figure 3 Two stage processing in Speech Recognition Sphinx 4 Tool The Sphinx Knowledge Base Tool CMU n d CMU 2011 was used to convert the training set into a language model by 
simply choosing the training set text file and Figure 2 Sphinx 4 Live Demo Interface CMU 2011 528 The acoustic model used within Sphinx 4 already provides a long list of word pronunciation or phonemes that may or may not include words needed for your individual purposes CMU 2011 provided a live test for Sphinx 4 see Figure 3 using a sample test file shown in Figure 4 the left most and closest the left most purple the green one right in the middle it s the uppermost of three in a row this is a green one this is the first behind the one which is the furthest away from you a purple one on the top right corner a purple one on the lower right corner it s a green one it s in the middle right after the two green ones in a row the closest green one the green one which is in the middle of the table the purple one on the left side the green one on the top right corner the green one which is in the middle on the left side there are three purple ones the lowest of the three purple ones the next one up Discussion and 
Future Work The ideal method to test the recognition of a language model is to use the Hello N Gram or the Live Connect demo The Hello N Gram demo unlike the Hello World demo uses a language mode instead of a grammar file offering more flexibility among the grammar used The Live Connect demo is an action tag demo that shows how to use an action tag parser once a speech is recognized The Live Connect could be used to plug in code allowing objects in the application to be activated and can therefore use a spoken language system to send accurately responses to be spoken by Snackbot using the ECMA Script logic The results of the Hello Word demo using the most common words spoken to Snackbot allowed us to get a feel of how Sphinx works and what files to manipulate reaping desirable results The accuracy of the test done was dependent on the pace the volume and how clearly words were spoken The more enunciated the words were using a standard pace of speaking the more capable the application was able to recall words 
Further research would allow more changes to be made to the class file allowing the voice generator used by the Snackbot to respond orally and using more than one tester to assess the recognition of words Acknowledgement This research was supported in part by the National Science Foundation CNS 1042466 and various corporate partners Any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF or our corporate partners Figure 4 Sample N Gram Test File first 20 lines Results To obtain preliminary results changes were made to Sphinx 4 Hello World demo files 1 for the sake of time A short grammar file was successfully used to test a few frequently used words with Snackbot These words include yes no orange cookie mystery snack snickers candy and a few others Some of the words such as snickers were not already a part of the dictionary Therefore changes were made to the dictionary to include these words with the 
proper phoneme using the Hidden Markov Model With this grammar the application was ran in Eclipse see Figure 5 and was able to recall sentences that were spoken though not at a 100 accuracy At this point though progress was made Snackbot was not able to respond orally due to the focus on recognition and less on incorporating the speech mechanism files and coding References CMU 2011 Sphinx Open Source Toolkit For Speech Recognition http cmusphinx sourceforge net CMU n d Sphinx Knowledge Base Tool http www speech cs cmu edu tools lmtool html Images Scientific Instruments Incorporated n d Build a Speech Recognition Circuit Retrieved November 21 2011 from http www imagesco com articles hm2007 SpeechRecognitionTu torial01 html Jurafsky D and Martin J 2008 Speech and Language Processing Upper Saddle River NJ Pearson Prentice Hall Kumar R 2008 Human Computer Interaction New Delhi India Firewall Media Lee M K Forlizzi J Rybski P E Crabbe F Chung W Finkle J Glaser E and Kiesler S 2009 The Snackbot Documenting the 
design of a robot for long term human robot interaction In Proceedings of HRI 2009 7 14 Li X 2005 Combination and Generation of Parallel Feature Streams for Improved Speech Recognition Ph D Thesis ECE Department CMU February 2005 1 http cmusphinx sourceforge net wiki tutorialsphinx4 529 
5	a	IMPROVING SEARCHABILITY OF AUTOMATICALLY TRANSCRIBED LECTURES THROUGH DYNAMIC LANGUAGE MODELLING by Stephen Marquard supervised by Dr Audrey Mbogho DISSERTATION SUBMITTED FOR THE PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE DEGREE OF MASTER OF PHILOSOPHY IN INFORMATION TECHNOLOGY IN THE DEPARTMENT OF COMPUTER SCIENCE UNIVERSITY OF CAPE TOWN December 2012 Abstract Recording university lectures through lecture capture systems is increasingly common However a single continuous audio recording is often unhelpful for users who may wish to navigate quickly to a particular part of a lecture or locate a specific lecture within a set of recordings A transcript of the recording can enable faster navigation and searching Automatic speech recognition ASR technologies may be used to create automated transcripts to avoid the significant time and cost involved in manual transcription Low accuracy of ASR generated transcripts may however limit their usefulness In particular ASR systems optimized for general speech 
recognition may not recognize the many technical or discipline specific words occurring in university lectures To improve the usefulness of ASR transcripts for the purposes of information retrieval search and navigating within recordings the lexicon and language model used by the ASR engine may be dynamically adapted for the topic of each lecture A prototype is presented which uses the English Wikipedia as a semantically dense large language corpus to generate a custom lexicon and language model for each lecture from a small set of keywords Two strategies for extracting a topic specific subset of Wikipedia articles are investigated a i Acknowledgements I extend my thanks to My supervisor Dr Audrey Mbogho for her patience and wise counsel My wife Pippa and son Cael for their support Developers of the open source toolkits used for this project in particular Nickolay V Shmyrev CMU Sphinx4 Radim ehek gensim and Josef Novak phonetisaurus for sharing freely both their code and expertise Timothy Carr and Andrew 
Lewis from ICTS who liberated me from the constraints of desktop computing Computations were performed using facilities provided by the University of Cape Town s ICTS High Performance Computing team http hpc uct ac za Copyright and License Stephen Marquard is the author of this dissertation and holds copyright in terms of the University of Cape Town s Intellectual Property Policy July 2011 http www uct ac za downloads uct ac za about policies intellect_property pdf This work is licensed by the author under a Creative Commons Attribution 2 5 South Africa License http creativecommons org licenses by 2 5 za deed en ii Contents Abstract i Acknowledgements ii Copyright and License ii 1 1 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 Introduction 1 Lecture recording in universities 1 Integration of speech recognition into a lecture capture system 1 Speech recognition accuracy 3 Improving searchability through adapting vocabulary and language models 4 Using Wikipedia as a linguistic resource for language model adaptation 6 
Implementation and evaluation 6 Open source 7 Research questions 7 2 2 1 2 2 Background 8 The field of speech recognition 8 Core concepts in speech recognition 9 2 2 1 2 2 2 2 2 3 2 2 4 2 3 2 4 2 5 2 6 2 7 2 8 Recognition scope 9 Acoustic and language models 9 Speech corpora 12 Supervised and unsupervised model adaptation 13 Applying speech recognition systems to lectures 13 Modelling the form style and content of lectures 14 Acoustic model adaptation 15 Language model adaptation 15 Measuring the accuracy of lecture transcripts 16 Prototype implementations of speech recognition for recorded lectures 17 2 8 1 2 8 2 2 8 3 2 8 4 2 9 ePresence 18 MIT 18 Synote 19 REPLAY 20 Alternate approaches and extensions 20 2 9 1 2 9 2 2 9 3 2 9 4 2 10 2 11 Real time transcription 20 Improving transcripts with user input 20 Indexing segmentation and searching 21 Improving manually generated transcripts with ASR 22 Remaining problems and future directions 22 Summary 23 3 3 1 3 2 3 3 3 4 3 5 3 6 3 7 3 8 3 9 Methodology 24 
Introduction 24 Aspects of searchability 24 Selection and definition of metrics 25 Generic speech recognition process 26 The CMU Sphinx ASR engine 27 Reference acoustic and language models 28 Selection of lectures 29 Recognition process with reference language model 30 Calculating metrics 32 iii 4 4 1 4 2 4 3 4 4 4 5 4 6 4 7 4 8 4 9 4 10 4 11 Topic and language modelling with Wikipedia 36 Introduction 36 Wikipedia as a linguistic resource 36 Creating a plain text corpus from Wikipedia 37 Goals for the adapted language model 40 Constructing a topic adapted language model from Wikipedia 41 Recognition process with a custom language model 42 Constructing the phonetic dictionary 44 Identifying a topic related subset of Wikipedia articles 44 The Wikipedia 5 5 1 5 2 5 3 5 4 5 5 5 6 5 7 5 8 5 9 5 10 5 11 Discussion and main findings 53 Introduction 53 Limitations on accuracy 53 Baseline performance with HUB4 54 Comparing the Wikipedia crawler behaviour and output 56 Recognition performance of 6 6 1 6 2 6 3 6 4 6 5 
Improvements and further directions 75 Improving recognition of common words 75 Iterative similarity modelling 75 Improving pronunciation accuracy 76 Generalizability to other languages 77 Examining user search behaviour 77 7 7 1 7 2 7 3 7 4 Conclusions 78 Implementation and analysis 78 Increasing transcript searchability with topic adapted language models created from Wikipedia articles harvested by the Similarity Crawler 78 Assessing the effectiveness of an article similarity metric when creating topic adapted language models using a Wikipedia article crawler 79 Overall 79 References 81 Appendix 1 Software and Data Sets 87 Appendix 2 Source code 89 Appendix 3 Open Yale Courses lectures 90 Appendix 4 Sphinx Configuration 92 Glossary 96 iv Tables Table 2 1 Characteristics of some common speech recognition applications 9 Table 2 2 The Arpabet phoneset without stress markers from the CMU Pronouncing Dictionary 10 Table 2 3 Examples of Arpabet pronunciations from the CMU Pronouncing Dictionary 11 Table 2 4 
Excerpts from a Trigram Language Model trained from the HUB4 Corpus 12 Table 3 1 Selected Open Yale Courses lectures 30 Table 3 2 Recognition metrics and artefacts 32 Table 3 3 Reference and hypothesis transcripts with alignment 33 Table 3 4 Example of transcript vocabulary OOV extraneous and unrecognized words 34 Table 3 5 Calculation of Ranked Word Correct Rate 35 Table 4 1 Examples of keywords for selected lectures 43 Table 4 2 Wikipedia crawler constraints 46 Table 5 1 Overlap between HUB4 Wikipedia and Google dictionaries 54 Table 5 2 Recognition statistics with HUB4 models 55 Table 5 3 Recognition accuracy with HUB4 models WER and WCR 56 Table 5 4 Wikipedia Crawler Statistics 57 Table 5 5 Articles and word comparison between v Figures Figure 1 1 A lecture recording showing slide thumbnails as a navigation aid 1 Figure 1 2 Architecture of the Opencast Matterhorn Lecture Capture System 2 Figure 1 3 Searching within lecture transcripts in the MIT Lecture Browser system 3 Figure 1 4 Comparison of word 
frequency by rank in a lecture blue and fictional text red 5 Figure 2 1 Prototype of ASR extensions to the ePresence system 18 Figure 2 2 The MIT Lecture Browser 19 Figure 2 3 The Synote Annotation System 19 Figure 2 4 Speech recognition in REPLAY 20 Figure 3 1 Speech recognition with a statistical HMM engine 26 Figure 3 2 The Sphinx4 Framework Sphinx 4 White Paper 27 Figure 3 3 Recognition process with a reference language model 31 Figure 4 1 Lycidas Wikipedia article as shown in a web browser 37 Figure 4 2 Lycidas Wikipedia article wiki markup text 37 Figure 4 3 Conditioned sentences from the Lycidas Wikipedia article 38 Figure 4 4 Generating a plain text corpus from Wikipedia 38 Figure 4 5 Creation of a custom language model from Wikipedia 42 Figure 4 6 Recognition process with a custom language model 43 Figure 4 7 Wikipedia crawler with vi 1 Introduction 1 1 Lecture recording in universities Lecture capture technologies are gaining popularity in higher education 1 Such systems record audio video and 
presentation slides or graphics from a lecture so that the lecture can be played back later by students or the general public However a single continuous recording is often unhelpful for users As students often use lecture recordings for revision or preparation for assessments 2 they may wish to play back a part rather than the whole of a lecture or identify lectures containing particular material or concepts To support this various indexing schemes have been used to enable faster navigation and searching For example slide images are commonly used to provide a visual index within the lecture Figure 1 1 Figure 1 1 A lecture recording showing slide thumbnails as a navigation aid However a transcript of the lecture provides even more possibilities as it enables quick navigation within the lecture discovery through text search across lectures within the lecture capture system for public lectures discovery through search engines and content aggregators 1 2 Integration of speech recognition into a lecture capture 
system In many contexts producing manual transcripts from audio recordings is not economically viable as it is time consuming and expensive Using automated speech recognition ASR technologies for transcription is thus an attractive lower cost approach 1 ASR may be integrated into an automated lecture capture system in the processing phase when recorded media are ingested and processed on a central cluster prior to distribution to end users Media analysis tasks in the processing phase may include segmenting video into slides optical character recognition OCR of text in slides and speech recognition Figure 1 2 shows the architecture of an open source lecture capture framework Opencast Matterhorn 3 4 Figure 1 2 Architecture of the Opencast Matterhorn Lecture Capture System The actual task of speech recognition and generating the transcript may be undertaken by a software component internal to the lecture capture system or could be performed by an external service for example provided by a vendor as a software 
as a service SaaS offering The time aligned transcript created by the ASR engine forms part of the recording metadata and may be exposed to the end user through the playback user interface indexed within the capture system to enable searching across lectures exposed to external harvesters through RSS OAI PMH or other metadata feeds An example of text based search and navigation within a recorded lecture is shown in Figure 1 3 from the MIT Lecture Browser prototype 5 6 2 Figure 1 3 Searching within lecture transcripts in the MIT Lecture Browser system 1 3 Speech recognition accuracy ASR systems are imperfect and may introduce many errors into a transcription Key factors affecting accuracy include 1 the audio quality of the recording influenced by the type of microphone used venue acoustics and amount of background noise 2 whether the recognition system has been trained for a particular speaker or is using a speaker independent acoustic model 3 for speaker independent systems the match between the speaker s 
accent and the acoustic model 4 the match between the vocabulary and pronunciation in the lecture with the language model and pronunciation dictionary In an automated lecture capture system in a university wide variations in all of the above factors are likely lectures take place in a range of venues with different equipment and acoustics many different lecturers are involved typically from diverse backgrounds and thus having a wide range of accents and lectures span a range of disciplines and topics One can therefore expect a correspondingly wide range in the transcription accuracy produced by a large vocabulary speaker independent continuous speech recognition system in this context 3 Many transcripts are thus likely to fall short of the accuracy threshold for readability 7 These transcripts are therefore unusable as a substitute for the recording itself i e for a student to read as an alternative to playing back the recording but may still be useful for search and navigation This project focuses on the 
application of ASR technologies for generating lecture transcripts for the primary purpose of information retrieval i e identifying lectures in which search terms occur and identifying the points within a lecture where search terms occur The most important optimizations of an ASR system in this context are therefore those which improve the usefulness of the system for a user searching by keyword or phrase the searchability of the transcript understood as the extent to which the transcript facilitates discovery and navigation For search purposes not all words in a transcript are of equal value For example a transcript which is accurate with respect to the terms most frequently used in searches may be more useful than a transcript with a higher overall accuracy but lower accuracy for the designated terms A more nuanced approach to accuracy is therefore required when the goal is to optimize searchability 1 4 Improving searchability through adapting vocabulary and language models This project focuses on the 
fourth factor affecting accuracy identified above i e the linguistic match between the lecture and the ASR system s language resources While a typical one hour lecture contains relatively few unique words in the order of 1000 it is likely to include an abundance of unusual words reflecting the specialist vocabulary of the discipline This may be seen in examining the distribution of words used in a lecture by word frequency rank i e the position of the word in a list of English words ranked in descending order of their frequency of use in general English Figure 1 4 shows a comparison of word frequency by dictionary rank for a lecture on a specialist topic Chemical Pathology of the Liver 8 compared to a fictional text Alice s Adventures in Wonderland 9 While frequency rank plots of texts in general exhibit a Zipfian distribution an inverse power law the lecture text in this example contains many more outliers than the fictional text These are shown circled indicating words with document frequency greater than 
3 and dictionary rank from approximately 25 000 to 1 000 000 For example in the Chemical Pathology lecture transaminases occurs 16 times with a word frequency ranking of 143 006 4 Figure 1 4 Comparison of word frequency by rank in a lecture blue and fictional text red These outlier words are disproportionately important as they are likely to be topic words reflecting the content of the lecture For search purposes it is therefore important that ASR engines correctly recognize the unusual words as they are likely to be strongly represented in keyword searches Furthermore for most ASR engines vocabulary represents a hard constraint while other factors such as audio noise or accent mismatch may be present to a greater or lesser degree and influence the accuracy accordingly if a word is not contained in the dictionary and language model it will never be recognized While intuitively it may seem desirable to use a very large dictionary for speech recognition the sample lecture above would require a dictionary of 
more than a million words to encompass more than 99 of the unique words used 10 This would in turn require a correspondingly large statistical language model Unfortunately large generic language models present significant performance and accuracy challenges for the current generation of ASR systems The larger the model the greater the search space which slows down recognition and degrades accuracy given that there are many more hypotheses for each word More importantly such models lose the essential advantage of context for example that a lecture on the Chemical Pathology of the Liver is unlikely to include a discussion of existential philosophy A desirable goal therefore is a language model and dictionary specific to the topic of the lecture highly attuned to the context allowing the language model to be small enough 5 to achieve good performance and accuracy while being optimized for the recognition of terms in the lecture most likely to be used for search and navigation This project therefore investigates 
the unsupervised adaptation of language models to the topic of a lecture with the assumptions that lectures may cover a wide range of topics and disciplines in the context of a largely automated enterprise lecture capture system little would be known about the content of the lecture in advance other than the course name and lecture title 1 5 Using Wikipedia as a linguistic resource for language model adaptation Text corpora used for language modelling are often curated from within a specific genre for example the HUB4 corpus derived from broadcast news 11 By contrast the loosely curated English Wikipedia is an attractive linguistic resource for this application because it is extremely large containing millions of articles constantly evolving wideranging in content and semantically dense through an abundance of inter article links A subset of Wikipedia articles is identified which relate to the topic of the lecture The text from those articles is then used as language corpus to create a topic specific 
language model As the topic of a lecture is not a well defined concept nor is a definitive mapping possible from topic to vocabulary two fuzzy measures are adopted Firstly a small set of keywords is identified from the course title and lecture title and used as search terms for the Wikipedia search service to locate a set of seed articles Next two alternate methods are explored to identify a set of articles to harvest starting from the seed articles The first method uses a 1 6 Implementation and evaluation A prototype Wikipedia article crawler is implemented to harvest text from a set of Wikipedia articles using the strategies described above The CMU Sphinx4 ASR engine is then used to generate three ASR transcripts for each of thirteen recorded lectures from Open Yale Courses using the following language models the open source HUB4 language model as a reference a language model adapted for each lecture using Wikipedia articles harvested using the 6 Three standard 1 7 Open source A secondary goal of the 
project is to demonstrate how an ASR system with dynamic topic adaptation could be incorporated into an open source lecture capture framework Therefore software toolkits language resources and data sets have been selected which have appropriate open source or open content licenses 1 8 Research questions The main research question is How can English Wikipedia be used as a language corpus for the unsupervised topic adaptation of language models to improve the searchability of lecture transcripts generated by an automatic speech recognition engine Sub questions are To what extent do topic adapted language models created from Wikipedia produce more searchable transcripts than those created using a generic reference language model To what extent do topic adapted language models created from Wikipedia using a crawler strategy bound by an article similarity metric produce more searchable transcripts than those created from Wikipedia using a 7 2 Background 2 1 The field of speech recognition Automatic speech 
recognition ASR is a broad field encompassing technologies used for multiple applications and problem domains Rabiner and Juang s detailed account of the fundamentals of speech recognition 12 dates the first research in the field to the early 1950s when researchers at Bell Labs built a system to recognize single digits spoken by a single speaker Since then the field has drawn on the disciplines of signal processing acoustics pattern recognition communication and information theory linguistics physiology computer science and psychology Three approaches to speech recognition have been explored The acoustic phonetic approach aimed to identify features of speech such as vowels directly through their acoustic properties and from there build up words based on their constituent phonetic elements The statistical pattern recognition approach measures features of the acoustic signal and compares these to existing patterns established from a range of reference sources to produce similarity scores which may be used to 
establish the best match Artificial intelligence AI approaches have been used to integrate different types of knowledge sources such as acoustic lexical syntactic semantic and pragmatic knowledge to influence the output from a pattern recognition system to select the most likely match Of these approaches the statistical pattern recognition approach produced significantly better accuracy than the acoustic phonetic approach and is now the dominant paradigm for speech recognition augmented by various AI approaches A key element in pattern recognition is the use of Hidden Markov Models HMMs which enables recognizers to use a statistical model of a pattern rather than a fixed template ASR systems are known to perform best on audio recorded using a close talking microphone in a noise free environment transmitted through a clear channel and recorded with a high sampling frequency 16 KHz or greater However as these conditions are seldom available in real life a range of strategies have been investigated to 
compensate for the effects of noise reverberation and variation in conditions between the reference recordings used for training recognizers and actual recordings While acoustic issues are not explored in depth here they remain a significant constraint on recognition performance 13 8 2 2 Core concepts in speech recognition 2 2 1 Recognition scope Speech recognition applications can be broadly characterised in three ways speaker dependent or independent small or large vocabulary and isolated or connected recognition Speaker dependent systems are designed to recognize speech from one person and typically involve a training exercise where the speaker records sample sentences to enable the recognizer to adapt to the speaker s voice Speaker independent systems are designed to recognize speech from a wide range of people without prior interaction between the speakers and the recognition system Small vocabulary systems are those where only a small set of words is required to be recognized for example fewer than 100 
and permissible word sequences may be constrained through a prescriptive grammar Large vocabulary systems are those designed to recognize the wide range of words encountered in natural speech for example up to 60 000 words Finally isolated recognition systems are intended to recognize a discrete word or phrase typically as an action prompt in an interaction between person and system whereas connected recognition systems are intended to recognize continuous words and sentences following each other without interruption Three possible applications and their characteristics are shown in Table 2 1 Application Dictation Command and control system Lecture transcripts Speaker Dependent Independent Independent Vocabulary Large Small Large Duration Connected Isolated Connected Table 2 1 Characteristics of some common speech recognition applications The subfield relevant to the creation of automatic transcripts from lecture speech is thus characterised as speaker independent SI large vocabulary connected or continuous 
speech recognition LVCSR 2 2 2 Acoustic and language models Recognition systems following the dominant statistical pattern recognition paradigm make use of four related resources for a given language and speaker population 1 2 3 4 A set of phonemes A phonetic dictionary An acoustic model A language model 9 A phoneme is a unit of sound making up an utterance The most general representation of phonemes is that provided by the International Phonetic Alphabet IPA which includes orthography for phonemes found in all oral languages 14 However for speech recognition applications ASCII representations of phonemes are more practical A widely used ASCII set is the Arpabet Table 2 2 created by the Advanced Research Projects Agency ARPA to represent sounds in General American English 15 The Arpabet comprises 39 phonemes each represented by one or two letters with optional stress markers represented by 0 1 or 2 Arpabet symbol AA AE AH AO AW AY B CH D DH EH ER EY F G HH IH IY JH K Sound type vowel vowel vowel vowel vowel 
vowel stop affricate stop fricative vowel vowel vowel fricative stop aspirate vowel vowel affricate stop Arpabet symbol L M N NG OW OY P R S SH T TH UH UW V W Y Z ZH Sound type Liquid Nasal Nasal Nasal Vowel vowel stop liquid fricative fricative stop fricative vowel vowel fricative semivowel semivowel fricative fricative Table 2 2 The Arpabet phoneset without stress markers from the CMU Pronouncing Dictionary The relationship between words and phonemes is captured in a phonetic dictionary or pronouncing dictionary which maps each word to one or more sets of phonemes Table 2 3 shows examples with stress markers for aardvark tomato and Zurich including an alternate pronunciation for tomato from the CMU Pronouncing Dictionary 0 7a 16 10 Word AARDVARK TOMATO TOMATO 1 ZURICH Arpabet Pronunciation AA1 R D V AA2 R K T AH0 M EY1 T OW2 T AH0 M AA1 T OW2 Z UH1 R IH0 K Table 2 3 Examples of Arpabet pronunciations from the CMU Pronouncing Dictionary Pronouncing dictionaries are used both for speech to text applications 
speech recognition and text to speech applications speech synthesis An acoustic model associates features from the sound signal with phonemes As the pronunciation of an individual phoneme is affected by co articulation effects how sounds are pronounced differently when voiced together many systems model phoneme triples i e a phoneme in context of the phonemes preceding and following it As the exact pronunciation and sound of a phoneme may vary widely even from a single speaker acoustic models reflect probabilities that a set of acoustic features may represent a particular phoneme or set of phonemes Acoustic models are trained from a speech corpus consisting of audio recordings matched with a transcription The transcription typically contains time alignment information to the word or phoneme level Speaker independent models are trained with audio from a wide range of speakers for example with a mix of male and female speakers and regional accents Speaker dependent models may be trained from a single speaker 
or more commonly created by adapting a speaker independent model to a given speaker However acoustic models alone are insufficient to achieve acceptable levels of accuracy as can be illustrated by the challenges of disambiguating between homonyms and similar sounding phrases such as wreck a nice beach and recognize speech Linguistic context is thus an additional and indispensable resource in generating plausible recognition hypotheses The dominant approach to language modelling is the n gram language model LM Such language models are trained from a text corpus and give the probability that a given word will appear in a text following the n 1 preceding words Smoothing techniques are often applied to the initial model to adjust the probabilities to compensate for the fact that less frequent words which have not been seen in the training text may also occur For example Table 2 4 shows the probabilities for words which might follow your economic in a trigram 3 word language model in ARPA format 11 Log10 
Probability 2 0429 1 2870 2 0429 1 7585 1 7585 1 1613 2 0429 1 5947 2 0429 2 0429 1 3695 Trigram YOUR ECONOMIC ADVISERS YOUR ECONOMIC FUTURE YOUR ECONOMIC GROWTH YOUR ECONOMIC POLICIES YOUR ECONOMIC POLICY YOUR ECONOMIC PROGRAM YOUR ECONOMIC PROGRAMS YOUR ECONOMIC PROPOSALS YOUR ECONOMIC REFORM YOUR ECONOMIC REFORMS YOUR ECONOMIC TEAM Table 2 4 Excerpts from a Trigram Language Model trained from the HUB4 Corpus In this example where the recognizer is assessing which hypothesis is most likely for a word following your economic the language model would favour program rather than programs and team over the homonym teem However the model would give no advantage to the recognizer in distinguishing between singular and plural forms of reform and policy as they are equally likely in the model Language models are used in a range of natural language processing applications including spell checkers to suggest the most likely correction for a misspelt word and machine translation systems translating words phrases and 
sentences from one language to another 2 2 3 Speech corpora Training acoustic and language models require appropriate corpora Notable corpora used in speech recognition research have included The TIMIT corpus of American English speech 1986 which consists of a set of sentences each read by a range of different speakers 15 The Wall Street Journal WSJ corpus 1992 derived largely from text from the Wall Street Journal newspaper from 1987 1989 read aloud by a number of speakers 17 The HUB4 English Broadcast News Speech corpus 1996 7 generated from transcriptions of news programmes broadcast in the United States on CNN CSPAN and NPR 11 18 The Translanguage English Database TED corpus 2002 created from lectures given by a range of speakers at the Eurospeech 93 conference 19 The above examples have each been carefully curated to serve research purposes and are derived from specific genres or application domains Models trained from such corpora may be less effective when applied to different contexts For example 
acoustic models trained by American English speakers may be less effective for recognizing 12 speech from other parts of the world and language models trained on broadcast news may be less effective when applied to a different genre such as poetry 2 2 4 Supervised and unsupervised model adaptation To improve the alignment between acoustic and or language models and the speaker and genre of text being recognized it can be more effective to adapt an existing model with a limited amount of new training data rather than create an entirely new model This is especially the case if the volume of new training data is insufficient to create a robust model from scratch Supervised adaptation refers to the process of adapting models with some manual intervention based on prior knowledge of the target speaker and domain Examples include adapting an acoustic model with transcribed sentences from the speaker or adapting a language model with material from a textbook related to the topic being spoken about Supervised 
adaptation may produce good results but limit the generality of the approach Unsupervised adaptation is when the recognition system adapts the acoustic and or language models in response to the input data provided for the recognition task Such adaptation is often performed iteratively with output data from a first pass recognition attempt used to modify the models used for subsequent recognition passes Further examples are given in 2 5 and 2 6 2 3 Applying speech recognition systems to lectures Over the last decade a number of research groups and projects have undertaken systematic work in applying speech recognition to lectures progressively investigating multiple techniques and approaches Major programmes include work by the Spoken Language Systems Group in the Computer Science and Artificial Intelligence Laboratory CSAIL at MIT 5 work by Cosmin Munteanu and colleagues in the Computer Science Department at the University of Toronto 20 the Science and Technology Agency Priority Program in Japan Spontaneous 
Speech Corpus and Processing Technology supporting work particularly at Kyoto University and the Tokyo Institute of Technology 21 the Liberated Learning Project 22 the Net4voice project under the EU Lifelong Learning Programme 23 the Computers In the Human Interaction Loop CHIL project under the EU FP6 24 The starting point of speech recognition research for lectures is usually recognition systems developed for earlier applications These include broadcast news and meeting transcription systems or speaker dependent systems such as those used for dictation Speaker independent systems are typically trained with widely available speech and language corpora such as those described in 2 2 3 13 As initial results in applying the recognition systems and accompanying acoustic and language models to lecture speech usually produced poor results characterised by high error rates much of the related research effort has focused on improving the effectiveness of speech recognition for lectures through different types of 
generalization and specialization of earlier systems and approaches Generalization approaches have examined ways of accounting for the larger vocabulary including specialized terms and greater variation in delivery style characteristic of spoken lectures Specialization approaches have looked at features specific to many lectures such as the use of presentation slides and using these attributes to know more about the content of the lecture and thus improve recognition accuracy and usefulness A further class of research starts by accepting the imperfect nature of automatically generated transcripts and examines how to involve users in improving transcript accuracy and where possible use correction feedback to further improve subsequent automated recognition tasks 2 4 Modelling the form style and content of lectures The form and linguistic style of lectures present both challenges and opportunities for ASR systems For example Yamazaki et al note the high level of spontaneity in lectures which are characterized 
by strong coarticulation effects non grammatical constructions hesitations repetitions and filled pauses 25 Glass et al note a colloquial style dramatically different to that in textbooks characterized by poor planning at the sentence level and higher structural levels 26 Lectures additionally exhibit a high number of content specific words which may not occur in a general speech corpus Spoken and written forms of language may diverge differently in different languages for example Akita and Kawahara note significant linguistic differences between spoken and written Japanese 27 These variations have presented recognition difficulties and a range of strategies have been explored to compensate Structural features in the genre have been observed and exploited to improve recognition performance Such features include rhetorical markers the use of presentation slides a close correspondence between speech and slides or textbook and affinity between the content of related lectures and between lectures and associated 
material available on the web Most models of the lecture for ASR systems assume a single speaker engaged in a monologue in a single language accounting for students or the audience only so far as they constitute a potential source of noise Birnholtz notes a lack of systematic study of face to face behavior in the research related to webcasting systems focusing particularly on audience interactivity and how turn taking changes in floor control is dynamically negotiated 28 14 Although a sub field of speech recognition known as speaker diarization is devoted to identifying multiple speakers in audio typically in the context of meetings or conferences 29 the potential requirement for ASR systems to transcribe not only the speech of the lecturer but also that of people asking questions or interjecting in a lecture is largely unexplored 2 5 Acoustic model adaptation Acoustic models derived from the broadcast and news genres may be a poor fit for lecture recordings and thus a class of research has focused on how to 
adapt acoustic models to more accurately reflect the characteristics of lecture speech Adaptation strategies which have shown some success include accounting for nonlinguistic speech phenomena filler sounds 30 dynamically adjusting the model to account for speaking rate 31 unsupervised adaptation to account for new speakers 32 and using discriminatively trained models for language identification and multilingual speech recognition 33 2 6 Language model adaptation Researchers have investigated strategies for generating and adapting the language model LM to improve recognition accuracy for lectures on the assumption that a model which closely reflects the context of the utterances is likely to outperform a more generic language model Adaptations have been investigated for three levels of context at the macro level for all lectures treating spoken lectures as a genre with distinct characteristics at the meso level for a single lecture taking advantage of prior knowledge about the lecture topic or speaker at the 
micro level for a part of a lecture using knowledge about segments or transitions within a lecture Many adaptation strategies make use of some prior knowledge or parallel media This could include information about the topic or knowledge domain of the lecture a textbook or instructional materials related to the course or the lecture presentation slides Use of such information may provide specific improvements at the expense of the generality of the technique for example not all lectures may be accompanied by slides Kato et al investigated the use of a topic independent LM created by creating a large corpus of text from lecture transcripts and panel discussions with topic specific keywords removed 34 The model is then adapted to specific lectures by using the preprint paper of the lecture to be delivered when available Willett et al propose two iterative methods of unsupervised adaptation 35 36 Both methods show improvements in accuracy up to the second iteration of application A first method identifies texts 
from a large corpus which are considered close to the first pass recognition text by using a Term 15 the document term frequency but adjusted to avoid words which are common across all documents such as a and the from dominating the score A second method uses a minimum discriminant estimation MDE algorithm to adapt the LM following the thesis that seeing a word uttered at some place within the speech increases the likelihood of an additional appearance MDE is a technique for adapting a language model to more closely match the distribution of words seen in the recognized text while minimizing the variation from original to adapted model using a measure of distortion or discrimination information known as the Kullback Leibler distance 37 Nanjo and Kawahara report similar work and further explore adaptations to the lexicon and LM to account for variant pronunciations 38 The use of lecture slides for adapting the LM has been explored by several research groups Yamazaki et al note that a a strong correlation can 
be observed between slides and speech and explore first adapting the LM with all text found in the slides then dynamically adapting the LM for the speech corresponding to a particular slide 25 Munteanu et al pursue an unsupervised approach using keywords found in slides as query terms for a web search The documents found in the search are then used to adapt the LM 39 Kawahara et al investigate three approaches to adapting the LM viz global topic adaptation using Probabilistic Latent Semantic Analysis PLSA adaptation with web text derived from keyword queries and dynamic local slide by slide adaptation using a contextual cache model They conclude that the PLSA and cache models are robust and effective and give better accuracy than web text collection because of a better orientation to topic words 40 Latent Semantic Analysis is an approach to document comparison and retrieval which relies on a numeric analysis of word frequency and proximity Akita and Kawahara propose a statistical transformation model for 
adapting a pronunciation model and LM from a text corpus primarily reflecting written language to one more suited for recognizing spoken language 27 While n gram language models are the dominant paradigm in ASR systems they offer a relatively coarse model of language context Newer research is exploring more accurate statistical representations of deep context for example accounting for connections between related but widely separated words and phrases 41 2 7 Measuring the accuracy of lecture transcripts The most widely used accuracy metric for recognition tasks is the Word Error Rate WER computed as the Levenshtein distance edit distance between the recognized text and a reference transcript This is the number of insertions deletions and substitutions required for the hypothesis to match the reference transcript as a proportion of the number of words in the reference transcript A related measure is the Word Correct Rate WCR which ignores insertion errors 16 WER is often used as a measure of readability and 
thus comprehension task performance Munteanu investigated the usefulness of transcripts with a range of different error rates showing that transcripts with a WER of 25 were perceived to be as useful as manually generated transcripts When examining user scores on a quiz testing information recall after viewing a video with transcript a linear relationship emerged between WER and quiz performance At a lower bound of 45 quiz performance was worse than having no transcript at all However the study reports that users perception of transcript quality is subjective coarse grained and taskdependent 20 While its widespread use makes WER a useful measure to compare competing approaches it may often not account for the actual impact of errors for the application at hand For example some errors may be more trivial than others and easily overlooked while keyword accuracy may be disproportionately significant Bourlard et al have taken issue with WER s dominance in the field arguing that reliance on reporting WER may in 
fact be counter productive undermining the development of innovative new approaches and deviant research paradigms 42 McCowan et al point out that WER characterises recognition performance as a string editing task whereas for many applications speech recognition is better understood as supporting information retrieval tasks 43 Cited weaknesses of WER include that it is not a proper rate as it can range below 0 and above 1 is not easily interpretable and cannot be decomposed in a modular way Park et al examine automatic transcripts from the perspective of information retrieval IR investigating the effects of different recognition adaptations on WER and the IR measures precision and recall which relate to matches of keyword query strings in the recognized text 44 Results show that good retrieval performance is possible even with high error rates and conversely that adapting the language model with spontaneous speech data improves accuracy but is of marginal value to information retrieval tasks Wang et al argue 
against WER reporting results where an alternate language model produced higher word error rates but better performance with an alternate task oriented metric slot understanding error 45 McCowan et al propose four qualities for an improved metric that it should be a direct measure of ASR performance calculated in an objective automated manner clearly interpretable in relation to application performance and usability and modular to allow application dependent analysis 43 2 8 Prototype implementations of speech recognition for recorded lectures A number of prototype applications have integrated ASR systems with lecture recording and playback systems enabling the end user to interact with the transcript text These use the time alignment information generated by the ASR process to synchronize the transcript with the video playback Words or phrases in the transcript text act as index markers into the recording allowing the user to click on a point in the transcript to play 17 back the audio and or video from the 
corresponding point A further common feature is text search which highlights matching points in the transcript or on a timeline 2 8 1 ePresence At the University of Toronto Cosmin Munteanu and colleagues extended the ePresence system 46 with transcripts generated by the SONIC ASR toolkit 47 48 The project is comprehensively described in Munteanu s PhD thesis 20 as well as in a number of separate papers exploring accuracy rates 7 web based language modelling 39 collaborative editing for improving transcripts 49 and the application of transformation based rules for improving accuracy with minimal training data 50 The integration of the transcript in the user interface is shown in Figure 2 1 51 Figure 2 1 Prototype of ASR extensions to the ePresence system 2 8 2 MIT The Spoken Language Systems Group in the MIT Computer Science and Artificial Intelligence Laboratory CSAIL has investigated multiple dimensions of speech recognition in a long running research programme The group s SUMMIT recognizer 52 53 has been 
applied to lecture recordings to produce the Lecture Browser shown in Figure 2 2 54 Publications from the group have reported inter alia on experimental results with lecture recordings 55 and explored the linguistic characteristics of lectures 26 vocabulary selection and language modelling for information retrieval 44 pattern discovery in lectures 56 approaches to error correction 57 pronunciation learning from continuous speech 58 and approaches to crowdsourcing the generation of language models and transcripts 59 60 18 Figure 2 2 The MIT Lecture Browser 2 8 3 Synote Wald and colleagues at the University of Southampton and the Liberated Learning Consortium have focused on using automated transcripts to make lectures more accessible for deaf and hard of hearing students 61 The Synote application uses a recognizer developed by IBM and the LL Consortium ViaScribe presenting a web user interface which also allows bookmarks and annotations shown in Figure 2 3 55 The project has focused on real time display in 
teaching venues while noting that as ASR engines may be configured to optimize for accuracy over speed the same lecture could be re processed afterwards to create a more accurate transcript for online use Figure 2 3 The Synote Annotation System 19 2 8 4 REPLAY REPLAY is an open source lecture capture system developed at ETH Figure 2 4 Speech recognition in REPLAY The project examined software implementation strategies appropriate metadata formats for storing timestamped transcripts and audio and word based optimization strategies for improving accuracy 2 9 Alternate approaches and extensions 2 9 1 Real time transcription A decade of research in the Liberated Learning Project has highlighted the value of realtime transcription and captioning using commercially available recognition engines 22 61 65 66 Whereas many systems with integrated transcripts are designed for post lecture viewing and recall real time systems support the participation of deaf or hard of hearing students during the lecture itself 2 9 2 
Improving transcripts with user input Despite the many incremental improvements in recognition performance described above ASR systems are still not regarded as being robust or accurate enough to always produce usable and useful transcripts Researchers have therefore turned to user input as a strategy to close the error gap which is in the range of 10 25 Munteanu first explored whether students would be prepared to invest effort in correcting transcripts online finding positive results with a range of incentives in a field trial using a wiki like online editor 84 of all transcript lines were edited by users 20 20 2 9 3 Indexing segmentation and searching Many ASR systems aim to generate a complete transcript of a lecture and have the goal of optimizing the accuracy of the transcript However a transcript is often just the means to an end A range of alternate approaches have been proposed which pursue the goals of indexing keyword extraction segmentation or search directly without requiring an accurate or 
complete transcript Yamamoto et al observe that a lecture may consist of several topics closely aligned with a textbook and demonstrate a method of topic segmentation by comparing topic vectors from speech and textbook constructed using nouns weighted using a TF IDF measure Lin et al investigate segmentation using multiple linguistic features 67 Five contentbased features and two discourse based features are used to create feature vectors which are used to compare the similarity of adjacent sections of text Kawahara et al approach segmentation using presumed discourse markers expressions that are characteristic to the beginning of new sections in lectures and oral presentations describing an unsupervised training approach for the markers 68 Seide et al describe a non textual approach to search Rather than matching search text against a transcription the recognizer generates lattices of phonetic word fragments against which keywords are matched phonetically 69 The vocabulary and domainindependent approach is 
shown to be as accurate as vocabulary domain dependent systems and has the advantage of maintaining this accuracy for out of vocabulary OOV words Ngo et al describe a segmentation approach for video where the camera field of view captures both the speaker and projected slides 70 The video is segmented by identifying the slide transitions recognizing text on projected slides and extracting phrases and keywords to constrain speech recognition to identify only the sought after words for the purpose of aligning topics with video segments Repp and Meinel describe techniques for semantic indexing of lectures 71 A generated thesaurus is used to associate common phrases with pedagogically meaningful meta phrases for which the authors suggest example explanation overview repetition and exercise Identified meta phrases in the recognized text are then used as index keys to the video allowing students to navigate according to their learning objective Repp et al further explore another indexing technique using word 
repetitions as an indicator of thematic cohesion 72 Chain indexes are created for an audio search tool which presents results as matching segments of the video timeline Park and Glass apply approaches suggested by developmental psychology and comparative genomics to identify recurring speech patterns directly in the acoustic signal 56 The patterns are grouped into clusters which correspond to lexical entities such as words and short phrases The identified clusters are shown to have high relevance to the content of the lecture 21 Liu et al explore unsupervised approaches to keyword extraction from transcripts 73 TF IDF weighting part of speech word clustering and sentence salience scores are shown to be of value using a variety of evaluation methods Many of the surveyed techniques are designed to exploit particular features of the media for example linguistic attributes of a lecture and thus attain improved results at the expense of the generality of the technique On the other hand techniques which appear 
more generalizable introduce other types of constraints For example audio search algorithms require a specialized search service which could limit the visibility of published media to text based search engines on the open web Thus while alternate approaches for indexing segmentation and searching appear promising they present partial solutions which do not yet provide the full range of affordances of a complete text transcript 2 9 4 Improving manually generated transcripts with ASR Hazen outlines an inverted approach where it may be feasible to obtain imperfect human generated transcriptions quickly or cheaply 74 Speech recognition technologies may then be used for automatic alignment of the text with the speech to discover and automatically correct transcription errors The results show an improvement in word error rate for the adjusted transcript over that produced by a human transcriber but also show that most of the corrections represent re insertion of omitted words which are mostly not of significance 
for comprehension of the text 2 10 Remaining problems and future directions With respect to speech recognition in general Baker et al suggest six grand challenges for the field 13 everyday audio greater robustness in adverse conditions rapid portability to emerging languages self adaptive language capabilities detection of rare key events cognition derived speech and language systems spoken language comprehension With respect to speech recognition of lectures Munteanu suggests inter alia investigating topic specific language modelling approaches and refining ASR transcriptions for lectures and presentations that do not use slides or make use of other visual aids developing a user motivated measure of transcript quality maximizing the trade off between user editing and ASR improvements and exploring other collaborative approaches to webcast usability improvement 20 22 Tibaldi et al present a methodology focused on exploiting and assessing the impact of Speech Recognition technology in learning and teaching 
processes an under examined but central topic if the full benefits of ASR systems are to be realized in educational contexts 75 2 11 Summary A number of prototype systems have shown that ASR systems can be integrated with lecture capture systems to produce useful results However the central issue in the application of speech recognition to lectures is recognition performance Much research has focused on demonstrating small incremental improvements to accuracy rates through innovation in algorithms or smarter adaptations to the acoustic or language models Many posited improvements take advantage of particular features of lectures specific to the language domain content supporting media or style of presentation Most experimental results are reported on in relation to a narrow corpus in controlled conditions The most widely used measure for accuracy Word Error Rate is not regarded as optimal for information retrieval tasks and application specific alternate metrics are seen as more helpful in evaluating the 
success or failure of adaptations to language models Promising work has been done on examining user needs and behaviour more closely Productive directions include harnessing human intelligence to close the recognition gap and identifying the best ways to use imperfect recognition results effectively rather than seeking completely faithful transcription 23 3 Methodology 3 1 Introduction Chapters 3 and 4 describe the methodology used to investigate and evaluate the research questions posed in Chapter 1 An applied experimental research design is used This follows common practice in the field and enables the impact of different language models on accuracy and searchability to be assessed across a set of real world test cases In this chapter the concept of searchability is characterised leading to the identification of related metrics A generic speech recognition process is set out followed by details of the CMU Sphinx speech recognition engine and the selected reference acoustic and language models A set of 
recorded lectures is identified for experimentation and the speech recognition process used with reference and custom language models is shown Finally the process of calculating the evaluation metrics is set out 3 2 Aspects of searchability Whether a lecture transcript is more or less searchable has multiple dimensions Consider three scenarios A member of the public would like to know more about the work of John Milton and so does a Google Search on his name A student enrolled in an English Literature course has been set an essay on a particular work by Milton and would like to find the lecture about Milton which she recalls attending She searches for Milton on the university s video portal A student is playing back the recording of a 45 minute lecture but doesn t have much time and would like to skip to the parts where Milton s friend Charles Diodati is mentioned In these examples the search scope ranges from the entire Internet to a single recording In the first two examples the objective is 
discoverability the lecture should appear in the set of search results In the last example the objective is better navigation within the media Searchability here thus encompasses discoverability does the transcript facilitate the user finding the lecture and usefulness does the transcript provide fine grained indexing Many factors could affect the outcome of the user s search including the search terms chosen by the user and the indexing ranking and search algorithms of the search engine which may be opaque proprietary and evolve frequently The best case for maximising a lecture s searchability given that user and search engine behaviour are both unknown to a degree is therefore a completely accurate transcript 24 However given two imperfect transcripts of the same lecture as will be the case for ASR generated transcripts in the foreseeable future which is more likely to be searchable For the purposes of this investigation three assumptions are made Users develop expertise in online searching and form a 
mental model which leads them to prefer search terms with greater discriminatory power to avoid being swamped with irrelevant search results Thus users search for keywords specific to the content being sought An alternate strategy could be searching for a distinctive sequence of words such as a short quotation Thus words which occur in the document but are less frequent in general English or the set of documents within the search scope are disproportionately important Thus in a transcript Milton is a more valuable word for the purposes of maximising discoverability than here The introduction of extraneous words into a transcript may be harmful for the quality of search results overall but is not significant in considering the searchability of an individual document Thus false negatives words incorrectly not matched are more important than false positives words incorrectly matched 3 3 Selection and definition of metrics Three primary metrics are used to assess the likely searchability of a lecture transcript 
Each metric is derived from a word by word comparison of a reference transcript to an imperfect hypothesis transcript Evaluation is thus automated and quantitative and does not take into account human factors or the influence of algorithms in the selection and application of search terms Two metrics used which are common in speech recognition research are Word Error Rate WER WER is an accuracy metric calculated from the edit distance between two documents the number of word insertions deletions and substitutions required to transform the reference to hypothesis as a proportion of the word count in the reference Word Correct Rate WCR WCR is the number of words correctly recognized as a proportion of total word count in the reference WCR thus ignores the effect of insertions To better characterise searchability in terms of keyword recognition a new metric is introduced Ranked Word Correct Rate RWCR RWCR calculates the total recognition rate of those words in the transcript which occur below a given frequency 
rank in general English Thus the recognition accuracy of unusual words e g Comus affects the recognition score while the recognition accuracy of common words e g a the and is ignored 25 The method of calculation and examples of the above metrics are shown in 3 9 Secondary metrics which give insight into aspects of the recognition process are Vocabulary coverage expressed by the number of out of vocabulary words i e words found in the transcript which are not included in the recognition dictionary and language model Vocabulary recognition expressed by the number of unrecognized words words in the transcript which are in the recognition dictionary and language model but do not occur in the hypothesis and extraneous words words which do not occur in the reference but were incorrectly introduced to the hypothesis The perplexity of the language model evaluated against a reference text which is an information theory measure expressing the extent of the uncertainty which the recognizer might face in selecting word 
hypotheses 3 4 Generic speech recognition process Figure 3 1 illustrates a generic speech recognition process using a Hidden Markov Model HMM recognition engine with a statistical language model Acoustic Model Audio Alternate recognition hypotheses Phonetic Dictionary Speech Recognition Engine Confidence scores Time alignment information Transcript best hypothesis n gram Language Model Figure 3 1 Speech recognition with a statistical HMM engine The recognizer makes use of an acoustic model phonetic dictionary and n gram language model to recognize audio from an input file Depending on configuration the recognizer may output several different recognition hypotheses for each word or phrase with confidence scores or a single best hypothesis Time alignment information may be output for use in applications such as search navigation or video subtitling For this investigation only the best hypothesis plain text transcript is used 26 3 5 The CMU Sphinx ASR engine CMU Sphinx is an open source speech recognition 
toolkit from Carnegie Mellon University The version of Sphinx selected for this project is Sphinx4 a highly customized recognition engine written in Java Figure 3 2 illustrates the Sphinx4 architecture as described in the Sphinx4 White Paper 63 Figure 3 2 The Sphinx4 Framework Sphinx 4 White Paper Each module may be implemented by different classes each with its own set of parameters allowing many different ways to use and configure Sphinx For this project Sphinx was configured for large vocabulary continuous speech recognition using the following modules FrontEnd o audioFileDataSource dataBlocker speechClassifier speechMarker nonSpeechDataFilter premphasizer windower fft melFilterBank dct batchCMN and featureExtraction Decoder o WordPruningBreadthFirstSearchManager o ThreadedAcousticScorer o SimplePruner 27 LexTreeLinguist o TiedStateAcousticModel o FastDictionary o LargeNGramModel The FrontEnd manages the audio input source and pipeline and is configured to read a WAV file with a single audio channel 
encoded with pulse code modulation PCM at 16 KHz The LexTreeLinguist manages the acoustic and language models The LargeNGramModel class is used configured for a trigram model The dictionary and language model approach used in this architecture constrain the recognition process to a single word vocabulary model for example process and processing are distinct words This means that the dictionary and language model must contain all word variants to be recognized The Decoder generates recognition hypotheses and results The Decoder configuration can have a significant impact on performance and accuracy for example by increasing or decreasing the search space and number of hypotheses evaluated In general accuracy may be improved at the expense of performance the recognizer requires more memory and is slower As this project investigates the relative performance of competing language models reasonable defaults were used for the Decoder but further configuration related optimizations were not explored The full 
configuration of Sphinx4 used is included as Appendix 4 3 6 Reference acoustic and language models The reference acoustic and language models used are the HUB4 models provided with Sphinx The HUB4 Acoustic Models are US English models generated from 140 hours of audio from the 1996 7 HUB4 corpus The specific model used is an 8 gau 6000 senone tristate HMM packaged in hub4opensrc cd_continuous_8gau zip 76 The model uses the cmudict_0 6d phoneset without stress markers and a silence phone totalling 40 phones The HUB4 Language Model packaged in HUB4_trigram_lm zip is a trigram LM generated from a variety of permitted sources including broadcast news 76 with a vocabulary of 64000 words The models produce reasonable word error rates within the reported range for Sphinx4 when used for continuous speech recognition of US English speakers 28 3 7 Selection of lectures As the project investigates the performance of different language models sample lectures were selected with the goals of minimizing the influence of 
extraneous variables on the recognition process while ensuring a reasonable spread of topics and speakers Requirements for sample lectures were thus Good quality audio recorded with a close talking microphone minimal reverberation or background noise Speakers with a North American English accent likely to be a reasonable match with the reference acoustic model Lectures should be from a higher education institution on a range of topics matching the application domain Lectures should be in the form of a continuous monologue thus no or little audience interaction or third party media such as film clips to reduce the impact of different speakers or variable quality audio The Open Yale Courses OYC site was identified as a suitable collection containing many lectures matching the above requirements and helpfully includes transcripts for all lectures Audio recordings and transcripts are licensed with a Creative Commons Attribution Non Commercial ShareAlike license which facilitates their use in research 
applications and the downstream publication of derivative works such as modified transcripts A subset of OYC lectures was selected to ensure a diversity of knowledge domains a range of speakers and recordings of a consistent length approximately 50 minutes A subjective listening test was used to further select recordings with the best audio quality Assessing the performance of language models across different domains and topics is considered important because some disciplines use many more specialist words than others which is likely to affect recognition and thus search performance Table 3 1 shows the final set of 13 selected lectures References for each are listed in Appendix 3 29 1 Course ASTR 160 Frontiers and Controversies in Astrophysics BENG 100 Frontiers of Biomedical Engineering BENG 100 Frontiers of Biomedical Engineering EEB 122 Principles of Evolution Ecology and Behavior ENGL 220 Milton ENGL 291 The American Novel Since 1945 ENGL 300 Introduction to Theory of Literature HIST 116 The American 
Revolution HIST 202 European Civilization 1648 1945 PHIL 176 Death PLSC 114 Introduction to Political Philosophy PSYC 110 Introduction to Psychology RLST 152 Introduction to New Testament History and Literature Lecture title Dark Energy and the Accelerating Universe and the Big Rip Cell Culture Engineering Biomolecular Engineering Engineering of Immunity Mating Systems and Parental Care Lycidas Thomas Pynchon The Crying of Lot 49 The Postmodern Psyches The Logic of Resistance Maximilien Robespierre and the French Revolution Personal identity Part IV What matters Socratic Citizenship Plato Apology What Is It Like to Be a Baby The Development of Thought The Afterlife of the New Testament and Postmodern Interpretation Lecturer Professor Charles Bailyn 2 3 4 5 6 7 8 9 10 11 12 13 Professor Mark Saltzman Professor Mark Saltzman Professor Stephen Stearns Professor John Rogers Professor Amy Hungerford Professor Paul Fry Professor Joanne Freeman Professor John Merriman Professor Shelly Kagan Professor Steven Smith 
Professor Paul Bloom Professor Dale Martin Table 3 1 Selected Open Yale Courses lectures 3 8 Recognition process with reference language model Figure 3 3 illustrates the process followed to execute the speech recognition process for a recorded lecture and the reference HUB4 language model The audio is downloaded from the OYC collection and converted from the published mp3 format to the 16 KHz mono WAV format required by Sphinx The lecture transcript is downloaded and conditioned into a continuous set of unpunctuated words as the reference transcript Sphinx is configured with the HUB4 acoustic model HUB4 LM and accompanying dictionary as described in 3 5 and Appendix 4 and then run with the input audio file producing a hypothesis transcript The reference and hypothesis transcripts are then compared and evaluated to generate the metrics used for analysis such as WER 30 Select a lecture from Open Yale Courses Download mp3 audio file Download transcript from HTML page as plain text Convert mp3 audio to 16KHz wav 
format Condition transcript convert to a continuous set of words Run Sphinx Recognizer HUB4 AM HUB4 LM Align recognition output with transcript Evaluate output and calculate metrics Figure 3 3 Recognition process with a reference language model 31 3 9 Calculating metrics As described in 3 3 three primary and four secondary metrics are used to evaluate the results of the techniques applied Primary metrics are Word Error Rate WER Word Correct Rate WCR Ranked Word Correct Rate RWCR Secondary metrics are Out of vocabulary OOV words representing vocabulary coverage Unrecognized words representing vocabulary recognition Extraneous words representing vocabulary recognition Language model perplexity representing the complexity and alignment of the language model in relation to the target text The metrics are calculated from the reference transcript hypothesis transcript as produced by the recognizer language model and in the case of RWCR also a frequencyranked English dictionary as shown in Table 3 2 It is assumed 
that the pronunciation dictionary is equivalent to or a superset of the language model s vocabulary Reference transcript Hypothesis transcript Language model Frequencyranked dictionary Metric Resource WER WCR RWCR OOV words Extraneous words Unrecognized words Perplexity Table 3 2 Recognition metrics and artefacts Word Error Rate is calculated as where S substitutions D deletions I insertions N word count of reference transcript Calculating S D and I requires the reference and hypothesis transcripts to be aligned as illustrated in Table 3 3 32 Reference The best way I think to introduce the central issues of this wonderful poem Lycidas is to return to Milton s Comus So yet once more and I promise this will be one of the last times that we look back at Milton s mask but yet once more let s look at Comus Now you will remember that the mask Comus was everywhere concerned with questions of the power of well the strangely intertwined questions of the power of chastity on the one hand and the power of poetry on the 
other Hypothesis the best way to buy thank to introduce the central issues of of it s a wonderful column was so this is is to return set to milkens common so we can once more i promise this will be one of the last times that we look back at hilton s masked but what s yet once more let s look at our comments making remember now the mass comments was everywhere concerned with questions of the power of well because strangely intertwined questions of the power of chassis on the one hand the power of poetry on the other Aligned Hypothesis the best way TO BUY THANK to introduce the central issues OF of IT S A wonderful COLUMN WAS SO THIS IS is to return SET to MILKENS COMMON so WE CAN once more i promise this will be one of the last times that we look back at HILTON S MASKED but WHAT S yet once more let s look at OUR COMMENTS MAKING remember NOW the MASS COMMENTS was everywhere concerned with questions of the power of well BECAUSE strangely intertwined questions of the power of CHASSIS on the one hand the power of 
poetry on the other Hypothesis deleted a word incorrectly Upper case insertion or substitution Aligned Reference the best way I THINK to introduce the central issues of THIS wonderful POEM LYCIDAS is to return to MILTON S COMUS so YET once more AND i promise this will be one of the last times that we look back at MILTON S MASK but yet once more let s look at COMUS NOW YOU WILL remember THAT the MASK COMUS was everywhere concerned with questions of the power of well THE strangely intertwined questions of the power of CHASTITY on the one hand AND the power of poetry on the other Hypothesis inserted a word incorrectly Upper case word substituted Table 3 3 Reference and hypothesis transcripts with alignment In the above example the reference transcript contains 90 words and the recognition hypothesis has 18 substitutions 3 deletions and 9 insertions The Word Error Rate is thus 18 3 9 90 33 3 Word Correct Rate is calculated as the number of correct words as a proportion of total word count so in the above example 
WCR is 69 90 76 6 Here WCR is higher than the inverse of WER as WER reflects errors by the recognizer in identifying whether a phoneme sequence constitutes one or two words Where the hypothesis has significantly fewer or more words than the reference WER will also diverge further from inverse of WCR The transcript alignment and resulting WER and WCR metrics are calculated by the NISTAlign class from CMU Sphinx4 33 The RWCR OOV extraneous and unrecognized words metrics are calculated with custom written scripts source code links are provided in Appendix 2 Table 3 4 shows examples of OOV extraneous and unrecognized words using the above example Transcript vocabulary Comus I Lycidas Milton s and at back be best but central chastity concerned everywhere hand intertwined introduce is issues last let s look mask more now of on once one other poem poetry power promise questions remember return so strangely that the think this times to was way we well will with wonderful yet you 54 OOV words Extraneous words 
comments what s thank set our milkens mass masked making it s hilton s common column chassis can buy because a 18 Unrecognized words comus milton s lycidas mask and you think poem chastity 3 6 Table 3 4 Example of transcript vocabulary OOV extraneous and unrecognized words OOV words are those occurring in the transcript but not in the language model reflecting limitations in the language model vocabulary Extraneous words are those which occur in the hypothesis transcript but are not in the reference transcript These may reflect recognition difficulties or a language model which has too large a vocabulary or is too diverse Unrecognized words are those which are in the language model and the reference transcript but not in the hypothesis These may reflect audio related recognition difficulties for example from background noise or language related recognition difficulties arising from poor alignment between the language model and the genre style of speech or topic of the recorded speech The perplexity of the 
language model in relation to the reference transcript is calculated by the evaluate ngram tool in the mitlm language modelling toolkit Finally the Ranked Word Correct Rate is calculated by taking into account the dictionary rank of each word and including recognition scores only for those words below a given frequency cut off as illustrated in Table 3 5 for a cut off rank of 10 000 34 Word STRANGELY INTERTWINED CHASTITY MILTON S COMUS LYCIDAS Total Dictionary frequency rank 17238 25037 29904 41755 91192 157200 Recognized 1 1 0 0 0 0 2 Unrecognized 0 0 1 2 3 1 7 Table 3 5 Calculation of Ranked Word Correct Rate Here only the recognition rates of words which occur below the dictionary frequency rank cut off value are considered The RWCR for this example is thus 2 2 7 22 2 This reflects the application specific assumption that search terms are more likely to be less common words for example Comus rather than everywhere and therefore these words are more valuable for recognition 35 4 Topic and language 
modelling with Wikipedia 4 1 Introduction This chapter introduces Wikipedia as a linguistic resource and describes the process used for converting a set of Wikipedia articles into a plain text corpus suitable for generating or adapting a language model Topic modelling in Wikipedia is then introduced A technique is described for identifying and harvesting a set of related Wikipedia articles using article similarity metrics generated through latent semantic indexing enabling the creation of topic specific custom language models 4 2 Wikipedia as a linguistic resource The English Wikipedia hereafter Wikipedia is used to create three types of resources for this project 1 a dictionary of English words with word frequency counts 2 a generic language model approximating general English usage 3 topic specific language models approximating English usage in a topic area Advantages of using Wikipedia for this purpose include It is a large corpus containing more than 4 million articles and over 1000 million words 
although other language versions of Wikipedia are smaller 77 78 It is thus of a similar order of magnitude to resources such as the English Gigaword Corpus 79 It has been shown to be a usable language resource for other natural language processing tasks 80 Wikipedia articles include semantic metadata through inter article links and other tags such as categories This semantic structure can be used to select subsets of Wikipedia articles It has broad topic coverage It is updated continuously and thus dynamic and contemporary Wikipedia text is available at no cost and published with a permissive license allowing derivative works to be freely redistributed 81 The principle disadvantage is that it is a loosely curated resource and thus contains a greater number of typographical spelling formatting and classification variations and errors than other published texts which have been edited in a more traditional and centralized manner For applications such as this one which make use of Wikipedia as source data for 
statistical models these types of errors are less significant provided they are of relatively low frequency 36 4 3 Creating a plain text corpus from Wikipedia Users interact with Wikipedia as a set of article web pages for example as shown in Figure 4 1 Each page contains global navigation links links to article metadata such as the history and discussion pages links to other articles within the article body text and reference information such as footnotes To create a plain text corpus only the actual body text is of interest Wikipedia articles are stored in wiki markup format rather than HTML illustrated in Figure 4 2 Wiki markup is preferable as a source format for further processing because the wiki markup typically has more semantic value than the equivalent HTML representation and can thus be processed more reliably Figure 4 1 Lycidas Wikipedia article as shown in a web browser For the genus of jumping spiders Lycidas genus Lycidas is a poem by John Milton written in 1637 as a pastoral elegy It first 
appeared in a 1638 collection of elegies entitled Justa Edouardo King Naufrago dedicated to the memory of Edward King British poet Edward King a collegemate of Milton s at Cambridge who drowned when his ship sank in the Irish Sea off the coast of Wales in August 1637 The poem is 193 lines in length and is irregularly rhymed While many of the other poems in the compilation are in Greek and Latin Lycidas is one of the poems written in English ref Womack Mark On the Value of Lycidas Studies in English Literature 1500 1900 1997 119 136 JSTOR 3 Nov 2008 http www jstor org stable 450776 ref Milton republished the poem in 1645 Figure 4 2 Lycidas Wikipedia article wiki markup text However for continuous speech recognition language modelling purposes where the model should be trained on sentences approximating how people speak punctuation and references are unwanted and so further text conditioning the process of converting text to a consistent canonical form is applied to transform wiki markup into a list of 
unpunctuated upper case sentences illustrated in Figure 4 3 37 LYCIDAS IS A POEM BY JOHN MILTON WRITTEN IN 1637 AS A PASTORAL ELEGY IT FIRST APPEARED IN A 1638 COLLECTION OF ELEGIES ENTITLED JUSTA EDOUARDO KING NAUFRAGO DEDICATED TO THE MEMORY OF EDWARD KING A COLLEGEMATE OF MILTON S AT CAMBRIDGE WHO DROWNED WHEN HIS SHIP SANK IN THE IRISH SEA OFF THE COAST OF WALES IN AUGUST 1637 THE POEM IS 193 LINES IN LENGTH AND IS IRREGULARLY RHYMED WHILE MANY OF THE OTHER POEMS IN THE COMPILATION ARE IN GREEK AND LATIN LYCIDAS IS ONE OF THE POEMS WRITTEN IN ENGLISH MILTON REPUBLISHED THE POEM IN 1645 Figure 4 3 Conditioned sentences from the Lycidas Wikipedia article This list of sentences provides the source material from which a language model and dictionary with word frequency counts can be generated Figure 4 4 illustrates the complete process of creating a plain text corpus from Wikipedia 82 English Wikipedia Current Articles Offline Dump Start Convert article markup to plain text Identify sentence boundaries and 
split into sentences List of sentences all articles Yes More articles No Condition sentences List of conditioned sentences Generate vocabulary Dictionary Word Frequency End Figure 4 4 Generating a plain text corpus from Wikipedia 38 Article markup text is available from Wikipedia in two forms for an individual article through the Wikipedia API for a set of articles as a compressed XML dump file containing a snapshot of all articles and their metadata The most efficient method to access the body text for a large set of articles is to download and process the dump file For this process a dump of the current revision of all articles labelled enwiki latest pages articles is used The gwtwiki toolkit is used to parse the Wikipedia XML dump and extract the article titles and wiki markup text Further details of the software toolkits and datasets used are contained in Appendix 1 The following text conditioning is applied to each article to produce a list of sentences Headings and references are removed gwtwiki s 
PlainTextConverter is used to render the remaining wiki markup to plain text removing for example markup used for inter article links The OpenNLP toolkit is used for statistical sentence boundary detection This is more reliable than using a regular expression parser as English punctuation can be ambiguous for example a full stop may be used for abbreviations within a sentence A further set of rules is applied to restrict the output set as far as possible to well formed sentences o sentences must contain 5 or more words o sentences must start with a capital letter A Z and end with a full stop question mark or exclamation mark o sentences containing or are rejected which excludes URLs Sentences are capitalized and punctuation is removed A further word filter is applied when the dictionary and word counts are generated Words must satisfy English orthography consisting only of letters a z hyphen and apostrophe and English apostrophization rules Words must be three letters or longer The latter condition of course 
excludes many valid English words a an to which are restored to the resulting dictionary from a list of 2 letter words contained in the CMU Dictionary The above rules filter out many non words and non sentences in Wikipedia articles introduced both through meaningful content such as text in tables bulleted lists and abbreviations and through misspellings or syntactic errors The filtering process compensates to a degree both for noise in the data and at a higher level for the fact that Wikipedia as a written visual hypertext genre is being used to model language use in continuous speech a linear oral genre 39 Two language resources are created from the above process a large set of conditioned plain text upper case sentences from all articles an extract of which is shown in Figure 4 4 and a frequency ranked word dictionary 4 4 Goals for the adapted language model When creating a custom language model adapted to a specific topic the goal is not necessarily to create a larger model but to create a well adapted 
model that is a model which is closely aligned to the recognition target text in genre vocabulary linguistic style and other dimensions The size of an n gram language model is initially determined by the number of different n grams combinations of n words encountered in the training text Models may be limited in size by constraining the vocabulary in which case words in the training text which are not in the given dictionary will be modelled as unknown and applying a frequency cut off to the n grams in which case n grams which occur fewer than a certain number of times in the training text will not be included in the model In general a larger language model increases the search space for the recognizer and for the Sphinx4 recognition engine larger models lead to an increase in both runtime a consequence of the larger search space and memory requirements a consequence of needing to load the entire model into memory A further consequence of increasing the search space with a larger model is that accuracy can 
be reduced as the model leads to the recognizer introducing extraneous words and phrases To enable the most accurate comparison between recognition performance with the adapted and reference language models the adapted models are created with the same vocabulary size as the HUB4 reference model 64000 words However owing to limitations in the language modelling toolkit used a frequency cutoff was not applied to the adapted language model This leads to the adapted model having a larger number of bi grams and tri grams than the reference model and overall being about twice the size as presented in section 5 4 Table 5 6 40 4 5 Constructing a topic adapted language model from Wikipedia Figure 4 5 illustrates the steps taken to construct a topic adapted language model from Wikipedia In Step 1 a set of articles is identified from Wikipedia which relate to the topic keywords The article selection process is described further in 4 8 to 4 11 In Step 2 the output text from each of the articles is conditioned into plain 
text sentences using the techniques described in 4 3 Steps 3 and 4 create a target vocabulary for the adapted language model This is done by merging two frequency ranked vocabularies one the more specialized derived from the output text from the selected set of Wikipedia articles and the other more general derived from a plain text corpus of all Wikipedia articles The merged list starts with all words which occur 5 or more times in the specialized word list and is supplemented with words from the general list in descending order of frequency until the list reaches the target size of 64 000 For the word cutoff threshold and a number of other parameters for example the Wikipedia crawler constraints listed in Table 4 2 and the similarity threshold applied in 4 11 reasonable default values have been chosen informed by experimental results Further experimentation would be required to establish the impact of varying these choices and whether the selected values are optimal Step 5 creates a phonetic dictionary for 
the target vocabulary described further in 4 6 The adapted language model is then created in Steps 6 and 7 using the mitlm language modelling toolkit As the amount of training text available from the set of topic related Wikipedia articles is relatively small a more general language model is first created from a larger Wikipedia corpus restricted to the target vocabulary Step 6a The input corpus used for this model is 5 of all Wikipedia text selected using 1 from every 20 sentences yielding a total of around 75 million words A topic specific language model is then created from the conditioned text output from the topic related Wikipedia articles again restricted to the target vocabulary Step 6a The two language models are then merged using linear interpolation to create the third adapted language model Step 7 41 Start Topic keywords 1 Select a set of related articles from Wikipedia and output list of sentences 2 Condition output text 3 Create vocabulary from the output text 4 Merge vocabulary with reference 
word list to create a custom dictionary 5 Create pronunciations for any words not in CMUdict 6a Create a language model from output text with restricted vocabulary 6b Create a language model from a large Wikipedia sample with restricted vocabulary 7 Interpolate the two language models to create a custom language model End Figure 4 5 Creation of a custom language model from Wikipedia 4 6 Recognition process with a custom language model Figure 4 6 illustrates the process followed to execute the speech recognition process for a recorded lecture with a custom adapted language model The method of language model adaptation here is unsupervised adaptation based only on minimal information about the lecture in the form of up to 5 keywords derived from the lecture topic It is assumed that a suitable set of keywords could always be selected possibly in an automated way from the subject area of the lecture for example the from the name of the department and title of the course and the title of the lecture 42 Example 
keywords following this approach for three lectures are shown in Table 4 1 Course ASTR 160 Frontiers and Controversies in Astrophysics ENGL 220 Milton PLSC 114 Introduction to Political Philosophy Lecture Dark Energy and the Accelerating Universe and the Big Rip Lycidas Socratic Citizenship Plato Apology Keywords astrophysics dark energy accelerating universe english literature milton lycidas political philosophy socratic citizenship plato Table 4 1 Examples of keywords for selected lectures The custom language model is created automatically as described in 4 8 to 4 11 seeded by the given keywords The recognition process in all other respects is the same as that for the reference language model 3 8 Select a lecture from Open Yale Courses Download mp3 audio file Choose 5 keywords for the lecture from course and lecture title Download transcript from HTML page as plain text Convert mp3 audio to 16KHz wav format Create a Wikipediagenerated language model and dictionary Condition transcript convert to a 
continuous set of words Run Sphinx Recognizer HUB4AM Custom LM Align recognition output with transcript Evaluate output and calculate metrics Figure 4 6 Recognition process with a custom language model 43 4 7 Constructing the phonetic dictionary The base phonetic dictionary used is the CMU Pronouncing Dictionary CMUDict 0 7a which contains phonetic representations for slightly over 123 000 words However it is to be expected that new words will be encountered which are not in the CMU Dictionary for example because of their relative scarcity or because they are neologisms or variants of known words such as new hyphenations As less common words are expected to be significant to the topic it is important to recognize them where possible and thus a method is required to generate pronunciations for unknown words For this project the phonetisaurus grapheme to phoneme g2p converter is used Phonetisaurus uses a weighted finite state transducer WFST approach to generate pronunciation hypotheses for a word a technique 
claimed to produce results comparable in accuracy to other state of the art systems 83 The model used by phonetisaurus for this application is trained from CMUDict 0 7a and thus phonetisaurus is in effect extrapolating from the implicit pronunciation rules represented in CMUDict Only the best hypothesis generated by phonetisaurus is used Stress markers are used in the training process to create the FST model but ignored in the output This approach is supported by experimental results suggesting that Sphinx recognition accuracy is adversely affected by using stress markers but that g2p models trained from CMUdict with stress markers produce better accuracy even when stress markers in the output are ignored 76 84 4 8 Identifying a topic related subset of Wikipedia articles To collect a set of articles from Wikipedia related to the topic of a lecture a web crawler strategy is used A web crawler starts at a seed page and iteratively follows all the links encountered on the page In this case the crawler is seeded 
with the top five search results produced by executing a Wikipedia search using the keywords selected from the topic title The crawler then follows a breadth first search by adding links it encounters on each page to the end of an article queue and iterating through the article queue Only links to other Wikipedia articles are followed the crawler thus does not follow links to external sites Two different crawler strategies are investigated 1 The 44 As Wikipedia articles typically contain links to both related and unrelated topics it is expected that the set of articles indexed by the 4 9 The Wikipedia The operation of the 45 Start Topic keywords 1 Use Wikipedia Search with topic keywords to get seed documents 2 Add seed articles to search queue End Yes Reached max depth article or sentence count No 4 Convert Wikipedia markup to plain text and identify sentence boundaries add sentences to 5 Get set of links to other articles 3 Pull article title from top of search queue and get full text List of sentences 6 
Add titles of articles not already visited to search queue Figure 4 7 Wikipedia crawler with Parameter Maximum seed pages from search results Minimum seed article word count Maximum article depth Maximum number of articles Maximum number of output sentences Value 5 250 5 2500 200 000 Table 4 2 Wikipedia crawler constraints 46 Figure 4 8 illustrates the Open Yale Courses Lecture Prof John Rogers Course title Milton Lecture title Lycidas Keywords english literature milton lycidas More relevant article Less relevant article Wikipedia Search top 5 results Depth 0 5 articles Lycidas Baptista Mantuanus C A Patrides John Milton Pastoral Elegy Edward King British Poet Cambridge Irish Sea Milton s 1645 Poems Pastoral elegy history Depth 1 126 articles John Milton Cheapside First language Areopagitica Cambridge Non metropolitan district City status in the United Kingdom Sentences 200 065 plain text sentences from 2028 articles Depth 2 2204 articles Cheapside First language Figure 4 8 Wikipedia Crawler for lecture on 
Lycidas 4 10 Topic modelling with article similarity When adapting a language model to a given topic two goals are to improve the vocabulary coverage of the model for the given topic i e to include as many words as possible which are likely to be used in the context of the topic and to model the style of language and typical word combinations used in the context of the topic 47 It is therefore advantageous to collect as much text as possible from contexts here Wikipedia articles which are related to the target topic And as this process should be unsupervised it must be possible to establish relatedness in an automated way without human subjective judgement or interpretation This section describes an article similarity metric which gives the degree of similarity measured from 0 to 1 between two Wikipedia articles This metric is then used to improve the discrimination of a Wikipedia article crawler such that only similar articles are included in the links which are followed This approach described further 
below aims to gather a large set of articles using search seeding and transitive similarity Topic keywords Article A from Wikipedia Search links to Article B similar to Article A links to Article C similar to Article B List of sentences all articles Figure 4 9 Seeded search with transitive similarity As shown in Figure 4 9 a search is seeded using keywords with subsequent articles being included in the search net through similarity to the parent article Article B is similar to Article A and Article C is similar to Article B The text from all such articles is then used to train a language model for the target topic Latent semantic indexing LSI is used to derive an article similarity metric LSI also known as Latent Semantic Analysis LSA is a technique widely used in information retrieval applications to identify related documents in large corpora 85 86 LSI uses singular value decomposition to train a model from the corpus which relates individual words to a set of topics The set of topics is of a fixed size 
and arbitrary in that the topics are mathematical abstractions which emerge from latent semantic clustering in the data Each topic is defined through a set of words and their respective contribution weights to the topic A related method Latent Dirichlet Allocation LDA works in a similar way but is not explored here Using the model a document may then be expressed as a set of topic values representing the relative strength of each topic in the document or equivalently as a set of n values representing a position in n dimensional space where n is the number of 48 topics in the model The similarity between two articles is then understood to be the distance between the two article vectors in an n dimensional space To apply LSI to Wikipedia and generate article similarity scores the open source gensim vector space modelling toolkit is used 87 gensim is designed to handle large corpora such as Wikipedia which exceed available memory and in addition is well documented and actively maintained Figure 4 10 illustrates 
the initial process to train the LSI model from Wikipedia a modified version of the recipe described in the gensim documentation 88 Start English Wikipedia Current Articles Offline Dump List Word ID Word Pass 1 Generate vocabulary of most frequent 100K words List Article ID Article Title Pass 2 Generate bag of words representation for each article Sparse Matrix ArticleIDs x Word IDs Pass 3 Perform Latent Semantic Analysis of bag ofwords matrix LSI model weighting of words for 400 topics End Figure 4 10 Generating a bag of words index and LSI model from Wikipedia with gensim This requires three passes through an offline dump of all Wikipedia articles 3 345 476 articles in total from the Wikipedia snapshot used This is a time consuming process but only needs to be executed at the start and possibly at intervals thereafter to take account of gradual evolution of the corpus 49 In Pass 1 a vocabulary is created of the most frequent 100 000 words Only these words will be regarded as significant for LSI modelling 
with any remaining words or tokens being ignored Outputs from this pass are the list of words each with a numeric identifier and list of article titles also each assigned a numeric identifier In Pass 2 a bag of words representation of each article is generated This represents the article as the set of distinct words from the chosen vocabulary which occur in it irrespective of frequency word position or sequencing This is the simplest article representation which can be used with this technique other approaches such as using a TF IDF measure can be used but are not explored here The output of this pass is a sparse matrix of words by articles In Pass 3 the sparse matrix is used to create the LSI model for a fixed number of topics 400 The model parameters of 100 000 terms and 400 topics follow the gensim defaults informed by empirical results on dimensionality for semantic indexing applications suggesting an optimal range of 300 to 500 for topic size 89 Figure 4 11 illustrates the process followed to generate 
scores for similarity between a parent article and one or more linked articles using gensim with the LSI model List of Article IDs from the Wikipedia Crawler Start Load model and word matrix Article bag ofwords matrix LSI model Generate a small sub corpus with only matrix rows for the requested Article IDs List of article similarity scores Transform sub corpus to LSI space and calculate similarity to first article Figure 4 11 Generating article similarity scores with gensim and an LSI model While the initial creation of the LSI model from Wikipedia is time consuming upwards of 24 hours calculating similarity between a document and the set of documents to which it links is relatively efficient at approximately 72ms per comparison This makes it a computationally tractable approach for generating custom language models on demand requiring neither a significant memory footprint nor long runtime By comparison pre computing pair wise article similarity for the approximately 3 3 million articles in the Wikipedia 
snapshot would require a set of 5 6 x 1012 tuples which would take just under 13 000 processor years to calculate 50 For efficiency the similarity calculation engine is designed to execute as a long lived process with which the crawler communicates so that the LSI model and word matrix are only loaded once rather than per article or per comparison 4 11 The Wikipedia Similarity Crawler Figure 4 12 illustrates the operation of the Wikipedia Similarity Crawler This resembles the Topic keywords Start 1 Use Wikipedia Search with topic keywords to get seed documents 2 Add seed articles to search queue End Yes Reached max depth article or sentence count 7 Add the titles of articles exceeding the similarity threshold and not already indexed to the search queue No 4 Convert Wikipedia markup to plain text and identify sentence boundaries add sentences to 5 Get set of links to other articles 3 Pull article title from top of search queue and get full text List of sentences 6 Invoke Similarity Scorer to get similarity 
scores for the set of target articles to the parent article Figure 4 12 Wikipedia Crawler with Similarity Scorer The similarity threshold applied is 0 7 0 025 article_depth Thus articles linked from depth 0 articles those returned by the keyword search need to have similarity 0 7 to be included in the index queue whereas for links from depth 1 articles a threshold of 0 725 is applied and so on This is intended to counteract topic divergence as distance from the seed articles increases 51 Figure 4 13 illustrates the Similarity Crawler applied to the lecture on Lycidas as in Figure 4 8 with the same crawler constraints as in Table 4 2 Open Yale Courses Lecture Prof John Rogers Course title Milton Lecture title Lycidas Keywords english literature milton lycidas Wikipedia Search top 5 results Depth 0 5 articles Lycidas Baptista Mantuanus John Milton Apollo 77 Jupiter mythology 71 Milton s 1645 Poems Pastoral elegy history Similarity threshold 70 Cambridge 37 Irish Sea 20 Depth 1 76 articles Apollo Hermes 90 Zeus 
89 Dionysus 87 Pan god 86 Jupiter mythology Mars mythology 88 Ceres mythology 82 Summanus 81 Flamen 80 Sentences 200 156 plain text sentences from 2335 articles Depth 2 974 articles Hermes Theogony 84 Persephone 84 Serpent symbolism 83 Zeus Uranus mythology 87 Castor and Pollux 86 Cronus 86 Depth 3 973 articles Theogony Persephone Figure 4 13 Wikipedia Crawler for lecture on Lycidas Similarity Crawler In this example the crawler outputs 200 065 sentences having processed 2028 articles at a maximum depth of 3 The effect of the similarity scorer is that the crawler added just under 9 of articles considered to the indexing queue 2533 out of 21229 articles considered For example from the top level article Lycidas links to Apollo and Jupiter mythology are followed article similarity scores of 0 77 and 0 71 respectively while links such as Cambridge and Irish Sea are rejected article similarity scores of 0 37 and 0 2 52 5 Discussion and main findings 5 1 Introduction This chapter presents the key results of the 
experimental work described in Chapters 3 and 4 Limitations on the accuracy of the recognition process are noted and the baseline recognition performance of the Sphinx 4 speech recognition engine with the reference HUB4 acoustic and language models is presented The behaviour and output of the 5 2 Limitations on accuracy The nature of the data and methodology used introduce some limitations on the resulting accuracy figures Five factors affecting accuracy are noted here transcript fidelity text conditioning Sphinx configuration dictionary variation and corpus quality and drift In general these factors are likely to lead to a slight understatement of absolute accuracy but do not affect relative accuracy between models in a significant way 1 Transcript fidelity the source transcripts for the Open Yale Courses are edited for readability and thus may exclude disfluencies repetitions or filler words such as um Should these occur in the recognition output they would be considered incorrectly as insertions 2 Text 
conditioning is the process of converting text to a consistent canonical form suitable for language modelling and comparing the output of recognizers with the reference transcript For this investigation only minimal text conditioning has been applied which means for example literal numerals in transcripts 58 may not match word recognition output fifty eight 3 A number of Sphinx configuration parameters affect its behaviour and the resulting output and the optimal configuration for one recording may be less optimal for another 4 Dictionary variation English Wikipedia as a whole is considered representative of general English for this project and is used inter alia for generating frequency ranked word lists However a comparison of three dictionaries Table 5 1 shows significant 53 divergence and surprisingly low overlap The HUB4 vocabulary shares only two thirds of its words with the top 64 000 words from English Wikipedia and the Google books ngram data set while all three dictionaries have only 58 of words in 
common 90 91 Dictionaries HUB4 Google ngram 64K HUB4 Wikipedia 64K Google ngram 64K Wikipedia 64K HUB4 Google ngram 64K Wikipedia Words in common 43 173 42 099 45 129 37 021 Percentage of words in common 67 66 71 58 Table 5 1 Overlap between HUB4 Wikipedia and Google dictionaries 5 Corpus quality and drift Wikipedia itself is of variable quality and by design is loosely curated Therefore it may contain many misspellings or syntactic errors in the text or markup which affect the quality of the resulting vocabularies and language models As it is also under constant revision any results which depend on Wikipedia as an online resource are subject to some degree of drift as the articles evolve and static offline resources such as a pre computed latent semantic indexing model as described in 4 10 diverge from the related online data 5 3 Baseline performance with HUB4 The HUB4 acoustic and language models are used as the reference models The HUB4 language model contains 64 000 unigrams CMUdict 0 7a is used as the 
baseline pronunciation dictionary Pronunciations have been estimated for 177 words which are contained in the HUB4 language model but are not in CMUdict Table 5 2 presents a set of recognition statistics of the thirteen sample lectures with the HUB4 models Transcript words is the number of words in the reference transcript OOV words is the number of words in the reference transcript which are not contained in the dictionary Perplexity is the calculated perplexity of the language model for the transcript without sentence markers Length is the length of the audio recording RT ratio is the ratio of the time spent by the recognizer runtime in relation to the length of the audio Transcript sentences is the number of sentences in the reference transcript Output segments is the number of lines output by the recognizer Output words is the number of words in the hypothesis transcript 54 Lecture astr160 beng100a beng100b eeb122 engl220 engl291 engl300 hist116 hist202 phil176 plsc114 psyc110 rlst152 Sum Average 
Transcript words 6 704 7 385 6 974 5 795 7 350 6 201 6 701 7 902 6 643 6 603 5 473 7 085 8 196 89 012 6847 Perplexity OOV continuous words transcript 47 54 62 97 143 96 116 54 131 31 48 70 58 1 007 77 325 228 307 211 331 535 379 274 309 252 475 357 275 286 Length mm ss 45 47 52 21 46 32 40 55 51 51 49 27 52 49 47 44 49 56 48 52 45 34 48 46 47 40 Transcript RT ratio sentences 2 50 2 10 2 18 2 82 2 63 1 61 1 60 1 84 1 73 1 99 1 27 1 78 2 34 447 336 334 370 330 274 280 332 466 464 249 467 393 4742 Output segments 415 552 400 484 556 503 299 492 427 509 536 641 391 6205 Output words 7 312 8 166 7 706 5 935 8 293 6 596 7 992 7 797 8 031 6 676 5 976 7 156 8 614 96 250 48 20 2 03 Table 5 2 Recognition statistics with HUB4 models Notable here is that the number of output segments where the recognizer inserts a line break in the output based on elapsed time between utterances exceeds the number of sentences by around 30 and the correlation between sentences and output segments is relatively weak 0 12 This implies 
that the recognizer has difficulty identifying sentence boundaries in the speech and therefore that sentence markers in the language models will have limited value Word Error Rate and Perplexity have thus been calculated across the whole transcript without sentence boundaries rather than per sentence The recognizer has also output approximately 8 more words than in the original transcript suggesting a bias towards shorter words in the Sphinx configuration regulated partly by the wordInsertionProbability parameter and or language model Table 5 3 presents recognition accuracy for the sample lectures in terms of Word Error Rate and Word Correct Rate Edit distance is the Levenshtein distance between the reference and hypothesis transcripts number of insertions deletions and substitutions required for the hypothesis to match the reference and is used to calculate the Word Error Rate WER Words correct is the number of words in the reference transcript which are correctly recognized in the reference transcript and 
is used to calculate the Word Correct Rate WCR 55 Lecture astr160 beng100a beng100b eeb122 engl220 engl291 engl300 hist116 hist202 phil176 plsc114 psyc110 rlst152 Average Transcript words 6 704 7 385 6 974 5 795 7 350 6 201 6 701 7 902 6 643 6 603 5 473 7 085 8 196 Edit distance 2 360 2 617 2 228 2 312 3 185 2 015 3 162 3 131 4 059 3 230 1 739 2 984 3 379 Word Error Rate WER 35 2 35 4 31 9 39 9 43 3 32 5 47 2 39 6 61 1 48 9 31 8 42 1 41 2 40 8 Words correct 5 185 5 780 5 675 3 935 5 312 4 738 5 006 5 184 4 293 3 856 4 355 4 661 5 730 Word Correct Rate WCR 77 3 78 3 81 4 67 9 72 3 76 4 74 7 65 6 64 6 58 4 79 6 65 8 69 9 71 7 Table 5 3 Recognition accuracy with HUB4 models WER and WCR Both measures show relatively wide variation in accuracy with WER ranging from 31 8 to 61 1 and WCR ranging from 58 4 to 81 4 Audio factors which could account for this variation include the degree of background noise and reverberation in the recording determined by room acoustics and microphone position and the extent of 
alignment between the acoustic model and the speaker s accent A difficulty in examining the impact of changes in the recognition process is in understanding the extent to which acoustic or language factors dominate recognition accuracy Nevertheless the average WER here of around 40 is consistent with reported results from other projects and recognizers 5 51 5 4 Comparing the Wikipedia crawler behaviour and output Table 5 4 summarises the results of executing the 56 Docs indexed is the number of articles converted to plain text and added to the output corpus Total sentences is the number of plain text sentences added to the output corpus Total Words is the number of words in the output corpus Links considered Links queued Links queued Docs indexed Total sentences Lecture Total Words astr160 beng100a beng100b eeb122 engl220 engl291 engl300 hist116 hist202 phil176 plsc114 psyc110 rlst152 Average 2 592 2 909 2 589 2 583 2 609 2 606 2 624 2 634 2 548 2 662 2 528 2 540 2 762 2 630 2592 2909 2589 2583 2609 2606 
2624 2634 2548 2662 2528 2540 2762 2 630 100 100 100 100 100 100 100 100 100 100 100 100 100 2462 1852 2500 2500 2335 1752 2291 2500 1845 2253 2162 1739 2155 2 180 200 113 200 123 115 229 158 760 200 156 200 067 200 008 178 178 200 013 200 095 200 339 200 200 200 228 188 731 4 769 510 4 458 388 2 689 101 3 527 469 4 752 762 4 782 250 4 738 475 4 325 914 4 836 736 4 724 906 4 695 799 4 712 681 4 829 361 4 449 489 Similarity Crawler astr160 beng100a beng100b eeb122 engl220 engl291 engl300 hist116 hist202 phil176 plsc114 psyc110 rlst152 Average 10 282 14 850 23 869 13 876 21 299 31 750 14 765 26 782 19 715 11 492 19 033 12 892 12 755 17 951 2619 2515 2557 2539 2533 2535 2501 2519 2514 2525 2500 2576 2508 2 534 25 17 11 18 12 8 17 9 13 22 13 20 20 14 2500 2500 2500 2500 2028 2292 2083 1490 1656 2103 2047 2333 2197 2 171 180 300 184 621 163 045 135 829 200 065 200 011 200 358 200 087 200 044 200 127 200 088 200 094 200 010 189 591 4 372 655 4 174 711 3 852 979 2 881 927 4 851 670 4 742 750 4 766 094 4 866 393 4 
846 217 4 769 079 4 788 145 4 657 753 5 033 929 4 508 023 Table 5 4 Wikipedia Crawler Statistics The Similarity Crawler followed between 8 and 25 of links considered highlighted in purple above Applying the similarity threshold to be more selective about which article links to follow means that the Similarity Crawler considered many more articles 57 in order to reach the same number of indexed articles or output sentences as the Articles by depth naive crawler 100 80 60 40 20 0 1 2 3 4 5 6 7 8 9 10 11 12 13 Articles by depth similarity crawler 100 80 60 40 20 0 1 2 3 4 5 6 7 8 9 10 11 12 13 Depth 2 Depth 1 Depth 3 Depth 2 Depth 1 Figure 5 1 Percentage of Wikipedia articles by depth for The variation across lectures in the percentage of links queued and thus total number of links considered and depth reached in order to generate an equivalent number of output sentences suggests that topic coverage and connectedness is uneven in Wikipedia As the similarity metric is a measure of distance topic coverage in 
Wikipedia could be expressed in terms of density Where seed articles have a large number of links to strongly related other articles reflecting both semantic relevance and connectedness topic density could be regarded as high whereas for topics where similarity scores are lower and there are fewer links the topic has sparser coverage While the concept of variations in topic density is not explored further here one could perhaps expect that more robust language models would be created from topics with higher density coverage in Wikipedia than those for which coverage is sparse Table 5 5 shows the impact of the two crawler strategies on the resulting vocabulary Articles in both Lecture astr160 beng100a beng100b eeb122 engl220 engl291 engl300 hist116 hist202 phil176 plsc114 psyc110 rlst152 Average Articles in both Vocabulary in both Words not in Wikipedia 64K Words not in Wikipedia 64K similarity crawler 5 192 7 550 4 251 7 505 7 963 5 651 6 244 5 312 6 119 6 774 7 143 6 202 6 999 6 377 Table 5 5 Articles and 
word comparison between While the 59 Total size is the number of entries in the language model unigrams bigrams and trigrams Unigrams 64 001 64 219 64 219 Bigrams 9 382 014 9 139 981 9 126 390 Trigrams 13 459 879 32 600 005 32 605 170 Total size 22 905 894 41 804 205 41 795 779 Model HUB4 LM Average of Wikipedia Table 5 6 Average sizes of the HUB4 and Wikipedia Language Models The custom models are sized to have the same vocabulary number of unigrams as the HUB4 model for comparison purposes and have a similar number of bigrams but approximately twice as many trigrams While the design of the HUB4 model is not documented in detail this difference is most likely explained by the application of a frequency cut off for HUB4 trigrams which has not been applied to the custom models 5 5 Recognition performance of To investigate whether the Similarity Crawler produces better language models than the Sum of unique OOV words 791 735 Average Perplexity 297 248 Language models Average WER 41 9 41 6 Average WCR 68 0 68 4 
Table 5 7 Comparison of recognition performance for The Similarity language models thus outperform the 60 To investigate whether the Similarity Crawler s language models are uniformly better than the Decrease in unique out of vocabulary words 3 21 8 1 24 3 9 1 12 0 4 11 3 4 Lecture astr160 beng100a beng100b eeb122 engl220 engl291 engl300 hist116 hist202 phil176 plsc114 psyc110 rlst152 Average Decrease in perplexity 7 314 167 11 20 18 15 2 2 17 17 44 13 49 Decrease in WER absolute 0 7 1 7 1 2 0 5 1 5 0 1 0 4 0 2 0 1 0 0 1 1 0 9 0 5 0 4 Increase in WCR absolute 0 7 1 5 1 0 0 7 0 6 0 1 0 4 0 1 0 0 0 3 1 0 0 7 0 5 0 4 Table 5 8 Relative performance of For the seven lectures highlighted above shaded green the similarity LM outperforms the 5 6 Recognition performance of HUB4 and Similarity language models To investigate whether the Similarity Crawler language models outperform the reference HUB4 language model the recognition performance of the Similarity language models is compared to that of the reference HUB4 
language model across four metrics in Table 5 9 61 Language models HUB4 Similarity Sum of unique out of vocabulary words 1 007 735 Average Perplexity 324 248 Average WER 40 8 41 6 Average WCR 71 7 68 4 Table 5 9 Comparison of recognition performance for HUB4 and Similarity LMs Table 5 10 shows the performance increase positive highlighted green or decrease negative highlighted red by individual lecture across the same metrics Decrease in unique out of vocabulary words 27 36 1 37 15 1 48 10 34 1 22 28 38 21 Lecture astr160 beng100a beng100b eeb122 engl220 engl291 engl300 hist116 hist202 phil176 plsc114 psyc110 rlst152 Avg Decrease in perplexity 53 106 33 87 191 60 28 35 18 189 148 25 87 76 Decrease in WER absolute 0 6 2 2 4 7 1 0 1 7 3 3 0 3 2 8 0 8 0 7 1 5 2 4 0 2 0 8 Increase in WCR absolute 3 2 0 9 6 4 3 0 1 1 4 2 3 6 4 5 3 7 3 5 1 3 5 1 2 9 3 3 Table 5 10 Relative performance of HUB4 and Similarity LMs per lecture One lecture beng100b is worse across all metrics whereas the other 12 show improved 
perplexity but mixed or worse performance in vocabulary WER and WCR Thus on average the Similarity language models outperform HUB4 in the two languagerelated metrics out of vocabulary words and perplexity but recognition performance for the Similarity LMs reflected in WER and WCR is actually worse with an increase in WER of 0 8 and decrease in WCR of 3 3 5 7 Effect of estimated pronunciation As an aim of the Wikipedia crawler is to introduce specialist vocabulary into the topicadapted language models it is likely that a number of such words will not occur in the relatively small CMU pronouncing dictionary around 123 000 words Pronunciations for such words are therefore estimated in this project through the phonetisaurus grapheme to phoneme tool using a model trained from CMUdict 83 92 62 As these estimations are extrapolations of implicit rules in CMUdict they may be inaccurate for unusual or foreign vocabulary and thus produce poor recognition results For example Table 5 11 shows word recognition results 
for the word Lycidas from the lecture on Milton with estimated machine generated and manual humangenerated pronunciations Pronunciation L AY S AH D AH Z L IH S IY D AH S Word recognition count Wikipedia Similarity LM 0 7 47 Table 5 11 Recognition of Lycidas with variant pronunciations With the estimated pronunciation the word is not recognized at all although occurring 47 times With a manual pronunciation closer to how the word is spoken the recognition rate improves to 7 instances out of 47 To investigate whether pronunciations estimated by phonetisaurus lead to lower word recognition rates than those for words with pronunciations in the CMU Dictionary the recognition rates of the respective word types are compared in Table 5 12 Words from CMUdict is the number of words in the transcript which have pronunciations in the CMU Pronouncing Dictionary CMUdict CMUdict words recognized at least once is words found in CMUdict which are recognized correctly at least once i e occur in the output transcript CMUdict 
word recognition rate is the word correct rate for words found in CMUdict number of words recognized correctly as a percentage of total words Est words is the number of words in the transcript without pronunciations in CMUdict and thus for which pronunciations have been estimated by phonetisaurus Est words recognized at least once is words with estimated pronunciations which are recognized correctly at least once Est word recognition rate is the word correct rate for words with estimated pronunciation 63 Words from CMUdict astr160 beng100a beng100b eeb122 engl220 engl291 engl300 hist116 hist202 phil176 plsc114 psyc110 rlst152 Total Average 975 1050 1078 1107 1503 1437 1283 1417 1406 787 1101 1231 1312 15687 1207 CMUdict words recognized at least once 85 89 87 77 82 80 82 79 71 76 87 79 81 CMUdict word recognition rate 76 80 78 67 75 75 75 64 64 57 81 62 69 Est words 28 30 11 35 43 27 51 7 48 14 25 29 40 388 Est words recognized at least once 54 60 45 51 47 37 43 43 15 50 40 62 63 Est word recognition rate 69 
45 47 51 36 30 26 27 15 52 36 58 59 81 71 30 46 39 Table 5 12 Recognition rate of words with estimated pronunciation For CMUdict pronunciations 81 of all words are recognized at least once whereas only 46 of words with estimated pronunciations are ever recognized correctly Estimating pronunciation is therefore only partially effective If the recognition rate of estimated pronunciation words matched those with CMUdict pronunciations i e increased from 39 to 71 an additional 6 unique words or 22 additional words in total could be recognized correctly on average per lecture While the absolute number of estimated pronunciation words is quite small an average of 30 per lecture these words are likely to be significant for searchability 5 8 Introduction of extraneous words Recognition failures not only lead to the omission of a correct word in the hypothesis transcript but they also introduce an incorrect word A class of incorrect words are those which do not occur at all in the reference transcript identified here 
as extraneous words Considered in terms of the information retrieval measures recall and precision the introduction of extraneous words in a transcript lowers the precision of search results by increasing false positives i e matches to words in the transcript which should not be there To investigate the extent to which the Similarity LMs introduce extraneous words the number of extraneous words introduced by the HUB4 and Similarity LMs respectively is shown in Table 5 13 64 Lecture astr160 beng100a beng100b eeb122 engl220 engl291 engl300 hist116 hist202 phil176 plsc114 psyc110 rlst152 Total Extraneous words HUB4 LM 422 486 415 595 671 586 624 625 806 588 381 510 617 7 326 Extraneous words Similarity LM 459 504 582 666 749 733 756 829 967 759 401 635 715 8 755 increase in extraneous words 9 4 40 12 12 25 21 33 20 29 5 25 16 20 Table 5 13 Extraneous words introduced by the HUB4 and Similarity LMs On average the Similarity LM introduces 20 more extraneous words than the HUB4 LM although as the number of 
extraneous words is strongly correlated with Word Error Rate with a correlation of 0 84 this effect is worst for lectures where the Similarity LM led to an increase in overall WER An example of the introduction of extraneous words in the recognition outputs for the lecture on Milton engl220 is shown in Table 5 14 which lists extraneous words which occur three or more times in the transcripts produced by the HUB4 and Similarity LMs respectively Of note for this lecture is that while the absolute number of extraneous words increased by 12 the distribution for these words in the results from the Similarity LM contains a longer tail than those from the HUB4 LM as the Similarity results show slightly fewer unique extraneous words with frequency 3 or more These words are additionally slightly less common than those introduced by HUB4 with a median rank of 3582 vs 1856 again showing that the Wikipedia derived language models include a more specialized vocabulary than that of HUB4 A number of words listed here are 
artefacts of variant spellings for example Masque vs Mask or text conditioning differences sixteen vs 16 while other content terms such as Odyssey Iris Hera and Kant are probable search terms and could cause the transcript to incorrectly appear in search results 65 Extraneous words introduced by both language models freq 3 Dict rank 11498 1463 185170 4174 5280 3061 2913 920 679 207 Doc freq 13 11 8 5 4 4 3 3 3 3 Extraneous words introduced only by HUB4 LM freq 3 Dict rank 1448 2761 1624 952 68444 14238 862 590 30973 11684 10026 8548 4396 2903 2611 1856 1753 1724 943 366 311 1856 Doc freq 8 5 5 5 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 Extraneous words introduced only by Similarity LM freq 3 Dict rank 52 27997 22670 745 212 109012 40806 26785 25182 10794 8171 4727 3582 3581 590 326 276 271 213 Doc freq 6 5 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 Word ODYSSEY LISTS SOLICITOUS THIRTY SIXTEEN SELF COLUMN BILL GOT HOME Word DOCTOR HAPPEN BOB EIGHT HILTON S EIGHTY OLDER REPORT ORIOLE DAY S ANALOGY LISTENERS POLL BOMB SCENES 
YOUNGER CRIME GUY BAD COURT FIVE Word USED HERA SONNETS LISTED WITHIN ABHORRENCE KANT S MASQUE TANGLED IRIS ABBOT PALM WRIGHT HANDLE REPORT WHITE AIR EDITS MEMBER Median rank 2987 Median rank Median rank 3582 Table 5 14 Comparison of extraneous words in recognition output of Lycidas lecture 5 9 Relation to searchability The results in 5 6 show that the topic adapted language models have a net negative effect on WER and WCR The resulting transcripts are therefore likely to be less readable and in information retrieval terms have lower precision with the introduction of more extraneous words However in relation to the goal of improving searchability not all words are created equal users are more likely to use less common words as search terms Do the topicadapted language models therefore lead to more searchable transcripts or defined in information retrieval terms provide better recall To gain insight into possible qualitative differences in recognition performance between language models differential word 
recognition rates are examined in the Milton lecture Table 5 15 shows the top and bottom groups of words for this lecture where the recognition rate diverges most between the Similarity and HUB4 language models Dictionary Rank is the word s position in a frequency ranked Wikipedia English dictionary 66 Word in CMU Dict indicates whether the word is found in the CMU Pronouncing Dictionary if not pronunciation has been estimated Word in HUB4 Dict indicates whether the word occurs in the vocabulary of the HUB4 language model Recognized with HUB4 LM is the number of times that the word was correctly recognized using the HUB4 language model Recognized with Wikipedia Similarity LM is the number of times that the word was correctly recognized using the Similarity language model Word Freq in Doc is the number of times that the word occurs in the transcript Recognition increase or decrease is the difference in word recognition count between the Similarity and HUB4 language models Word in CMU Dict Word in HUB4 Dict 
Recognized with HUB4 LM Recognized with Wikipedia Similarity LM Word Freq in Doc Recognition increase or decrease Word Dictionary Rank Top 15 words where recognition rate with Wikipedia Similarity LM exceeds HUB4 LM MILTON S POEM GOD HEAVEN KING POET ELDER HIS ORPHEUS COMUS EDWARD MILTON MUSE THEOCRITUS DIODATI 41755 2999 865 4191 291 1919 3913 9 21669 91192 1150 6092 10609 111859 187129 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 5 12 0 6 20 8 43 8 0 5 65 1 0 0 29 17 21 9 14 27 14 49 14 5 10 70 6 5 4 35 44 22 14 18 34 15 64 17 10 16 74 10 5 6 29 12 9 9 8 7 6 6 6 5 5 5 5 5 4 Bottom 15 words where recognition rate with Wikipedia Similarity LM is lower than HUB4 LM THERE S ABLE ALL CAN T DEATH I M NOW THAN FOR HERE LOOK THIS AND THAT IT S 1009 520 28 768 272 273 98 57 4 154 563 8 2 5 164 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 10 40 4 21 5 16 9 49 12 12 114 127 136 39 0 6 36 0 17 1 12 5 44 7 7 108 109 118 19 6 11 44 7 24 9 22 11 54 32 15 144 175 161 48 3 4 4 4 4 4 4 4 5 
5 5 6 18 18 20 Table 5 15 Word recognition comparison for Lycidas lecture with HUB4 and Similarity LMs 67 It is notable here that words whose recognition rates are increased by the Similarity LM are generally less common as reflected by dictionary frequency rank and longer words whereas those for which the recognition rate has decreased are more common shorter words This example suggests that the adapted LM may be improving recognition of specialist vocabulary at the expense of overall recognition better recall but poorer readability To explore this hypothesis systematically across all lectures word recognition performance is examined across four word rank frequency groupings using a 1 5 million word frequency dictionary derived from English Wikipedia words of 3 more or characters ordered from most to least frequent Words with dictionary frequency rank of 1 to 1 000 Words with dictionary frequency rank from 1 000 to 10 000 Words with dictionary frequency rank from 10 000 to 100 000 Words with dictionary 
frequency rank above 100 000 Figure 5 2 shows the distribution of all words in the set of transcripts across these rank frequency groups with around 90 of words falling into the top two categories and the remaining 10 in the bottom two 1K 10K 25 10K 100K 9 100K 1 1K 65 Figure 5 2 Transcript word distribution by frequency rank group 68 To illustrate the analysis of recognition performance by word frequency rank Table 5 16 shows the transcript and recognition hypotheses with the HUB4 and Similarity language models of the opening sentences of the Milton lecture Words with frequency rank of 10 000 and below are highlighted Transcript before conditioning The best way I think to introduce the central issues of this wonderful poem Lycidas is to return to Milton s Comus So yet once more and I promise this will be one of the last times that we look back at Milton s mask but yet once more let s look at Comus Now you will remember that the mask Comus was everywhere concerned with questions of the power of well the 
strangely intertwined questions of the power of chastity on the one hand and the power of poetry on the other the best way to buy thank to introduce the central issues of of it s a wonderful column was so this is is to return set to milkens common so we can once more i promise this will be one of the last times that we look back at hilton s masked but what s yet once more let s look at our comments making remember now the mass comments was everywhere concerned with questions of the power of well because strangely intertwined questions of the power of chassis on the one hand the power of poetry on the other the best ways i think to introduce the central issues that of this wonderful paul was a bus is used to return set to milton s comus odette whence more i promise this will be while the last times that we look back at milton s masque that once yet once more lives lookout a comments making remember that the masque comus was everywhere concerned questions of the power of boudica strangely intertwined questions 
of the power of chassis on the one hand the power of poetry on the other HUB4 language model Wikipedia similarity language model Table 5 16 Transcription of opening sentences of Lycidas lecture In this short example the Similarity LM has recognized 6 of the 9 highlighted words correctly compared to only 2 of 9 for the HUB4 LM Of these Milton and Comus are likely search terms whereas strangely and intertwined are memorable but are less likely to be used for discovery purposes Figure 5 3 presents the Word Correct Rate for words in each frequency rank group for each lecture by language model For example the WCR for 69 astr160 100 80 60 40 20 0 1K 1K 10K 10K 100K 100K 100 80 60 40 20 0 1K beng100a 100 80 60 40 20 0 1K 10K 10K 100K 100K 1K beng100b 1K 10K 10K 100K 100K eeb122 100 80 60 40 20 0 1K 1K 10K 10K 100K 100K 100 80 60 40 20 0 1K engl220 100 80 60 40 20 0 1K 10K 10K 100K 100K 1K engl291 1K 10K 10K 100K 100K engl330 100 80 60 40 20 0 1K 1K 10K 10K 100K 100K 100 80 60 40 20 0 1K hist116 100 80 60 40 20 0 1K 
10K 10K 100K 100K 1K hist202 1K 10K 10K 100K 100K phil176 100 80 60 40 20 0 1K 1K 10K 10K 100K 100K 100 80 60 40 20 0 1K plsc114 100 80 60 40 20 0 1K 10K 10K 100K 100K 1K psyc110 1K 10K 10K 100K 100K rlst152 100 80 60 40 20 0 1K 1K 10K 10K 100K 100K Figure 5 3 Partial Word Correct Rate by word frequency rank groups 70 5 10 Ranked Word Correct Rate Metric Examining recognition accuracy by word frequency rank is therefore helpful in providing a better characterisation of the performance of topic adapted language models in recognizing less common words To simplify such analysis a single metric is proposed Ranked Word Correct Rate RWCR n RWCR n is defined as the Word Correct Rate for all words in the document which are not found in the first n words in a given general English word dictionary d with words ranked from most to least frequent At n 0 RWCR is identical to WCR and may diverge as n increases thus To illustrate the effect of a value for n of 10 000 Figure 5 4 shows the cumulative Word Correct Rate for 
three example lectures by language model where there are significant moderate and negligible differences in recognition performance between the models In these graphs the transcript words are arranged in inverse frequency rank order from least frequent x 1 to most frequent right most word The dotted red line indicates the position of the word with frequency rank 10 000 and thus the separation between language models given by the metric RWCR 10K While the cut off value of 10 000 is in some senses arbitrary the examples suggest that this metric provides reasonable insight into differences in recognition performance while still being based on a sufficient proportion of total words so as not to be too idiosyncratic Table 5 17 shows the performance of the HUB4 and Similarity LMs across all lectures for five metrics the four metrics shown in 5 6 Table 5 9 and the RWCR 10K metrics Linguistic metrics Language models HUB4 Similarity Difference Sum of unique OOV words 1 007 735 272 Average Perplexity 324 248 76 
Recognition metrics Average WER 40 8 41 6 0 8 Average WCR 71 7 68 4 3 3 Average RWCR 10K 46 0 54 9 9 0 Table 5 17 Comparison of recognition performance for HUB4 and Similarity LMs RWCR 10K improves by 9 from the HUB4 to Similarity LM even though WER worsens on average by 0 8 and overall WCR worsens by 3 3 Using the RWCR 10K metric it appears therefore the topic adapted language models are successful in improving recognition of less common words although they do so at the expense of recognition of more common words and thus overall accuracy 71 100 astr160 80 60 40 20 0 1 100 101 201 301 401 501 601 701 801 901 1001 engl220 80 60 40 20 0 1 100 101 201 301 401 501 601 701 801 901 1001 1101 1201 1301 1401 1501 beng100b 80 60 40 20 0 1 101 201 301 401 501 601 701 801 901 1001 1101 dotted line dictionary frequency rank of 10 000 Figure 5 4 Cumulative Word Correct Rate by inverse word frequency rank 72 To investigate whether the RWCR 10K metric shows a uniform improvement from HUB4 for the Wikipedia language models 
the recognition performance of the three language models using the RWCR 10K metric for individual lectures is presented in Table 5 18 with the best performing LM highlighted green Lecture astr160 beng100a beng100b eeb122 engl220 engl291 engl300 hist116 hist202 phil176 plsc114 psyc110 rlst152 Average RWCR 10K HUB4 LM 47 1 55 2 68 0 35 2 44 2 46 2 40 2 53 5 31 0 27 2 59 0 38 4 52 4 46 0 RWCR 10K Table 5 18 Ranked Word Correct Rate 10K by lecture and language model In all cases both the Similarity and 5 11 Correlation of metrics To investigate how the different metrics relate to each other the relative performance of the Similarity LM to the HUB4 LM is shown in Table 5 19 across five metrics OOV words Perplexity WER WCR and RWCR 10K with positive effects highlighted in green RWCR 10K exhibits a greater range than either WER or WCR and appears the most helpful metric in characterising the performance of the topic adjusted language models in relation to searchability A decrease in out of vocabulary words is a 
reasonable predictor of RWCR performance where the number of OOV words actually increases improvement in RWCR is lowest One exception to this is hist202 which shows improvement in OOV but still a relatively low RWCR improvement 2 3 possibly on account of the high initial word error rate for that recording 61 1 73 Lecture astr160 beng100a beng100b eeb122 engl220 engl291 engl300 hist116 hist202 phil176 plsc114 psyc110 rlst152 Average Absolute HUB4 WER 35 2 35 4 31 9 39 9 43 3 32 5 47 2 39 6 61 1 48 9 31 8 42 1 41 2 40 8 Decrease in unique OOV words 27 36 1 37 15 1 48 10 34 1 22 28 38 21 Decrease in perplexity 53 106 33 87 191 60 28 35 18 189 148 25 87 76 Decrease in WER 0 6 2 2 4 7 1 0 1 7 3 3 0 3 2 8 0 8 0 7 1 5 2 4 0 2 0 8 Increase in WCR 3 2 0 9 6 4 3 0 1 1 4 2 3 6 4 5 3 7 3 5 1 3 5 1 2 9 3 3 Increase in RWCR 10K 22 9 14 2 1 1 13 9 8 6 1 8 7 8 4 3 2 3 5 7 11 2 12 1 11 1 9 0 Table 5 19 Relative performance of Similarity LM to HUB4 LM by lecture across four metrics Finally to investigate the strength of the 
relationship between the metrics Perplexity WER WCR and RWCR 10K the statistical correlation is calculated using the set of results for the 13 lectures each with the HUB4 Correlation Perplexity WER WCR RWCR 10K 0 05 0 12 0 27 0 73 0 75 0 63 Perplexity WER 0 05 WCR 0 12 0 73 RWCR 10K 0 27 0 75 0 63 Table 5 20 Correlation of WER WCR Perplexity and RWCR 10K metrics Perplexity shows weak correlation with all other metrics which is somewhat counterintuitive but suggests that optimizing language models for low perplexity is not necessarily a good strategy for improving recognition performance WER WCR and RWCR are strongly correlated with each other 0 63 to 0 75 showing that they are related but different measures For RWCR 10K the strong correlation with WCR and WER suggests that the choice of 10 000 as a frequency cut off has validity 74 6 Improvements and further directions 6 1 Improving recognition of common words The analysis of the recognition results presented in Chapter 5 shows that the topicadapted language 
models are less successful than the reference model at recognizing more common words especially the top 1000 by frequency rank for example as shown in Figure 5 3 This clearly impacts on readability of the resulting transcript As an ideal result would be to generate transcripts which are both more searchable and more readable than with the generic reference model improving recognition rates for common words is a worthwhile goal Analysis of differential word recognition rates for example as in Table 5 15 shows that recognition failures of words in this group are not a question of vocabulary i e whether the words are included in the language model or not but are a consequence of the language model s construction Areas to investigate include Altering the size and shape of the language model in particular the number of unigrams vocabulary size and the number of trigrams which can be limited by applying a frequency cut off A key question is whether excluding low frequency trigrams would negatively impact 
recognition of specialist vocabulary Adjusting the size and composition of the input corpus for example using a larger set of sentences for the generic language model adjusting the vocabulary balance between the generic and topic derived language model and the method of interpolation used to create the topic adapted language model Closer alignment of genre HUB4 is derived from broadcast news transcripts which in some respects may be closer in genre to spoken lectures than Wikipedia articles A collection of lecture speech transcripts ideally verbatim transcripts so that disfluences and repetitions are more accurately modelled could be used as an additional source of language modelling data for the topicadapted language models 6 2 Iterative similarity modelling The current approach generates a language model for a topic by proceeding from lecture metadata such as title to keywords from keywords to seed articles and from seed articles to related articles Article similarity is further determined transitively if 
A is similar to B and B is similar to C then A is similar to C While this may be a weak assumption the strength of the relationship between the seed articles and the lecture contents is also unknown so the method uses a broad and coarse net to gather possibly related articles However once a transcript has been created it is possible to short circuit the chain of inferences above and re run the article harvesting process against a large set of articles calculating similarity to the hypothesis transcript directly rather than to the parent article This process could be repeated iteratively as illustrated in Figure 6 1 each time generating a progressively more adapted language model 75 First pass Start Iterative process Transcript hypothesis Initial topic keywords Expanded set of topic keywords Seed articles from Wikipedia search Seed articles from Wikipedia search Wikipedia Crawler follows links to articles similar to parent article Wikipedia Crawler follows all links Articles similar to transcript are selected 
Collection of Wikipedia Articles Collection of Wikipedia Articles Custom Language Model Custom Language Model Speech Recognition Engine Speech Recognition Engine Figure 6 1 Iterative Similarity Modelling 6 3 Improving pronunciation accuracy Table 5 12 shows that the recognition rates of words with estimated pronunciation is significantly poorer than for words contained in the CMU Dictionary Approaches which could yield improvements include Increasing the size of the pronunciation dictionary i e adding manually vetted pronunciations to the dictionary Including more than one pronunciation hypothesis in the dictionary Identifying words and names which may originate from other languages and using the appropriate non English pronunciation dictionary or estimated pronunciation using a model trained from the appropriate language 76 6 4 Generalizability to other languages Although the approach described here has been investigated only for lectures in English the technique should in theory be generalizable to other 
languages The main constraint is likely to be the size of the Wikipedia for the target language While English Wikipedia is currently the largest and projected to plateau at around 4 4 million articles 9 other Wikipedias have in excess of 750 000 articles and seem likely to continue to grow over time as illustrated in Figure 6 2 78 Figure 6 2 Article Count for the Ten Largest Wikipedias 2001 2007 Other language specific resources and requirements include A pronouncing dictionary and model for g2p estimation Sentence boundary detection rules or model Punctuation rules or other constraints for text conditioning UTF8 support in all tools 6 5 Examining user search behaviour The methodology and evaluation followed here have rested on the assumption that users are likely to search with less common terms and that these can be characterised in general by their position in a frequency ranked dictionary Applying the discipline of user centred design to examine user search behaviour may provide insights as to how users 
construct search terms for particular goals and navigate the results and thus possibly generate new approaches to optimizing recognition performance to improve search success 77 7 Conclusions 7 1 Implementation and analysis To investigate the research questions a prototype system was developed to generate topic adapted dictionaries and language models from Wikipedia for each of a set sample lectures from Open Yale Courses Two strategies for extracting topic specific subsets of Wikipedia articles were investigated a 7 2 Increasing transcript searchability with topic adapted language models created from Wikipedia articles harvested by the Similarity Crawler Returning to the research questions posed in 1 8 the first sub question is To what extent do topic adapted language models created from Wikipedia produce more searchable transcripts than those created using a generic reference language model With the methodology described in Chapters 3 and 4 the evaluation results discussed in Chapter 5 show that topic 
adapted language models created from Wikipedia articles harvested by the Similarity Crawler produce results of marginally worse accuracy than those produced by the reference HUB4 language model Table 5 10 with an average increase in Word Error Rate WER of 0 8 absolute and an average decrease in Word Correct Rate WCR of 3 3 absolute However examination of the accuracy of the topic adapted language models by word frequency rank shows that the topic adapted language models increase recognition accuracy for less frequent words while decreasing accuracy for more frequent words Figure 5 3 The notion of searchability is formalized through the metric Ranked Word Correct Rate RWCR following the assumption that searchability is associated with recall word accuracy for less frequent words Using the metric RWCR 10K Ranked Word Correct Rate for words below 10 000 in a frequency ranked dictionary the topicadapted language models created using the Wikipedia Similarity Crawler outperform the reference HUB4 language model by 
an average of 9 absolute Table 5 18 Thus while the topic adapted language models produce transcripts which are less accurate overall and thus less readable the improvement in RWCR suggests that they are more searchable than those produced by the HUB4 reference language model 78 A positive relationship between RWCR and searchability is assumed here However this hypothesis should be confirmed through further research such as user trials which fell outside the scope of this preliminary study 7 3 Assessing the effectiveness of an article similarity metric when creating topic adapted language models using a Wikipedia article crawler The second sub question identified in 1 8 is To what extent do topic adapted language models created from Wikipedia using a crawler strategy bound by an article similarity metric produce more searchable transcripts than those created from Wikipedia using a 7 4 Overall The overall research question is How can English Wikipedia be used as a language corpus for the unsupervised topic 
adaptation of language models to improve the searchability of lecture transcripts generated by an automatic speech recognition engine Chapter 4 describes a process for using Wikipedia to generate topic adapted language models for a speech recognition engine to improve the searchability of lecture transcripts The evaluation results in Chapter 5 show the resulting transcripts are less accurate than those produced by the reference language model but are potentially more searchable as they have greater accuracy with respect to less frequent words which are more likely to be used as search terms 79 Chapter 6 suggests avenues for investigation to improve accuracy further so that it is not necessary to trade off readability for searchability Possible strategies include adjusting how the language models are created to improve their quality using iterative similarity modelling to further refine the language model from the initial hypothesis and improving pronunciation accuracy 80 References 1 2 Jan Martin Lowendahl 
Hype Cycle for Education 2011 Gartner Inc 29 Jul 2011 J Copley Audio and video podcasts of lectures for campusbased students production and evaluation of student use Innovations in Education and Teaching International vol 44 no 4 pp 3 4 5 6 7 8 9 10 Marquard Stephen Recognizing specialized vocabulary with large dictionaries Truly Madly Wordly Open source language modelling and speech recognition 25Mar 2011 Online Available http trulymadlywordly blogspot com 2011 03 recognizing specialist vocabulary html 11 J Gauvain G Adda L Lamel and M Adda Decker Transcribing broadcast news The limsi nov96 hub4 system in Proc ARPA Speech Recognition Workshop 1997 vol 56 p 63 12 L Rabiner and B H Juang Fundamentals of speech recognition Prentice Hall Englewood Cliffs New Jersey 1993 13 J Baker L Deng J Glass S Khudanpur C H Lee N Morgan and D O Shaugnessy Research developments and directions in speech recognition and understanding Part 1 IEEE Signal Processing Magazine vol 26 no 3 pp 81 17 D B Paul and J M Baker The design 
for the wall street journal based CSR corpus in Proceedings of the workshop on Speech and Natural Language HLT 91 Harriman New York 1992 p 357 18 Jonathan Fiscus John Garofolo Mark Przybocki William Fisher and David Pallett 1997 English Broadcast News Speech HUB4 Linguistic Data Consortium Philadelphia 19 E Leeuwis M Federico and M Cettolo Language modeling and transcription of the TED corpus lectures 2003 Online Available http doc utwente nl 55870 Accessed 05 Nov 2010 20 C Munteanu Useful Transcriptions of Webcast Lectures PhD Thesis University of Toronto 2009 21 S Furui K Maekawa H Isahara T Shinozaki and T Ohdaira Toward the realization of spontaneous speech recognition introduction of a Japanese priority program and preliminary results in Sixth International Conference on Spoken Language Processing 2000 22 K Bain S H Basson and M Wald Speech recognition in university classrooms Liberated Learning Project in Proceedings of the fifth international ACM conference on Assistive technologies Assets 02 
Edinburgh Scotland 2002 p 192 23 E Luppi R Primiani C Raffaelli D Tibaldi I Traina and A Violi Net4voice new technologies for voice converting in barrier free learning environments Final Report Net4Voice Project 135446 LLP 1 2007 1 IT KA3 KA3MP Jul 2010 24 L Lamel G Adda E Bilinski and J L Gauvain Transcribing lectures and seminars in Ninth European Conference on Speech Communication and Technology 2005 25 H Yamazaki K Iwano K Shinoda S Furui and H Yokota Dynamic language model adaptation using presentation slides for lecture speech recognition in Proc Interspeech 2007 pp 33 T Niesler and D Willett Language identification and multilingual speech recognition using discriminatively trained acoustic models in Multilingual Speech and Language Processing 2006 34 K Kato H Nanjo and T Kawahara Automatic transcription of lecture speech using topic independent language modeling in Sixth International Conference on Spoken Language Processing 2000 35 T Niesler and D Willett Unsupervised language model adaptation for 
lecture speech transcription presented at the 7th International Conference on Spoken Language Processing Denver Colorado 2002 vol 144 p 413K 36 D Willettt T Niesler E McDermott Y Minami and S Katagiri Pervasive unsupervised adaptation for lecture speech transcription in Acoustics Speech and Signal Processing 2003 Proceedings ICASSP 03 2003 IEEE International Conference on 2003 vol 1 p Proceedings ICASSP 03 2003 IEEE International Conference on 2003 vol 1 p 84 64 S Atitallah Speech indexation in Replay Bachelor s thesis ETH Zurich 2009 65 R Kheir and T Way Inclusion of deaf students in computer science classes using real time speech transcription in Proceedings of the 12th annual SIGCSE conference on Innovation and technology in computer science education ITiCSE 07 Dundee Scotland 2007 p 261 66 M Wald Synote Accessible and Assistive Technology Enhancing Learning for All Students Computers Helping People with Special Needs pp 85 80 S P Ponzetto and M Strube Knowledge derived from Wikipedia for computing 
semantic relatedness Journal of Artificial Intelligence Research vol 30 no 1 pp 86 Appendix 1 Software and Data Sets Open Source Software Toolkits Sphinx 4 1 0 beta https cmusphinx svn sourceforge net svnroot cmusphinx trunk sphinx4 Last Changed Rev 10998 Last Changed Date 2011 05 31 12 38 52 0200 Tue 31 May 2011 SphinxBase 0 7 sphinx_lm_convert https cmusphinx svn sourceforge net svnroot cmusphinx trunk sphinxbase Last Changed Rev 11011 Last Changed Date 2011 06 06 15 26 59 0200 Mon 06 Jun 2011 MIT Language Modeling Toolkit v0 4 http mitlm googlecode com svn trunk Last Changed Rev 48 Last Changed Date 2010 11 29 23 59 06 0200 Mon 29 Nov 2010 Phonetisaurus grapheme to phoneme g2p framework http code google com p phonetisaurus Revision 895e2ba6b4b0 28 May 2011 Gensim topic modelling toolkit v0 7 8 http radimrehurek com gensim https github com piskvorky gensim zipball 0 7 8 gwtwiki toolkit v3 0 16 http code google com p gwtwiki Used for parsing Wikimedia markup to plain text OpenNLP toolkit v1 5 1 incubating 
http incubator apache org opennlp Used for sentence boundary detection Sphinx HUB4 Models HUB4 acoustic model http sourceforge net projects cmusphinx files Acoustic 20and 20Language 20 Models US 20English 20HUB4 20Acoustic 20Model HUB 4 Binary Trigram Language Model in HUB4_trigram_lm zip available from http sourceforge net projects cmusphinx files Acoustic 20and 20Language 20 Models US 20English 20HUB4 20Language 20Model 87 Language Resources Wikipedia full text dump enwiki latest pages articles xml bz2 as at 15 Feb 2011 downloaded from http download wikimedia org enwiki latest Google books n gram dataset English 20090715 http books google com ngrams datasets The CMU Pronouncing Dictionary version 0 7a available from https cmusphinx svn sourceforge net svnroot cmusphinx trunk cmudict FST phonetisaurus model trained from CMUdict 0 7a available from http www gavo t u tokyo ac jp novakj cmudict 0 7a tgz opennlp English sentence detector model http opennlp sourceforge net models 1 5 en sent bin 88 Appendix 2 
Source code Selected source code used in this project is available from the svn repository maintained by the Centre for Educational Technology UCT The links below are to the versions of the code used in the project with the specific versions of the packages listed in Appendix 1 Source code and scripts are licensed under the Apache 2 0 license http www apache org licenses LICENSE 2 0 html except where noted otherwise Java and python code for Wikipedia plain text export and Wikipedia Crawler http source cet uct ac za svn people smarquard wikicrawler trunk p 10355 Scripts used to generate the custom language models set up recognition jobs evaluate the output and calculate metrics http source cet uct ac za svn people smarquard sphinx scripts p 10355 89 Appendix 3 Open Yale Courses lectures Selected Lectures Audio recordings and transcripts of lectures from Open Yale Courses at http oyc yale edu used in accordance with Terms of Use described at http oyc yale edu terms Charles Bailyn Frontiers and Controversies in 
Astrophysics Yale University Open Yale Courses http oyc yale edu Accessed June 8 2011 License Creative Commons BYNC SA Mark Saltzman Frontiers of Biomedical Engineering Yale University Open Yale Courses http oyc yale edu Accessed June 8 2011 License Creative Commons BYNC SA Stephen Stearns Principles of Evolution Ecology and Behavior Yale University Open Yale Courses http oyc yale edu Accessed June 8 2011 License Creative Commons BY NC SA John Rogers Milton Yale University Open Yale Courses http oyc yale edu Accessed June 8 2011 License Creative Commons BY NC SA Amy Hungerford The American Novel Since 1945 Yale University Open Yale Courses http oyc yale edu Accessed June 8 2011 License Creative Commons BY NC SA Paul Fry Introduction to Theory of Literature Yale University Open Yale Courses http oyc yale edu Accessed June 8 2011 License Creative Commons BY NC SA Joanne Freeman The American Revolution Yale University Open Yale Courses http oyc yale edu Accessed June 8 2011 License Creative Commons BY NC SA 
John Merriman European Civilization 1648 1945 Yale University Open Yale Courses http oyc yale edu Accessed June 8 2011 License Creative Commons BY NC SA Shelly Kagan Death Yale University Open Yale Courses http oyc yale edu Accessed June 8 2011 License Creative Commons BY NC SA Steven Smith Introduction to Political Philosophy Yale University Open Yale Courses http oyc yale edu Accessed June 8 2011 License Creative Commons BY NC SA Paul Bloom Introduction to Psychology Yale University Open Yale Courses http oyc yale edu Accessed June 8 2011 License Creative Commons BY NC SA Dale Martin Introduction to New Testament History and Literature Yale University Open Yale Courses http oyc yale edu Accessed June 8 2011 License Creative Commons BY NC SA 90 Lecture Transcripts Reference transcripts from the selected Open Yale Courses lectures derivative works in terms of the Creative Commons BY NC SA license are archived at http source cet uct ac za svn people smarquard datasets oyc transcripts Source URLs for the 
lecture transcripts are http oyc yale edu astronomy frontiers and controversies inastrophysics content sessions lecture21 html http oyc yale edu biomedical engineering frontiers in biomedicalengineering content sessions session 5 cell culture engineering http oyc yale edu biomedical engineering frontiers in biomedicalengineering content sessions session 9 biomolecular engineering engineering of http oyc yale edu ecology and evolutionary biology principles of evolution ecologyand behavior content sessions lecture34 html http oyc yale edu english milton content sessions session 6 lycidas http oyc yale edu english american novel since 1945 content sessions session 12thomas pynchon the crying of lot 49 http oyc yale edu english introduction to theory ofliterature content sessions lecture15 html http oyc yale edu history the american revolution content sessions lecture08 html http oyc yale edu history european civilization 16481945 content sessions lecture06 htm http oyc yale edu philosophy death content sessions 
lecture13 html http oyc yale edu political science introduction to politicalphilosophy content sessions lecture02 html http oyc yale edu yale psychology introduction topsychology content sessions lecture05 html http oyc yale edu religious studies introduction to newtestament content sessions lecture26 html 91 Appendix 4 Sphinx Configuration http source cet uct ac za svn people smarquard datasets sphinx4 hub4 oyc sphinx custom xml xml version 1 0 encoding UTF 8 Sphinx 4 Configuration file HUB4 language model config frequently tuned properties property name logLevel value INFO Used in standardActiveListFactory initial 30000 1E 60 property name absoluteBeamWidth value 30000 property name relativeBeamWidth value 1E 80 Used in wordActiveListFactory initial 22 1E 30 property name absoluteWordBeamWidth value 22 property name relativeWordBeamWidth value 1E 30 Used property property property in LexTreeLinguist and LargeNGramModel name languageWeight value 10 5 name wordInsertionProbability value 2 name 
silenceInsertionProbability value 1 Component names property name frontend value epFrontEnd property name recognizer value recognizer word recognizer configuration component name recognizer type edu cmu sphinx recognizer Recognizer property name decoder value decoder propertylist name monitors propertylist component The Decoder configuration component name decoder type edu cmu sphinx decoder Decoder property name searchManager value wordPruningSearchManager component The Search Manager component name wordPruningSearchManager type edu cmu sphinx decoder search WordPruningBreadthFirstSearchManager property name scorer value threadedScorer property name pruner value trivialPruner property name acousticLookaheadFrames value 1 8 property name logMath value logMath property name activeListManager value activeListManager property name buildWordLattice value false property name relativeBeamWidth value 1E 65 property name growSkipInterval value 8 property name linguist value lexTreeLinguist property name 
checkStateOrder value false 92 property name keepAllTokens value true component The Active Lists component name activeListManager type edu cmu sphinx decoder search SimpleActiveListManager propertylist name activeListFactories item standardActiveListFactory item item wordActiveListFactory item item wordActiveListFactory item item standardActiveListFactory item item standardActiveListFactory item item standardActiveListFactory item propertylist component component name standardActiveListFactory type edu cmu sphinx decoder search PartitionActiveListFactory property name logMath value logMath property name absoluteBeamWidth value absoluteBeamWidth property name relativeBeamWidth value relativeBeamWidth component component name wordActiveListFactory type edu cmu sphinx decoder search PartitionActiveListFactory property name logMath value logMath property name absoluteBeamWidth value absoluteWordBeamWidth property name relativeBeamWidth value relativeWordBeamWidth component The Pruner component name trivialPruner 
type edu cmu sphinx decoder pruner SimplePruner TheScorer component name threadedScorer type edu cmu sphinx decoder scorer ThreadedAcousticScorer property name frontend value frontend property name isCpuRelative value true property name numThreads value 0 property name minScoreablesPerThread value 10 property name scoreablesKeepFeature value true component The linguist configuration component name lexTreeLinguist type edu cmu sphinx linguist lextree LexTreeLinguist property name wantUnigramSmear value true property name wordInsertionProbability value wordInsertionProbability property name silenceInsertionProbability value silenceInsertionProbability property name fillerInsertionProbability value 2 property name unitInsertionProbability value 1 0 property name addFillerWords value false property name languageModel value ngramModel property name languageWeight value languageWeight property name logMath value logMath 93 property property property property property property component name dictionary value 
dictionary name unigramSmearWeight value 1 name cacheSize value 0 name generateUnitStates value false name acousticModel value hub4 name unitManager value unitManager The Dictionary configuration component name dictionary type edu cmu sphinx linguist dictionary FastDictionary property name dictionaryPath value file models cmudict cmudict 0 7a_SPHINX_40 property name fillerPath value file models wsj noisedict property name addenda value file models extradict extra hub4 saurus dic property name addSilEndingPronunciation value false property name allowMissingWords value false property name createMissingWords value true property name wordReplacement value lt sil gt property name unitManager value unitManager component The Language Model configuration component name ngramModel type edu cmu sphinx linguist language ngram large LargeNGramModel property name location value file models hub4lm language_model arpaformat DMP property name unigramWeight value 0 7 property name maxDepth value 3 property name logMath value 
logMath property name dictionary value dictionary property name wordInsertionProbability value wordInsertionProbability property name languageWeight value languageWeight component The acoustic model configuration component name hub4 type edu cmu sphinx linguist acoustic tiedstate TiedStateAcousticModel property name loader value sphinx3Loader property name unitManager value unitManager component component name sphinx3Loader type edu cmu sphinx linguist acoustic tiedstate Sphinx3Loader property name logMath value logMath property name unitManager value unitManager property name location value file models hub4opensrc cd_continuous_8gau property name dataLocation value component The unit manager configuration component name unitManager type edu cmu sphinx linguist acoustic UnitManager 94 The frontend configuration component name epFrontEnd type edu cmu sphinx frontend FrontEnd propertylist name pipeline item audioFileDataSource item item dataBlocker item item speechClassifier item item speechMarker item item 
nonSpeechDataFilter item item premphasizer item item windower item item fft item item melFilterBank item item dct item item batchCMN item item featureExtraction item propertylist component component name audioFileDataSource type edu cmu sphinx frontend util AudioFileDataSource component name dataBlocker type edu cmu sphinx frontend DataBlocker component name speechClassifier type edu cmu sphinx frontend endpoint SpeechClassifier component name speechMarker type edu cmu sphinx frontend endpoint SpeechMarker component name nonSpeechDataFilter type edu cmu sphinx frontend endpoint NonSpeechDataFilter component name premphasizer type edu cmu sphinx frontend filter Preemphasizer component name windower type edu cmu sphinx frontend window RaisedCosineWindower component name fft type edu cmu sphinx frontend transform DiscreteFourierTransform component name melFilterBank type edu cmu sphinx frontend frequencywarp MelFrequencyFilterBank property name minimumFrequency value 133 3334 property name maximumFrequency 
value 6855 4976 property name numberFilters value 40 component component name dct type edu cmu sphinx frontend transform DiscreteCosineTransform component name batchCMN type edu cmu sphinx frontend feature BatchCMN component name featureExtraction type edu cmu sphinx frontend feature DeltasFeatureExtractor Miscellaneous components component name logMath type edu cmu sphinx util LogMath property name logBase value 1 0001 property name useAddTable value true component config 95 Glossary AI AM API ARPA Arpabet ASCII ASR CHIL CMU CMUdict CNN CSAIL CSPAN ETH IPA IR KHz LDA LL LM LSA LSI 96 LVCSR MDE MIT MP3 NIST NLP NPR OOV OYC PCM PLSA REPLAY RT RWCR SaaS SI SONIC Sphinx4 SUMMIT TED TF IDF TIMIT US USA UTF8 WAV WCR WER WFST WSJ XML Large Vocabulary Continuous or Connected Speech Recognition Minimum Discrimant Estimation Massachusetts Institute of Technology Moving Picture Experts Group MPEG Audio Layer 3 audio format National Institute of Standards and Technology Natural Language Processing National Public Radio 
broadcaster Out of vocabulary words Open Yale Courses Pulse code modulation digital audio format Probabilistic Latent Semantic Analysis Lecture capture system developed at ETH 97 
6	a	1 Feature Fusion for Efficient Content Based Video Retrieval Machiel Visser Student Member IEEE Abstract Content based video retrieval is a complex task because of the large amount of information in single items and because databases of videos can be very large In this paper we explore a possible solution for efficient similar item retrieval In our experiments we combine relevant feature sets together with a learned Mahalanobis metric while using an efficient nearest neighbor search algorithm The efficient nearest neighbor algorithms we compare are Locality Sensitive Hashing and Vantage Point trees The two options are compared to several baseline systems in the general video retrieval framework We used three sets of features to test the system SURF features color histograms and topics The topics where extracted using a Latent Dirichlet Allocation topic model We show that fusing the individual feature sets with a learned metric improves the performance upon the best individual feature set The feature fusion 
can be combined with an efficient nearest neighbor search algorithm to reduce the number of exact distance computations with limited impact on retrieval performance Index Terms Content based video retrieval feature fusion metric learning efficient retrieval nearest neighbor search locality sensitive hashing vantage point trees I I NTRODUCTION To use a large database of videos effectively it is essential to have a good retrieval mechanism that enables the user to focus on only that what is important to him Retrieval can be active querying of the database by the user but it could also mean that the user gets recommendations based on his interests or search history Video retrieval has as goal to find and only find the relevant items to a query It is a very broad area of research and has overlap with many other research areas such as information retrieval speech recognition pattern recognition and computer vision Most of the techniques used for video retrieval are not unique to that domain The task of video 
retrieval can be divided in two video main categories 1 exact copy retrieval and 2 similar item retrieval The main problem of exact copy retrieval is to distinguish between similar items and manipulated versions of the same item For similar item retrieval it is important to distinguish between similar and dissimilar items with respect to a chosen definition of similarity This paper considers the problem of using content based features from videos in a similar item retrieval system using an example video as a query We do not consider the detection of certain a priori concepts within a video Content based features are the features that are extracted from the video itself The other common source of features is meta data which is information that is attached to the video 13 Both sources of information are suitable to base the similarity measure between videos on Content based features can be used in combination with meta data to enhance the performance but here we will look solely at the implications of building 
a retrieval system based on content based features Multiple features can be extracted from a video such as visual region descriptors or output from a speech recognizer 14 To use those features for a similarity measure for videos they need to be combined in a certain manner Combining features for a classification or comparison task is called feature fusion The features can be fused directly which is called early fusion or the decisions can be fused after using the individual feature sets for retrieval which is called late fusion Using a similarity measure for retrieval can become slow when the database to retrieve from is large For a useful retrieval system it is important that the time complexity does not scale linearly with the number of items in the database To accomplish this there exist several more efficient near neighbour search algorithms that make use of efficient structures to reduce the number of exact distance computations In this paper we will show that it is possible to improve retrieval 
performance in an early fusion scheme with a learned metric over the feature space Second we will show that this system can be improved by an efficient nearest neighbour algorithm in terms of the number of exact distance computations with limited impact on retrieval performance It is important that the fusion scheme and the near neighbour algorithm are compatible This paper is organized in the following way We will start with an overview of the related work about features similarity measures feature fusion and efficient near neighbour search Then the specific techniques that we use are discussed in more detail Followed by the experiments we have conducted to answer the questions raised and the results of these experiments The paper ends with a conclusion and discussion on the results of the experiments II R ELATED WORK A Similarity There are two distinct information sources that can be used to derive a similarity measure for similar item retrieval meta data and content based information Meta data is the 
information that is attached to an item such as title tags comments or usage statistics Content based information is information extracted from the item itself The former is more common nowadays because the information is directly available without extraction techniques and is a more compact representation of the video compared to content based features The main problem with meta data is the manual annotation step that makes the features less reliable Content based retrieval can 2 be useful when large amounts of data need to be annotated automatically or to complement the meta data based retrieval to improve performance 26 For content based retrieval systems the definition of similarity between items is very important Sometimes there is a domain specific measure of similarity already available for the task But in most situations the choice of similarity measure is not obvious 23 The similarity of two items can be defined as a domain specific function over the items or in a more general way as a measure of 
how close items are in the feature space The similarity of two items is then related to how close they are in the feature space given a measure of distance But closeness in a given space is not a well defined property there are multiple ways to quantify closeness or distance in a space 6 Distances over vector spaces are typically defined by a metric or the norm of the difference between two vectors Most common is the Euclidean distance metric or L2 norm Besides general metrics that can be applied to every Rd space it is possible to use side information and construct a Mahalanobis metric from that specific to one Rd space Side information can be class labels or a pair or triple wise similarity There exist several methods to learn a metric based on side information 34 B Modalities and features A video can be seen as a combination of several modalities of data Modalities here can be understood as data observed trough different senses such as visual information and auditory information Each modality contains 
information that can be relevant for the task 27 To capture the information from a modality one or multiple features can be extracted Individual features are generally not able to capture all the relevant information from an item therefore often multiple features are used In the visual modality there are several classes of features such as structure color and motion features The classes of features in the auditory domain are speech music and general audio features 10 The optimal set of features is different for every application and is constrained by the desired performance computation time and storage space requirements Although computation speed and storage space are increasing they are not unlimited and therefore choices need to be made for practical applications 36 C Fusion of features After the extraction of features possibly from different modalities the features need to be combined for the task Features can be used directly for certain tasks but it is often better to process the features first using 
domain knowledge to make them more suitable Several fusion schemes are possible but the main distinction is between early and late fusion In an early fusion scheme the features from different modalities are first combined in for example one feature vector and then used for classification or comparison A late fusion scheme uses the individual features first for classification or comparison and then combines the results of this 28 Other fusion schemes are possible such as double fusion which is a combination of complete early and late fusion schemes 21 It was also proposed to adapt the fusion scheme to the incoming query by grouping queries in several classes with their own optimal fusion scheme 18 There is no consensus on whether early or late fusion is better in general it really depends on the application at hand the features that are used and the type of task one wants to accomplish 19 Early fusion is capable of exploiting the feature correlations that will be lost after the decision stage but late fusion 
is in general a lot simpler and therefore often preferred D Metric learning One way to do early fusion is with a learned metric because it scales the distances of features with different ranges The general idea of metric learning is to construct a metric for a certain space using side information By using side information it is expected that the metric reflects certain characteristics of the feature space better than standard metrics in the context of a specific task And later in the decision stage this should result in a lower error A Mahalanobis distance metric is one such metric and most common in this setting 17 There are several types of side information possible for metric learners Most ideal would be to have the real similarities for pairs of items as similarity constraints But since the real similarity is often ill defined and manual annotation would take too much time constraints must be based on available information Information that is available or would be easy to obtain are the class labels 
Classes can be the categories or genres of the items Most metric learners require pair wise similarity constraints but others require triple wise constraints 34 A Mahalanobis metric A can be used directly in a nearest neighbor setting to measure distance between items But it can also be decomposed to a transformation G on the feature space such that Xnew GX The original metric and the transformation are equivalent in that they both contain all information and are related as A GT G 17 The decomposition can be done for example with Singular Value Decomposition SVD as A U SV and since metrics are symmetric U and V are identical Transformation G can then be obtained from the SVD as G U S T Learning a metric based on a set of constraints is a complex problem and given a set of constraints it in general impossible to satisfy them all 9 Because of this metric learning algorithms are framed as optimization problems that go for approximations to the exact solution The earlier approaches made use of semi definite 
programming methods such as 35 but often use eigenvalue decompositions to solve the problem which is quite expensive Other noteworthy methods are Relevant Component Analysis 25 that works under the assumption that class covariances are equal The Large Margin Nearest Neighbour 32 algorithm uses triplets with a similar and a differently labelled example as constraints and tries to create a margin between them Learning algorithms exist also to directly learn the feature space transformation G such as Neighborhood Component 3 Analysis NCA 12 More recent methods use iterative updates of the metric to satisfy the constraints while minimizing a regularized loss function between the learned metric A and a standard metric A0 such as the Euclidean distance The Pseudo Metric Online Learning Algorithm POLA 24 still uses eigenvector decompositions in this setting Other algorithms such as LogDet Exact Gradient Online LEGO use the exact gradient in the update step 16 Information Theoretic Metric Learning ITML uses Bregmans 
cyclic projection algorithm to update the metric which is quite efficient compared to other metric learners 9 Therefore we will use the ITML metric learner in this paper E Time complexity and reduction For practical retrieval applications it is important that the answer to a query is retrieved in real time There are two aspects of the retrieval process that can be optimized for efficient retrieval 22 First the number of items n that are compared is high if an exhaustive search conducted There exist several efficient retrieval algorithms that reduce the number of comparisons such as space partitioning methods and hashing methods The other aspect is the feature space dimensionality d that influences the time a pair wise comparison takes Several dimension reduction techniques exist Dimension reduction aims to reduce the original set of features to a smaller set of features One way is to select the best n features and throw out the remaining features Another way is to extract new features from the original 
features such that the new features contain as much of the relevant information as possible and not much irrelevant information This transformation of the input data can be linear such as PCA or non linear For video retrieval it is more important to use space partitioning or hashing methods to reduce the time complexity because n is in real systems much larger than d though it can be preceded by a dimension reduction step In an exhaustive search algorithm the query is compared to every item in the database and the results are then ranked This is almost intractable for practical applications The order of the time complexity of a query search can be reduced significantly by using an appropriate indexing structure Space partitioning structures are good for this but hashing methods that use an approximate similarity measure are also common 1 Space partitioning methods divide the space in a set of smaller spaces This is often done hierarchically and the resulting structure is then a tree Vantage Point trees which 
are a specific type of Binary Space Partitioning are an example of this Hashing methods such as locality sensitive hashing create one or more hashes of the items 2 It is important that those hashes capture the chosen definition of similarity approximately III A PPROACH The main idea of our approach is to combine an early feature fusion scheme with an efficient nearest neighbour search algorithm for content based video retrieval We use the Information Theoretic Metric Learner ITML for fusion of the features to transform the feature space after the concatenation to a single feature vector With the fused feature vectors we do the retrieval using two different methods 1 Locality Sensitive Hashing LSH and 2 Vantage Point VP trees The first step of the entire system is processing a video before inserting it in the database or using it to query the database Processing consist of segmentation and the extraction of several features from the visual and audio streams The next step is the fusion of the different 
features into one set of features This set of features can then be used for retrieval The two systems are compared with each other and with a couple of baseline systems that will be explained in the experiments section A Fusion with Information Theoretic Metric Learning The idea of ITML is to satisfy distance constraints while minimizing a loss function The purpose of the loss function is to ensure that the resulting metric generalizes well to other data This is done by minimizing the difference of the metric A from a standard metric A0 to prevent over training J V Davis et al 9 formulate the problem of metric learning as minimizing the differential relative entropy also called Kullback Leibler divergence between two Gaussians parametrized by A and A0 The Gaussian corresponding to A 1 1 exp 2 dA x This minimization problem is than subjected to the following similarity and dissimilarity constraints over items in the feature space where S and D are the sets of similar and dissimilar items respectively and u 
and l are thresholds dA xi xj u dA xi xj l i j S i j D 2 3 In 9 it is shown that the problem of minimizing the relative entropy can in this case be expressed as minimizing a LogDet matrix divergence over A and A0 which is a convex function over the cone of positive definite matrices The constraints can be written as products over the feature space items and the metric And to guarantee feasibility of the problem slack variables are introduced in the formulation of the problem where regulates the trade off between the minimization and the constraints This leads to the following minimization problem min Dld A A0 Dld diag diag 0 s t A 0 4 tr A xi xj xi xj T c i j tr A xi xj xi xj c i j T i j S i j D To solve this problem iterative projections are made of the current metric onto one single constraint by Bregman 4 projections until a set convergence threshold is reached One update using a single projection is as follows where is the Lagrange multiplier for the current constraint At 1 At At xi xj xi xj T At 5 The 
parameters for this algorithm that are not automatically chosen by the algorithm are hF rT x b 8 w For p we use 2 because we work in a Euclidean space and the b in the numerator is to randomize the bin boundaries P stable distributions have the desirable property that the p norm of i i d variables has the same distribution up to scaling and location as the individual variables h r A x Fig 1 LSH with p stable hash function One projection from a high dimensional space captures only a small portion of the discriminative information that the items have To capture enough discriminative information in the hashes of items LSH uses multiple hash functions and concatenates them into one hash g g x h1 x hk x 9 Pr h x h y sim x y 6 Several different hash functions can be used in this setting Most practical implementations make use of projection based hash functions A projection based hash function projects items onto a randomly oriented line in the high dimensional feature space The standard projection based functions 
divide the line in two and assigns one bit to the item x h r A x sign rT x 7 Another option is the family of p stable distribution functions that divide the line in several bins with a chosen width w and assigns an integer to the item x The probability that similar items end up in the same bucket gets smaller when k is larger To counter this effect LSH uses also multiple tables with their own hash function To retrieve from an LSH structure every table is queried and the results from all the tables are ranked together In the case of p stable projection based hash functions which we will use here there are three parameters to be set before an LSH index can be constructed The number of hash functions k that determines how discriminative the buckets are The buckets should not be too small otherwise nothing will be retrieved The number of hash tables L that increases the probability of retrieving similar items but only up to a certain level because otherwise it becomes an exact nearest neighbour search again The 
last parameter is the bin width which strongly depends on the type of data Because only a small number of the buckets are nonempty it is common to apply a second level standard hash to the buckets This way the LSH structure does not grow exponentially in memory with the number of hash functions k The transformation can be included in the random hash planes of projection based hash functions Due to integration within the hash functions no transformation of the input features is required Both the standard projection based hash function as well as the p stable hash functions can be used with the transformation G 5 hr A x sign r Gx rT Gx b hr A x w T 10 11 is always larger than k A metric cannot be integrated in the algorithm itself but a combination can be made by first transforming the features before applying the algorithm IV E XPERIMENTS A Dataset The experiments are performed on a dataset of videos from the on line video sharing site blip tv The set consisted of 1967 videos with varying length and 
resolution some videos were only a minute long while others were over an hour long All videos came in the Ogg container format using Theora for video encoding and Vorbis for audio encoding From the total set we took 50 for training and 50 for testing the systems For testing particular configurations we drew random subsets from the test set All videos came with genre labels speech transcripts and meta data The speech transcripts were extracted using a state of the art speech recognizer To set the parameters for optimal retrieval performance we varied all three parameters k L and w over the range of possible values From this three dimensional space we tested 100 configurations at random to find the best configuration 31 2 Vantage Point trees Peter N Yianilos 37 is the first to refer to this algorithm but it was introduced a couple of years earlier What makes Vantage Point trees more suitable for some applications than other Binary Space Partitioning methods is the spherical partitioning of the space General 
Binary Space Partitioning algorithms divide the space recursively into two parts by dividing the space with a hyper plane Because the partitioning is done with hyper planes the exact distances are not preserved in the structure which is important for retrieval systems that rank the results according to their distance to the query Vantage Points trees partition the space by recursively taking a point and dividing the remaining region into two parts One part is the region that is within a certain threshold from chosen point and the remaining region is the other part of the binary partition Fig 2 2 dimensional VP tree 20 A search through the structure for nearest neighbours with respect to a query is started at the root node The depth of the search is controlled by parameter Children of a node are considered as potential nearest neighbours if they lie on the right side of the threshold of the current node The algorithm maintains a short list of size k and parameter is set at the distance of the last node in the 
short list As soon as the shortlist is filled parameter is lowered such that only nodes closer than the farthest in the short list are considered The number of exact distance computations in this algorithm depends on how balanced the tree is and the position of the query in the tree Because the algorithm guarantees to find k items given that there are at least k items available the number of exact distance computations cannot be reduced up to zero A minimum number of distance computations is required that Fig 3 Examples of extracted video frames B Features Feature extraction was done using a couple of open source tools and libraries The visual features were extracted using functions from the OpenCV libraries for C in combination with FFmpeg for extraction from the container format And the audio stream was extracted and converted to the right format with FFmpeg To extract the visual features from a video we divided them into segments that have similar visual characteristics The boundaries of these segments 
were detected where there was a low histogram correlation between the first and last frame 6 within a sliding window which is a simple but effective way to detect segment boundaries 11 We used the middle frame for feature extraction but another option would be to use multiple frames and combine the extracted features 1 Region descriptors To describe local structures in the extracted frames we used 128 dimensional Speeded Up Robust Features SURF 3 We used SURF features instead of ScaleInvariant Feature Transforms SIFT because it is significantly faster and can be done on the GPU Because of the dimensionality and the variation it is not possible to use SURF features directly or even by calculating a histogram from them The common approach is to use the Bagof Words model so that the actual features are a histogram over clusters of SURF features A Bag of Words model consists of a codebook that defines which SURF features are assigned to what cluster A lot of variations are possible in the Bag of Words approach 
with SURF features such as different clustering algorithms the number of clusters soft versus hard cluster assignment multiple scales or integration of color We used a variant on mean shift clustering for non uniform clustering of a set of training features into 2000 clusters 29 The features were only extracted from the grayscale layer and the codebook used soft cluster assignment 2 Colour histogram To complement the SURF features with a representation of the colors in the frames we used a histogram of the hue values from the HSV color space with 180 bins We chose to use only the hue values because they describe colors and are expected to be more discriminative and relevant to measure similarity While the saturation and value layers of the HSV color model are less discriminative and might vary depending on post processing choices 3 Topics Although the given speech transcripts were of good quality we chose another speech recognizer to extract slightly better speech transcripts in a more suitable format The 
speech was extracted using the open source speech recognizer Sphinx4 8 with the Gigaword trigram language model 30 using a 20k dictionary and the HUB4 acoustic model 7 We used a topic model to determine the most prominent topics in the spoken content A topic model describes the relations between words and a set of topics Latent Dirichlet Allocation LDA 5 was used to build the topic model using approximately 3 million English Wikipedia articles for training 4 The actual features extracted using this model from the videos are the histogram values of the topics We used a model with 250 topics Using fewer topics would reduce the performance while increasing the number of topics would not improve performance 31 C Baseline systems For both aspects of our problem statement fusion of the features and efficient nearest neighbour search we need good baseline systems to compare them with The most important baseline for the fusion of features with a metric is retrieval with just the concatenation of features in an 
exhaustive search This way we show the effect of using a learned metric for feature fusion The efficient nearest neighbour search algorithms are tested with and without metric and compared to the corresponding exhaustive search systems As an absolute baseline we did retrieval with random results using only the class priors This is the absolute minimum that a system without any features will have We compared this with the retrieval systems using the individual feature sets to show what can be done without any kind of fusion Besides just concatenating the features and early fusion with a learned metric we also tested a system with a late fusion scheme Late fusion is done by first converting the distances Dx obtained with the individual feature sets to probabilities of being similar using Bayes rule 33 where x identifies the feature set P S Dx P Dx S P S P Dx 12 P S can be taken out because it is independent with respect to the distances The individual probabilities are then fused by multiplying them to obtain 
one score P Dx S is estimated using the class depend distances and P Dx is estimated using all distances D Measures For practical systems the precisions at k where k is often set at 10 is a very useful measure because it takes only the results into account that are shown on the first page of results in a practical application Recall is important because although a typical application can only show a certain number of results on the first page we still want to retrieve as many of the relevant items as possible Relevant Precision at 10 Recall items of first 10 retrieved 10 Relevant items retrieved Total relevant items We combined the precision at 10 and the recall by taking their harmonic mean or F1 measure such that we can set it out against the number of exact distance computations We use the number of exact distance computations as a measure of the how efficient the retrieval algorithms are V R ESULTS We have analysed the two systems by comparing their performance to the baselines as mentioned in the 
previous section For all systems that use an exhaustive search the number of exact distance computations is always the maximum Therefore it is not useful to set them out against the number of distance computations unless they are compared to the efficient retrieval algorithms Because all items are retrieved the recall will always be 1 The absolute baseline random retrieval using the genre priors has a harmonic mean of precision at 10 and recall of 0 220 7 A Early fusion with metric learning Below in table I we report the performance of several systems that use an exhaustive search We have separate system for the individual feature sets that show how well suited the features are for this task Then we concatenate the features into one feature vector For all these four systems we add a learned metric to evaluate the improvement due to the metric For both cases with and without metric we reported the performance with the harmonic mean of precision at 10 and recall In the last column we report the relative 
improvement of the system due to the metric The use of metric learning in the exhaustive search setting for transformation of the concatenated features as well as for transformation of the individual feature sets improves the retrieval performance significantly The improvement due to a learned metric on the concatenation of all features results in the largest relative improvement But it is also the best system in absolute performance System Topics Colour histogram SURF features Concatenated Baseline 0 552 0 466 0 380 0 456 With metric 0 627 0 484 0 502 0 670 Improvement 13 6 3 9 32 1 46 9 System Late fusion Baseline 0 448 With metric 0 537 Improvement 19 9 TABLE II P ERFORMANCE WITH LATE FUSION TABLE I P ERFORMANCE WITH AND WITHOUT METRIC The advantage of late fusion for metric learning is the lower feature space dimensionality in the learning stage This means that learning the metrics takes less time but also that not all relations between features can be exploited by the metric learner Without a metric 
there is no use for late fusion because it does not improve upon the concatenation of features Both with and without a metric the performance of the systems with late fusion is a weighted average of the results of the individual feature sets Especially in the case with the metric the difference with the concatenation of features becomes clear because that system can improve upon the individual features sets and late fusion not We have to note that we only tested one option for late fusion In 33 it is shown that supervised late fusion of retrieval results can outperform unsupervised static fusion rules They give several supervised late fusion schemes of which we only tested one However they also note that none of the fusion schemes both supervised and unsupervised can consistently outperform the individual retrieval results C Efficient retrieval Two options for efficient retrieval were tested Locality Sensitive Hashing and Vantage Point trees We compared them with and without metric The number of exact 
distance computations of efficient nearest neighbour algorithms is not a fixed value but can be varied indirectly by adjusting the parameters of the algorithms For both algorithms we tested several configurations with different parameter settings All features were used for these systems The small variation between different configuration with almost the same number of exact distance computations is due to random selection of test queries In figure 4 below we see random retrieval as baseline Large black dot the exhaustive search without metric Large red dot and exhaustive search with metric Large green dot The small dots are the different configurations of efficient nearest neighbour algorithms where the light red and blue dots are LSH and the darker dots the VP trees The VP trees algorithm reduces the number of exact distance computations by searching only through part of the tree structure that has potential neighbours But because the structure can only be searched in a top down fashion starting from the 
root node and not starting at the query position there is always a minimum number of exact distance computations required LSH can reduce the number of distance computations even further because it starts the search at the query position Although there are differences between the two tested efficient nearest neighbour algorithms both are useful for retrieval purposes They both reduce the number of distance computations significantly without reducing the performance much The effect holds both for the case without a metric as well as with a metric And the behaviour of both algorithms Not every feature set can benefit equally from a learned metric The color histograms improve with only 3 9 while the SURF features improve 32 1 This indicates that bins of the color histogram are equally discriminative while the clusters of SURF features are not equally discriminative The topic features are by themselves the most discriminative which is to be expected because they operate at the semantic level In the table of 
results we see that just concatenating the features has a performance that is a weighted average of the individual features It cannot improve upon the best individual feature set but is not worse than any of them either By applying the metric on the set of concatenated features we get a performance that is better than the best individual feature set The metric learner we used has one disadvantage that could be a problem in some cases The learning stage scales quadratic with the number of feature dimensions We had 2506 feature dimensions in the system with concatenated features but for practical systems with more different feature sets the number of dimensions could be up to 10 times more In that case a different metric learner that scales linear with the feature dimensions is necessary B Late fusion of retrieval results The system with late fusion of the retrieval results of the individual feature sets performs not as good as the systems with concatenated features as can be seen in table II Both with and 
without a metric it performs less than the concatenation of features However using learned metrics does improve the performance 8 1 0 9 0 8 0 7 Concatenation with metric LSH with metric VP tree with metric Concatenation LSH VP tree Random retrieval F1 measure 0 6 0 5 0 4 0 3 0 2 0 1 0 0 0 5 1 1 5 2 2 5 3 3 5 x 10 4 Distance computations Fig 4 Performance of LSH and VP trees with different configurations does not change due to the combination with a metric Precision at 10 and recall against the number of distance computations gives even more insight in the effects of the efficient nearest neighbour algorithms than the harmonic mean of them The precision at 10 does not decrease almost until there are not more than 10 items returned which is expected because only 10 items are needed if the right items are retrieved But the recall drops slowly while reducing the number of distance computations which is to be expected because to keep recall high most instances of the given class need to be retrieved The 
performance of both algorithms is similar in terms of the number of distance computations But there are differences between the algorithms that are relevant when they are used in a practical application LSH requires more memory because it has to store the hash functions for every table while there is almost no overhead over the items themselves for VPtrees Another problem with LSH is the complex parameter configuration 2 or 3 parameter that depend on each other need to be set while the VP trees algorithm in its basic form has only one parameter which is actually not a parameter of the structure but only used for a search through the structure For both algorithms it is possible to update the structure for insertion or deletion of individual items In a LSH structure the item can simply be added or removed from a bucket though this also needs to be done for the second level hash if it is the only item in the bucket Tree structures are in general easy to change by removing an item and connecting the parent and 
the child or by adding an item between an existing pair of parent and child VI C ONCLUSION In this paper we presented a system for video retrieval with content based features The videos we want to retrieve are those that share the same genre label as the query video We focus on two aspects of such a system the fusion of features and an efficient retrieval algorithm We showed that using learned metrics for the fusion of the features from the individual feature sets results in a significant improvement over simple feature vector concatenation Both in the early fusion setting as well as in a late fusion setting the metric gave an improvement in terms of the harmonic mean of precision at 10 and recall But more improvement was possible in the early fusion setting than in the late fusion setting with 46 9 and 19 9 respectively With early fusion the performance of the whole system is above that of the individual features while late fusion cannot improve upon the best individual feature set However learning a metric 
has a time complexity that is quadratic in the number of feature dimensions For a reasonable number of feature dimensions 1 5000 this is not a problem on modern computers but in the domain of multimedia retrieval the number of features can be even higher In that case late fusion might become preferable depending on the hardware available Or a different metric learner that learns in linear or close to linear time with respect to the dimensionality The use of a more efficient nearest neighbour search algorithm gives an improvement for a multimedia retrieval system compared to an exhaustive search in terms of the number of exact distance computations We tested two options for an efficient nearest neighbour algorithm Vantage Point trees and Locality Sensitive Hashing Both reduce the number of exact distance computations per query significantly but the way they do this is completely different However the reduction in performance in terms of less distance computations is the same for both techniques 9 A Further 
research One direction that can be researched further is the combination with unsupervised dimension reduction methods In the proposed system we transform the feature space before or in combination with efficient retrieval But initial tests suggest that the dimensionality of the feature space could be reduced significantly without reducing performance This depends of course on the chosen features Besides the standard unsupervised dimension reduction methods available a different metric learning algorithm that simultaneously reduces the number of dimensions might also be an option Another direction for further research could be the effect of larger databases Here we used a relatively small database compared to real systems It is worth to investigate the behaviour of the efficient nearest neighbour algorithms with different amounts of videos It might also be more difficult to find the right parameter configuration when it takes more time to test different settings For practical applications of this approach 
the aspect that needs attention is the choice of features Here we focused on the fusion of features and efficient retrieval but for practical applications there exist better features A lot of research is done on semantic concept detectors for images and videos These kinds of features might perform better because they close the semantic gap 31 R EFERENCES 1 A Andoni Nearest neighbor search the old the new and the impossible PhD thesis 2009 2 A Andoni and P Indyk Near optimal hashing algorithms for approximate nearest neighbor in high dimensions In Foundations of Computer Science 2006 FOCS 06 47th Annual IEEE Symposium on pages 17 P Jain B Kulis and K Grauman Fast image search for learned metrics In Computer Vision and Pattern Recognition 2008 CVPR 2008 IEEE Conference on pages 
7	a	Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions c ROVER I MPROVING ROVER USING AUTOMATIC E RROR D ETECTION K Abida1 F Karray1 W Abida2 1 Department of Electrical and Computer Engineering University of Waterloo Ontario CANADA Enabling Systems Technology Inc Ontario CANADA 2 Voice ICASSP 2011 1 30 Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions Outline 1 2 ROVER Proposed Approach PMI based Error Detection Integrating Error Detection within ROVER Simulation Results Experimental Framework Experimental Results Summary and Future Directions 3 4 2 30 Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions Motivations Ubiquitous use of Large Vocabulary Continuous Speech Recognition LVCSR Requirements for high accuracy robustness and optimized speech decoders Solutions Enhancing feature extraction techniques Combining speech features Optimizing speech decoders Combining speech decoders 3 30 Introduction ROVER 
Proposed Approach Simulation Results Summary and Future Directions Decoders Output Combination Combine two or more speech decoders output Recognizer Output Voting Error Reduction ROVER Confusion Network Combination CNC Minimum Time Frame WER fWER Compile a new output that yields a reduced Word Error Rate WER 4 30 Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions Outline 1 2 ROVER Proposed Approach PMI based Error Detection Integrating Error Detection within ROVER Simulation Results Experimental Framework Experimental Results Summary and Future Directions 3 4 5 30 Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions ROVER Combination Technique Recognizer Output Voting Error Reduction Developed by NIST in 1997 Goal produce a composite ASR output when the outputs of multiple ASR are available to provide lower WER Baseline technique in system combination field Every new technique reports performance compared to ROVER 6 30 Introduction ROVER 
Proposed Approach Simulation Results Summary and Future Directions ROVER Process Two stage process Combines multiple output into single Word Transition Network WTN Browse the WTN by a voting process to select the output sequence Three voting schemes Frequency of occurrence Frequency of occurrence and Average word confidence Frequency of occurrence and Maximum word confidence 7 30 Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions ROVER Voting Schemes At each location in the composite WTN a score is computed using Score w N w i 1 C w i Ns i is the current location in the WTN Ns is the total number of combined systems N w i the frequency of word w at the position i C w i is the confidence value for word w at the position i is set to be the trade off between using word frequency and confidence scores is optimized via a training stage 8 30 Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions ROVER Voting Schemes cont d Frequency of Occurrence No 
word confidences used 1 Frequency of occurrence used to select the winning word Problem Ties occur very frequently and are broken randomly Frequency of Occurrence and Average Word Confidence Used to overcome the ties problem Overall confidence is obtained by averaging confidence of all occurrences of the same word Frequency of Occurrence and Maximum Word Confidence Overall confidence is obtained by selecting the maximum confidence 9 30 Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions ROVER Shortcomings Scoring mechanism only work if the errors produced by each ASR are different from one system to the other Iterative combination of word transition networks does not guarantee the optimal composite output Vulnerable technique because of the use of confidence values not reliable and much research is still underway to come up with a reliable and robust confidence measuring technique Unable to use more than 1 best word sequence from each recognizer Unable to outvote the 
erroneous ASR systems when only one single ASR output is correct 10 30 Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions PMI based Error Detection Integrating Error Detection within ROVER Outline 1 2 ROVER Proposed Approach PMI based Error Detection Integrating Error Detection within ROVER Simulation Results Experimental Framework Experimental Results Summary and Future Directions 3 4 11 30 Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions PMI based Error Detection Integrating Error Detection within ROVER Proposed Approach Build the composite WTN Remove erroneous token from the composite WTN Apply ROVER voting on the new WTN 12 30 Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions PMI based Error Detection Integrating Error Detection within ROVER Outline 1 2 ROVER Proposed Approach PMI based Error Detection Integrating Error Detection within ROVER Simulation Results Experimental Framework Experimental 
Results Summary and Future Directions 3 4 13 30 Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions PMI based Error Detection Integrating Error Detection within ROVER PMI based Error Detection Neighborhood N w PMI wi wj log P wi P wi wj P wi P wj c wi wj c wi and P wi wj N N Semantic Coherence Harmonic mean SC wi i j n 1 PMI wi wj PMI wi wj i j 1 Arithmetic mean SC wi n i j Maximum SC wi max PMI wi wj Sum SC wi i j 14 30 PMI wi wj Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions PMI based Error Detection Integrating Error Detection within ROVER PMI based Error Detection Algorithm 1 PMI based Error Detection 1 2 3 4 5 Identify the neighborhood N w Compute PMI scores PMI wi wj for all pairs of words wi wj in the neighborhood N w including w Compute Semantic Coherence SC wi for every word wi in the neighborhood N w by aggregating the PMI wi wj scores of wi with all wj wi Define SCavg to be the average of all the semantic coherence measures 
SC wi in N wi Tag the word w as an error if SC w K SCavg 15 30 Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions PMI based Error Detection Integrating Error Detection within ROVER Outline 1 2 ROVER Proposed Approach PMI based Error Detection Integrating Error Detection within ROVER Simulation Results Experimental Framework Experimental Results Summary and Future Directions 3 4 16 30 Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions PMI based Error Detection Integrating Error Detection within ROVER 17 30 Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions PMI based Error Detection Integrating Error Detection within ROVER Integrating Error Detection within ROVER Algorithm 2 cROVER Procedure 1 2 3 4 5 6 Create the composite WTN by aligning the WTNs from the different recognizers for all slots in the composite WTN do Apply the PMI based error detection Remove the detected erroneous words from the slot and 
replace them with the NULL transitions end for Apply ROVER voting mechanism on the new composite WTN 18 30 Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions Experimental Framework Experimental Results Outline 1 2 ROVER Proposed Approach PMI based Error Detection Integrating Error Detection within ROVER Simulation Results Experimental Framework Experimental Results Summary and Future Directions 3 4 19 30 Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions Experimental Framework Experimental Results Outline 1 2 ROVER Proposed Approach PMI based Error Detection Integrating Error Detection within ROVER Simulation Results Experimental Framework Experimental Results Summary and Future Directions 3 4 20 30 Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions Experimental Framework Experimental Results Simulation Framework Nuance V9 and Sphinx 4 HUB4 testing framework Combination Settings V9 with LM98T28 Sphinx 4 
with LM98T28 Sphinx 4 with LMBN99 PMI counts Google one trillion token corpus 13 5 million uni grams 314 8 million bi grams 21 30 Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions Experimental Framework Experimental Results Metric Measures D S I N TP TP and Rec Prec TP FP TP FN Prec Rec Fmeasure 2 Prec Rec TN TP NPV and PPV TN FN TP FP WER 22 30 Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions Experimental Framework Experimental Results Outline 1 2 ROVER Proposed Approach PMI based Error Detection Integrating Error Detection within ROVER Simulation Results Experimental Framework Experimental Results Summary and Future Directions 3 4 23 30 Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions Experimental Framework Experimental Results Error Detector Assessment Figure F measure as of function of K and Aggregation methods with different Context window Sizes 24 30 Introduction ROVER Proposed Approach 
Simulation Results Summary and Future Directions Experimental Framework Experimental Results Error Detector Assessment Figure PPV and NPV Rates Window 20 Max Aggregation 25 30 Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions Experimental Framework Experimental Results cROVER Assessment Figure c ROVER vs ROVER 2 decoders 26 30 Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions Experimental Framework Experimental Results cROVER Assessment Figure c ROVER over ROVER 3 decoders 27 30 Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions Outline 1 2 ROVER Proposed Approach PMI based Error Detection Integrating Error Detection within ROVER Simulation Results Experimental Framework Experimental Results Summary and Future Directions 3 4 28 30 Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions Summary Novel approach to improve ROVER cROVER Augmenting ROVER with contextual analysis 
PMI based error classifier Up to 1 5 in WER reduction 29 30 Introduction ROVER Proposed Approach Simulation Results Summary and Future Directions Future Directions Use alternative error classifiers Combine error classifier Use alternative ASRs and Testing paradigms Investigate computational complexity and scalability of cROVER 30 30 
8	a	Zemanta service Everything you need to know about Zemanta API beside the specification by Andraz Tori Tomaz Solc 01 17 11 Table of Contents Introduction 2 Basics 3 Roles in Zemanta ecosystem 5 Information for authoring application developers 7 Information for publishing platform owners 7 Information for other integrators 7 Suggestions 8 Images 8 Sources of images 8 Licenses 8 Formats and dimensions 9 Description 9 Inclusion guidelines 9 GUI guidelines 10 Related articles 10 Sources of articles 10 Licenses 11 Article metadata 11 Inclusion guidelines 11 In text links 11 Sources of in text links 11 Licenses 12 Link description types and metadata 12 Inclusion guidelines 12 GUI guidelines 13 Tags 13 Sources of tags 13 Licenses 13 Tag delivery 13 Inclusion guidelines 13 GUI guidelines 14 Signature 14 Zemified icon 14 Reblog 14 Hidden pixie 14 User author s preferences 15 Web service introduction 15 POST request 15 1 Obtaining an API key 16 Developer API key 16 API keys for content management systems and platforms 
16 General API description 16 Examples of API usage 16 Python 16 XML encoding example 16 JSON encoding example 17 PHP 17 Perl 19 C 21 Java 23 Proxy server 29 Developer and user support 29 Dictonary 30 Introduction Digitalization of content started by putting written word into ASCII form HTML and web eventually enabled linking and interleaving with other types of media such as images sound and video Flash and Javascript further enabled interactive widgets such as map views Lately the content on the web is moving into direction of explicitly exposing relations between pieces of data General intention of explicit relations is to allow computers to comprehend what pages are saying and use that knowledge to offer better service to humans when interacting with them While authoring text comes naturally for educated human beings many reasons exist why creating fully featured web content is still cumbersome experience Those reasons can be split into two main categories One issue is efficiently of finding the right 
content that should be included or connected to This usually takes a lot of time The other issue is efficiently telling the computer the nature of relationships between our content and external content and data This usually requires skills and knowledge from depths of specifications and standards Zemanta is the service that tries to resolve those two issues by providing semi automatic process of content enrichment to be more appealing to humans and at the same time placing it in correct relations to other content in a way computers can understand At one point in future we would imagine for computers to be able to help us manage our life with the knowledge they have about us Imagine a computer that sends you on a date or one that automatically discovers new career opportunities and arranges a job interview for you Or suggests how to better manage your finances But first things first We need to put structure and candy into plain old text This document tries to provide needed information to a developer that 
wants to use Zemanta API Explanations do not try to formally document the API since that documentation is found at http developer zemanta com docs but provide a broader view about how Zemanta suggestions work and how they should be presented to the author inside authoring application From the infrastructure viewpoint Zemanta web service is a server that application sends content to and gets suggestions from HTTP protocol is used with standard JSON or XML response formats 2 Authoring application such as a content management system provides suggested content to the author and she can select appropriate pieces for incorporation into her own content It is important to be aware that when author publishes her work the content suggestions are baked inside it and Zemanta service does not participate when content is served or read Author Text Suggestions Author Image Published content Tags Related Links Illustration 1 Authoring process with Zemanta This document first describes what Zemanta offers as suggestion what 
information accompanies those suggestions and how that information should be used Zemanta works from infrastructure and technological perspective At the end there are examples of how to use Zemanta in different programming languages Basics Content on the web is generally authored in language called HTML HyperText Markup Language or languages that are later translated to HTML such as BBCode These languages enable linking and inclusion of other materials into web pages 3 For the purpose of this document content consists of title main body of text and supporting content such as links images and tags Zemanta service is a web service that authoring software submits title and text into and gets suggestions as the response When we were deciding on what information to send back to the API client we had the following in mind human needs to decide if suggestion is correct and desired human needs to decide if planned usage complies with the license of suggested content and computer needs to be able to correctly 
incorporate the suggestion into the existing content Zemanta service currently supports four fundamental types of suggestions images related articles in text links markup tags keywords Each of those suggestions can mean qualitative improvement of existing submitted content Images are meant to illustrate or present the topic or concept being talked about Related articles help readers find more about the topic at hand and also make it easy for writer to become engaged and interwoven in the sub web of writers with common interest In text links help readers when they don t know what the people places or phrases mean or let them explore details of very specific concepts mentioned in the text Tags help users navigate between topics inside and across sites Also tags make indexing of content easier and more accurate Basis for Zemanta s suggestions are state of the art algorithms for processing natural language machine learning information retrieval and similar Text at hand is related to background knowledge Newly 
generated and gathered information is used to create context which is in turn used to query internal and external repositories for suggestions 4 Illustration 2 Four basic types of Zemanta suggestions shown in blog writing application Zemanta creators imagined first adopters were going to be bloggers and other people authoring large amounts of web content That is why Firefox browser extension was created as the first delivery vehicle of this technology However we have no illusions about being able to imagine your way of using Zemanta that is why Zemanta API was born API enables integration into open source and proprietary Content Management Systems and content hosting platforms Suggestions can be integrated into word processing applications or email clients Maybe into a game or maybe into a mobile platform All this is possible as long as you adhere to conditions specified in Terms of Service at http developer zemanta com API_terms_of_use If there is anything unclear throughout this document about Zemanta API 
or about Zemanta in general please do let us know via email at support zemanta com if you have technical questions please leave them at developers forums at http developer zemanta com forum if questions are about browser extensions leave them at http getsatisfaction com zemanta Roles in Zemanta ecosystem Five primary roles participate in classical Zemanta ecosystem The diagram shows them in orange 5 Zemanta platform Zemanta web service API Zemanta Publishing platform CMS creator vendor Authoring application CMS Publishing platform owner Content serving Author Reader Published content Reader Reader Reader Illustration 3 Roles in classical Zemanta ecosystem It is not uncommon that a single person or organization acts in many different roles Roles represent Author a person creating content and using Zemanta suggestions CMS creator person or organization providing a Content management software that has Zemanta service and user experience integrated as part of the authoring process Platform owner a person or 
organization owning the specific hosting platform that runs the Content management software supporting Zemanta service and user experience Reader a person or organization consuming the content Zemanta a service provider during content creation process If a single programmer both writes his own CMS runs it and creates content with it he is fulfilling three different roles This is a common usage pattern for enthusiasts experimenting with Zemanta When commercial entities are involved there are cases where CMS creator and platform provider is the same organization some big blogging platforms and cases where publishing platform owner is a different company from CMS creator who sold or open sourced the CMS software which publishing platform owner is using 6 In some cases CMS creator is split into two different entities This happens when existing open or closed source CMS is extended with Zemanta experience by an integrator In such a situation we regard integrator as CMS creator for the scope of this document 
Reader can be wide internet public or when Intranet is concerned it can be just the limited public that has access to a specific document Information for authoring application developers When you step into Zemanta ecosystem you get full access to Zemanta developer portal where you can find documentation about API and this document As long as you are testing the API you should use the API key provided to you When you want to deploy your application for your end users each of them should get their own Zemanta API key so he can control preferences and have personalized suggestions Keep in mind that each single API key allows just 1000 requests per day Please contact Zemanta at support zemanta com for information on how to implement automatic assignment of API keys to each separate author using your software Also contact us if you need larger quota of requests per day Information for publishing platform owners If you want to enable your users to experience Zemanta you need to install a special plug in onto your 
CMS platform and enable it for all users Plug ins are currently available for Wordpress 2 x 3 x and Movable type 4 x 5 x Drupal and Joomla If you are not using any of those extending support to your platform of choice is part of possible commercial relationship There are other options how you can enable your users to experience Zemanta on one by one basis You can recommend to your users to install Zemanta browser extension Currently Zemanta browsers extension is available for Firefox Internet Explorer Chrome Also available is a Windows Live Writer plug in Each of the authors using your platform should get their own Zemanta API key so they can control preferences and have personalized suggestions Please contact Zemanta at support zemanta com for information on how to implement automatic assignment of API keys to each separate author using your software Information for other integrators There are many use cases for use of Zemanta suggestions outside of classical content authoring process We cannot imagine all 
of them and we are eager to hear about them Please contact us at support zemanta com with information about what you created and what you need from Zemanta to make your software even better We are very flexible and would like your input on the Zemanta ecosystem on Zemanta API and the quality of suggestions 7 Suggestions As already explained Zemanta service returns different types of suggestions each of them is discussed here in detail Images A picture is worth thousand words It makes it easy for the reader to associate specific page with his imagination it can explain the topic much more efficiently or it can simply decorate the article it accompanies Sources of images There are multiple sources which Zemanta uses as a basis for image suggestions Wikipedia and Flickr are probably most comprehensive while images from stock image providers are of higher aesthetical quality Zemanta s image collection from Wikipedia includes 5 million images While Flickr provides over two billion photos unfortunately not all of 
them can be used by authors using Zemanta service Most of images on Flickr have license forbidding reuse so that leaves around 160 million images as possible suggestions Because Zemanta uses Flickr API it can not take advantage of Zemanta s internal representation of concepts This means less topically accurate images are suggested The same goes for other external image providers Additional issue is proliferation of intentional and unintentional mistagging of Flickr images This can sometimes cause completely unrelated images being suggested author of this document notes that semantic tagging of Flickr images would probably be the greatest contribution Yahoo could provide for progress of Semantic web It is important to know that images indexed in Zemanta s database and even those offered as suggestions via external sources are not guaranteed to exist at specified URLs Therefore authoring application presenting the images must check if those resources exist and in case they do not fill the space with suggested 
images further down the list Licenses When suggesting images Zemanta tries to filter out those that would undoubtedly need image owner s approval Most of the time that means suggesting images under CreativeCommons or similar licenses public domain images and images that could be used as fair use Zemanta also suggests images from stock photo providers when they allow their low resolution images to be used Zemanta provides necessary license information which author can interpret and decide whether intended use of the image is allowed under the terms of the license It is important for both developers using the API and authors using the suggestions to know that it is impossible for computer to reliably determine if usage of image in specific case is allowed Therefore maximal effort of informing the author is and should be done and the author is responsible for making the final decision API conveys the license along each image inside an attribute named license This attribute is HTML formated and should be 
presented directly to the author as Terms of service demand It sometimes includes a link to location where author can get further information Zemanta can not guarantee that supplied image licenses are correct since they all come from outside sources 8 Formats and dimensions Suggested images are always raster images This means they are either in JPG PNG or GIF format This happens even when original image is vector based SVG since Zemanta suggests links to converted image Each image suggestion consists of three URL attributes one for small medium and large version of the image Zemanta cannot fully control the sizes of images available from different repositories We strive to respond with the small image width near 75 pixels middle sized picture width near 200 pixels and large size image to be the highest resolution of the image available under the given license Attributes height and width contain dimensions of the image available at the URL of the original wlarge version Description Each image suggestion 
includes description attribute This is textual description of what image represents Description may be inaccurate in some cases especially with third party image sources Further information about the image can be found at URL in the attribute source_url This url points to the page inside the image repository that has further human readable information about the image Inclusion guidelines Each image suggestion includes attribute attribution which describes source and author of the image when those are available Data inside the attribute is HTML formated and where possible it includes link to the page of the author image or source Application that provides Zemanta image suggestions to authors has to automatically include that attribution along the image when it is inserted If it does not do so it must provide other ways to allow authors to easily comply with the image license conditions and following precisely Terms of service Zemanta acknowledges that application writers cannot control what happens after 
attribution is inserted into the editing area Authors responsibilities cannot be controlled by software Authors themselves are bound by Terms of service and licenses of specific images From technical viewpoint when author decides to use specific image suggestion application should try to retrieve the inserted image and have it served from author s content serving platform However in certain situations it is not possible to do so Zemanta also strongly suggest to place link to the source_url on the whole image as an anchor Images added via Zemanta should have an upper element enclosing both image and its attribution Class of that enclosing element should be zemanta img This enables Zemanta to behave correctly when already enriched text is submitted to the service and using that information as feedback in the future It is suggested that authoring applications make a copy of image and store it on authoring application server before publishing the image This will guarantee that the image removal or change at the 
primary source such as Wikipedia or Wikimedia commons will not affect presentation of the published content 9 Related articles Letting the reader know aobut related articles accomplishes many things for a publisher Firstly reader experience is better since readers are able to dig deeper into their topic of interest Happy readers are returning readers Secondly important effect of actively linking to sub web of common interest is building loosely knit communities around specific topics Thirdly a lot of blogs support so called trackbacks By linking to those blogs author could automatically get a back link to his page While linking to other sites could be seen as counterintuitive to those monetizing through advertising and benefiting from keeping the user around for as long as possible it is an established fact that on long term sites that are good net citizens receive more visitors Zemanta allows at the same time automatic search for related articles and full control of the author over article inclusion From 
user feedback Zemanta has come to know that many authors only read suggested related articles by themselves and use gained information to write better content while not explicitly linking to sources This is acceptable use of Zemanta service Because of this usage pattern and because of the need of authors to check what articles they are linking to authoring applications should allow for easy visiting of suggested articles before they are included into main body of content Article is probably not the perfect term for describing the piece of content Zemanta suggests a link to albeit the suggestion usually has the form of the article It could also take different form such as a tweet a photo album a photo blog famous quote or similar However articles present the wast majority of these suggestions Sources of articles Zemanta aggregates articles from many different internet sources Those include both mainstream news sites and wider blogosphere with emphasis on highly regarded blogs Zemanta tries to include articles 
from multiple domains multiple world regions and multiple world views If you believe there are topic areas that are covered unsatisfactory please let Zemanta know at support zemanta com In January 2011 around 50000 sources were being followed with new ones being added each day Every source is manually vetted in order to prevent spam from creeping in Zemanta also adds sources because of commercial relationships with the publishers Estimated time for a new article to be included in the index from which article suggestions are made is from few minutes to maximum of two hours We try to have index as fresh as possible Currently January 2011 index consists of all articles seen in last three months but we are trying hard to expand the timeframe Old articles are not being purged but there is bias toward more recent articles in suggestion algorithm Licenses Zemanta only suggests link to the article not its content Therefore author or publisher has no need to comply with any licensing conditions Basic metadata about 
the article is provided by Zemanta title is a string representing the title of the article url points to the article page Zemanta tries to resolve url redirects but that might not always 10 be possible published_datetime represents a date and time when article was published If this information was not available first time article was aggregated by Zemanta harvest time is used Datetime might be in future in some exceptional cases due to wrong data provided by publishers usually because of wrong usage the timezone Larger deviations of published time are heuristically adjusted by Zemanta text_highlight and title_highlight represent search snippet which establishes a logical connection between source article and suggested article Inclusion guidelines Related articles should be added to the main body of text in a way that makes it clear that they represent a connection to a larger body of knowledge about the topic Zemanta browser plug in achieves that by putting the articles into a list with a title saying 
Related articles Related articles added via Zemanta should have class defined as zemanta article If you are using outer element such as fieldset to define a group of related articles use zemanta related as its class This enables Zemanta to behave correctly when already enriched text is submitted to the service and using that information as feedback in the future In text links In text links represent links inside the main body of text that lead reader to the information about very specific concepts and topics that were mentioned Zemanta currently provides links to entities but not to other blogs as in text links Word markup is used to refer to in text links inside the API and API documentation due to more general nature of the term markup Sources of in text links Zemanta uses knowledge databases to establish connection between mentions of specific concepts or topics and the text involved Such databases are Wikipedia IMDB MusicBrainz Amazon book listings and similar Understanding of text is used to find out 
whether specific phrase should be linked to specific page In one context Zemanta may decide a connection is desirable in the other the same link might not be suggested Context is also used to disambiguate between different possibilities when linking for example Apollo the space program and Apollo the Greek god Links to some information providers may include affiliate ID Affiliate ID information is part of preferences of each user Licenses Zemanta does not suggest any content of the pages pointed to by links Therefore there are no need to comply with any licensing conditions for the author and publisher Link description types and metadata Links are not anchored to specific location in the text but instead to substrings of the text This is done because original text might change before author decides to apply a link and it would be extremely hard for authoring software to correctly do the necessary bookkeeping That s why anchor to which a link 11 should be attached is provided as an attribute For each anchor 
it might be sensible to link to multiple destinations and just one should be chosen by the author Possible destination links for specific anchor text are stated in target list of links Each target has a url title and type attribute Some of the types are wikipedia amazon imdb youtube homepage geolocation Types are self explaining New types might be added by Zemanta and authoring software must gracefully handle unknown types New attributes might be available for precise information about specific types The engine does not suggest links to anchors that are already linked with a href HTML statement when content is submitted Inclusion guidelines In text links added via Zemanta should have their class defined as zem_slink This enables Zemanta to behave correctly when already enriched text is submitted to the service including using that information as feedback in the future When Zemanta finds a possible link and the anchor of the link is already partly or fully marked up Zemanta will not return that link as a 
suggestion For example b we are in Washington b D C will prevent Zemanta suggesting correct link to Washington D C page Exception to this rule are links with class zem_slink If anchor to be linked is entirely enclosed in the bold tag link will be returned Zemanta will also not return new link which would have anchor overlapping with already existing link If you want to get maximum accuracy of in text links for further processing strip input text of HTML tags before sending it to Zemanta service GUI guidelines Authoring application creators should pay close attention to enable simple way of unlinking previously selected text Zemanta suggests that GUI element that created the link should change state and on the second click unlink that same link Tags Tag is a relevant keyword or term associated with specific content Labeling by keywords has long been used in scientific publications Its web comeback happened when web users and developers realized tags are a very efficient method of attaching metadata to the 
infromation While lacking formal 12 structure tags can provide valuable navigational enhancements and make it easy for web search engines to comprehend content more fully Lack of formal structure also made it very easy for everyone to tag their content and even content published by others like del icio us does Zemanta makes tagging even simpler User does not need to make up tags instead he is offered a number of possible tags and he just needs to choose appropriate ones preferably via single click From information processing viewpoint tags are metadata that accompanies main body of information They are used by CMS to provide navigational facilities and by search engines as informal clues on what content is about Sources of tags While some other services only try to find the most overrepresented rare words or proper names in the text Zemanta goes deeper when processing content Zemanta offers both tags based on words and phrases that can be found inside author s text and also those that are only topics that 
could represent the content as a whole but are not explicitly mentioned It goes even further and tries to find very concrete items and concepts that are related to what is being said but are only connected through a third piece of information Therefore author can expect topics names and concepts as tags Licenses Tags are pieces of content that are generally not regarded as creative work and therefore not protected under copyright However authors should be aware that brand names can be offered as tags Tag delivery Zemanta currently does not provide any additional information about reasoning why specific tag was suggested There is a confidence factor but it has to be noted that it might be unreliable Zemanta s tags can include whitespaces but will never include commas Inside API and API documentation keyword is a name used for tags since that is more general and understandable outside web domain Inclusion guidelines Authoring application should store selected tags as metadata separately from main body of 
content This is the standard practice in content management systems Zemanta suggests that when tags are rendered they should be marked up semantically so advanced reading applications can make use of them look at Technorati tags specification GUI guidelines Authoring applications should allow for inclusion and exclusion of tags with a single click When there is a longer text field in which tags can be entered and edited authoring application should provide each suggested tag as clickable button 13 Signature Each call to Zemanta suggest method also returns an attribute called signature This has to be automatically added to each published article to fulfill Zemanta Terms of service Authors are then allowed to manually delete the signature from each article Signature is available in different forms which can be chosen via preferences discussed in next capture Main forms of signature are Zemified icon Hidden pixie Zemified icon Zemified icon simply reveals to the reader that author has used Zemanta during the 
process of authoring it represents a way of giving credit to Zemanta s hard work Reblog Reblog functionality was deprecated in summer 2010 Hidden pixie Both Zemified icon and Reblog allow Zemanta to detect which articles were written with its help and eventually include those blogs into its aggregation index User author s preferences Zemanta allows authors to set some preferences regarding how the service works Some preferences affect suggestion algorithms while other affect just the user interface Preferences are currently tied to each separate API key When authoring application calls function suggest preferences one of the attributes that are part of the answer is config_url it points to Zemanta s server where user can set his specific settings Authoring application should give the user ability to open the page at that address in new window Currently user can set following options alignment of the image left or right Amazon ID and if it is set at all styled HTML or unstyled XHTML insertion of suggestions 
type of signature discussed in previous chapter 14 Those preferences that should affect authoring applications are returned as attributes in responsof a preferences API call Please use them accordingly If there are any doubts don t hesitate to contact Zemanta Web service introduction Zemanta web service is a classical type of web service Using standard HTTP GET or POST protocol client sends a request via port 80 to api zemanta com and gets response encoded in JSON XML or RDF XML format Client identifies itself with API key API key is a string that uniquely identifies specific instance of the application that is using the service For example Firefox extension stores API key in the extensions permanent configuration and uses that API key on every request to Zemanta service no matter which blogging platform the user visits the same is true for other browser extensions Zemanta web service includes limitations on the number of requests per day and number of requests per second First one depends on type of the 
account you have default developer accounts allow for 1000 posts per day and 1 post per second When you go beyond these limits you will get back the appropriate error message POST request For long texts it is not possible to encode them into a standard GET request due to limitations in URL length of some web servers and proxies In those cases users can use POST HTTP requests Due to security measures browsers in general prevent cross site POST requests requests to sites different than one from which the page is loaded from However Zemanta correctly sets allow origin headers and therefore all modern browsers allow javascript to easily use POST requests to Zemanta API Zemanta also provides helpers inside the API which enable so called window name json return format and thus POST requests can be used even in older browsers Obtaining an API key There are two ways of obtaining API keys One is meant for developers and the other is available for applications to automatically assign keys to their users authors 
Developer API key Developers and testers initially get only one key for their own usage As a developer you first have to create an account at Zemanta developer portal at http developer zemanta com and then continue to create your own API key You have to enter API keys for content management systems and platforms Zemanta provides a way to assign separate key to each end user author of Zemanta service when 15 Zemanta is integrated into third party CMS or content authoring platform Please contact Zemanta at info zemanta com about the details General API description API is officially documented on Zemanta Developers portal http developer zemanta com docs Please consult documents there for official and formal descriptions of the API This document is mainly concerned about giving you all the info you need on top of formal API description Examples of API usage Examples of how to use Zemanta API in different programming languages Wherever you see string YOUR_API_KEY you should replace it with your own API key Python 
XML encoding example Following example is a simple call to the suggest function import urllib gateway http api zemanta com services rest 0 0 args method zemanta suggest api_key YOUR_API_KEY text Cozy lummox gives smart squid who asks for job pen format xml args_enc urllib urlencode args print urllib urlopen gateway args_enc read JSON encoding example Here is the simplest way to use the API in python using simple json encoding Simplejson library is needed import urllib import simplejson gateway http api zemanta com services rest 0 0 args method zemanta suggest api_key YOUR_API_KEY text Cozy lummox gives smart squid who asks for job pen format json args_enc urllib urlencode args response_raw urllib urlopen gateway args_enc read response simplejson loads response_raw print response 16 print Suggested links for link in response markup links print s link anchor for target in link target print s target url print Suggested related for article in response articles print s n s article title article url print 
Suggested images for image in response images print s n s image description image url_l print Suggested keywords print t for keyword in response keywords print s keyword name PHP This code example comes from Hubert Moreau He posted it to Zemanta developer forums by hmoreau php This are the vars you may need to modify Some may be placed in conf files Some may be generated by your application url http api zemanta com services rest 0 0 Should be in a conf file format xml May depend of your application context text Place here the text you want to be parsed by Zementa May depend of your application context key your_zementa_api_key Should be in a conf file method zemanta suggest May depend of your application context It is esayer to deal with arrays args array method method api_key key text text format format Here we build the data we want to POST to Zementa data foreach args as key value data data data urlencode key urlencode value Here we build the POST request params array http array method POST 17 Content type 
application x www form urlencoded Content length strlen data content data Here we send the post request ctx stream_context_create params We build the POST context of the request fp fopen url rb false ctx We open a stream and send the request if fp Finaly herewe get the response of Zementa response stream_get_contents fp if response false response Problem reading data from url php_errormsg fclose fp We close the stream else response Problem reading data from url php_errormsg Tags Php sample code 18 Here we build the data we want to POST to Zementa data foreach args as key value data data data urlencode key urlencode value Initialisation of curl ch curl_init Setup of the url curl_setopt ch CURLOPT_URL url We want a post request curl_setopt ch CURLOPT_POST 1 Here we give to curl the data we want to send to Zementa curl_setopt ch CURLOPT_POSTFIELDS data We setup the response method of curl curl_setopt ch CURLOPT_RETURNTRANSFER true Execute curl and fetch the result response curl_exec ch Close curl connection 
curl_close ch Perl There is a Net Zemanta module available from CPAN which is a thin Perl wrapper around the API methods http search cpan org avian Net Zemanta It provides a simple object oriented interface and is the preferred way of calling Zemanta from Perl code The following is a minimal example using the module use Net Zemanta Suggest use Data Dumper my zemanta Net Zemanta Suggest new APIKEY YOUR_API_KEY my suggestions zemanta suggest Cozy lummox gives smart squid who asks for job pen print Dumper suggestions If you do not want to introduce the Net Zemanta dependecy you can craft your own code for calling the API Bellow is a minimal example without any error checking which you can use as a template use use use use LWP UserAgent HTTP Request Common JSON Data Dumper 19 my gateway http api zemanta com services rest 0 0 my args method zemanta suggest api_key YOUR_API_KEY text Cozy lummox gives smart squid who asks for job pen format json my ua LWP UserAgent new my response ua request POST gateway args my 
result from_json response content print Dumper result C This code is courtesy of Tom Altman it has been submitted to Zemanta developer forum by him Thank you very much By Tom Altman Please search for yourapihere and replace with your api Good luck This is the _textZ aspx page Page Language C AutoEventWireup true CodeFile _textZ aspx cs Inherits _textZ DOCTYPE html PUBLIC W3C DTD XHTML 1 0 Transitional EN http www w3 org TR xhtml1 DTD xhtml1 transitional dtd html xmlns http www w3 org 1999 xhtml head runat server title TEST Page title head body form id frm runat server table width 800 align center cellpadding 0 cellspacing 0 border 1 tr td align left valign top width 66 asp Label ID lblContent runat server td td align left valign top width 34 asp Label ID lblZem runat server asp label ID CloudMarkup runat server td tr table form body html ok now the _textZ aspx cs using System 20 using using using using using using using using using using using using using System Collections System Configuration System Data 
System IO System Net System Text System Web System Web Security System Web UI System Web UI HtmlControls System Web UI WebControls System Web UI WebControls WebParts System Xml using Newtonsoft Json Converters using Newtonsoft Json Utilities public partial class _textZ System Web UI Page string str I have a dream that one day this nation will rise up and live out the true meaning of its creed We hold these truths to be self evident that all men are created equal I have a dream that one day on the red hills of Georgia the sons of former slaves and the sons of former slave owners will be able to sit down together at the table of brotherhood I have a dream that one day even the state of Mississippi a state sweltering with the heat of injustice sweltering with the heat of oppression will be transformed into an oasis of freedom and justice I have a dream that my four little children will one day live in a nation where they will not be judged by the color of their skin but by the content of their character I have 
a dream today protected void Page_Load object sender EventArgs e lblContent Text str string json getZemanta zemanta suggest str json string xml getZemanta zemanta suggest str xml XmlDocument xmldoc new XmlDocument xmldoc LoadXml xml XmlNodeList xmlnode xmldoc GetElementsByTagName article lblZem Text lblZem Text ul for int i 0 i xmlnode Count i XmlAttributeCollection xmlattrc xmlnode i Attributes XML Attribute Name and Value returned Example Book id 001 lblZem Text lblZem Text xmlattrc 0 Name lblZem Text lblZem Text t xmlattrc 0 Value lblZem Text lblZem Text li a href xmlnode i url InnerText xmlnode i title InnerText a li br lblZem Text lblZem Text ul 21 populateCloud public static string getZemanta string whichMethod string whatContent string whatFormat Uri address new Uri http api zemanta com services rest 0 0 we need to create the web request HttpWebRequest wreq WebRequest Create address as HttpWebRequest we want to post the data so set that here wreq Method POST wreq ContentType application x www form 
urlencoded build string with data for REST StringBuilder d new StringBuilder d Append method HttpUtility UrlEncode whichMethod d Append api_key HttpUtility UrlEncode yourapihere d Append text HttpUtility UrlEncode whatContent d Append format HttpUtility UrlEncode whatFormat break it down to a byte array byte bd UTF8Encoding UTF8 GetBytes d ToString set length write wreq ContentLength bd Length using Stream ps wreq GetRequestStream ps Write bd 0 bd Length response using HttpWebResponse wres wreq GetResponse as HttpWebResponse capture the response StreamReader r new StreamReader wres GetResponseStream return the results return r ReadToEnd Java Example comes from Thomas Francart who posted it in Zemanta developer forums File ZemantaWrapper java package com mondeca test zemanta import import import import import import java io BufferedReader java io ByteArrayInputStream java io IOException java io InputStream java io InputStreamReader java io OutputStreamWriter 22 import import import import java io 
UnsupportedEncodingException java net URL java net URLConnection java net URLEncoder import javax xml parsers DocumentBuilder import javax xml parsers DocumentBuilderFactory import javax xml parsers ParserConfigurationException import org w3c dom Document import org xml sax SAXException author thomas francart mondeca com public class ZemantaWrapper private static String ZEMANTA_API_URL http api zemanta com services rest 0 0 your API key private String key public ZemantaWrapper String key super this key key Calls the suggest API on the given input text with XML format Parse result and return it as DOM param text text to be processed return throws ZemantaException public Document zemanta_suggest String text throws ZemantaException try build parameters String data URLEncoder encode method UTF 8 URLEncoder encode zemanta suggest UTF 8 data URLEncoder encode api_key UTF 8 URLEncoder encode this key UTF 8 data URLEncoder encode text UTF 8 URLEncoder encode text UTF 8 data URLEncoder encode format UTF 8 URLEncoder 
encode xml UTF 8 return doZemantaCall data catch UnsupportedEncodingException e you must be kidding e printStackTrace return null 23 Calls the suggest feedback API on the given input text with XML format Parse result and return it as DOM param text text to be processed return throws ZemantaException public Document zemanta_suggest_feedback String text throws ZemantaException try build parameters String data URLEncoder encode method UTF 8 URLEncoder encode zemanta suggest feedback UTF 8 data URLEncoder encode api_key UTF 8 URLEncoder encode this key UTF 8 data URLEncoder encode text UTF 8 URLEncoder encode text UTF 8 data URLEncoder encode format UTF 8 URLEncoder encode xml UTF 8 return doZemantaCall data catch UnsupportedEncodingException e you must be kidding e printStackTrace return null Calls the preferences API with XML format Parse result and return it as DOM param text text to be processed return throws ZemantaException public Document zemanta_preferences String format throws ZemantaException try build 
parameters String data URLEncoder encode method UTF 8 URLEncoder encode zemanta preferences UTF 8 data URLEncoder encode api_key UTF 8 URLEncoder encode this key UTF 8 data URLEncoder encode format UTF 8 URLEncoder encode format UTF 8 return doZemantaCall data catch UnsupportedEncodingException e you must be kidding e printStackTrace return null 24 private Document doZemantaCall String data throws ZemantaException Document d null try URL url new URL ZEMANTA_API_URL String result readURL url data UTF 8 d parse new ByteArrayInputStream result getBytes UTF 8 catch UnsupportedEncodingException e you must be kidding e printStackTrace catch IOException e throw new ZemantaException Error while calling service result e catch SAXException e throw new ZemantaException Error while parsing service result e catch ParserConfigurationException e throw new ZemantaException Error while parsing service result e return d Utility method Parses given input stream and turn that into a DOM param input return throws SAXException 
throws IOException throws ParserConfigurationException private static Document parse InputStream input throws SAXException IOException ParserConfigurationException DocumentBuilderFactory dbf DocumentBuilderFactory newInstance dbf setNamespaceAware true DocumentBuilder domFactory dbf newDocumentBuilder return domFactory parse input Utility method Calls the given URL with the given POST data param url param data param charset return throws java io IOException private static String readURL URL url String data String charset throws java io IOException StringBuffer buf new StringBuffer 10000 URLConnection c url openConnection c setRequestProperty User Agent 25 c setDoOutput true OutputStreamWriter wr new OutputStreamWriter c getOutputStream wr write data wr flush InputStreamReader sourceContent new InputStreamReader c getInputStream charset BufferedReader in new BufferedReader sourceContent buf setLength 0 String inputLine while inputLine in readLine null buf append inputLine n in close return buf toString File 
ZemantaTest java package com mondeca test zemanta import import import import import import import import import import import import import java io BufferedReader java io File java io FileInputStream java io IOException java io InputStreamReader java io StringWriter java io UnsupportedEncodingException javax xml transform OutputKeys javax xml transform Transformer javax xml transform TransformerException javax xml transform TransformerFactory javax xml transform dom DOMSource javax xml transform stream StreamResult import org w3c dom Document import org w3c dom Node public class ZemantaTest public void test throws Exception System out println Testing zemanta service change with your own API key ZemantaWrapper zemanta new ZemantaWrapper 1234567890AZERTYUIOP Document d zemanta zemanta_suggest readFileContent new File C inputzemanta txt UTF 8 pretty print result System out println toString d UTF 8 26 Pretty prints a DOM param node param encoding return throws TransformerException public static String toString 
Node node String encoding throws TransformerException StringWriter writer new StringWriter Transformer tr TransformerFactory newInstance newTransformer tr setOutputProperty OutputKeys INDENT yes tr setOutputProperty OutputKeys ENCODING encoding tr transform new DOMSource node new StreamResult writer return writer toString Read content of a File into a String param f param charset return throws IOException public String readFileContent File f String charset throws IOException fetch content of the stream StringBuffer buf null try buf new StringBuffer 10000 InputStreamReader sourceContent new InputStreamReader new FileInputStream f charset BufferedReader in new BufferedReader sourceContent buf setLength 0 String inputLine while inputLine in readLine null buf append inputLine n in close catch UnsupportedEncodingException e encoding not supported e printStackTrace return buf toString param args public static void main String args try ZemantaTest test new ZemantaTest test test 27 catch Exception e e 
printStackTrace File ZemantaException java package com mondeca test zemanta author thomas francart mondeca com public class ZemantaException extends Exception public ZemantaException String arg0 Throwable arg1 super arg0 arg1 public ZemantaException String arg0 super arg0 public ZemantaException Throwable arg0 super arg0 Dictonary Zemanta service Author Authoring application Authoring software same as authoring application Reader Developer Platform CMS Flickr 28 Wikipedia 29 
9	a	Using Open Source Automatic Speech Recognition for Conceptual Knowledge Monitoring Ruben Lagatiea Fridolin Wildb Patrick De Causmaeckera Piet Desmeta Peter Scottb KULeuven Kulak Etienne Sabbelaan 52 Kortrijk 8500 Belgium The Open University Walton Hall Milton Keynes MK7 6AA United Kingdom Abstract Useful in many applications text mining techniques offer the possibility to analyze texts and discover their semantic meaning Unfortunately most of these methods only work on textual data but they are equally useful when dealing with spoken conversations In this contribution we show how an open source and freely available Automatic Speech Recognition engine can be used out of the box as a preprocessing step for keyword spotting by first transcribing spoken conversations Can lower quality transcriptions still prove useful for keyword extraction a b 1 Introduction Online video conferences provide an excellent opportunity for students to interact with each other and work together on a project being it to improve their 
language skills writing a paper or designing a software application One of the downsides of this interaction however is that it is still very hard to provide users with relevant feedback Teachers using these tools often replay all recorded meetings and manually grade and provide feedback obviously a time consuming task Moreover in this case immediate feedback would be more useful than delayed feedback Playback of these online events is also important and to assist the user in finding the relevant parts in the vast amount of digital content available to them we need to provide and automatically construct a catalog showing how they are related to each other For both of these tasks techniques exist that provide this functionality such as keyword extraction text categorization text clustering latent semantic analysis and many more which can be summarized as text mining techniques Unfortunately most of these techniques only work on textual data An obvious solution is to introduce a preprocessing step to 
transcribe the spoken text Transcribing speaker and domain independent free speech however is still a daunting task and although many recent advances have improved recognition accuracy it is still far from perfect Luckily for many language processing algorithms an imperfect transcript may be sufficient A good example is keyword spotting Even with a word error rate WER as high as 0 6 the chance of not recognizing any of three subsequent words is only 0 216 WER3 As a result we might not need a stateof the art commercial Automatic Speech Recognition ASR system The objective of this study is therefore to evaluate whether freely available and possibly open source software is mature enough to be used out of the box for keyword spotting It is important that we test this theory on real life preexisting recordings to evaluate the robustness of these systems against low quality data It is well known that while ASR works well in a controlled environment it is still unable to produce a high quality transcription of 
conversational speech 1 In Lagatie et al 2 we showed how keywords generated by this application can be used as feedback for students and as a grading tool for teachers for online spoken conversations by visualizing them as conceptual diagrams In this contribution however we focus on the more technical aspects of this endeavor 2 Related work Back in 1998 Ehsani and Knodt 3 investigated the applicability of ASR in Computer Assisted Language Learning CALL They stated not only that speech technology is an essential component of CALL but that it is in fact ready to be deployed successfully in second language education provided that the current limitations of the technology are understood and systems are designed in ways that work around these limitations They focus however on the use of ASR in very confined situations where the vocabulary and grammar can be easily predicted In our case the conversations are completely open there is no right or wrong answer We are not dealing with evaluating vocabulary or grammar 
use but the conceptual knowledge of the students Detecting conceptual knowledge means being able to detect the key concepts and their relations In recent years a number of different techniques have been proposed and implemented for automated essay scoring An overview is given by Rudner et al 4 Hearst 5 Valenti et al 6 and Dikli 7 These techniques all require textual data so in this paper we are investigating if and how ASR can be used as a preprocessing step for this Algorithms exists specifically developed to perform keyword spotting Most of these algorithms look for a few predefined keywords for instance when doing topic spotting Some contributions therefore construct a finite state grammar that contains the keywords to be detected and include a garbage state for all utterances that were not confidently recognized as part of the grammar 8 Others do not limit their vocabulary and use Large Vocabulary Continuous Speech Recognition LVCSR but look for keywords in the lattice afterwards 9 When the possible 
keywords are not limited and performance is less important we believe the best method is to simply transcribe the text and then use a keyword extraction algorithm on this text afterwards A similar approach was used by Roy and Subramaniam 10 to construct a domain model for call centers They showed that even from noisy transcriptions useful information can be extracted They however used a state of the art ASR system and their taxonomy generation is more than simply finding keywords So the question still remains is an open source ASR system sufficient for keyword spotting 3 Technologies We will be using three different technologies online conferencing speech recognition and keyword extraction It is important we test our theory on real life data so we will be using recordings made by the webbased conferencing tool Flashmeeting developed at the Open University These recordings can then be transcribed using speech recognition There are a number of possible open source ASR engines available From these generated 
transcriptions we can then extract keywords describing the topic of the conversation Again a number of possible algorithms can be used The main criteria here is robustness because errors in the transcriptions are inevitable The results are then passed back to Flashmeeting to be visualized In the remainder of this section we will give a short overview of the technologies we will use in this paper 3 1 Flashmeeting Flashmeeting is an academic research project aimed at understanding the nature of online events and helping users to meet and work more effectively 1 It records all meetings for research purposes and allows users to replay past public meetings Flashmeeting is often used for group discussions by students working on a joint project such as a paper an essay or a software application Flashmeeting is also auto moderated only one speaker is able to broadcast at a time Each speaker is recorded in a separate file and some metadata is kept in a database such as real name timestamp or duration 1 http 
flashmeeting open ac uk Next to broadcasting video and audio users can also send text messages using the chat This is often used whenever a participant wants to add something to the conversation but does not want to interrupt the current speaker It has also proven to be very useful for sending hyperlinks and files Using the techniques described here Flashmeeting is now able to present students and teachers a conceptual graph showing relationships between meetings based on common keywords In Lagatie et al 1 we showed how these graphs can serve as automatically generated additional feedback for students and help teachers in evaluating online spoken conversations 3 2 Open Source Automatic Speech Recognition There are three popular open source automatic speech recognition engines HTK Hidden Markov Model Toolkit CMU Sphinx and Julius HTK has many state of the art ASR features such as vocal tract length normalization VTLN heteroscedastic linear discriminant analysis HLDA and discriminative training with maximum 
mutual information MMI and minimum phone error MPE criteria 11 The Sphinx Toolkit released by Carnegie Mellon University CMU contains less advanced features but in contrast to HTK Sphinx focuses on speed and was one of the first ASR systems to offer support for speaker independent Large Vocabulary Continuous Speech Recognition LVCSR 12 There are currently four versions The latest Sphinx 4 is although less efficient than Sphinx 3 more flexible and it is therefore easier to develop and maintain applications with 13 Julius is a high performance two pass LVCSR decoder for researchers and developers 14 Julius focuses on three factors performance modularity and availability Unfortunately it is at the time of writing only distributed with Japanese models VoxForge offers an English acoustic model for Julius but they have yet to create a dialog manager to transcribe speech2 When comparing these engines there is no real winner Accuracy and performance are very similar especially for LVCSR As a result the choice is 
often based on the available models environmental factors the programmers preference and the development time For all these reasons and because Sphinx is easier to configure we opted to use Sphinx 4 for our test In section 4 we elaborate on the use and setup of this system 3 3 Keyword extraction We use the term keyword here as a synonym of descriptor It is contextual metadata a word or phrase that describes the topic of a text In our case any word can be a keyword except stop words such as the and a There are again a number of available techniques to extract keywords from a text Zhang et al 15 defines 4 categories simple statistical approaches linguistic approaches machine learning approaches and hybrid approaches A similar technique is keyword suggestion or recommender systems Instead of simply extracting keywords from a given text it suggest related concepts which might also be interesting for the user A survey of such systems is given by Adomavicius and Tuzhilin 16 Zemanta3 is a good example of such a 
keyword suggestion engine In Bosnic et al 17 Zemanta is categorized as a semantic entity extraction service The authors compared Zemanta with The Yahoo Term Extraction web service for recommending learning objects Zemanta uses semantic relationships to find related keywords As a result it is more robust than a simple term extraction engine Which natural language processing NLP techniques Zemanta uses is unknown Being a recommender system Zemanta not only finds words in a text it also suggests keywords which might even be missing from the test In our case for example the words Dublin Core were never correctly recognized in one of our meetings but because words like metadata LOM and application profile were the engine suggested Dublin Core as one of the keywords 2 3 http www voxforge org home downloads http www zemanta com 4 Keyword Spotting We perform keyword spotting in two separate steps We first transcribe the meetings and then extract the keywords that are their likeliest descriptors 4 1 Transcribing As 
already mentioned one benefit of keyword spotting is that there is no need for a perfect recognition like in dictation applications In our case precision is more important than recall so we can ignore words that have a low confidence score If it is a keyword chances are that it is pronounced multiple times by different persons in the same conversation increasing the chance of being correctly recognized at least once Writing an application using the Sphinx library that produces transcripts has proven to be relatively easy The API is well documented and the example applications are a good starting point In Flashmeeting each speaker is recorded individually so we also transcribe each segment individually 4 2 Keyword suggestion From these individual transcripts relevant keywords are then extracted by running them through Zemanta Doing this for each segment separately gives us more keywords which we can then filter using what is known as semantic co occurrence filtering 18 By doing so keywords that are not 
related to the other keywords are deleted because they are most likely wrong or not really relevant Extracting keywords per segment can also be useful if we want to compare individuals to suggest possible conversation partners It also enables us to give an overview of the interests of a single person and even show its evolution over time Zemanta offers a Java library to communicate with their web service making it easy to integrate it in our Java application which already uses the Sphinx library The output of this step is a list of keywords for each segment and their relevance scores 5 Sphinx Configuration The recognizer of Sphinx 4 comprises three components the linguist the decoder and the frontend They are all summarized in Walker et al 13 Because we have constructed our own model we will give a short overview of which configuration we used in our setup 5 1 Linguist Sphinx offers multiple implementations of the linguist component depending on the task at hand One of them is the LexTreeLinguist which is 
appropriate for large vocabulary recognition tasks that use large n gram language models 13 Using LVCSR in an ASR system requires a number of linguistic resources First a dictionary is needed with all words to be recognized and their pronunciation Second the system requires two models a so called acoustic model and a language model 5 1 1 Dictionary The domain and vocabulary of the recorded meetings vary and they contain many words that are not present in the English dictionary As a result the dictionary needs to be extended with these words Unfortunately we do not have enough transcripts to extract the words that have actually been said but there is an extensive amount of data available from the text chat The recorded meetings are not necessarily English spoken so foreign languages should be removed from the text chat as well For this we use a classifier trained on part of the Leipzig Corpora Collection implemented using the LingPipe library To increase accuracy we discard sentences with less than 70 
characters Some filtering is also necessary to remove emotional abbreviations e g LOL ROFL and emoticons Chat data also contains a lot of typos so each word occurring infrequently less than 7 times is checked against the default dictionary If it is present there we know it is not a typo and can be added to our dictionary After all this filtering we retained 65 204 sentences Containing 9 790 words of which 1 232 were absent from the default dictionary CMU dict4 In our case we don t need a perfect transcription so we opted not to extend the default dictionary but simply use the dictionary we constructed This means that some words will have a low acoustic score because the correct word does not exist in the dictionary nor does one that sounds alike These words will be filtered out before keyword extraction A limited vocabulary increases speed and many of the words in the default dictionary are rarely used anyway 5 1 2 Acoustic model The acoustic model of Sphinx is implemented as a phonetic Hidden Markov Model 
HMM 12 Such a model describes each phone as a graph with probabilities assigned to the transitions This model thus assigns sounds to each phone used in the dictionary It is typically trained with a corpus of audio recordings and their text transcriptions Flashmeeting has been in use since 2003 At that time a tradeoff was made between quality and network load and as a result all audio has been encoded at 11kHz frequency It is known that recognition performance degrades severely when the input was recorded at a different sampling frequency than the speech signals the system was trained on 19 Unfortunately we do not have enough transcribed audio to train our own 11kHz model and we were unable to find an open source one so we have decided to downsample our input files to 8kHz and use the 8kHz Wall Street Journal WSJ acoustic model This approach was also used in 20 The WSJ model is implemented in Sphinx as a TiedStateAcousticModel 5 1 3 Language model The language model we use is a LargeTrigramModel which offers 
support for true n gram models An n gram describes the probability of a sequence of n words Using this we can more confidently select which word is more probable to have been pronounced Note that the trigram model of sphinx contains unigrams bigrams and trigrams The default language model just as the dictionary is inappropriate The bigram Dublin core would for instance have a very low probability or even be missing from the language model As a result we had to train a new language model again using the data available from the text chat A script was written to filter the corpus file as described above construct the vocabulary and a trigram and then convert the model to the correct binary format required by CMU Sphinx Training the model and constructing the dictionary was done using the CMU Statistical Language Modeling SLM Toolkit 21 5 2 Frontend The frontend is in charge of transforming an input signal to a sequence of features We use the default settings here so we won t deal with these settings in detail 5 
3 Decoder The decoder forms a bridge between the frontend and the linguist It uses features provided by the frontend in conjunction with the searchgraph from the linguist to generate result hypotheses The main component of the decoder is the searchmanager which implements a certain search algorithm to find features In our case we opted to use the WordPruningBreadthFirstSearchManager which performs a frame synchronous Viterbi search as explained in 22 4 http www speech cs cmu edu cgi bin cmudict 6 Fine tuning It is often said that tuning Sphinx is somewhat of a black art5 There over a dozen variables that can be changed often related to each other Most of these allow developers to prioritize speed or accuracy Two of these parameters are highly dependent on the language model The first is the weight of the language model score compared to the acoustic score when calculating the total score The second is the weight of unigrams compared to bi and trigrams in the language model As a preliminary study we will 
limit fine tuning to these two parameters but want to automate the process to a certain extent In future work we will take more parameters into account and use more data The resulting accuracy is highly dependent on the length of the samples the topic the speakers the audio quality and many more factors To counter this effect we took a random set of speakers for a random set of meetings We made sure the segments contained an equal distribution of audio quality length and speaker gender In total we used 42 minutes with an average of 7 minutes and four speakers per meeting These conversations where then manually transcribed To fine tune the two parameters we decided not to use the Word Error Rate WER as is used often in evaluating dictation systems When fine tuning it is not so important to us whether or not the transcription is correct we just want meaningful keywords For this reason we compare the list of keywords constructed for our manual transcription to the keywords found for the transcription provided 
by the system We use the same keyword suggestion algorithm for both transcriptions which means that if the automatic transcription is perfect the same keywords will be found Using the F1 score we can compare both sets of keywords Our goal for this study is then to set the language and unigram weight so that the F1 score is as high as possible There is no clear correlation between the weights and the resulting score A quick test showed that changing a weight resulted in unpredictable changes to the accuracy To find the optimal setting we could use a Random Search RS method However these algorithms can take a long time to converge and automatically transcribing a meeting takes about the same time as its duration so 42 minutes We therefore decided to perform a preliminary test and try to visualize how the F1 is influenced by the two values In figure 1 a we plotted the F1 score for all integer language weights between 0 and 15 From this we concluded that the highest language weight was reached between 8 and 9 so 
in figure 1 b we plotted the F1 score for all language weights between 8 and 9 with increments of 0 1 The best result is reached when the language weight is set to 8 5 but the unigram weight is still left undetermined all we can say is that it should be between 0 2 and 0 8 To better determine this value we looked at the average f score of the keywords found for each speaker segment separately instead of those for each complete meeting Optimizing this would result in a better handling of individual users and shorter audio fragments The results are depicted in figure 2 With a language weight of 8 5 the best results are achieved when using a unigram weight of 0 4 or 0 5 a Figure 1 b 5 http www speech cs cmu edu sphinx models Figure 2 7 Evaluation In the fine tuning process which can be seen as a preliminary effectiveness evaluation we achieved an average F1 score of 87 5 The evaluation of our application could now be done the same way we finetuned the system simply take a random set of meetings manually 
transcribe them an see what f score is achieved The downside of this is that although the f score in this case depicts the quality of the transcriptions it says little about the quality of the keywords The found keywords may be different but therefore not necessarily bad If a number of words are not correctly recognized other keywords might be suggested but they can be just as useful to the user So in our evaluation we would like the usefulness of the keywords to the user be the quality indicator Furthermore we want to know which keywords are redundant or missing and how we can configure the system do deal with these problems Obviously there is no formula to calculate this value automatically so we are planning a user study where we will present keywords to the participants of a meeting and ask them specific questions to evaluate these keywords The results and setup of this user study do not fit into this more technical paper and will therefore be described in future work 8 Conclusions In this contribution 
we have shown how two freely available tools such as the open source ASR system Sphinx and the online semantic entity extraction service Zemanta can be combined to perform keyword spotting for online spoken conversations Preliminary results show that after some fine tuning an average F1 score of 87 5 is achieved In future work we will further fine tune and evaluate this system We are currently working on putting this topic spotting application into the backend of Flashmeeting so that we can collect more evaluation data about how users like the keywords At the moment we process the information on a separate server at night to minimize interference with the online conversations A future approach could be to do this analysis in real time which would allow for more interaction References 1 K Myers and S Singh A Boosting Approach to Topic Spotting on Subdialogues In proceedings of ICML 00 17th International Conference on Machine Learning 2000 2 R Lagatie F Wild P De Causmaecker P Scott Exposing Knowledge in 
Speech Monitoring Conceptual Development in Spoken Conversation IST Africa 2010 Conference Proceedings 2010 3 F Ehsani and E Knodt Speech technology in computer aided language learning Strengths and limitations of a new CALL paradigm Language Learning and Technology 2 1 45 60 1998 4 L Rudner P Gagne E C on Assessment and Evaluation An overview of three approaches to scoring written essays by computer ERIC Clearinghouse on Assessment and Evaluation 2001 5 M Hearst The debate on automated essay grading Intelligent Systems and their Applications IEEE 15 5 22 37 2002 6 S Valenti F Neri and A Cucchiarelli An overview of current research on automated essay grading Journal of Information Technology Education 2 319330 3 118 2003 7 S Dikli An overview of automated scoring of essays Journal of Technology Learning and Assessment 5 1 2006 12 2006 8 I 
10	a	Proceedings of The National Conference On Undergraduate Research NCUR 2011 Ithaca College New York March 31 April 2 2011 Development of the Speech Capabilities of a Tour Guide Robot Fabian Okeke Computer Science Fisk University Nashville Tennessee 37208 USA Faculty Advisor 1Dr Wyatt Newman 1 Electrical Engineering and Computer Science Case Western Reserve University Cleveland Ohio 44106 USA Abstract The EECS Department at Case is developing a tour guide robot In addition to its localization and locomotion capabilities verbal communication with the robot will make interactively attractive Speech capability is being pursued in two parts speech synthesis and speech recognition Speech synthesis involves the conversion of words into vocal sounds For this development open source software Festival and Espeak text to speech TTS systems are being utilized together with VLC media player These speech synthesizers convert simple sentences and text files into wave files and these wave files are played and controlled 
using the VLC interface Speech production is implemented within the system as a network resource which allows for modular development and extensibility Upon receiving a network request to orate a specific script the speech service produces the desired output In continuing work speech recognition is being developed to enable commanding robot behaviors through spoken commands Speaker independent speech recognition is desired for which the Sphinx4 or PocketSphinx open source software is being used on the Ubuntu Linux Operating System The process involves conversion of continuous acoustical signals into words using recognition of phonemes Words are recognized using a grammar library which is manually populated with commands to be recognized by the robot An ongoing challenge will be how to expand this library while maintaining reliable recognition given that a larger database of words will contain more opportunities for audible confusion Through the use of open source software and network based modules this 
project is enabling interactive speech capability for a tour guide robot while providing a foundation for future growth of sophistication Keywords Text To Speech Synthesizer Independent Speech Recognizer Open Source 1 Introduction The EECS Department at Case is developing a tour guide robot In addition to its localization and locomotion capabilities verbal communication with the robot will make interactively attractive For effective communication between the robot and tourists the robot needs to talk and listen appropriately He also needs to process certain commands and carry out corresponding actions when faced with different scenarios For instance In a situation in which Roberto s pathway is blocked by tourists or any obstacles he quickly reacts with Please excuse me so I can move In order to implement all these functions this research work utilizes speech synthesizers in speaking to end users and speech recognizers in listening to them Although other non open source 1 1 Speech Synthesizer Speech Synthesis 
involves the conversion of words and text files into sounds and wave files respectively Frequency slicing is a recent technique applied in speech synthesis It involves the production of wave forms through the combination of one or more incomplete speech segments It can also be used in the production of incomplete speech segments which refers to missing frequency components in a frequency range If it produces incomplete frequencies then it is called Frequency Splicing Components FSC On the other hand complete speech segment contains all the desired frequency components and this can be used in creating Frequency Sliced Segment FSS 2 In this research Espeak and Festival speech synthesizers are utilized These synthesizers are selected amongst other existing speech tools because it is inexpensive and it properly integrates with the other libraries used in this research work The speech synthesizer converts written words or text files to sound files Festival Text To Speech TTS synthesizer receives text words from 
either a text file or from the Linux bash command line Then the received texts are broken down into identifiable dictionary words Based on the selected speaker s 1 2 Speech Recognizer A speech recognizer converts sound waves into words It is the direct opposite of a speech synthesizer Over the years speech recognition has changed through different techniques ranging from the use of Digital Filter DF Fast Fourier Transformer FFT Dynamic Time Warping DTW and the Hidden Markov Model HMM Recent research works have combined Mel Frequency Cepstrum Coefficient MFCC and DTW in order to improve the results of speech recognition3 For this project Sphinx4 speech recognizer is utilized Just like the speech synthesizer Sphinx 4 is selected from the list of numerous speech recognizers because it is free it works properly with the other tools for this project and it can be expanded based on need Sphinx 4 has its beauty in its modularity and plug ability Besides implementing continuous and non continuous large vocabulary vs 
smaller vocabulary an xml configuration file allows varied and dynamic behavior from the speech engine without a need for modifying the source code or recompiling 304 Figure 1 Sphinx 4 Framework Figure 1 above shows the architecture of the Sphinx 4 and its replaceable modules The speech engine of Sphinx 4 consists of 3 main modules the Front End the Decoder and the Linguist The Front End parameterizes input signals into a sequence of Features The Linguist generates the SearchGraph using a language structure and the topological structure of the AcousticModel4 The Acoustic Model consists of a set of Hidden Markov Models HMMs with one HMM per unit5 The Linguist may utilize a Dictionary to map words from the LanguageModel into sequences of Acoustic Model elements The ConfigurationManager deals with the structure of modules at run time while the SearchManager in the Decoder combines the FrontEnd and the SearchGraph from the Linguist to perform actual decoding The behavior of each of these modules can be 
individually configured in the xml configuration file The breakdown of these modules supports the performance of the Sphinx 4 which is a reconstruction and enhancement of other versions of recognizers including Sphinx 3 Any application incorporating Sphinx appears to go through 5 basic steps as in the Load in xml configuration file create Configuration Manager which interprets xml use lookup on Configuration Manager to create Recognizer object the speech engine and audio input data stream go into loop that is based on audio events where the Recognizer analyzes speech to return Results call Result methods to convert speech analysis into text strings or do more advanced operation 6 This research ensures that the returned texts from the 305 2 Methodology 2 1 Design The input device microphone collects a user s input sound and sends them to Sphinx4 PocketSphinx speech recognizer Then the speech recognizer interprets the input into words commands and selects a corresponding number mapped to each command 
Thereafter the new number is sent from the client side to the server side via network resourcing On receipt of the integer from client server sends confirmatory information back to client Then the received command is transferred from the server to a Mapper function The Mapper function relying on the Mapper module calls a database Figure 2 Architecture of Project The breakdown of the design into different modules gives way for easy expansion of the project For instance separating the code implementation into Client and Server side connection allows convenient modification and extension of the received input which occurs on the Client side and the implementation of the interpreted commands on the server side Hence one can adjust one network section without affecting the other In addition the use of a Mapper file makes the addition of more commands easy for even a non programmer In order to add more commands the yaml file which is similar to a text file can be edited and numbers with corresponding commands can 
be added Even though Festival could have been used in handling the returned spoken commands VLC provides a more convenient programming interface With VLC media player several sound output could be implemented and run using program threads 2 2 Algorithm Implementation The implementation of the algorithm for this project was written in two programming languages python and Java For speech recognition CMU Sphinx 4 can only be conveniently programmed in Java since the recognizer was implemented in Java On the other hand PocketSphinx is the equivalent of Sphinx 4 but in Python In addition Espeak speech synthesizer already comes installed with the Ubuntu Linux Operating System thus making the tools for the speech development readily available This choice of tools and programming languages were mainly based on research preference and better synchronization of all the tools used for this project 306 The codes and diagram below give a glimpse of the project implementation and workspace 2 2 1 mapper file 1 command_type 
say_string play_media hello there how are you priority 5 control_command stop 2 command_type say_wav play_media home fabiano Desktop VLC_NEW sound_files MJ Akon mp3 priority 6 control_command 3 command_type say_text play_media home fabiano Desktop VLC_NEW text_files MAE txt priority 8 control_command Figure 3 Section of Yaml File The mapper file shows a few commands and their corresponding numbers When the mapper function is called it loads the above yaml file and then returns a tuple of instructions of any corresponding number which is required Based on the priority of the returned tuple the tuple is sorted into a queue If a certain command with its given priority say 5 is running and another command with a higher priority say 3 is inserted into the queue then the command pauses and executes the most recent command with a higher priority In practical terms the robot could be talking about the history of a building assuming the priority of this command is 5 Then if a tourists stands in the robot s pathway 
while the robot is moving the excuse me please 2 2 2 python code implementation def say_text self text_file_path Read any given text file Must collect the pathway of the file and confirm it exists update the status of text_file_status if there is any given file path check given file path and confirm it is valid If valid assign it to self file_path for reuse later check_path find text_file_path print check_path 307 os system check_path self text_file_status True self file_path text_file_path self convert Figure 4 Python Function Connecting Mapper and Speech Modules Figure 4 is one of the python functions which also exist in the yaml file Depending on the interpreted results from the yaml file the say_text function or any other function could be returned as a tuple and sent to the Speech Module for appropriate execution For instance from Figure 3 if the number three returns its generated tuple then the say_text function is called This function collects the pathway of a text file verifies the availability of 
the file as shown in Figure 4 and then converts the file into a sound wave form which is controlled using VLC interface 2 2 3 VLC media interface The VLC media player library uses python bindings which are ctypes based7 The library consists of numerous functions that start with the creation of an Instance object With this object triggered events can be handled by the VLC media player at any time 2 2 4 java Sphinx 4 recognizer in action Figure 5 Workspace of Speech Recognizer Figure 5 shows the workspace of the speech recognizer and its elementary implementation of simple commands which are written on screen after the speaker says them At elementary stage words can be added to the grammar file for more recognition but as the vocabulary increases a vocabulary database would be required as opposed to individual addition of words 308 3 Discussion of Results Although satisfactory level of speech synthesis and recognition is yet to be attained due to the time frame of conducting the research work there is still 
room to attain optimum results in future work At the time of research the robot can talk about any area it tours it can also comfortably respond to simple instructions and read any text file assigned to it However the greatest challenge lies in the speech recognition as it is supposedly efficient irrespective of the accent of the tourists users speaking to the robot Unfortunately some people could speak to the robot and have their commands quickly interpreted while others could speak and have a delay in the breakdown and interpretation of phonemes Another challenge posed by the speech recognizer is the confusion of similar sounding words For instance keywords tall and call could be mixed up Consequently the commands tied to tall could be implemented for call In order to tackle the problem of speech analysis of everyone irrespective of his her accent users would be told keywords that can be used whenever their commands cannot be understood Tourists will also have the option of calling numbers to implement 
certain commands as opposed to saying the commands For example instead of tell me about the oldest Engineering laboratory the user would say a 4 Conclusion A speech system has been illustrated that optimally utilizes a combination of network connection a text to speech synthesizer a media player and speech recognition software on the Ubuntu Linux Operating System The principal advantage of the technique of speech development discussed in this research paper lies in the development of an efficient speech system based on open source software In this research the use of different modules gives room for future extension for larger projects For future work concentration will be on the expansion of the grammar library while concurrently reducing possible audible confusions in the speech recognizer 5 Acknowledgments The author would like to thank the Department of Electrical Engineering and Computer Science of Case Western Reserve University especially his advisor Dr Wyatt Newman for the opportunity to embark on an 
exciting research area Also the Case Western ACES summer program for an excellent summer 2010 research is highly appreciated Finally I appreciate my family for their support and encouraging words which always propel me to study and give my contributions in order to make the world a better place 6 References 1 Sjolander Kare and B J 2000 Wavesurfer An Open Source Speech Tool Proceedings of ICSLP vol 4 309 5 Evandro Gouvea et al 2002 Sphinx 4 for the JavaTM platform Architecture Notes http www speech cs cmu edu cmusphinx twiki Sphinx4 WebHome Architecture pdf 6 Voicerecog Sphinx 4 architecture 2006 http www xncroft com blog lyceum voicerecog 2006 07 05 sphinx 4 architecture 7 VLC Media python bindings 2011 http liris cnrs fr advene download python ctypes 310 
11	a	Using Speech Recognition Software to Increase Writing Fluency for Individuals with Physical Disab Garrett Jennifer Tumlin Heller Kathryn Wolff Fowler Linda P Alberto Paul A Fredrick Laura D Journal of Special Education Technology 2011 26 1 ProQuest Central pg 25 Reproduced with permission of the copyright owner Further reproduction prohibited without permission Reproduced with permission of the copyright owner Further reproduction prohibited without permission Reproduced with permission of the copyright owner Further reproduction prohibited without permission Reproduced with permission of the copyright owner Further reproduction prohibited without permission Reproduced with permission of the copyright owner Further reproduction prohibited without permission Reproduced with permission of the copyright owner Further reproduction prohibited without permission Reproduced with permission of the copyright owner Further reproduction prohibited without permission Reproduced with permission of the copyright owner 
Further reproduction prohibited without permission Reproduced with permission of the copyright owner Further reproduction prohibited without permission Reproduced with permission of the copyright owner Further reproduction prohibited without permission Reproduced with permission of the copyright owner Further reproduction prohibited without permission Reproduced with permission of the copyright owner Further reproduction prohibited without permission Reproduced with permission of the copyright owner Further reproduction prohibited without permission Reproduced with permission of the copyright owner Further reproduction prohibited without permission Reproduced with permission of the copyright owner Further reproduction prohibited without permission Reproduced with permission of the copyright owner Further reproduction prohibited without permission Reproduced with permission of the copyright owner Further reproduction prohibited without permission 
12	a	published as HJ Yang C Oehlke Ch Meinel German Speech Recognition A Solution for the Analysis and Processing of Lecture Recording In Proc of 10th IEEE ACIS International Conference on Computer and Information Science ICIS 2011 Sanya Heinan Island China May 2011 German Speech Recognition A Solution for the Analysis and Processing of Lecture Recordings Haojin Yang Christoph Oehlke Christoph Meinel Hasso Plattner Institut HPI University of Potsdam P O Box 900460 D 14440 Potsdam e mail Haojin Yang Meinel hpi uni potsdam de Christoph Oehlke student hpi uni potsdam de Abstract Since recording technology has become more robust and easier to use more and more universities are taking the opportunity to record their lectures and put them on the Web in order to make them accessable by students The automatic speech recognition ASR techniques provide a valueable source for indexing and retrieval of lecture video materials In this paper we evaluate the state of the art speech recognition software to find a solution for 
the automatic transcription of German lecture videos Our experimental results show that the word error rates WERs was reduced by 12 8 when the speech training corpus of a lecturer is increased by 1 6 hours Keywords automatic speech recognition e learning multimedia retrieval recorded lecture videos I INTRODUCTION In the past decade more and more universities recorded their lectures and presentations by using state of the art recording systems such as tele TASK 3 and made them available over the internet Recording this kind of content quickly leads to very large amounts of multi media data Thus the challenge of finding lecture videos on the internet or within video portals has become a very important and challenging task Content based retrieval within lecture video data requires textual metadata that has to be provided manually by the users or has to be extracted by automated analysis Many research projects have experimented with these data and determined two main questions How can we access the content of 
multi media lecture videos easily 1 4 6 7 8 and how can we find the appropriate semantical information within them 4 5 Speech is the most natural way of communication and also the main carrier of information in nearly all lectures Therefore it is of distinct advantage that the speech information can be used for automatic indexing of lecture videos The studies described in 1 4 are based on outof the box commercial speech recognition software Their proposed indexing and searching functionalities are closely related to recognition rates Concerning such commercial software it is not easy to adapt it for a special working domain and custom extensions are rarely possible 2 and 9 focus on English speech recognition for TED technology entertainment and design lecture videos and webcasts Their approaches do not use the vocabulary extension method Therefore their training dictionary can not be extended or optimized periodically 8 proposed a solution for creating an English speech corpus basing on lecture audio data 
But they have not dealt with a complete speech recognition system 7 introduced a spoken document retrieval system for Korean lecture search Their automatically generated search index table is based on lecture transcriptions However they do not consider the recognition of topic related technical foreign words which are important for keywordsearch Overall most of these lecture recognition systems have a low recognition accuracy the WERs of audio lectures are approximately 40 80 1 2 7 8 9 The poor recognition results limit the usability of their approaches If we regard German speech we see that it is much harder to be recognized than English speech This is because of the different language characteristics Compared to the English language German has a much higher lexical variety The out of vocabulary OOV rate of a 20k German lexicon is about 7 5 higher than it is for an appropriate English lexicon 10 German is a compound language and has a highly inflective nature Because of these peculiarities a German 
recognition vocabulary is several times larger than a corresponding English one and it is hard to resolve word forms that sound similar In addition German lecture videos in specific domain e g computer science are more difficult to recognize than common contents like TV news This is because there are many topic related technical terms which are out of the standard vocabulary A lot of them are foreign words sometimes even pronounced with English pronunciation rules e g the words server and World Wide Web are often mentioned in German lectures about computer science We have compared the current state of the art speech recognition software and developed a solution for the automatic transcription of German lecture videos Our solution enables a continued improvement of recognition rate by creating and refining new training data The topic related technical terms have been added to the training vocabulary In addition we developed an automatic vocabulary extension procedure for adding new speech training resources 
In the experiments we have determined that the training time of our speech corpus influences the recognition accuracy Table I C OMPARISON BETWEEN STATE OF THE ART SPEECH RECOGNITION SOFTWARE 12 Criteria Recognition rate 60 of the total score Plattform independence 15 Cost 5 Modularity 15 Actuality 5 Total score IBM ViaVoice 5 6 7 5 0 5 Dragon Naturally Speaking 8 2 4 1 8 5 85 Sphinx 4 6 5 8 10 10 7 7 45 Julius 8 9 10 10 7 8 5 HTK 6 8 7 10 6 6 35 significantly The paper is organized as follows In Section II we discuss several state of the art speech recognition software in more detail and evaluate them for our task In Section III the development of our German lecture speech recognition system is presented Experimental results are provided in Section IV Section V concludes the paper with an outlook on future work II SPEECH RECOGNITION SOFTWARE After a detailed study of state of the art speech recognition software we selected IBM ViaVoice Dragon Naturally Speaking CMU Carnegie Mellon University Sphinx Julius 
and HTK Hidden Markov Model Toolkit for our evaluation In 12 a comparison between several pieces of speech recognition software is given We have referenced their evaluation in our research results and illustrated them in Table I where higher score means better performance The evaluation of recognition rates was taken from 12 13 IBM ViaVoice is basing on Windows and Mac OS The active vocabulary includes 30000 words plus an add on capacity of further 34000 words For control and command applications the average WER of ViaVoice is lower than 7 The software costs about 45 but its development has been terminated for several years Dragon Naturally Speaking DNS is another competing commercial software where costs go from 120 to over 1000 Its active vocabulary includes 30000 words The average WER is about 5 for command control and small dictation applications DNS is based on Windows only and programming interfaces are not supported at all HTK Hidden Markov Model Toolkit is a portable toolkit for building and 
manipulating hidden Markov models It is used mostly for speech recognition and copyrighted by Microsoft Altering the software for the licensee s internal use is allowed CMU Sphinx is an open source toolkit for speech recognition and is an ongoing project by Carnegie Mellon University As the same as HTK the Sphinx toolkit can be used for training both acoustic models and language models which will be discussed in detail in section III Sphinx 4 has a generalized pluggable language model architecture and its average WER for a 5000 words vocabulary is lower than 7 Julius is an open source large vocabulary continuous speech recognition engine that is developed primarily for Japanese language To run Julius recognizer a language model and an acoustic model have to be provided Julius accepts acoustic models in HTK ASCII format Moreover both Julius and Sphinx accept n gram language models in ARPA format In addition to the results in Table I there is another important reason that affected our evaluation Because of the 
particular characteristics of lecture videos mentioned in section I we need to update the recognition vocabulary and retrain the system from time to time for including newly added words This process is much more limited by commerical software than by open source software Moreover lecture recognition is a long term task There might be more and more special requirements that need to be met so it is important that we have the absolute control over the refining process Therefore we eventually decided not to consider the commercial software for our research After the comparison between HTK and Sphinx according to 14 15 16 on Chinese Mandarin English and Hindish the following conclusions have been mentioned Table II C ONVERSION OF ESPEAK PHONEME FORMAT INTO OUR ASCII PHONEME FORMAT eSpeak phoneme a O i U N Our ASCII phoneme a oo i uu nn Description long a modification of the vowel e short o long i short u modification of the consonant n Figure 1 Phoneme Representation of German and English Words using software to 
create statistical representations of the sounds that make up each word A language model tries to capture the properties of a language and to predict the next occurrence in a speech sequence Our speech recognition system makes use of phone based continuous HMMs Hidden Markov Models for acoustic modeling and n gram statistics based on German plaintext that has been extracted from different databases for language modeling The language vocabulary is created by refining and extending an open source German vocabulary This is discussed in more detail in the next subsection A Vocabulary Vocabulary is a very important part of an ASR system We have investigated three open source German dictionaries HADI BOMP lexicon1 that has been developed by speech and communication workgroup of University Bonn Voxforge lexicon2 and Ralf s German dictionary3 Unfortunately none of them is immediately useable for our task This is the case because in our preliminary training we only have a small speech corpus about 25 hours and an 
appropriate vocabulary size should be about 5000 15000 words Too many foreign entries will lead to higher WER because of confusion with similar sounding words Finally we decided to create a new vocabulary basing on Ralf s German dictionary for our working domain The creation is based on the following steps 1 http www sk uni bonn de forschung phonetik sprachsynthese bomp 2 www voxforge org 3 http spirit blau in simon 2010 05 13 ralfs german dictionary 0 1 9 3 Figure 2 Dictionary Extending Procedure size was reduced from over 380000 words down to 13300 words Adding 413 technical terms related to computer science published on German Wiktionary4 into the dictionary Generating a phoneme representation for each word by using eSpeak5 and automatically adapting these eSpeak phonemes into our ASCII phoneme format The phoneme creation steps are illustrated by two sample words from our dictionary in Figure 1 The phonetic output of eSpeak is in a special format containing additional information like how to stress each 
syllable Table II shows the 4 http de wiktionary org wiki Verzeichnis Informatik 5 http espeak sourceforge net Table III R ECOGNITION RESULTS ARE SHOWN AS WER USING BIGRAM AND TRIGRAM LM S LM LM1 LM2 LM3 LM4 LM5 Corpora Leipzig Wortschatz transcripts DBPedia transcripts DBPedia Leipzig Wordschatz German daily news transcripts German daily news transcripts transcripts Corpora Size 3 05M words 2 6GB texts 3 2GB texts 1 05M words 50K words WER Bigram 81 2 81 9 81 2 76 81 9 WER Trigram 81 82 3 82 75 5 81 9 AM 3000 senones 16 tied state conversion of consonants and vowels of the German word Datenkomprimierung from eSpeak format to our ASCIIphoneme format The phoneme representations of English technical terms like delay in Figure 1 were manually adapted In addition we have trained technical terms more carefully because a high recognition rate of technical terms can improve the accuracy of speech transcript based keyword search for lecture videos 1 Before adding our newly created speech corpus to our training set 
an automatic vocabulary extension algorithm is used Figure 2 shows its workflow First the new transcript file is automatically parsed for determining all words which are not included in our current dictionary Subsequently the spell check word correction and the language check processes are performed This is essential since we want to avoid spelling mistakes and need to separate German words and foreign words All foreign words have to be processed manually whereas the conversion of German words to the correct phonemes is handled automatically Finally all collected words plus their phonemes are merged into a new dictionary B Acoustic Model The Acoustic Model AM is trained using speech utterances and their transcriptions A good speech corpus is the basis for developing speech recognition systems and it is usually characterized by the following features recording hours speech type number of speakers background environment etc The AM training is usually performed by dictating e g reading and recording a 
predefined text but the actual input that has to be recognized is an audio track containing free speech which differs significantly from the dictation text Experimental results from 1 show that the quality and the format of the speech recording strongly affect the recognition accuracy 1 also concluded that the training is useful but only if done under the same conditions This implies that better training results can be achived by using audio input which has the same signal frequency and the same background noise as the recognition audio 8 pointed out that the alignments of the 16KHz broadband speech provide optimal quality for the training data According to these conclusions we decided to create our speech corpus by using our lecture videos in an appropriate format 16KHz16Bit In order to ensure that the trainer can detect most of our lecture audio data we have segmented it to many pieces of speech utterances that are mostly between 1 and 3 seconds long then selected utterances that have good audio quality 
manually transcripted them and finally added them to the training corpus This work is very time consuming It is therefore very difficult to generate large amounts of speech data in a short time In order to carry out the preliminary experiment we decided to add about 23 hours of the Voxforge open source German speech corpus6 to our training set The AM is trained using CMU Sphinx Acoustic Model trainer C Language Model A Language Model LM is trained using a large and structured set of texts After a detailed investigation we have collected text corpora from the following sources Extracted plain texts from German DBPedia7 The extracting process has been performed according to 17 About 2 6 GB texts were extracted Text corpus from Leipzig Wortschatz8 3M words German daily news 1996 2000 from radio about 1M words Audio transcripts 50K words Because there are still many foreign sentences and special characters in extracted DBPedia corpus and the LeipzigWortschatz corpus we have performed the following refinements 
text preprocessing 6 http www voxforge org 7 http wiki dbpedia org Downloads351 8 http corpora informatik uni leipzig de download html Foreign words and filtering of special characters All the words in the DBPedia text corpus are filtered with the help of a predefined large German vocabulary about 420000 words including all technical terms V CONCLUSION Since the increasing amounts of the lecture videos and recorded presentations were made accessable the indexing of these multi media data has become a great challenge The speech information is one of the best resources for semantic indexing However the speech recognition is still an active research area and almost none of the existing lecture speech recognition systems have achieved a good recognition rate In this paper we have evaluated the state of the art speech recognition software and proposed a solution for a recognition system for German lecture videos As further work we plan to continue generating and collecting speech data from our lectures and 
retrain the system periodically We also consider to make the speech resource available to the research community In order to achieve a better recognition accuracy a word decomposition algorithm can be utilized in combination with our training so the German vocabulary size and the OOV rates can also be reduced The actual use of the automatically transcripted audio for a lecture video portal such as transcript based semantic search automatically created video subtitles a recommendation system based on speech information etc will be developed in the future R EFERENCES 1 W We have trained several LMs using the CMU statistical language modeling toolkit SLM toolkit 9 Their experimental evaluation is discussed in the next section IV EXPERIMENTAL RESULTS The preliminary experiments were carried out on 50 undetected speech sentences of our lecturer that were randomly selected from his computer science lecture Our experiments follow two main goals evaluation of different language models find out how training time of 
our speech corpus influences recognition accuracy We use the common metric WER for the evaluations It can be calculated as follows W ER S D I N where S is the number of subsitutions D is the number of the deletions I is the number of insertions N is the number of words in the groundtruth In experiment I the AM is trained using a 24 5 hours speech corpus 23 hours voxforge speech corpus plus 1 5 hours of our corpus and a 59K words vocabulary LMs are trained using different combinations of text corpora Table III shows the results of this experiment The LM that has been trained using the German daily news corpus plus transcripts has the best recognition accuracy Using the trigram language model tends to improve the recognition performance The results show that the recognition accuracy is not directly related to the size of text corpus In the second experiment we have used the best LM from experiment I A pruned 1 3K words vocabulary is used AMs are trained with different sizes of speech corpora The test results 
are illustrated in Table IV We have found out that the recognition rate is very closely related to the amount of training time performed with the lecturer The WER is reduced by 12 8 when the speech training corpus of a lecturer is increased by 1 6 hours As already mentioned in section 3 2 the creation of our lecture speech corpus is very time consuming work We have only built a small training corpus so far The AM that is trained with this corpus therefore still does not meet the requirements of a real application However our experiments show that through a continuous extension of the speech corpus in our working domain a considerable increase of the recognition acurracy is predictable 9 http www speech cs cmu edu SLM toolkit html Table IV R ECOGNITION RESULTS ARE SHOWN AS WER USING TRAINED AM S AM AM1 AM2 AM3 AM4 Corpora voxforge speech corpus voxforge 30 minutes hpi corpus voxforge 1 hour hpi corpus voxforge 1 6 hours hpi corpus Corpora Size 23 3 hours 23 8 houts 24 3 hours 24 9 hours WER 82 9 78 5 77 1 70 
1 LM trained with German daily news corpus and transcripts 1 05M words 8 J Glass T J Hazen L Hetherington C Wang Analysis and Processing of Lecture Audio Data Preliminary Investigations In HLT NAACL Workshop on Interdisciplinary Approaches to Speech Indexing and Retrieval 2004 9 C Munteanu G Penn R Baecker Y C Zhang Automatic Speech Recognition for Webcasts How Good is Good Enough and What to Do When it Isnt In Proc of the 8th international conference on Multimodal interfaces 2006 10 M Adda Decker G Adda L Lamel J L Gauvain Developments In Large Vocabulary Continuous Speech Recognition of Germa In Proc IEEE ICASSP 96 1996 11 R Hecht J Riedler G Backfried Fitting German into NGram Language Models In Proc of the 5th International Conference on Text Speech and Dialogue pp 
13	a	Automated Tagging to Enable Fine Grained Browsing of Lecture Videos K Vijaya Kumar 09305081 under the guidance of Prof Sridhar Iyer June 28 2011 1 66 Outline Outline 1 2 3 4 5 6 7 8 9 Introduction Motivation Example Lecture Video Repositories Problem Definition Solution Approach System Architecture Implementation Details Experiments and Evaluation Results Conclusion and Future Work 2 66 Introduction Outline 1 2 3 4 5 6 7 8 9 Introduction Motivation Example Lecture Video Repositories Problem Definition Solution Approach System Architecture Implementation Details Experiments and Evaluation Results Conclusion and Future Work 3 66 Introduction Introduction Lecture video recordings are widely used in distance learning To make best use of the available videos a system called Browsing System is required Purpose of the browsing system is to provide search facility in the lecture video repository Problem Statement To develop a browsing system which is useful for users to find their required video content easily 4 66 
Introduction Video Browsing System It takes keywords from users and gives them lecture videos matching their keywords 5 66 Motivation Outline 1 2 3 4 5 6 7 8 9 Introduction Motivation Example Lecture Video Repositories Problem Definition Solution Approach System Architecture Implementation Details Experiments and Evaluation Results Conclusion and Future Work 6 66 Motivation Text Search Example a Query b Results c Finding Info Figure Google Search 7 66 Motivation Can we do the same in Lecture Videos Yes We can provide the same type of search facility in lecture videos based on their contents Example Scenarios Portion of video where Matrix Multiplication is discussed in a programming course lecture Searching for a video which discusses Quick Sort in a Data Structures course videos Finding video results containing Double Hashing in lecture video repository 8 66 Motivation Techniques for Searching in Lecture Videos Meta data based Uses data such as video title description or comments associated with the video 
Content based Based on data extracted from lecture videos which represents contents present within it 9 66 Motivation How You Tube Searches Videos Youtube video search is based on meta data associated with videos Meta data include video title description and tags 10 66 Example Lecture Video Repositories Outline 1 2 3 4 5 6 7 8 9 Introduction Motivation Example Lecture Video Repositories Problem Definition Solution Approach System Architecture Implementation Details Experiments and Evaluation Results Conclusion and Future Work 11 66 Example Lecture Video Repositories Example Lecture Video Repositories CDEEP 5 No search feature NPTEL 16 No search feature freelecturevideos com 8 videolectures net 20 Lecture Browser MIT 13 Some more Academic Earth 1 Youtube Edu 23 Link to list of available educational video repositories is at 15 12 66 Example Lecture Video Repositories Slide Index feature in NPTEL Recently launched Through a video processing company called videopulp 21 13 66 Example Lecture Video Repositories 
freevideolectures com Provides Google custom search to index textual data Topic Looked for Double Hashing 14 66 Example Lecture Video Repositories freevideolectures com Keyword double hashing Result Your search double hashing did not match any documents 15 66 Example Lecture Video Repositories freevideolectures com Keyword hashing Result 6 video results 16 66 Example Lecture Video Repositories freevideolectures com First video Duration 61 22 Found at 42 32 17 66 Example Lecture Video Repositories videolectures net Provides free online access to lecture video recordings of various universities Has hyper links to slide change timings 18 66 Example Lecture Video Repositories Lecture Browser Provides free on line access to lecture videos available in MIT Open Course ware Has Content based Search feature and highlights relevant segments of each video 19 66 Example Lecture Video Repositories Our System User Interface 20 66 Example Lecture Video Repositories Features in Lecture Video Repositories Repository CDEEP 
NPTEL freelecturevideos com videolectures net Lecture Browser MIT Our System Search No No Meta data Meta data Content Content Navigation Features No No No Slide Index Manual Speech Transcript Speech Transcript Slide Index Automated Table Lecture Video Repositories Comparison 21 66 Example Lecture Video Repositories Problems with existing systems freevideolectures com No indication of where exactly searched keywords occur within the video Takes more time to find required information videolectuers net Uses manual process for Synchronization of the slides 22 66 Example Lecture Video Repositories Why can t we use lecture browser Can not be applied directly to our lecture videos Requires speech recognition engine adaptation for non native english speakers Not an open source tool Their speech recognition engine is also not publicly available 23 66 Example Lecture Video Repositories How our system is different Provides automatic synchronization of slides Improved user interface with more navigation features It 
combines features in videolectures net and lecture browser Open source application by integrating available speech recognition and text search engines Tune Sphinx speech recognition engine to recognize and transcribe Indian accents English 24 66 Problem Definition Outline 1 2 3 4 5 6 7 8 9 Introduction Motivation Example Lecture Video Repositories Problem Definition Solution Approach System Architecture Implementation Details Experiments and Evaluation Results Conclusion and Future Work 25 66 Problem Definition Input keywords Output List of videos matching the keywords In each video portions where the keywords occur in the speech are highlighted When user clicks on a particular portion video starts playing in the media player Along with the media player user interface also shows slide index and speech transcript 26 66 Problem Definition Scope of the project Only deals with lecture videos which are in English and related Computer Science domain Reason Speech Recognition Engine Figure Sphinx 4 Recognizer 27 66 
Problem Definition Steps in Speech Recognition 28 66 Solution Approach Outline 1 2 3 4 5 6 7 8 9 Introduction Motivation Example Lecture Video Repositories Problem Definition Solution Approach System Architecture Implementation Details Experiments and Evaluation Results Conclusion and Future Work 29 66 Solution Approach Solution Approach 30 66 Solution Approach Content Extraction a Optical Character Recognition b Speech Recognition 31 66 Solution Approach Speech Recognition Engines Sphinx 4 18 Hmm Tool Kit HTK 9 Reasons for choosing Sphinx Provides Java API Application Programmable Interface s so it can be integrated easily into any application CMU Sphinx provides support for various tools useful in speech recognition Has easy configuration management where we need to set various parameters related to speech recognition Supporting tools are available for generation of acoustic and language models Completely written in java it is highly modular and platform independent 32 66 Solution Approach Indexing Query 
Handling 33 66 Solution Approach Text Search Engines Lucene 3 Indri 10 Xapian 22 Zettair 24 Reasons for choosing Lucene It creates index of smaller size and search time is also very less 17 Supports ranked searching best results returned first Can handle many powerful query types phrase queries wild card queries range queries and more Mostly used text search engine List of more than 150 applications and websites that are using Lucene to provide search facility 14 34 66 System Architecture Outline 1 2 3 4 5 6 7 8 9 Introduction Motivation Example Lecture Video Repositories Problem Definition Solution Approach System Architecture Implementation Details Experiments and Evaluation Results Conclusion and Future Work 35 66 System Architecture System Components 36 66 Implementation Details Outline 1 2 3 4 5 6 7 8 9 Introduction Motivation Example Lecture Video Repositories Problem Definition Solution Approach System Architecture Implementation Details Experiments and Evaluation Results Conclusion and Future Work 37 
66 Implementation Details Audio Extraction Input Video file Output Audio file Command line tools provided by FFmpeg 7 Running ffmpeg ffmpeg i CS101 L10 Strings mp4 1 CS101 L10 Strings wav ar 16000 ac 38 66 Implementation Details Speech Recognition Input Audio file Output Time aligned transcript in XML format Open source Java library for Sphinx 4 Speech Recognizer from CMU Sphinx 18 Requires language model acoustic model and a pronunciation dictionary 39 66 Implementation Details Language model creation Large amount of text corpus related to the domain of speech recognition is required CMU SLM Toolkit 6 is useful for creating language model from the text corpus Figure Framework for creating large amount of text corpus 40 66 Implementation Details Language model creation Collected text corpus related to Computer Science domain Wiki Index Randomly generated queries consisting of terms from CS and searched in Lucene Indexes Text books Data structures Algorithms Computer Networks DBMS and OS Manual Transcriptions 
Available in MIT OCW 4 Converted PDF files to Text using Java library provided from PDFBox 11 41 66 Implementation Details Acoustic model development Requires audio files and corresponding manual transcriptions Developing new acoustic modeling takes large amount of time Adaptation of acoustic model is an option which requires an existing model CMU Sphinx provides WSJ and HUB4 models useful for recognizing US English Sphinx Train and Sphinx Base are set of tools useful for development for acoustic model 42 66 Implementation Details Acoustic model development We have to adapt an acoustic model to match our speakers to get better recognition accuracy Time consuming which requires small audio files each having a sentence and manual transcription of each of the audio file Created 150 wav files for adaptation from CS101 lectures of Prof Deepak Phatak Each of the wav file duration is 2 to 5 seconds and gave manual transcriptions for them 43 66 Implementation Details Speech Transcript Generation Configured the 
Sphinx 4 recognizer with the created language model and acoustic model Transcribed audio files of CS101 lectures and generated time aligned transcripts Transcribing of an audio file took approximately double the duration of the file The transcription speed can be increased but gives low recognition accuracy 44 66 Implementation Details Example Speech Transcript transcript tt text deals with text time 7 time tt tt text searching text time 11 time tt tt text of lectures text time 14 time tt transcript 45 66 Implementation Details Video Frames Extraction Input Video file Output Frames extracted from the video at specified intervals ffmpeg can be used for the frame extraction ffmpeg i CS101 L10 Strings mp4 image 4d jpeg r 1 f image2 46 66 Implementation Details Slide Detection Input Video frames of a lecture Output Slides of the lectures along with their title and time of occurrences Designed an algorithm based on slide title matching which uses OCR for slide text extraction Found an OCR tool called tesseract 
ocr 19 which gives better recognition accuracy among available the Open Source tools 47 66 Implementation Details Example frame from a video lecture 48 66 Implementation Details After applying OCR Overview Engineering Education He earchar1 iUrilmu lhinkirng lnirucluctivn tc the course Oui 49 66 Implementation Details Title Matching algorithm for Slide Detection Title Time overview 0104 Will be identified as starting of a slide overview 0105 overview 0106 overview 0107 overview 0108 overview 0109 overview 0110 engineering 0135 Will be identified as starting of next slide engineering 0136 engineering 0137 engineering 0138 engineering 0139 engineering 0140 50 66 Implementation Details Title Matching algorithm for Slide detection while i titles length 1 begin if titles i equals prev matchesNextTwo titles i indices add i i findNextSlide titles title i i 3 if i 1 return endif prev titles i indices add i i i 2 endif i i 1 end 51 66 Implementation Details Example Slide Index slides slide title Overview title time 13 
time slide slide title Introduction title time 79 time slide slides 52 66 Implementation Details Indexing Input Transcript file and Slide index file Output Creates an Index or adds to existing indexes Apache Lucene 3 provides Java library for indexing text documents Parsed the transcript and slide index file which are in XML format Indexed CS101 lectures of Autumn 2009 and created indexes are of size 2 5MB 53 66 Implementation Details Query Handling Input User given queries Output List of lectures matching the query Apache Lucene 3 is also include Java classes for searching the indexes Technologies Java Server Pages JSPs and Java Servlets Web Server Apache Tomcat 6 0 24 2 Operating System Ubuntu Lucid Lynx 10 04 LTS 54 66 Implementation Details User Interface Created web pages using HTML and Java Script Using a freely available version of JW Player 12 for playing videos in the interface Figure User Interface of our System 55 66 Implementation Details User Interface Figure Search Results for query binary 
search 56 66 Implementation Details User Interface Figure playing selected video with the navigation 57 66 Implementation Details Content Repository Recorded videos of lectures Speech transcripts Slide Index files Lucene indices 58 66 Experiments and Evaluation Results Outline 1 2 3 4 5 6 7 8 9 Introduction Motivation Example Lecture Video Repositories Problem Definition Solution Approach System Architecture Implementation Details Experiments and Evaluation Results Conclusion and Future Work 59 66 Experiments and Evaluation Results Slide Detection Results Video L 01 L 02 L 03 L 04 L 05 Total Actual slides 14 20 12 32 32 110 Detected slides 14 20 11 30 30 105 Correctly detected 12 16 11 26 28 93 Duplicates 0 6 2 9 5 18 Recall 100 100 91 6 93 7 93 6 95 4 Prec 85 80 100 86 6 93 3 88 5 Table Slide Detection results 60 66 Experiments and Evaluation Results Speech Recognition Results Adaptation files 0 30 60 90 120 150 Words in test files 127 119 124 120 110 123 Matches 22 43 70 76 69 82 Accuracy 13 31 52 59 61 62 
Table Speech Recognition results 61 66 Experiments and Evaluation Results Video Retrieval Results No of queries tested Avg Search seconds Recall Avg Precision 30 0 004 0 72 0 91 Table Search Quality Results 62 66 Conclusion and Future Work Outline 1 2 3 4 5 6 7 8 9 Introduction Motivation Example Lecture Video Repositories Problem Definition Solution Approach System Architecture Implementation Details Experiments and Evaluation Results Conclusion and Future Work 63 66 Conclusion and Future Work Conclusion and Future Work Built a system for providing search facility in CS101 Autumn 2009 lectures Speech recognition accuracy can be improved through more adaptation Slide Detection method can be improved to reduce duplicate slides More lectures can be added to the repository 64 66 Conclusion and Future Work Academic Earth http academicearth org Apache An Open Source Web Server http tomcat apache org Apache Lucene http lucene apache org java docs index html Audio Video Lectures from MIT OCW http ocw mit edu 
courses audio video courses electrical engineering and computer science CDEEP IIT Bombay http www cdeep iitb ac in CMU Statistical Language Modeling Toolkit Documentation http www speech cs cmu edu SLM toolkit_ documentation html 64 66 Conclusion and Future Work FFmpeg http www ffmpeg org freevideolectures com http www freevideolectures com HTK http htk eng cam ac uk Indri http www lemurproject org indri Java PDF Library http pdfbox apache org JW Player http www longtailvideo com players jw flv player Lecture Browser MIT 64 66 Conclusion and Future Work http web sls csail mit edu lectures List of Applications that are using Lucene http wiki apache org lucene java PoweredBy List of educational video websites http en wikipedia org wiki List_of_educational_ video_websites nptel http www nptel iitm ac in Open Source Text Search Engines Evalution Results http wrg upf edu WRG dctos Middleton Baeza pdf sphinx http www speech cs cmu edu tesseract ocr http code google com p tesseract ocr 64 66 Conclusion and Future 
Work videolectures net http www videolectures net VideoPulp Official Partners for Slide Index feature in NPTEL http www videopulp in xapian http xapian org Youtube Edu http www youtube com education b 400 zettair http www seg rmit edu au zettair 65 66 Conclusion and Future Work Thank You 65 66 Conclusion and Future Work 66 66 
14	a	Pers Ubiquit Comput 2010 14 ORIGINAL ARTICLE Design and implementation of a VoiceXML driven wiki application for assistive environments on the web Constantinos Received 31 December 2008 Accepted 5 October 2009 Published online 12 March Abstract In this paper we describe the design and implementation of an audio wiki application accessible via both the Public Switched Telephone Network and the Internet The application exploits mature World Wide Web Consortium standards such as VoiceXML Speech Synthesis Markup Language and Speech Recognition Grammar Specification toward achieving our goals The purpose of such an application is to assist visually impaired technologically uneducated and underprivileged people in accessing information originally intended to be accessed visually via a personal computer PC Users may access wiki content via fixed or mobile phones or via a PC using a Web Browser or a Voice over IP service This feature promotes pervasiveness to collaboratively created content to an extremely large 
population i e those who simply own a telephone line Keywords C Kolias Abbreviations ABNF Augmented 1 Introduction Wikis are generally considered as collaboration platforms where users access or contribute knowledge on specific topics Wikis are mostly implemented as web applications that allow registered and sometimes even unregistered 123 528 Pers Ubiquit Comput 2010 14 users to create edit hyperlink and organize their content Wiki applications are also used in many companies to provide effective Intranets and for knowledge management Beyond doubt the huge success of Wikipedia 1 which is perhaps the most famous wiki application nowadays proves in practice the suitability of wikis in exchanging knowledge The idea that wiki applications enable the collaborative writing of articles of common interest is simple enough yet this simplicity entails a profound impact on the flow of information among their users Their general philosophy is the facilitation of access and contribution to knowledge However their 
collaborative and in many cases open nature raises important issues such as accessibility content validity and security 2 In most of the existing wiki implementations access is as simple as browsing on a simple web page Editing is also straightforward and it can be done by inserting information written in a specific usually very simple syntax in the appropriate web forms that the wiki interface offers Since wiki content can be changed by anyone depending on the implementation its validity is consigned either to the users themselves or to specific users who are considered as experts As far as security is concerned it is typically achieved by utilizing standard security mechanisms to accomplish authorization authentication and integrity in order to relate changes of wiki content with specific users or to ensure that the presented information is original Most of the existing wiki implementations are primarily dependent on web standards such as the Hypertext Markup Language HTML HTML represents information in a 
visual manner and as a result visually impaired individuals are unable to access their content Beyond doubt the vast majority of modern web applications neglect the special needs of disabled people Until now visually impaired individuals rely on the features of the operating system they use e g ShowSounds for the Windows XP operating system While these features comprise a typical feature of most of the modern operating systems they lack of support for dynamically generated content such as content originating from the World Wide Web Such individuals are limited in using high cost systems that often require special training as well A fundamental requirement of using wiki applications is having some sort of access to the Internet This automatically makes access to its content prohibited to a large population such as various semi literate and illiterate people in cities and rural areas of emerging economies or technologically uneducated people such as the elderly Thus the inability of underprivileged people to 
afford computers or web enabled handheld devices as well as the disinterest of some people to acquire basic information technology IT skills lead to an undesirable blockade to a large amount of knowledge Since wikis share a large amount of information among different people the need to be accessible from the widest range of devices possible is immanent However the integration of wikis to personal digital assistants PDAs or smartphones is rather complicated This is due to the great number of different standards technologies and operating systems that exist for such devices in the market today Web based wiki implementations are usually developed focusing on desktop scale HTML based browsers This fact generates many problems when wiki content is accessed from mobile devices equipped with browsers supporting certain subsets of standard HTML like Wireless Markup Language WML 3 or i Mode HTML 4 Speech is the most natural and innate communication means available to people and when exploited it maximizes the 
effectiveness of 2 Previous work During the recent past numerous efforts have been made to acoustically access information which was originally intended to be accessed visually Applications were developed motivated by the need to aid visually impaired 123 Pers Ubiquit Comput 2010 14 529 individuals and at the same time serve population who do not afford access to the Internet In 6 a Wiki application similar to the one described in this paper is presented It is based on the observation that mobile phones have penetrated more than the Internet into young people becoming a fashion and in some countries like the developing ones has clearly dominated over it Under this assumption the proposed wiki service waits for a short message service SMS signal from the user s mobile phone with the title of the article he wishes to hear After a while the service calls the user to his mobile phone and speaks the article content via a synthetic voice During the call the user can navigate to the different sections of the 
article by pressing keys in his phone The application is totally based on open source software components such as MediaWiki 7 This application is addressed to students in emerging economies who usually are not familiarized with the use of PCs or simply cannot afford one On the other hand the fact that such a wiki is based on a mobile phone feature it makes it inaccessible to people e g the elderly who do not own one or cannot or do not know how to send an SMS Although mobile phones are very popular nowadays ordinary Public Switched Telephone Network PSTN phones are still used by the majority of people In 8 authors propose a client server architecture to integrate speech technology into web pages to be used for e learning purposes The production of voice to text and vice versa is done on a central speech server where the services speech synthesis speech recognition and speaker verification are installed The content produced is presented to the user in a web browser in which is embedded a Java applet that 
implements audio input and output capabilities However the only requirement on the client side for this approach i e a JavaScript enabled browser and Sun s Java plugin is uncertain to be supported by current mobile devices Additionally access to the Internet is not guaranteed when the user is on the move or simply when he does not have an Internet connection or a PC In 9 authors claim that the collaborative nature of wikis is not well served because it limits its users to computer environments or when deployed in mobile environments it restricts them by means of input keyboard stroke or stylus and output small screen They identify that synchronous communication technologies like teleconferences or simple calls are gaining attention as channels to carry out collaborative tasks Therefore the combination of the wiki collaboration paradigm with audio communication means can improve the overall usability of wikis Under this context they propose a wiki implementation to facilitate asynchronous audio mediated 
collaboration when on the move It is based solely on the manipulation of audio files Despite the fact that it enhances collaboration with a more personalized feeling each user contributes with his own voice this implementation does not have any web counterpart and therefore it cannot be accessed by any means other than acoustic In 10 a non visual web browser is presented It enables visually impaired people to navigate contents of web pages acoustically A synthetic speech feature transforms the contents of web pages into sound and the open source Sphinx voice recognition engine 11 transforms the user s voice into signals which are recognized by the system In this way one can navigate through pages with his voice only and can avoid the sequential narration of their content This application provides significant advantages especially to partially blind users who are not willing to invest time and effort to learn new communication means or acquire IT skills On the other hand it runs exclusively on PCs making it 
inaccessible to users who do not own or do not know how to use a computer In addition there is no lightweight version targeting to mobile devices making it inappropriate for roaming users thus reducing its pervasiveness DAISY Consortium 12 is an organization the mission of which is to develop integrate and promote standards technologies and implementation strategies to enable global access by people with reading disabilities to information provided by mainstream publishers governments and libraries A DAISY Digital Talking Book DTB is a file with a specific format which can be accessed either from an appropriate software in a PC or from a special device with DTB playback capability Though feasible to integrate this approach in a wiki platform would require a conversion of the material to this specific format Unlike VoiceXML DTB is not a widely adopted standard and therefore it is not appropriate for developers or suitable for dynamic systems like wikis Additionally from a user s point of view DTB requires 
software which in turn requires a PC and the related skills or a special device Both lead to extra costs for the end user 3 Related standards Our proposed application is based on VoiceXML in order to become accessible acoustically by a standard phone or by a computer Besides VoiceXML our application exploits the power of more relative W3C standards such as SRGS 13 and SSML 14 in order to increase the overall effectiveness of the application and enhance the end user experience Hereunder we present a brief description of each one of these standards 3 1 VoiceXML VoiceXML is an eXtensible Markup Language XML based language that aims to function as a tool for the 123 530 Pers Ubiquit Comput 2010 14 development of interactive special elements called dialogs which control the flow of voice applications Each dialog specifies the next dialog or document that the call will transit to by using Universal Resource Identifiers URIs If a dialog does not specify a successor then the execution is terminated Dialogs can be 
forms or menus Forms are used for presenting information and accepting user input while menus are used for presenting the user with a set of choices and transits to the appropriate dialog based on the result of the user choice Table 1 presents a sample of a basic VoiceXML document in which bold and italic text style represents embedded SRGS and SSML elements respectively Each dialog must have one or more grammar files associated with it Grammars contain lists of words that the interpreter may consider valid during the time a user uses that specific dialog Speech and or DTMF grammar files exist for recognizing voice commands or digital tones as user input respectively The role of grammar files is very important for the development of dialogs and a different W3C recommendation exists to specify their structure and purpose In the following paragraph we elaborate on grammars 3 2 SRGS The speech recognition grammar specification SRGS is a W3C recommendation for the creation of grammar documents within 123 Pers 
Ubiquit Comput 2010 14 xml version 1 0 vxml version 2 0 xmlns http www w3 org 2001 vxml form block prompt Welcome to the emphasis Voice Wiki emphasis break time 500ms Please provide a category grammar rule id category one of item history item item science item item environment item one of rule grammar prompt block form vxml 531 when the expected terms may change be added amended or deleted very often This is the case for the proposed wiki application in which the article names that the user may search for change frequently Currently there are two main formats for writing SRGS grammar files Augmented implementation SSML tags are relatively small in number thus SSML code is embedded in the VoiceXML file 4 System architecture Figure 1 depicts a high level view of the system architecture The proposed system is comprised of four major components Database The database is a key system s architecture component in which application data are stored and from where they are retrieved It holds information regarding the 
contents of the articles their previous versions the registered users the users who have the privileges to create new articles etc The complexity of the database scheme is kept low It is stressed that the content of the articles is stored in the database mixed with presentation information The latter are nothing but special sequences of characters usually referred to as wikitext that a user inserts during the editing of an article Web server On the web server resides the Wiki engine which is a web application responsible for the following tasks 1 To constantly wait for the user s requests either from a web browser thus starting a web page session or from a voice browser thus starting a voice session 123 532 Pers Ubiquit Comput 2010 14 Fig 1 System architecture 2 3 4 5 To communicate with the database to retrieve or update article data In the case of voice session the wiki engine has to generate a dynamic grammar of the available article titles one of which will be the user s choice for presentation In a wiki 
application the number of existing articles and as a result the terms that the user is expected to ask for might take large proportions That would lead to the generation of a large grammar file which would affect the overall performance of the system and consequently the user s overall experience For this reason articles are categorized and the user is requested to first choose the category of the article and then ask for the article s title In this way grammar files are kept smaller To transform wikitext sequences retrieved from the database either in XHTML in the case of a web page session or in VoiceXML in the case of a voice session An example of the transformation of wikitext into XHTML or VoiceXML is presented in Table 2 To send the generated XHTML files to the user s web browser or the VoiceXML files to the system s Voice Server for further processing 4 time it may forward the text meant to be spoken to the TTS engine Generates the request made by the user and forwards it to the web server Voice 
server The voice server is the component responsible for the transformation of text documents to audio data The voice server consists of a voice browser a TTS engine and an Automatic Speech Recognition ASR engine Additionally a VoIP gateway is an additional component that plays an important role during the transformation but is not an actual component of the voice server itself The voice browser 1 2 3 Accepts request from the user and in response proceeds to the appropriate actions Receives VoiceXML and grammar files from the web server Specifies the execution flow according to the instructions in the VoiceXML file For instance at a given The TTS engine receives the text from the VoiceXML file meant to be spoken transforms it into streaming sound and sends it to the VoIP gateway to forward it to the enduser On the other hand the ASR engine receives a grammar which is a set of terms that is able to recognize along with the client prompt and identifies if the prompt corresponds to any word in the grammar If 
true it returns the term textually The VoIP gateway receives calls from the PSTN converts PSTN signals to VoIP signals and forwards them to the voice server It is to be noted that the voice server will accept only VoIP signals Signals originating from the Internet from VoIP clients might be Session Initiation Protocol SIP 17 signals e g from Xlite softphone or signals of some proprietary VoIP protocol like Skype The VoIP gateway is also responsible for the transformation of VoIP signals from the Internet to the protocol that the voice server recognizes The web server and voice server components are depicted in Fig 2 Clients The system might accept different types of clients A client may be a typical web browser installed on the user s PC or PDA for instance Firefox Internet Explorer or IE mobile It might also be a VoIP client program installed on a PC like Skype Finally a client might be a fixed telephone or a wireless one that places its request through a PSTN network 4 1 Implementation aspects In the 
proposed application the MS SQL Server 2005 18 was used to store the wiki data A wiki engine application was developed on ASP NET scripts MS Speech Server 2007 was selected for the voice server MS Speech Server contains a powerful TTS engine as well as an advanced ASR engine The application was tested with the Xlite 123 Pers Ubiquit Comput 2010 14 WikiText Local Area Network Contents History Technical aspects See also References History The first LAN put into service occurred in 1964 at the Livermore Laboratory to support atomic weapons research LANs spread to the public sector in the late 1970s and were used to create high speed links between several large central computers at one site Of many competing systems created at this time td b Contents b td tr tr td ol li a href History History a li li a href Technical_aspects Technical aspects a li li a href See_also See also a li li a href References References li ol td tr table h2 a id History History a h2 The first b LAN b put into service occurred in 1964 at 
the Livermore Laboratory to support atomic weapons research LANs spread to the public sector in the late 1970s and were used to create highspeed links between several large central computers at one site Of many competing systems created at this time XHTML h1 Local Area Network h1 table tr VoiceXML You are now listening to article entitled emphasis Local Area Network emphasis break time 500ms menu choice next History accept approximate Say 1 to move to paragraph History choice choice next Technical_aspects accept approximate Say 2 to move to paragraph Technical aspects choice choice next See_also accept approximate Say 3 to move to paragraph See also choice choice next References accept approximate Say 4 to move to paragraph References choice menu break time 500ms form id History The first emphasis LAN emphasis put into service occurred in 1964 at the Livermore Laboratory to support atomic weapons research LANs spread to the public sector in the late 1970s and were used to create high speed links between 
several large central computers at one site Of many competing systems created at this time 533 123 534 Pers Ubiquit Comput 2010 14 Wiki Engine could be to add personalization and adaptive features to the application Users through the web interface could specify which content they would like to hear via phone and save time when they access it Additionally the application itself could create user profiles by keeping each user s browsing history and infer which content they consider interesting and which not Then the application could automatically present the content that is considered interesting and hide the irrelevant one Other features that could enhance the end user s experience could be VoiceXML WikiText HTML Document Voice Browser Grammar Text GrXML TTS Engine Voice ASR Engine Voice Fig 2 Web server and voice server components softphone 3 0 Since this particular software utilizes the SIP protocol and Speech Server 2007 requires the messages to be in SIP as well the use of VoIP gateway was not necessary 
The proposed wiki implementation converts article text to audio on the voice server and sends it to the client through the Internet or the PSTN This fact revokes the need for a voice browser with a TTS engine or any other software that performs analogous tasks to be installed on the client s PC Also it enables clients to access the wiki content via a phone However the client will receive voice streams instead of simple VoiceXML documents Normally voice streams have a large size resulting to longer response delays Also the architecture may have large implementation costs because it requires additional hardware and commercial software like TTS and ASR engines One aspect that should be taken into account is that the twofold nature of this implementation may raise various considerations regarding its effectiveness Audio as a temporal medium cannot be concurrently presented in contrast to text which can be presented in parallel At the same time grammar files can contain a small amount of terms and this may 
downscale the searching feature Therefore users when accessing an article via telephone they have to hear its content sequentially in order to find a unit of interest something that significantly deteriorates the overall user experience An approach to deal with this 123 Pers Ubiquit Comput 2010 14 535 integrates all these functions and thus collaboration or interface problems may occur when all these components are put together 4 2 Real life scenarios In this paragraph we present a scenario of a typical Table 3 Sample giving specific navigation commands such as BEGIN NEXT PREVIOUS 1 2 etc At any point the user is able to listen to the user manual by speaking the command HELP This case of course is very similar when a visitor of a museum wants to quickly know some additional details about the museum exhibits but has no laptop or internet connection He can use his cell phone to acquire this information wirelessly Table 3 presents a typical dialog like the one described above between a user and the application 
When a request is sent to the web server the wiki engine analyzes it and produces the appropriate query to retrieve the corresponding data from the database Using these data it creates the appropriate VoiceXML and grammar documents When a call is made to the voice server by a standard or mobile phone it starts to interact with its voice browser component Just like a normal web browser the voice browser places a request to the web server for a document containing the information requested The web server responds with a dynamically produced content Unlike the usual case where the web server produced an XHTML document in this case the server produces VoiceXML and grammar documents The voice browser receives it interprets the XML markup and redirects the result to its TTS engine component The TTS engine produces audio stream based on the results it received The voice server then converts the audio signals to packets according to a VoIP protocol such as SIP or to analog voice signals with the help of the 
appropriate hardware Welcome to the Voice Wiki Say HELP for navigation instructions or say SEARCH to search for article HELP Say BEGIN to read an article from the beginning While reading say CONTENTS to hear the article contents Say NEXT to skip to the next paragraph Say PREVIOUS to move back to the previous paragraph Say HELP to hear navigation instructions Say HELP for navigation instructions or say SEARCH to search for article SEARCH Please say one of the following categories ART COMPUTERS HISTORY SCIENCE COMPUTERS Did you say COMPUTERS YES Please provide your search term LOCAL AREA NETWORKS Did you say LOCAL AREA NETWORKS YES One article found Article name is Local Area Network Please say how to proceed BEGIN A local area network is a computer network covering small geographic area like home office or group of buildings NEXT The first LAN put into service occurred in 1964 at the Livermore Laboratory System User System User System User System User System User System User System User System 123 536 Pers 
Ubiquit Comput 2010 14 component installed on it If the client makes the call from a SIP soft phone then the voice data will be carried through the Internet to reach the client If the client makes the call from a fixed phone then the voice signals will be carried through the PSTN network instead Of course if the access to the wiki is done from a web browser then normal HTML pages will be generated and sent to the browser for display Figure 3 depicts the corresponding article as presented in the web browser 5 System evaluation Voice quality in VoIP systems is a multi dimensional and a non trivial problem For such measurements two kind of evaluation methods exist The subjective based methods in which speech samples are presented to an evaluation group of listeners who rate the quality of the vocal information using an integer opinion score All scores are then averaged to produce a mean opinion score MOS value 21 Subjective tests are highly reliable yet they are time consuming costly and the results may be 
biased to human perspective and the different test settings Thus the objective based testing methods have been developed as a solution for effectively measuring voice quality and dealing in parallel with all these issues These methods consist of algorithms carried out by devices involved in the VoIP architecture and do not require any intervention from evaluators Objective tests are further categorized in the intrusive and non intrusive methods based on whether a reference voice signal is used or not respectively In this paper we used a non intrusive objective evaluation method calculating parameters of the ITU T Recommendation G 107 Protocol also known as E model 22 Fig 3 The wiki application accessed from a web browser 123 Pers Ubiquit Comput 2010 14 537 E model was developed by the European Telecommunications Standards Institute ETSI and adopted by International Telecommunication Union ITU In the objectivebased methods several QoS parameters and metrics are involved for measuring voice quality such as the 
signal tonoise ratio SNR packet delays and inter arrival variations jitter etc Through these international standardized metrics the protocol provides a transmission rating factor R factor which varies between 0 and 100 and can be then interpreted in several subjective evaluation values such as the commonly used MOS value Equation 1 provides the estimated MOS value when using the objective E model through the calculated R factor defined in Equation 2 where Ro is a basic SNR value Is Id and Ie are metrics dedicated for the calculation of distortions that occurred in the voice signal combination impairment factor distortions caused by end to end delays and echoing delay impairment factor as well as distortions caused by for instant feedback even when traffic load or device parameters vary 23 For real time acquisition of the parameters and metrics needed we use CommView which is a tool capable for realtime monitoring in Internet and local area networks LANs as well as in analyzing activity of captured data 
network packets http www tamos com products commview Table 4 depicts the results in average values regarding a set of evaluation experiments conducted for measuring the call quality of our system The evaluation procedure was performed as follows We used 45 individuals separated in three groups namely A B and C each consisted of 15 users Group A and C consisted of VoIP users while Group B consisted of telephone users who either use PSTN or a mobile telephone During the first sets of experiments we recorded the averaged values of the Rfactor the available bandwidth of the voice server as well as the jitter between the two communication parts when a single VoIP session was performed from each user We then performed the same voice sessions for additional 10 times by generating traffic in the voice server through protocols UDP TCP and ICMP This was made in order to assess the performance of our system under different network stressing conditions The size of the generated packets was 42 54 and 106 Bytes for UDP 
TCP and ICMP protocols respectively while their generation rate was 30 packets s In the second set of evaluation the user groups involved used either a PSTN and VoIP call or a call from a mobile phone and VoIP call Group B and A respectively Calls were performed virtually simultaneously However since we were not able to measure the voice quality the users received through telephone calls we asked from the users to use the 5 points MOS scale for their evaluation Finally the experiments ended with the evaluation made for three simultaneously VoIP calls from users of Group A and Group C Thus the total amount of separate experiments conducted for voice quality evaluation over different context and networking conditions were 270 90 for single Table 4 Averaged call quality evaluation results Call context Evaluation parameters No stress i TCP Udp ii ICMP TCP iii ICMP UDP iv ICMP TCP UDP v Sing1e VoIP call 1 user R factor 93 3 91 4 92 3 92 8 88 4 Bandwidth 84 61 73 11 74 12 74 58 63 32 Jitter ms 3 97 4 87 4 63 5 67 
8 21 Same time calls PSTN or mobile phone and VoIP 2 users MOS R factor 4 3 91 0 4 0 90 0 4 0 90 3 4 0 90 1 3 8 88 6 Bandwidth 68 21 57 91 58 21 59 67 48 30 Jitter ms 6 39 6 25 6 78 6 34 8 63 Same time VoIP calls 3 users R factor 90 8 89 1 89 3 89 6 83 4 Bandwidth 69 41 60 78 61 75 62 24 39 23 Jitter ms 6 78 7 01 6 83 7 72 10 89 123 538 Pers Ubiquit Comput 2010 14 100 90 80 70 60 50 40 30 20 10 0 i voip 1 user ii iii iv v voip 3 users VoIP calls 90 for two simultaneous calls of different context and 90 for two simultaneous VoIP calls The values depicted in Table 4 are averaged values from 10 different experiments Figures 4 5 and 6 illustrate the averaged values for all three different kind of experiments explained above It was evaluated that in most cases R factor was not significantly influenced by traffic or stressing network conditions However a significant reduce of the R factor was measured when three users called simultaneously under the heaviest stressing condition ICMP TCD and UDP traffic generation 
Fig 4 case v Now as far as jitter is concerned Fig 5 clearly shows that this metric is totally correlated with the amount of simultaneous calls and or calls of different context as its averaged values considerably increases in all cases Once again the larger increase appeared in case v Finally Fig 6 shows that the percentage of available bandwidth presents a similar behavior in respect to the R factor variation and an inverse analogous relation with jitter variation in all cases available bandwidth pstn mobile voip 2 users Fig 6 Jitter evaluation over call context and traffic load R Factor evaluation 100 95 90 85 80 i voip 1 user ii iii iv v voip 3 users pstn mobile voip 2 users Fig 4 R factor evaluation over call context and traffic load ms 12 10 8 6 4 2 0 i voip 1 user ii Average jitter Nevertheless the slope in the R factor reduction is notably lower in comparison to the one of the available bandwidth in cases i ii iii and iv It was assessed that even if the available bandwidth fall down nearly 10 at the 
most in all three separate experiments 84 61 to 74 58 68 21 to 59 67 and 69 41 to 62 24 respectively R factor averaged values were dropped only by 2 units at the most 93 3 to 91 4 91 to 90 and 90 8 to 89 1 This was very encouraging indicating that our system is quite robust when it comes to provide high quality vocal services with two simultaneously calls It was also worth noticing that even if three users used simultaneously our system R factor was measured in acceptable levels even when some stressing network conditions were applied see cases i ii iii and iv in the last column of Table 4 In these cases R factor varied between 90 8 and 89 6 4 2 to 4 0 in the MOS scale Problems appeared when three simultaneous calls were made and the voice server was severely stressed with ICMP TCP and UDP generated packets In that case R factor was measured at the levels of 83 4 in average Even though this number is not so low for having an acceptable communication some users claimed that they did not clearly hear the 
system generated voice probably due to lower R factor values In other words this means that our system is capable of serving two users simultaneously at any call context Three users may also be supported but subject to no heavy stressing network conditions 6 Conclusions and future work In this paper a novel wiki application that can be accessed by virtually any wired or wireless phone as well as by a common web browser was presented By doing so a strong tool for collaboration such as a wiki was made accessible from practically everywhere Our application can equally support similar collaboration tools like Blikis iii iv v voip 3 users pstn mobile voip 2 users Fig 5 Jitter evaluation over call context and traffic load 123 Pers Ubiquit Comput 2010 14 539 8 Werner S Wolff M Eichner M Hoffmann R 2004 Integrating speech enabled services in a web based e learning environment In Proceedings of international conference on information technology coding and computing vol 2 ITCC 2004 24 and other emerging tools like 
Twitters 25 Since common telephones are installed in almost every home our application can bring wikis closer to a wider range of people who do not own or are not comfortable with the use of a PC The advantages of the proposed implementation over similar existing implementations are a it does not require installation of special software or a PC for someone to listen to wiki articles b it is cost efficient for the end user and c it accepts voice commands for navigation throughout articles and for controlling the application flow Currently we are focusing our efforts on creating a voice interface that will interact with the user in a more immediate way so that the user will spend as less time as possible searching locating and navigating throughout the various articles For that purpose we are exploring various adaptive hypermedia techniques in order to proactively adapt the content of articles to each user s likings The use of a more natural language for the interaction with the user is also desirable 
References 1 http www wikipedia org 2 Kolias C Demertzis S Kambourakis G 2008 Design and implementation of a secure mobile wiki system In Uskov V ed 7th IASTED international conference on web based education WBE 2008 March 2008 Innsbruck Austria pp 123 Reproduced with permission of the copyright owner Further reproduction prohibited without permission 
15	a	IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING VOL 18 NO 6 AUGUST 2010 1539 Statistical Transformation of Language and Pronunciation Models for Spontaneous Speech Recognition Yuya Akita Member IEEE and Tatsuya Kawahara Senior Member IEEE Abstract We propose a novel approach based on a statistical transformation framework for language and pronunciation modeling of spontaneous speech Since it is not practical to train a spoken style model using numerous spoken transcripts the proposed approach generates a spoken style model by transforming an orthographic model trained with document archives such as the minutes of meetings and the proceedings of lectures The transformation is based on a statistical model estimated using a small amount of a parallel corpus which consists of faithful transcripts aligned with their orthographic documents Patterns of transformation such as substitution deletion and insertion of words are extracted with their word and part of speech POS contexts and transformation 
probabilities are estimated based on occurrence statistics in a parallel aligned corpus For pronunciation modeling subword based mapping between baseforms and surface forms is extracted with their occurrence counts then a set of rewrite rules with their probabilities are derived as a transformation model Spoken style language and pronunciation surface forms models can be predicted by applying these transformation patterns to a document style language model and baseforms in a lexicon respectively The transformed models significantly reduced perplexity and word error rates WERs in a task of transcribing congressional meetings even though the domains and topics were different from the parallel corpus This result demonstrates the generality and portability of the proposed framework Index Terms Automatic speech recognition ASR language model LM pronunciation model spontaneous speech statistical transformation I INTRODUCTION HE targets of large vocabulary continuous speech recognition LVCSR research have been 
extended in recent years to spontaneous speech such as telephone conversations lectures and meetings Large corpora of conversational telephone speech CTS such as Switchboard and Fisher corpora were collected and a number of LVCSR techniques have been developed with these corpora 1 2 The NIST Rich Transcription RT project has dealt with conversational speech recognition of meetings 3 Such multiparty meetings have also been T Manuscript received March 31 2009 revised October 23 2009 First published November 24 2009 current version published July 14 2010 This work was supported in part by the Strategic Information and Communications R D Promotion Program SCOPE Ministry of Internal Affairs and Communications Japan The associate editor coordinating the review of this manuscript and approving it for publication was Prof Haizhou Li The authors are with the Academic Center for Computing and Media Studies Kyoto University Kyoto 606 8501 Japan e mail yuya media kyoto u ac jp kawahara i kyoto u ac jp Digital Object 
Identifier 10 1109 TASL 2009 2037400 investigated by the AMI AMIDA project conducted by European research institutes 4 A number of speech and linguistic studies for lectures have been conducted using the Corpus of Spontaneous Japanese CSJ 5 which is a collection of academic lectures and public speeches Oral presentations and seminars have also been recorded by European projects such as the TED corpus 6 and the CHIL project 7 The LVCSR of classroom lectures has also been tackled mainly by universities 8 9 Speeches made at public gatherings such as in parliaments and courts are yet another target of LVCSR 1558 7916 26 1540 IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING VOL 18 NO 6 AUGUST 2010 Fig 1 Concept underlying proposed statistical transformation In this paper we propose a novel approach to transform the language and pronunciation models to better model spontaneous speech in which the statistical characteristics of spontaneous speech are modeled separately from task dependent characteristics 
Large document archives such as those of meeting minutes and lecture proceedings are generally easy to access Although these archives are expected to match the target domain of LVCSR they do not have spoken style characteristics Spokenstyle models can be obtained from such large archives by modeling general transformations between orthographic document and spoken styles This transformation can be applied to various spontaneous speech recognition tasks because of the task independent framework Another advantage of the proposed approach is that since the transformation models are much smaller than conventional language and pronunciation models they can be trained with a smaller amount of training data This paper is organized as follows The basic concept underlying the proposed transformation framework is described in Section II Then the actual methods of transforming the language and pronunciation models are fully explained in Sections III and IV Section V describes typical characteristics of spontaneous 
Japanese which are modeled with the proposed approach The approach is evaluated for LVCSR with real data from congressional meetings Our experiments and the results are discussed in Section VI Section VII concludes the paper II BASIC CONCEPT UNDERLYING STATISTICAL TRANSFORMATION The basic idea behind the proposed transformation framework is illustrated in Fig 1 Spoken utterances in the conventional documentation process are transcribed once into a verbatim transcript and then formatted as a document When considering the formatting process as a translation from to an LVCSR system is followed by a statistical machine translation SMT 15 system to automatically produce these are documents Here a large number of spoken transcripts necessary to train the LVCSR system however faithful transcripts are expensive and the size of transcripts is actually limited Therefore direct estimates of the language and pronunciation models are virtually impossible with such small transcripts In contrast formatted documents such as 
the minutes of meetings which are manually edited are archived on a large scale These archives are often recorded in electronic form and are easily available Consequently the key idea behind the prointo language and pronunposed framework is to transform ciation models for spontaneous speech recognition by inverting the formatting process This inversion is modeled as a transformation model using together with corresponding formatted texts transcripts called a parallel corpus hereafter The transformation model for language model predicts N gram entries and estimates their occurrence statistics from document archives The transformation model for pronunciation model generates real pronunciation surface form entries from the orthodox pronunciation baseform of words found in the documents Pronunciation probabilities are also predicted and assigned to all pronunciation entries by the model The correspondences between spontaneous speech phenomena and orthographic expressions are solely targeted in both cases and the 
correspondences are extracted in a more generalized form than word level mappings Consequently the framework is expected to model transformation effectively and efficiently using a small amount of training data Both transformations are described in detail in the following sections III STATISTICAL TRANSFORMATION OF LANGUAGE MODEL We address the statistical transformation of the language model based on the framework described above The simulation and generation of spoken text from written or formal text have previously been proposed as an alternative to the baseline mixture based method mentioned in Section I For example Schramm et al 16 proposed generating a simulated spoken style text by randomly inserting fillers into written style text Petrik and Kubin 17 proposed restoring a literal transcript from non literal documents by using speech recognition and phonetic matching However predicting a verbatim transcript from a formatted text is not usually deterministic for example insertion of fillers is arbitrary 
though not random The reliability of N gram statistics was not always guaranteed in these approaches As the purpose of transformation for LVCSR is to build a spoken style language model and not to obtain a text itself it is more straightforward to estimate the language model statistics directly rather than producing a text Hori et al 18 proposed using a weighted finite state transducer WFST to transform a language model However linguistic expressions transformed into the spoken style were mostly handcrafted and variations and statistics were not well represented We extract the characteristics of the spoken style automatically from a corpus with a sufficient number of reliable statistics A Basic Formulation The concept underlying the proposed statistical transformation of the language model is illustrated in Fig 2 It is based on the framework of statistical machine translation 15 where a sentence of the target language is generated from a sentence of the source language which maximizes the posterior probusing 
Bayes rule ability 1 is usually computed with a translation model We consider document style and spoken style languages as different languages and denote the former as and the latter as AKITA AND KAWAHARA STATISTICAL TRANSFORMATION OF LANGUAGE AND PRONUNCIATION MODELS FOR SPONTANEOUS SPEECH RECOGNITION 1541 TABLE I MAJOR DIFFERENCES BETWEEN SPONTANEOUS SPEECH AND DOCUMENT STYLE TEXT Fig 2 Flow of language model transformation to generate transformation patterns and included in and to estimate probabilities In this paper we consider three types of transformation insertion deletion and substitution Suppose a filler ah is inserted in the middle of I think this is as shown in Fig 2 the words think and this are regarded as contexts and transformation think this and think ah this is modeled as Similarly deletion of a particle for from to wait for him now wait for him and wait him is expressed with and substitution of words am not to the colloquial expression ain t in today I am not fine because is modeled as I am 
not fine and I ain t fine Then N gram entries such as think ah think ah this and ah this are generated with input think this or think this where means any word If context words of the input N gram entries are not sufficient to provide transformed N gram entries for example ah this after insertion of ah in generating a trigram think this necessary context words are added Thus 3 is revised as follows 4 We can estimate spoken language model 1 by rewriting In case that can be generated from multiple entries the resulting occurrence count is a sum of counts estimated from respective by 4 2 5 The conditional probabilities and i e the transformation model can be estimated using a parallel corpus of faithful transcripts and corresponding document style texts When we assume N gram language model for both and actual estimation is carried out over N gram entries and their statistics because N gram probabilities are basically proportional to corresponding N gram statistics Thus 3 is derived from 2 to express an 
occurrence count of a spoken style N gram entry using that of the corresponding document style N gram entry found in an input corpus 3 where denotes the occurrence count of N gram entries When we further limit contextual information for and these probabilities are actually calculated as and for every pair of a spoken style word and a document style word sequence that are sequence found in the parallel aligned corpus The possibility of transformation depends on the context of the target spoken style expression therefore word contexts neighboring words are This context dependent model improves precision but encounters the problem of data sparseness because the parallel corpus is usually small To mitigate this problem we present three models based on back off linear interpolation and maximum entropy ME schemes which take into consideration part of speech POS information B Probabilities to be Estimated The language model of spontaneous speech is estimated within the above framework Types of transformation can be 
classified into three categories viz insertion deletion and substitution as listed in Table I We estimated conditional and for these cases Note probabilities that a detailed analysis of differences between spoken and orthographic Japanese is described in Section V A One of these three types is the insertion of words especially fillers Fillers are often observed at the beginning or the end of utterances and accompanied by a pause However their occurrence is not limited to these points and the frequency depends on actual filler words and contexts Hence insertion probability 1542 IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING VOL 18 NO 6 AUGUST 2010 must be estimated In contrast deletion probability is always one for fillers since fillers must be removed from transcripts for documentation The second type is the deletion of words such as particles Not all particles are deleted in utterances and particles cannot be inserted at all possible points when transforming a transcript and into a document style 
text Thus both must be estimated The third type is substitution of colloquial words and phrases Similar to the first case not all possible words are actually substituted however observed colloquial expressions must always be corrected in document style texts Therefore should be estimated while is set to one C Training of Transformation Model The basic word based transformation model is directly trained using the occurrence counts of word sequences In a training corpus spoken and document style word sequences are annotated and aligned beforehand Then and the statistics occurrence counts for document style word and those for corresponding spoken style sequence are calculated using this parallel word sequence aligned corpus Then word based transformation probabilities are estimated as follows 6 For reliability of probabilities and computational saving we whose count cut off transformation patterns is smaller than or equal to or probability is smaller than However the problem of data sparseness arises with this 
basic model We introduced a transformation model based on POS tags to improve coverage and estimation accuracy A POS tag is assigned to every contextual word by using a morphological analyzer ChaSen 19 We use top level categories of POS tags such as verb adjective and adverb except that nouns and particles are classified into subcategories such as general noun proper noun and pronoun The transformation probabilfor POS based patterns e g VERB ities VERB ah PRONOUN are estimated in PRONOUN and the same way as defined in 6 The same thresholds and are applied to counts and probabilities of POS based patterns and are Conditional probabilities also estimated in the same manner D Back Off Scheme The word based and POS based transformation models can be applied to the N gram entries of a document style language model using 4 A back off scheme is used as the simplest way of combining word based and POS based models exists 7 where is a pattern made from by changing its context words to corresponding POS tags Each word 
based pattern if exists else if POS in this scheme such as think this think ah this is first applied to N gram entries in turn and transformation is carried out with when the pattern matches If it does not match a POS based pattern such as VERB PRONOUN VERB ah PRONOUN is applied When the POS based pattern matches the POS contexts of a transformed entry are replaced with original context words such as think and this to produce a new N gram entry Unlike standard back off N gram modeling we in this scheme bedo not use back off weights to is small in most cases and therefore back off cause weights tend to be almost one E Linear Interpolation Scheme We introduce a scheme of linear interpolation of the two models as an alternative to the back off method The weighted sum of the word based and POS based probabilities is used as the transformation probability in this scheme and transformation is conducted if either the word based or POS based pattern matches an original N gram entry 8 F Maximum Entropy Scheme The 
word based and POS based models in the above schemes are first separately estimated and then combined We introduce the maximum entropy ME scheme 20 to better count the lexical and POS information in a more integrated manner A conis determined by ditional probability 9 where is a feature function is a feature weight and is a normalization factor We used the preceding and following words and their POS tags as features for ME The ME model is applied to every N gram entry of the document style model and a spoken style N gram is generated if the transformation probability is larger than a threshold G Generation of N Gram Entries and Language Model For every input word sequence matching with every transis performed For all matched patterns formation pattern spoken style word sequence is generated from which corresponds to the matched pattern The occurrence count of is then calculated in real numbers based on the transformation probabilities which are defined in either of back off interpolation and ME schemes as 
described above Then statistics of N gram entries are calculated over the generated word sequences Fractions of N gram occurrence counts are finally rounded off and resulting integer counts are used to train a language model in a standard manner There are a large number of N gram entries whose count is less than one and these entries are discarded in this process Although rounding errors are inevitable here their effect is empirically insignificant and this process leads to memory efficiency and compatibility with existing toolkits AKITA AND KAWAHARA STATISTICAL TRANSFORMATION OF LANGUAGE AND PRONUNCIATION MODELS FOR SPONTANEOUS SPEECH RECOGNITION 1543 IV STATISTICAL TRANSFORMATION OF PRONUNCIATION MODEL Next we will describe the transformation of the pronunciation model The design of a pronunciation lexicon was conventionally an empirical issue Manual editing of lexicons 21 is however extremely costly and not practical for LVCSR Therefore various frameworks of pronunciation modeling have been proposed to 
automatically generate a lexicon Previous studies include the knowledge based approach such as the application of phonological rules However this approach does not provide the probabilities of the rules which are necessary to suppress false matching caused by increased numbers of entries The data driven approach has also been studied e g pattern extraction using automatic phone recognition 22 Most of the previous work however has assumed that the domain and lexicon for the training data are the same as those of the test set Pronunciation modeling for spoken Japanese was studied using the CSJ 5 Faithful pronunciations of all speech materials in the CSJ have been transcribed as well as orthographic transcriptions Thus pronunciation variations observed in spontaneous speech can be extracted by matching these two kinds of transcriptions Nanjo and Kawahara have already addressed pronunciation modeling using the CSJ 23 where matching was done word by word and the pronunciation probability was estimated for all 
possible pronunciation variants of a word They also investigated language modeling that separately handled pronunciation variants 23 However these word based approaches are obviously limited to the vocabulary observed in the CSJ and cannot be applied to different tasks We investigate subword based modeling to achieve portability to other domains Pronunciation variation can be described as the transformation of one subword to another Surface forms are obtained by applying such a model to subword sequences of baseforms The decision tree 24 neural network 25 and confusion matrix 26 have been proposed as the modeling frameworks Phones are often used as the modeling units Although pronunciation variations depend on preceding and following contexts most methods have not considered the context or have only counted neighboring phones Moreover the three methods mentioned above Fig 3 Flow of pronunciation model transformation The proposed modeling method involves three steps First patterns in pronunciation variations 
are detected and the necessary statistics for variation patterns and their phone contexts are estimated Next a set of rewrite rules is derived with appropriate contexts and probabilities Finally these rules are applied to baseforms to generate new pronunciation entries surface forms A Extraction of Pronunciation Variations First phonetic transcriptions of spoken words in the training data are compared with baseforms to detect pronunciation variations The input text must be segmented into words to make comparisons and each word must have a pronunciation baseform In Japanese morphological analysis is applied to the input text to insert word boundaries and generate baseforms Then baseforms and phonetic transcriptions surface forms are automatically aligned for each word using a dynamic programming technique given utterance boundaries As Japanese words often have distinct multiple baseforms the most likely baseform is also determined by the alignment process As we can see from Fig 3 if a mismatched pair of a 
baseform and a surface form i e variant is found their phone sequences are identified Each variation is extracted together with its preceding and following phone context and the number of times it occurs is counted We considered up to two phones in both directions as the phone context The length of the context seemed reasonable since at most five phones quinphones are used as a modeling unit in context dependent acoustic modeling Note that the word boundary is also considered as a context because it provides useful information for pronunciation variations B Generation of Probabilistic Rewrite Rules Next probabilistic rewrite rules are generated based on the statistics of variations obtained in the previous step Let be a certain phone or phone sequence with phone context and be a variant of and correspond to the occurrence counts of baseform and surface form with to determine context A threshold is introduced for the adequate length of context so that the model has reliable statistics Namely patterns that are 
more frequent than i e 1544 IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING VOL 18 NO 6 AUGUST 2010 TABLE II EXAMPLES OF SPOKEN STYLE EXPRESSIONS AND THEIR OCCURRENCE COUNTS IN CONGRESSIONAL SPEECH are adopted as rules and their probabilities are computed as 10 are The contextual patterns eliminated by the threshold backed off to shorter context rules We used at most two phones as preceding and following contexts Let and be the respective lengths of the preceding be a set of rules whose conand following contexts and text length is and Rules are defined in a descending order to a context independent rule from the longest context set These once adopted should be excluded from the back off computation For example the adjusted frequency of variation which has preceding context and pattern following context is computed as 11 is all variation patterns applicable to the input where entry and the initial probabilities of are equal i e one divided by the number of baseforms The rules are applied to every 
possible position in the baseform and the probability of a new entry is calculated by multiplying probabilities of the actually applied rules to derive from 12 On the other hand the total amount of probabilities assigned to these new entries is deducted from the initial probability of the baseform entry 13 A resulting entry is discarded if its probability is smaller than a threshold to avoid false matching with rare entries during a decoding process V ANALYSIS ON CHARACTERISTICS OF SPONTANEOUS JAPANESE This section investigates spoken and formal Japanese using parallel aligned corpora to clarify the differences between them The comparison is made in lexical and phonetic sequences Note that we did not analyze the inversion and disorder of phrases and sentences as these were beyond the scope of N gram based LVCSR A Linguistic Characteristics in Spontaneous Japanese We used transcriptions and the minutes of congressional meetings as a parallel corpus for the lexical comparison We prepared faithful 
transcriptions of speech from meetings in the House of Representatives of the National Diet of Japan Most of the speech was extracted from the budget committee in 2003 where a variety of national issues were discussed Committee meetings in the Diet are more spontaneous than plenary sessions in parliament which were mainly dealt in the TC STAR project The House provides minutes of its meetings which were edited to meet the strict orthographic standards by professional stenographers They are not faithful transcripts since redundant expressions such as fillers and end of sentence expressions are deleted and several spoken expressions are modified for purposes of documentation They were manually compared and aligned to faithful transcriptions of the original speech and each edit was annotated The transcripts in this analysis contained 737 K words Table II lists typical differences and their occurrences in the parallel aligned corpus The most significant phenomenon in spontaneous speech is the insertion of 
fillers and end of sentence expressions as listed in Table II a A similar tendency has been reported in lectures in the CSJ 28 As these kinds of end of sentence expressions and conjunctives are often treated as fillers many such expressions are removed from minutes The rewrite rules for variation consist of context and individual rule entries have their sets Finally we also introduce a own probabilities threshold for the probabilities and rules that have probabilities larger than i e are adopted This threshold is intended to save computation at the time of application by discarding trivial rewrite rules C Application of Variation Rules Then new surface forms are generated by applying the set of rules to baseforms in a lexicon 1 Rules with longer contexts are applied with higher priority and then backed off to shorter contexts if necessary The probabilities of a resulting new pronunciation entry and the original for a lexical entry are updated as 12 and 13 1Using a finite state transducer FST would realize a 
more solid implementation but it was not adopted in this work AKITA AND KAWAHARA STATISTICAL TRANSFORMATION OF LANGUAGE AND PRONUNCIATION MODELS FOR SPONTANEOUS SPEECH RECOGNITION 1545 TABLE III EXAMPLES OF PRONUNCIATION VARIATIONS EXTRACTED FROM THE CSJ The deletions and substitutions in Table II b and c occur fewer times than insertions but have typical patterns Most deletions are postpositional particles which indicate the relationship between words or phrases such as the linguistic case structures Note that not all postpositional particles are deleted e g those indicating the nominative case such as wa and ga are often omitted while those indicating the possessive case are rarely dropped Lexical substitutions with colloquial expressions are often used for short smooth utterances just as the contracted forms of can t and ain t are often used in English conversation A large number of such substitutions appear at the ends of sentences which corresponds to verbs and auxiliary verbs in Japanese B 
Pronunciation Variations in Spontaneous Japanese We used the CSJ for phonetic comparison which has faithful transcriptions of speech from lectures We generated an orthographic pronunciation of speech using a morphological dictionary and then compared it against the faithfully transcribed pronunciation A total of 630 K words was used for this analysis Typical variations in the CSJ are listed in Table III One of the major differences occurs in vowels i e a short vowel or a diphthong becomes a long vowel and a long vowel becomes a short vowel Japanese syllables typically consist of a consonant followed by a vowel therefore vowels surrounded by consonants typically k sometimes vanish glottal stop Most of these variations cause us to speak faster Unvoiced consonants are often voiced in compound words of nouns However some consonants are also dropped to simplify pronunciation in limited phone contexts Some examples of derived rewrite rules are listed in Table IV The derived rule set includes typical cases of 
pronunciation variations that are phonologically predictable e g e i e diphthong to long vowel and k u q vanishing vowel However our results attach appropriate probabilities to these Moreover a number of variants that are characteristic to spontaneous speech and cannot be predicted by using phonology were also found VI EXPERIMENTAL EVALUATIONS We carried out experiments on real congressional speech to evaluate the proposed transformation schemes of language and pronunciation models First we preliminarily examined three modeling schemes for transforming the language model and TABLE IV EXAMPLES OF REWRITE RULES denotes word boundary they were then compared with conventional methods Finally we transformed the pronunciation model and evaluated it A Comparison of Methods of Transforming Language Model We preliminarily evaluated how effective the back off linearinterpolation and ME based methods were for transforming the language model We collected archives of the minutes of the National Diet over four years 1546 
IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING VOL 18 NO 6 AUGUST 2010 TABLE V PERPLEXITY FOR BUDGET COMMITTEE MEETING TEST SET TABLE VI SPECIFICATIONS OF 2006 CONGRESSIONAL SPEECH TEST SET TABLE VII SPECIFICATIONS OF LANGUAGE MODELS tuned depending on characteristics of and the target domain For the reference models in this experiment the best interpolation weight was separately chosen based on the perplexity over the test set 2 The perplexity PP out of vocabulary OOV rates and numbers of bigram and trigram entries for the two mixture models and the three transformed models are listed in Table V The combination with CSJ CSJ improved the perplexity and the OOV rate however the improvement against the baseline was smaller because it could not provide sufficient N gram entries for this task In contrast the combination with transcripts of the parallel corpus Transcript considerably improved perplexity These transcripts matched the task and N gram entries in the transcripts well covered spoken 
expressions in the test set although the increase of N gram entries was small All transformed models further reduced perplexity Among these models the back off model achieved smallest perplexity The reduction by this model over the baseline model was 37 5 and that over the conventional CSJ model was 34 1 The reduction over the Transcript model was 7 7 It was confirmed that the proposed transformation generated more N gram entries than the mixture based models and it led to these improvements We also confirmed that most of these N gram entries were generated by the POS based transformation When POS based transformation was omitted and word based transformation was solely applied only 3 70 M trigram entries were obtained and perplexity by this model was 75 2 When we compare the three transformed models there were fewer trigram entries in the linear interpolation model than in the other two transformed models because the effect of POS based transformation was reduced by the interpolation The ME model generated 
more N gram entries than the back off model however the perplexity by the former was larger Because the set of features for the ME model seemed too large the parameters could not be estimated appropriately Based on the results we adopted the back off scheme for transforming the language model B Transformation of Language Model in Different Topic Domains We applied the proposed transformation model to different topic domains in the National Diet to evaluate the generality We prepared new test sets for various committee meetings held in 2006 including those of the budget committee as listed in 2We did not prepare held out data for tuning but this would give a positive bias to the reference models Table VI There were a total of five meetings and the test set transcripts contained 144 K words in total The transformation model was retrained for this experiment using the entire parallel corpus 737 K explained in Section V A We obtained 6339 transformation patterns and 2310 36 of these were POS based We also 
enhanced the baseline language model used in the previous experiment by adding the minutes for year 2003 to 2005 to the training texts Then conventional mixture models were created by combining it with the transcripts in the CSJ or the parallel corpus Best mixture weights were preliminarily chosen for these models using perplexity on the test set The transformed model was generated based on the back off scheme The specifications for all models are listed in Table VII These models except the baseline model have the same vocabulary of 54 321 items The difference between this vocabulary and the baseline vocabulary was mostly fillers that were found only in the transcripts of the parallel aligned corpus The transformation added these words to the baseline vocabulary and resulting vocabulary was also applied to the mixture models The average OOV rate by this vocabulary over the test set is 0 15 as shown in Table VI Note that the baseline model was only used to mix with the other models and not to directly 
evaluate as it was obviously inappropriate for speech recognition The acoustic model in this experiment was newly trained using 140 hours of speech from meetings in the Diet with applying vocal tract length normalization VTLN 29 30 and minimum phone error MPE training 31 As we can see from Fig 4 perplexity was drastically reduced by the Proposed model over the CSJ and Transcripts models in all meetings The average perplexity by CSJ Transcripts and Proposed models corresponded to 52 0 49 1 and 41 2 Reduction in perplexity by the transformed model was 20 8 against the CSJ model and 16 1 against the Transcripts model Although the CSJ model had many more trigram entries 7 29 M than the other two models its perplexity was larger This suggests that a simple AKITA AND KAWAHARA STATISTICAL TRANSFORMATION OF LANGUAGE AND PRONUNCIATION MODELS FOR SPONTANEOUS SPEECH RECOGNITION 1547 Fig 4 Perplexity by language models Fig 5 Word error rates by language models mixture based language model does not sufficiently cover 
effective N gram entries The Transcripts model reduced perplexity over the CSJ model however the reduction was relatively smaller This is because mixture with the transcript yielded a limited number of trigram entries This result demonstrated efficient and effective prediction could be accomplished with the proposed transformation The reduction in perplexity with the proposed model against the Transcripts model was larger in this experiment than that obtained in the previous experiment 6 9 where the training and test set data were taken from the same budget committee and the date of the meetings were close Since the topics in both data were similar the mixture based Transcripts model could accomplish high coverage and small perplexity In contrast the test data in this experiment were chosen from a different year and thus the topics were also different even in the same budget committee While the Transcripts model could not greatly reduce perplexity the proposed model accomplished significant perplexity 
reduction This means that the proposed approach is general and even more effective in different topic domains Fig 5 is a bar graph of the word error rates WER for the three language models Here we used a pronunciation lexicon without the surface forms provided by our method that were described in Section IV The WERs obtained by the CSJ and Transcripts models were comparable whereas the proposed Transformed model significantly reduced the WER The reduced WER by using the Transformed model over the CSJ and Transcript models corresponded to 3 2 and 2 8 relative and these were statistically signifi The proposed method particularly reduced cant at errors around the fillers which might be inserted at many points The mixture based approach achieved limited error reduction around fillers since it could only provide N gram entries observed in the training corpus Actually we conducted an evaluation in which fillers were removed from automatic speech recognition ASR results and reference texts before comparison Here we 
obtained WERs of 21 6 21 5 and 20 9 by CSJ Transcripts and Transformed models respectively Improvements by the proposed transformation over CSJ and Transcripts models were 3 1 and 2 6 relative which were still statistically significant at This result demonstrates the advantage of the proposed method i e it could predict unseen N gram entries in the training corpus C Effect of Training Data Size In the proposed transformation the size of training data might affect the performance of the transformed language model To investigate this we made another experiment using transformation models that were trained with a half a quarter and one eighth portions of the parallel aligned corpus Average perplexity over the test set by language models transformed with these models were 42 0 43 1 and 44 4 respectively while the full size model achieved 41 2 as mentioned above This result suggests that the proposed method properly works even if the size of training data is one eighth i e smaller than 100 K words and the 
performance was almost saturated with the 737 K corpus D Transformation of Pronunciation Model Using the CSJ Finally we evaluated the proposed transformation scheme of the pronunciation model We used the CSJ in Section V B to train the transformation model We used all lectures in the CSJ 6 3 M words Thresholds and were determined as and based on our previous experiments on discussion tasks 32 As a result 265 kinds of variation patterns and 1381 rules were obtained By applying these rules the lexicon of 57 462 baseform entries was expanded to 64 857 entries by adding 7395 surface forms Fig 6 is a bar chart of the WER on the same test set by using the baseline pronunciation lexicon and the transformed lexicon which contains surface forms generated by the rewrite rules The Transformed language model in Table VII was used in this experiment Reduced WER was achieved in all meetings and the average reduction was relatively 4 6 This reduction is statis The training data used for the tically significant at 
transformation model was the CSJ which had a completely different domain from the Diet task Therefore the results demonstrated that the proposed framework could model spontaneous characteristics separately from the domain characteristics of the training data and achieved portability to other task domains The average WER by transforming the language and pronunciation models was 20 2 The improvement in both transformations over the CSJ language model and the baseline pronunciation lexicon was relatively 7 6 which is almost the 1548 IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING VOL 18 NO 6 AUGUST 2010 REFERENCES 1 G Zavaliagkos J McDonough D Miller A El Jaroudi J Billa F Richardson K Ma M Siu and H Gish The BBN Byblos 1997 large vocabulary conversational speech recognition system in Proc ICASSP 1998 pp Fig 6 Word error rates by pronunciation models same as the sum of individual improvement 3 2 and 4 6 This means that two transformations could work synergistically and effectively improved LVCSR VII 
CONCLUSION We proposed a novel approach of statistical transformation that efficiently generated a language and a pronunciation model for spontaneous speech recognition The transformation model for the language model contained context dependent probabilistic patterns of transformation from document style to spontaneous speech These patterns and their probabilities were determined based on occurrence statistics in a parallel corpus of faithful transcripts and their corresponding document style texts Since these training data were small contexts were backed off to the POS level which provided more robust prediction The transformation model was applied to the N gram counts of a document style language model and N gram entries for spontaneous speech were generated with the estimated number of occurrences The transformation model for a pronunciation model consisted of probabilistic rewrite rules with phone contexts of variable lengths The pronunciation variations between baseform and surface forms were extracted 
from a large scale spontaneous speech corpus CSJ then phone context dependent variation patterns and their occurrence probabilities were trained Since the probabilistic model was generalized it can be applied to any lexicon of new domains to generate appropriate surface forms with their probabilities In experimental evaluations on real congressional speech the proposed method efficiently and effectively generated a language model and a pronunciation model and reduced both perplexity and WER The transformed language model was tested on meetings whose topics and times were different from those in the training data and it accomplished reduced WER without side effects of increasing vocabulary The result demonstrated the generality of the proposed transformation of language model for new domains As for pronunciation model the transformation model was trained with the CSJ and tested on the Diet corpus Although the two corpora were completely different the generated pronunciation model reduced WER This clearly 
demonstrated the portability of our approach and thus transformation is expected to be applied to any spontaneous speech recognition tasks AKITA AND KAWAHARA STATISTICAL TRANSFORMATION OF LANGUAGE AND PRONUNCIATION MODELS FOR SPONTANEOUS SPEECH RECOGNITION 1549 26 D Torre L Villarrubia J Elvira and L Hernandez Gomez Automatic alternative transcription generation and vocabulary selection for flexible word recognizers in Proc ICASSP 1997 pp Tatsuya Kawahara M 
16	a	Lecture Notes in Artificial Intelligence Edited by R Goebel J Siekmann and W Wahlster 6233 Subseries of Lecture Notes in Computer Science Hrafn Loftsson Advances in Natural Language Processing 7th International Conference on NLP IceTAL 2010 Reykjavik Iceland August 16 18 2010 Proceedings 13 Series Editors Randy Goebel University of Alberta Edmonton Canada Library of Congress Control Number 2010931612 CR Subject Classification 1998 I 2 H 3 H 4 H 5 H 2 J 1 LNCS Sublibrary SL This work is subject to copyright All rights are reserved whether the whole or part of the material is concerned specifically the rights of translation reprinting re use of illustrations recitation broadcasting reproduction on microfilms or in any other way and storage in data banks Duplication of this publication or parts thereof is permitted only under the provisions of the German Copyright Law of September 9 1965 in its current version and permission for use must always be obtained from Springer Violations are liable to prosecution 
under the German Copyright Law springer Preface The research papers in this volume comprise the proceedings of IceTAL 2010 an international conference on natural language processing NLP IceTAL was the seventh in the series of the TAL conferences following GoTAL 2008 Gothenburg Sweden FinTAL 2006 Turku Finland EsTAL 2004 Alicante Spain PorTAL 2002 Faro Portugal VexTAL 1999 Venice Italy and FracTAL 1997 Organization IceTAL 2010 was organized by the Icelandic Centre for Language Technology ICLT Program Committee Walid El Abed Jan Alexandersson Jorge Baptista Tilman Becker Chris Biemann VIII Organization Caroline Brun Sylviane Cardey Robin Cooper Walter Daelemans Rodolfo Delmonte Markus Dickinson Mikel L Forcada Robert Gaizauskas Filip Ginter Peter Greenfield Philippe de Groote Xerox Corporation France University of Franche External reviewers Krasimir Angelov Organization IX Robert Nesselrath Anna B Gabriel Sekunda Jinsong Su Armando Xinyan Xiao Hao Xiong Conference Sponsors Table of Contents Invited Talks 
Reliving the History The Beginnings of Statistical Machine Translation and Languages with Rich Morphology Jan Haji c Harmonizing WordNet and FrameNet Christiane D Fellbaum 1 2 Research Papers A Morphosyntactic Brill Tagger for Inflectional Languages Szymon 15 27 39 45 57 67 79 XII Table of Contents Concept Based Representations for Ranking in Geographic Information Retrieval Maya Carrillo 85 97 103 115 121 127 138 150 162 167 179 185 Table of Contents XIII Symbolic Classification Methods for Patient Discharge Summaries Encoding into ICD Laurent Kevers and Julia Medori User Tailored Document 197 209 215 226 238 250 257 263 269 281 293 305 314 320 XIV Table of Contents Semi automatic Endogenous Enrichment of Collaboratively Constructed Lexical Resources Piggybacking onto Wiktionary Franck Sajous Emmanuel Navarro Bruno Gaume Laurent 332 345 357 369 381 393 401 406 418 431 Reliving the History The Beginnings of Statistical Machine Translation and Languages with Rich Morphology Jan Ha ji c Institute of Formal and 
Applied Linguistics School of Computer Science Charles University Prague Czech Republic hajic ufal mff cuni cz Abstract In this two for one talk first some difficult issues in morphology of inflective languages will be presented Then to lighten up this linguistically and computationally heavy issue a half forgotten history of statistical machine translation will be presented and contrasted with current state of the art in a rather non technical way Computational morphology has been on and off the focus of computational linguistics Only few of us probably remember the times when developing the proper formalisms has been in such a focus a history poll might still find out that some people remember DATR II or other heavyduty formalisms for dealing with the virtually finite world of words and their forms Even unification formalisms have been called to duty and the author himself admits to developing one However it is not the morphology itself not even for inflective or agglutinative languages that is causing the 
H Loftsson E Harmonizing WordNet and FrameNet Christiane D Fellbaum Department of Computer Science Princeton University Princeton USA fellbaum princeton edu Abstract Lexical semantic resources are a key component of many NLP systems whose performance continues to be limited by the lexical bottleneck Two large hand constructed resources WordNet and FrameNet differ in their theoretical foundations and their approaches to the representation of word meaning A core question that both resources address is how can regularities in the lexicon be discovered and encoded in a way that allows both human annotators and machines to better discriminate and interpret word meanings WordNet organizes the bulk of the English lexicon into a network an acyclic graph of word form meaning pairs that are interconnected via directed arcs that express paradigmatic semantic relations This classification largely disregards syntagmatic properties such as argument selection for verbs However a comparison with a syntax based approach like 
Levin 1993 reveals some overlap as well as systematic divergences that can be straightforwardly ascribed to the different classification principles FrameNet s units are cognitive schemas Frames each characterized by a set of lexemes from different parts of speech with Frame specific meanings lexial units and roles Frame Elements FrameNet also encodes cross frame relations that parallel the relations among WordNet s synsets Given the somewhat complementary nature of the two resources an alignment would have at least the following potential advantages 1 both sense inventories are checked and corrected where necessary and 2 FrameNet s coverage lexical units per Frame can be increased by taking advantage of WordNet s class based organization A number of automatic alignments have been attempted with variations on a few intuitively plausible algorithms Often the result is limited as implicit assumptions concerning the systematicity of WordNet s encoding or the semantic correspondences across the resources are not 
fully warranted Thus not all members of a synonym set or a subsumption tree are necessarily Frame mates We carry out a manual alignment of selected word forms against tokens in the American National Corpus that can serve as a basis for semi automatic alignment This work addresses a persistent unresolved question namely to what extent can humans select and agree on the context appropriate meaning of a word with respect to a lexical resource We discuss representative cases their challenges and solutions for alignment as well as initial steps for semi automatic alignment Joint work with Collin Baker and Nancy Ide H Loftsson E A Morphosyntactic Brill Tagger for Inflectional Languages Szymon Acedaski1 2 Institute of Informatics University of Warsaw ul Banacha 2 02 097 Warszawa Poland accek mimuw edu pl Institute of Computer Science Polish Academy of Sciences ul Ordona 21 01 237 Warszawa Poland 1 2 Abstract In this paper we present and evaluate a Brill morphosyntactic transformation based tagger adapted for 
specifics of highly inflectional languages Multi phase tagging with grammatical category matching transformations and lexical transformations brings significant accuracy improvements comparing to previous work Evaluation shows the accuracy of 92 44 for the Polish language which is higher than the same metric for the other known taggers of Polish stochastic trigram tagger 90 59 and hybrid tagger TaKIPI employing decision tree classifier and automatically extracted rule based tagger used for tagging the IPI PAN Corpus of Polish 91 06 Keywords PoS tagger Brill tagger inflectional language tagger morphosyntactic tagger lexical rules 1 Introduction Morphosyntactic tagging is a classic problem in NLP with applications in many higher level processing solutions namely parsing and then information retrieval speech recognition and machine translation Part of Speech tagging for English is already well explored and many taggers have been built with accuracy exceeding 98 In case of inflectional languages these numbers 
are much lower reaching 95 78 for Czech 1 and 92 55 for Polish per 2 evaluation by Karwaska and H Loftsson E 4 S Acedaski Table 1 Example tags in Brown s English Tagset and IPI PAN Polish tagset English VBD PPS Polish praet sg m1 perf verb past tense pronoun personal nominative 3rd person singular l participle singular human masculine perfective aspect ppron12 sg nom f pri 1st person pronoun singular nominative feminine the correct tags in some cases Because of this some corpora allow multiple tags to be assigned to a single segment whereas other require fully disambiguated tagging usually providing detailed instructions on how to do this This matter will be further discussed in the Evaluation section Several tagging techniques are commonly known The most frequently used approaches are stochastic e g based on Hidden Markov Models 7 and rulebased1 Brill 4 presents a transformation based Part of Speech tagger for English which automatically chooses good quality transformations given a number of general 
transformation templates and a training corpus The tagger used for morphosyntactic disambiguation of the current version of the IPI PAN corpus called TaKIPI 8 is a hybrid multiclassifier transformation based tagger Some of the transformations it uses were extracted automatically using machine learning algorithms and then reviewed and adjusted by linguists In this paper we describe and evaluate an implementation of the Brill s algorithm adapted for rich inflectional languages First steps towards this were described by Acedaski and Goluchowski in 2009 9 but that tagger was then rewritten with different approaches used in most parts As in previous work the adaptation involves splitting the process into phases so that at first only the part of speech and a few grammatical categories are disambiguated Remaining categories are determined in the second pass On top of it the new more general approach to transformation templates was developed and additional transformation templates allowing for transformations which 
look at particular grammatical categories of surrounding segments were added Also lexical transformations were used Finally the tagger was implemented using a new simplified algorithm based on FastTBL 10 and parallelized for better performance 2 The Original Brill Tagger Let us describe the original Brill s algorithm in some detail We assume that we are given three corpora a large high quality tagged training corpus smaller 1 Throughout this paper the term rule based tagger is used to denote systems using hand written rules For the algorithms involving automatic extraction of rules the term transformation based tagger is used A Morphosyntactic Brill Tagger for Inflectional Languages 5 tagged corpus called patch corpus and another one test corpus which we want to tag Brill also assumes that only one correct tag can be assigned to a segment Let s denote the tag assigned to i th segment as ti Tagging is performed in four steps 1 A simple unigram tagger is trained using the large training corpus 2 The unigram 
tagger is used to tag the patch corpus 3 There are certainly some errors in the tagging of the patch corpus Therefore we want to generate transformations which will correct as many errors as possible a We are given a small list of so called transformation templates Brill uses the following templates in his paper i ti A if ti B oO1 ti o C ii ti A if ti B oO2 ti o Co iii ti A if ti B and i th word is capitalized iv ti A if ti B and i 1 th word is capitalized 3 Adaptation for Inflectional Languages The algorithm described in the previous section was subsequently extended by applying a number of techniques targeted at improving accuracy of tagging of inflectional languages These techniques are 6 S Acedaski 3 1 Multi pass Tagging The first technique is used to reduce the size of the transformation space and to avoid too specific transformations in the first stage It is inspired by 9 where the authors split the tags into two parts sharing only the part of speech In the first run of the Brill tagger the tagset 
consists of only one of the parts of tags In the second run the tagset comprised of the other parts is used but the previously selected parts of speech are fixed for the second pass Also Tufi 11 proposes using a reduced tagset with easily recoverable grammatical categories not present to improve performance Out goal is different though we try to leave some of the hard to disambiguate categories for later stages so that the tagger already has more information from preceding phases We consider a sequence Ti i 0 k 1 of gradually simplified tagsets T0 is the original tagset and Tj 1 j 0 k 2 are some other tagsets Projections mapping specific tags to more general tags are also needed j Tj Tj 1 For each of the tagsets a separate pass of the Brill algorithm is performed The tag assigned to the i th segment in the p th pass p 1 k is denoted by tp i In the first pass the simplest tagset Tk 1 is used In the p th pass for i th p p 1 segment only tags tp i Tk p are considered such that k p ti ti In our experiments we 
used only two tagsets T0 being the original and T1 which had information about part of speech case and person only 0 was a natural projection i e the one which strips values of grammatical categories not present in T1 The produced software can be configured for more than two phases with different tagsets 3 2 Generalized Transformation Templates In the original Brill classifier all the transformation templates are of the following form Change ti to A if it is B and In our tagger we generalize the possible transformation templates by allowing other operations than changing the entire tag to be performed Also the current tag of a lexeme need not be fully specified in an instantiation of some transformation template A particular transformation template consists of a predicate template which specifies conditions on the context where the transformation should be applied and an action template describing the operation to be performed if the predicate matches For example in the transformation template change the tag 
to A if the tag is B the first part change the tag to A is the action template and the A Morphosyntactic Brill Tagger for Inflectional Languages 7 second part the tag is B is the predicate template The same nomenclature is applied to instantiated transformations This generalization was performed in order to allow using more general transformations than allowed by the original algorithm Let s denote by ti case the value of grammatical category case in the tag of the i th segment of the text Now consider the very robust linguistic rule if an adjective is followed by a noun then they should agree in case This rule may be composed p tp i T oO ti o U p ti T oO tp i o Uo p 1 ti T oO1 tp i o U p p ti pos P ti C X p tp i pos P ti C X p oO tp i o pos Q ti o C Y p oO ti o pos Qo tp i o C Yo and action templates 1 tp i V 2 tp i pos R 3 tp i C Z Additionally the actions were implemented in such a way that they were not applied if they were to assign a tag not reported by the morphological analyzer for a particular 
segment In case of actions 2 and 3 the nearest possible tag was used instead The metric used here is the number of matching values of grammatical categories but only tags with the expected part of speech are considered If no such tags are possible the action is not performed 3 3 Lexical Transformations Another extension which proved very useful are lexical transformation templates proposed by Brill in a later paper 12 Megyesi 13 subsequently explored them for Hungarian which is an agglutinative language with a number of affixes possessing grammatical functions The results were very promising The author used the following predicate templates 8 S Acedaski 1 2 3 4 tp i tp i tp i tp i T T T T orthi contains letter L orthi starts ends with S S 7 orthi with deleted prefix suffix S S 7 is a word orthi1 W orthi 1 W Here orthi simply denotes the orthographic representation of the i th segment Inspired by this work and after some experiments we extended the list of predicate templates by only the prefix suffix 
matching 1a tp i T orthi ends with S 1b tp i T orthi starts with S tp P tp i C X orthi ends with S 4a i pos p oO tp i o pos Q ti o C Y p tp i pos P ti C X p oO tp 4b i o pos Q ti o C Y orthi o ends with S where S and S are any strings no longer than 3 and 2 characters respectively This resulted in over 1 5 accuracy improvement over the Brill tagger with only generalized transformations as tested for Polish 3 4 Simplified FastTBL Implementation The idea behind the FastTBL algorithm 10 is the minimization of the number of accesses to data structures for storing Finally the tagger was implemented specifically for multiprocessing environment mostly because of the high memory requirements for storing the A Morphosyntactic Brill Tagger for Inflectional Languages 9 Algorithm 1 Pseudocode of the simplified FastTBL algorithm 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 Initializing good and bad data structures for i 0 to len text do for each transformation r 
which matches at position i do if r corrects the classification of i th segment then increase good r else if r changes the classification of i th segment from correct to wrong then increase bad r end if end for end for Main loop generating transformations loop b arg maxr good r bad r the best transformation if good b bad b threshold then return end if add b to the sequence of generated transformations text text after application of b for each position i in vicinity of the changes performed by b do for each transformation r which matches at position i in text do if r corrected the classification of i th segment in text then decrease good r else if r miscorrected the classification of i th segment in text then decrease bad r end if end for for each transformation r which matches at position i in text do if r corrects the classification of i th segment in text then increase good r else if r miscorrects the classification of i th segment in text then increase bad r end if end for end for text text end loop 10 S 
Acedaski 4 Evaluation The tagger was evaluated on two corpora of Polish the IPI PAN Corpus of Polish 6 and the new National Corpus of Polish 15 in preparation version dated Table 2 Evaluation results IPI PAN Corpus Sources 3 16 Full tags C WC P R F C WC PoS only P R F Tagger Trigram HMM 17 87 39 90 59 84 51 83 09 83 80 96 79 97 11 96 75 96 78 96 77 TaKIPI 2 Brill 9 2009 Brill this paper 88 68 91 06 90 94 83 78 87 21 96 53 96 54 96 58 96 71 96 65 89 46 83 55 86 40 90 00 92 44 92 44 86 05 89 13 98 17 98 18 98 18 98 16 98 17 Table 3 Evaluation results National Corpus of Polish full tags Thra Time s 2 6 a transformations P1 P2 2748 0 612 2 Acc 92 82 92 68 1450 5175 1 632 1422 6 The minimum good r bad r of the generated transformations Correctness C percent of segments for which the tagger assigned exactly the same set of tags as the golden standard Please note that in the IPI PAN corpus for some segments several tags are marked correct Weak correctness W C percent of segments for which the sets of 
interpretations determined by the tagger and the golden standard are not disjoint Precision P percent of tags given by the morphological analyzer which were both chosen by the tagger and the golden standard Recall R percent of golden tags which were chosen by the tagger 2P R F measure F F P R A Morphosyntactic Brill Tagger for Inflectional Languages 11 Accuracy Acc any of the above in the case of the National Corpus of Polish which has always one golden tag per segment The times in Table 3 were obtained on a multiprocessor machine with Xeon CPUs clocked at 2 4 GHz with 12MB cache 6 processes were run The tagger was compiled in 64 bit mode which probably negatively impacted performance due to almost doubled memory usage 1 2 GB per process compared to 0 7 GB in similar 32 bit setup but this was not verified It is also worth noting that only TaKIPI does not disambiguate words not known to the morphological analyser even if the input contains a number of possible morphosyntactic interpretations To provide a 
better insight into the classes of errors generated by the tagger detailed statistics are presented in Tables 4 5 and 6 It can be clearly seen that the most problems the tagger has concern case and gender Slightly fewer errors are reported for number This is similar to previous findings and not unexpected for Polish Nevertheless the introduction of lexical elements in transformation templates gave over 1 5 improvement in accuracy on the National Corpus of Polish Over 60 of all generated transformations do contain lexical matchers The vast majority of them is used for determining the correct case by matching nearby segments suffixes see Table 7 Also they are used for disambiguating rare flexemic classes like qub from conj There are also some categories of errors in the testing corpush which would not be disambiguated by a human looking at the same amount of context Let us present several examples Long nominal phrases especially near sentence or subordinate clause boundaries Table 4 Error rates for parts of 
speech shown only values 0 01 Expected PoS errs toks subst 3028 0 47 qub 1658 0 26 adj 1596 0 25 ger 1392 0 21 conj 652 0 10 adv 597 0 09 ppas 522 0 08 Expected PoS errs toks prep 471 0 07 pred 363 0 06 num 326 0 05 fin 268 0 04 pact 208 0 03 comp 172 0 03 Table 5 Error rates for grammatical categories shown only values 0 01 Category errs toks case 21259 3 28 gender 16151 2 49 number 4645 0 72 aspect 416 0 06 accommodability 193 0 03 12 S Acedaski Table 6 Specific errors in assignment of grammatical categories top 15 records Expected case nom case acc gender m1 case gen number sg number pl gender m3 gender m3 gender m1 gender f gender m3 gender f case gen case acc gender n Actual errs toks case acc 7188 1 11 case nom 4717 0 73 gender m3 2543 0 39 case acc 2533 0 39 number pl 2460 0 38 number sg 2185 0 34 gender m1 1989 0 31 gender n 1662 0 26 gender f 1375 0 21 gender m3 1243 0 19 gender f 1214 0 19 gender n 1115 0 17 case nom 1105 0 17 case gen 963 0 15 gender m3 907 0 14 Table 7 Sample lexical 
transformations generated by the tagger in the first pass No r Change case of preposition from acc to loc if it ends 3 with na in practice this asks for the particular preposition na in English on and one of two following segments has case of loc Change case of an adjective from loc to inst if one of 7 the three following segments has case of inst and ends with em good r bad r 2496 113 921 29 tego znaku Zamilowanie do sportu i this sign Passion for sport and Here the underlined word can have either nominal or accusative case Expressions with words like pastwo which may be either a noun country or a pronoun formal plural you o czym pastwo w tej chwili what you the country at the moment This calls for enlarging the lookup context in the future For example predicates like the nearest segment with part of speech P has category C equal X may be good candidates for inclusion This requires extending the vicinity parameter and therefore slows down the computations but may result in better accuracy 5 Conclusions and 
Future Work The paper presents and evaluates a number of techniques designed to adapt Brill tagger for inflectional languages with large tagsets Especially adding predicates A Morphosyntactic Brill Tagger for Inflectional Languages 13 and actions which allow matching or changing values of single grammatical categories as well as adding lexical transformations were the most valuable modifications of the original algorithm It is worth noting that the tagger does not need any linguistic knowledge provided except the specification of tagsets and the information about the grammatical categories which should be disambiguated in consecutive phases Rule templates are not designed for any specific language Even if some transformation templates are not suitable for considered language they may negatively impact only performance but not accuracy As far as the quality of the new tagger is concerned the reported numbers are at least 1 1 higher than for other existing taggers for Polish although this should be 
independently verified Also it may be an interesting experiment to use the tagger for other languages like Czech or Hungarian maybe after inclusion of all lexical transformations proposed by Megyesi 13 There are also some other places for improvement not explored yet namely 1 Experimenting with different simplified tagsets and more than 2 passes Tufi 11 proposes using an additional reduced tagset to collapse grammatical categories which are unambiguously recoverable from the lexicon This reduces the transformation space improving performance Others suggest joining some parts of speech or values of grammatical categories which have similar grammatical functions in the first pass to disambiguate them later For example in an intermediate phase one would use the value nom or acc for case 2 Simply enlarging the context of transformation templates may be a good way to go 3 Designing transformation templates which look for the nearest segment with a particular part of speech or value of some grammatical category 
may improve accuracy The full source code of the tagger is available under the terms of the GNU GPL v3 from its project page http code google com p pantera tagger Acknowledgments I d like to sincerely thank my academic advisor Prof Adam References 1 14 S Acedaski 3 Karwaska D Hybrid Syntactic Semantic Reranking for Parsing Results of ECAs Interactions Using CRFs Enzo Acerbi1 Guillermo 1 Julietta Research Group University of Seville enzoace gperez us es 2 University of Milano Bicocca stella disco unimib it Abstract Reranking modules of conventional parsers make use of either probabilistic weights linked to the production rules or just hand crafted rules to choose the best possible parse Other proposals make use of the topology of the parse trees and lexical features to reorder the parsing results In this work a new reranking approach is presented There are two main novelties introduced in this paper firstly a new discriminative reranking method of parsing results has been applied using Conditional Random 
Fields CRFs for sequence tagging Secondly a mixture of syntactic and semantic features specifically designed for Embodied Conversational Agents ECAs interactions has been used This approach has been trained with a Corpus of over 4 000 dialogues obtained from real interactions of real users with an online ECA Results show that this approach provides a significant improvement over the parsing results of out of domain sentences that is sentences for which there is no optimal parse among the candidates given by the baseline parse Keywords Embodied conversational agents natural language processing dialogue systems sequence tagging CRFs 1 1 1 Introduction Embodied Conversational Agents Conversational Agents CAs can be defined as communication technologies that integrate computational linguistics techniques with the communication channel of the Web to interpret and respond to statements made by users in ordinary natural language 1 Embo died Conversational Agents ECAs are empowered with a human representation that 
shows some degree of empathy smiling showing sadness disgust with the user as the dialogue go es on The fact of adding explicit anthropomorphism in Conversational Agents has some effects over the solution designed H Loftsson E 16 E Acerbi G The idea of discriminative reranking of parsing results is not new In 5 6 the authors propose a reranking process over the parsing results using a Maximum Entropy approach Also Collins 7 propose a similar strategy making use of Markov Random Fields and boosting approaches achieving significant improvement on error rate over the baseline system performance The approaches detailed in those papers are based on lexical and syntactic features describing the components of the parse tree and their syntactic relationship The reranking layer is applied over a set of candidates which are obtained with a classical generative parser In 8 an application of the previous proposals for semantic parsing is described In addition to the purely syntactic features the authors include semantic 
features on the reranking process obtaining partial improvements Reranking Parsing Results Using CRFs 17 In this paper radical a different strategy is proposed all parse tree structure is ignored and only terminal symbols are taken into account To our knowledge there is no previous work on reranking parsing results making use of sequence labeling as reranking metho d 1 3 Generative Parser The approach hereby described relies on a set of candidate parsing results provided by a generative parser The parser used in the experiments was 9 10 a unification grammar based context free parser inspired in the Lexical Functional Grammar formalism 11 The parsing results are therefore provided by means of two different structures the F structure and the C structure The first one is a set of language independent 2 Conditional Random Fields CRFs are probabilistic undirected graphical mo dels Each vertex of the CRF represents a random variable whose distribution is to be inferred and each edge represents a dependency 
between two random variables X is the sequence of observations and Y is the sequence of unknown state variables that needs to be inferred based on the observations In the application hereby described X is formed by all the words of the sentence while Y is the sequence of their corresponding labels CRFs are especially suitable for sequence labeling problems since independence assumptions are made among Y but not among X Thats is CRFs allow the use of strong interdependent and overlapping features as required by sequence labeling problems Formally CRFs can be defined as follows 12 Let G V E be a graph such that Y Yv vV so that Y indexed by the vertices of G Then X Y is a conditional random field in case when conditioned on X the random variables Yv obey the Markov property with respect to the graph p Yv X Yw w v p Yv X Yw wv where w v means that w and v are neighbors in G The likelihoo d probability for the CRF model is calculated in this way p y x 18 E Acerbi G exp eE k k fk e y e x v V k k gk v y v x 1 
Notice that and represent the model s parameters and f and g represent the feature functions that are provided to the model in the training phase 3 3 1 A New Approach Parse Trees as Lexical Sequences A key observation that allows this new approach is the fact that no pair of alternative trees provided by the generative parser share the same sequence of lexical categories This statement is true because the syntactic ambiguity is locally solved by the baseline parser before providing the alternative trees to the reranking mo dule In other words to distinguish one parse tree from another one can just look at the categories assigned to each word in the sentence Therefore the problem of finding the optimal parse boils down to finding the optimal assignment of the lexical category for each word in the sentence among those given by the parser Thus a new parse tree sequence representation is proposed The problem of reranking parsing results is therefore reduced to a word category assignment the new problem is to 
find the best assignment for the whole sentence which is a typical sequence labeling problem The sequence labeling problem is faced with up to 223 different labels 3 2 New Problem Characteristics The reranking approach described in this paper is conditioned by the following issues Reranking Parsing Results Using CRFs 19 4 4 1 The Proposed Solution Theory The strategy to keep the problem tractable despite the tagset dimension is based on helping the model in two ma jor ways The first one is through the introduction of highly informative features in order to reduce the tagset dimension for every specific word This goal is achieved by exploiting a priori knowledge about a term Secondly the model prediction is driven the mo del is not asked to directly predict the correct label sequence instead the likelihoo d of every sequence is used for optimal selection Additionally the training set size is high enough to ensure the presence of past cases for every label in the tagset Since words in a sentence are strongly 
interdependent the solution has to be able to model dependencies between entities moreover words can be linked to a big set of features that can help classification but dependencies may exist also between features One of the most well known approaches to sequence labeling is Hidden Markov Models 13 The potential problem using HMMs is that they calculate p x y where x is the word and y is the label The point is that what really needs to be mo deled is p y x A solution can be Maximum Entropy Markov Models MEMM where p y x is calculated using a maximum entropy model But MEMM can suffer the label bias problem CRFs are a suitable mo del for the task at hands since they do not suffer the label bias problem they are not per state normalized like MEMMs instead of being trained to predict each label independently they are trained to get the whole sequence correctly 4 2 Implementation Offline Due to the specificity of the problem the creation of an ad hoc training set has been necessary in order to take into account 
domain dependent semantic categories The training set is formed by a Corpus of over 4 000 dialogues of Human ECAs interactions and all the alternative parse trees for every sentence During the offline phase the correct alternative has been manually tagged for every sentence The tagging process was done by choosing among a set of sequencelike representation of the parse trees The tagger application graphically shows for each sentence all the possible sequences of lexical categories and allows to select the best sequence or the best combination of sequences Figure 2 shows a screenshot of the application If the sequence selected do es not include all the words in the sentence the excluded words are labelled as Not Used Sometimes the correct parse tree of a sentence is captured by a combination of two or more partial sequences In order to prevent the bad tendency of the mo del to predict too many words as Not Used words between two partial sequences are classified as Link Thus the mo del learns to distinguish 
between words that can be ignored namely Not Used and words that are functioning as a bridge between partial sequences namely Link Figure 3 shows the merging of the two partial sequences selected in Figure 2 20 E Acerbi G Fig 1 Offline processing of the proposed approach Sentences can be classified in two main categories Besides these two groups extremely bad formed sentences were classified as No Parse and discarded in the training phase 5 of the total amount of analyzed sentences The tagging application allows the user to choose the correct sequence in a time between 5 and 15 seconds approximately depending on the sentence complexity The final average tagging rate was 100 sentences per hour the entire tagging pro cess took over 50 hours Reranking Parsing Results Using CRFs 21 Fig 2 A detail of the application created for corpus tagging Fig 3 Merging sequences in a unique global sequence The MALLET library 14 was used for building the CRF mo del and has been mo dified to obtain the likelihood associated 
with each candidate The mo del was validated with a 5 fold cross validation details about the dataset are provided in the Experimental Results section Online The online phase refers to the real interactions between the ECA and a user Figure 4 shows the way the CRF model is used at running time the natural language sentence provided by the user is analyzed by the baseline parser and a set of candidate sequences is returned At this point all possible combinations of partial sequences have to be generated and added to the original set of candidates The CRFs mo del trained in the offline phase returns the likelihoo d probability asso ciated with each candidate sequences The highest likelihoo d sequence is identified as the optimal one and the related c structures one or more chosen Features As previously mentioned a set of highly informative features was intro duced in order to limit the number of possible labels for a specific word 22 E Acerbi G Fig 4 Online processing If it is known that word x can be 
classified only as tag1 tag2 tag3 and tag4 and this information is properly intro duced into the mo del it allows the mo del to focus the prediction on the specific subset ignoring remaining label assignments Three kind of features were used 5 Experimental Results As previously explained only in domain sentences were used to train the mo del but both in domain and out of domain sentences were used to test For the outof domain inputs the mo del is expected to choose a correct syntactic parse tree Reranking Parsing Results Using CRFs 23 which includes all the relevant terms in the sentence The presence of the main concepts in the syntactic tree is a key factor since the ECA will use them to search in the host web page To reduce risk of overfitting a 5 fold cross validation was applied on the indomain dataset The in domain dataset consisted of 4 096 propositions and was divided into 5 subset of approximately 820 sentences each Each time one of the 5 subsets was used to test while the remaining 4 subsets were 
used to train In this way each sentence in the dataset was used to test exactly once and 4 times to train The k fold technique for performance estimation is computationally very expensive but allows to obtain a more accurate estimate of true accuracy than classical hold out metho ds Out of domain sentences were tested using a mo del trained with the whole in domain dataset The best results have been obtained by setting the CRF s window size to 7 the number of tokens representing context surrounding a word An Intel Quad Core Q9400 2 66 GHz machine with 4 096 MB of RAM was used to train the mo del Training required a very long time to converge before intro duction of phrase based features about 90 hours were necessary to train with the whole in domain dataset After the intro duction of this kind of features traning time decreased to about 40 hours During the experiments a real risk of local minima was detected Performance of both rule based baseline and CRFs based reranking systems were evaluated in terms of 
accuracy F measure precision and recall The Table 1 shows the baseline rule system performance rules perform well on in domain sentences while on out of domain sentences the performance dramatically drops by losing 13 18 on accuracy and 8 89 on F measure Accuracy was calculated among the whole set of 223 categories while precision recall and F measures were calculated only for those categories that o ccurred more than 20 times in the dataset Table 1 Baseline rule based system performance Accuracy F measure Precision Recall 86 59 92 77 95 32 90 35 73 41 83 88 85 78 82 06 80 00 88 32 90 55 86 21 In domain Out of domain Mixed The CRFs based reranking performance is depicted in Table 2 CRFs perform worse than the baseline system when in domain sentences are considered while they perform better than the baseline system when out of domain sentences are considered In this case CRFs significantly improve the baseline system by obtaining a 5 21 increase on accuracy and a 4 98 increase on F measure Table 3 shows the 
performance evolution in relation to the training set size Due to considerable computational costs 5 fold cross validation was applied only for the biggest training set the remaining were tested with a simple hold out technique It is worthwhile that each training set in the table isn t a new training 24 E Acerbi G In domain Out of domain Mixed Table 3 Performance evolution on different training set sizes Training set size 800 1 756 2 480 3 600 4 096 Mixed Accuracy Mixed F measure 51 74 59 16 64 43 76 12 72 09 81 70 77 09 87 14 79 63 88 03 Fig 5 Performance evolution on different training set sizes set but only an extended version of the previous one Figure 5 shows how the performance improvements are slowly getting smaller as the training set size increase and is essentially stable around 4 000 sentences 6 Conclusions The performances achieved by the baseline system and the new proposal are quite mirrored rule based performance results are better for the in domain sentences while CRFs are better for the out 
of domain ones The reason why CRFs outperforms the baseline system on out of domain sentences is mainly because Reranking Parsing Results Using CRFs 25 they learn which terms are relevant for this particular domain even if they are to be parsed within a syntactic tree On the other hand the baseline system has no semantic knowledge when trying to rerank syntactic parse trees The CRFs approach on its own would provide similar results to the baseline system in terms of the overall performance However the baseline approach is still more suitable for this particular ECAs application since as previously explained in domain parsing failures are more harmful than out of domain ones But the work hereby described is not useless Actually the results presented in the previous section clearly suggest that a combination of both approaches rule based for the in domain sentence and CRFs for the out of domain ones would very much increase the overall performance of the system Moreover the relative importance of both 
approaches depends on the particular domain evaluated In section 4 2 a division in domain versus out of domain sentences of 80 20 was detailed This percentage is very much dependant on the domain and the particular coverage of the ECA application CRFs based approach would be more suitable for applications with higher out of domain input sentences percentage 7 Future Work The best way to make use of this approach is by combining it with the baseline References 1 Lester J Branting K Mott B Conversational Agents The Practical Handbook of Internet Computing Chapman and Hall Boca Raton 2004 2 Robinson S Traum D Ittycheriah M Henderer J What would you ask a Conversational Agent Observations of Human Agent Dialogues in a Museum Setting In Proceedings of the Sixth International Language Resources and Evaluation LREC 2008 Marrakech Morocco 2008 26 E Acerbi G 3 De Angeli A Johnson G Coventry L The Unfriendly User Exploring Social Reactions to Chatterbots In Proceedings of International Conference on Affective Human 
Factor Design pp Automatic Distractor Generation for Domain Specific Texts Itziar Aldabe and Montse Maritxalar IXA NLP Group University of the Basque Country Spain itziar aldabe montse maritxalar ehu es http ixa si ehu es Ixa Abstract This paper presents a system which uses Natural Language Processing techniques to generate multiple choice questions The system implements different methods to find distractors semantically similar to the correct answer For this task a corpus based approach is applied to measure similarities The target language is Basque and the questions are used for learners assessment in the science domain In this article we present the results of an evaluation carried out with learners to measure the quality of the automatically generated distractors Keywords NLP distractor for educational purposes semantic similarity 1 Introduction The generation of Multiple Choice Questions MCQ one of the measures used for formative assessment is difficult and time consuming The implementation of a system 
capable of generating MCQs automatically would reduce time and effort and would offer the possibility of generating a great amount of questions easily In our proposal we use Natural Language Pro cessing NLP techniques to construct MCQs integrated in didactic resources There are different NLP based approaches which have proved that the automatic generation of multiple choice questions is viable Some of them focus on testing grammar knowledge for different languages such as English 1 or Basque 2 Others apply semantic features in order to test general English knowledge 3 4 or knowledge of specific domains 5 Our work is fo cused on the automatic generation of MCQ in a specific domain i e science domain The target language is Basque The ob jective is to offer experts a helping tool to create didactic resources Human experts identified the meaningful terms i e words of a text which were to be the blanks of the MCQs Then the system applied semantic similarity measures and used different resources such as corpora 
and ontologies in the pro cess of generating distractors1 The aim of this work is to study different 1 The incorrect options of the MCQs H Loftsson E 28 I Aldabe and M Maritxalar metho ds to automatically generate distractors of high quality That is to say distractors that correspond to the vocabulary studied by learners as part of the curricula As there must be only one possible answer among the options of each MCQ experts had to discard those distractors that could form a correct answer Our purpose was to evaluate the system itself by means of an evaluation in a real situation with learners The results of a test exercise was used to measure the quality of the automatically generated distractors The evidence provided by the results will be used to improve the metho ds we propose The paper is organised as follows section 2 explains the scenario to generate and analyse the questions The metho ds we have used to generate distractors are explained in section 3 Section 4 presents the experimental settings and 
section 5 shows the results obtained when evaluating the questions with learners Finally section 6 outlines some conclusions and future work 2 Design of the Scenario We designed an experiment in which most of the external factors which could have an influence on the evaluation process were controlled The multiple choice questions were presented to learners together with the whole text Each MCQ is a stem and a set of options The stem is a sentence with a blank Each blank presents different options being the correct answer the key and the incorrect answers the distractors Example 1 shows an example of MCQs in the context of use Example 1 Espazioan itzalkin erraldoi bat ezartzeak bestalde Lurrari 6 egingo lioke poluitu gabe Siliziozko milioika disko 7 bidaltzea da ikertzaileen ideia Paketetan jaurtiko lirateke eta behin diskoak zabalduta itzalkin itxurako egitura handi bat osatuko lukete Hori bai 8 handiegiak izango lituzke 2 6 a babes b aterki c defentsa d itzala 7 a unibertsora b izarrera c galaxiara d 
espaziora 8 a kostu b prezio c eragozpen d zailtasun The pro cess of generating and analysing the questions consists of the following steps 2 6 a protection b umbrella c defense d shadow Automatic Distractor Generation for Domain Specific Texts 29 3 Distractor Generation When generating distractors the purpose is to find words which are similar enough to the key but which are incorrect in the context to avoid the generation of more than one correct answer We wanted to generate questions to test the knowledge on a specific domain i e the science domain The implemented metho ds are based on similarity measures For that the system employs the context in which the key appears to obtain distractors which are related to the it 3 1 Word Space Latent Semantic Analysis Similarity measures are usual in different NLP applications such us in generating distractors Two main approaches are used knowledge based metho ds and corpus based metho ds In fact some researches employ WordNet to measure semantic similarity 4 others 
use distributional information from the corpus 6 and finally there are some studies which exploit both approaches 5 Measuring similarity for minority languages has some limitations The main difficulty when working with such languages is the lack of resources In our case the main knowledge based resource for Basque 7 is not finished yet the Basque WordNet3 is not useful in terms of word coverage as it has 16 000 less synsets for nouns than WordNet 3 0 As a consequence we decided to choose as the starting point a corpus based metho d to carry out the experiments Nonetheless we also used a knowledge based approach to refine the distractor selection task cf Section 3 2 The system uses context words to compute the similarity deploying Latent Semantic Analysis LSA LSA is a theory and metho d for extracting and representing the meaning of words 8 It has shown encouraging results in a number of NLP tasks such as Information Retrieval 9 10 and Word Sense Disambiguation 11 It has also been applied in educational 
applications 8 and in the evaluation of synonym test questions 12 Our system makes use of Infomap software 13 This software uses a variant of LSA to learn vectors representing the meanings of words in a vector space known as WordSpace In our case it indexes the do cuments in the corpora it pro cesses and performs word to word semantic similarity computations based on the resulting model As a result the system extracts the words that best match a query according to the model Build Word Space and Search As the MCQs we work with are fo cused on the science domain the collected corpus consists of a collection of texts related 3 Nouns are the most developed ones 30 I Aldabe and M Maritxalar to science and technology 14 The corpus is composed of two parts For this work we used the balanced part 3 million words of the specialised corpus In the pro cess of building the model the matrix was created from the lemmatized corpus To distinguish between the different categories of the lemmas the matrix not only took into 
account the lemma but also its category The matrix contains nouns verbs adjectives and adverbs Once we obtained the mo del based on the specialised corpus we had to set the context to be searched After testing different windows we set the sentence as the context 3 2 Methods for Distractor Generation The words found in the mo del were the starting point to generate the distractors for which different metho ds can be applied The baseline metho d LSA metho d is only based on the output of LSA The rest of the metho ds combine the output of the mo del with additional information to improve the quality of the distractors LSA Method The baseline system provides InfoMap with the whole sentence where the key appears As candidate distractors the system offers the first words of the output which are not part of the sentence and match the same PoS In addition a generation pro cess is applied to supply the distractors with the same inflection form as the key LSA Semantics Morphology One of the constraints here is to 
avoid the possibility of learners guessing the correct choice by means of semantic and morphology information Let us see as an example a question whose stem is Istripua izan ondoren sendatu ninduen After the accident cured me the key is medikuak the do ctor and a candidate distractor is ospitalak the hospital Both words are related and belong to the same specific domain Learners could discard ospitalak as the answer to the question because they know that the correct option has to be a person in the given sentence The system tries to avoid this kind of guessing by means of semantic information Therefore applying this metho d the system do es not offer ospitalak as a candidate distractor The system uses two semantic resources a Semantic features of common nouns obtained with a semiautomatic metho d 15 The metho d uses semantic relationships between words and it is based on the information extracted from an electronic monolingual dictionary The extracted semantic features are animate human concrete etc and are 
linked to the entries of the monolingual dictionary b The Multilingual Central Repository MCR which integrates different local WordNets together with different ontologies 16 Thanks to this integration the Basque words acquire more semantic information to work with In this approach the system takes into account the properties of the Top Concept Ontology the WordNet Domains and the Suggested Upper Merged Ontology SUMO Automatic Distractor Generation for Domain Specific Texts 31 In a first step this metho d obtains the same candidate distractors as the LSA method and then it proposes only those which share at least one semantic characteristic with the key To do so the system always tries to find firstly the entries in the monolingual dictionary If they share any semantic feature the candidate distractor is proposed if not the system searches the characteristics in MCR which works with synsets By contrast the output of Infomap are words In this approach we have taken into account all the synsets of the words and 
checked if they share any characteristic That is if a candidate distractor and the key share any characteristic specified by the Top Concept Ontology the WordNet Domains or SUMO the candidate distractor is suggested One might think that after obtaining distractors which share at least one semantic characteristic with the key the system does not need any extra information to ensure that they are valid distractors However working with all the senses of the words may yield not valid distractors in terms of semantics Moreover there are some cases in which two words share a semantic characteristic induced from MCR but which would not be suitable distractors because of their morphosyntax In the last step the method takes the candidate distractors which share at least one semantic characteristic with the key and it takes into account morphosyntax Basque is an agglutinative language in which suffixes are added to the end of the words Moreover the combination of some morphemes and words is not possible For instance 
while the lemma ospital hospital and the morpheme ko form the word ospitaleko of the hospital it is not possible to combine the lemma mediku do ctor with the suffix ko since ko is only used to express the locative genitive case with inanimate words As the input text is previously analysed by a morphosyntactic analyser the system distinguishes the lemma and the morphemes of the key It identifies the case marker of the key and it generates the corresponding inflected word of each candidate distractor using the lemma of the distractor and the suffix of the key as basis Once distractors are generated the system searches for any occurrence of the new inflected word in a corpus If there is any occurrence the generated word becomes a candidate distractor The searching is carried out in a Basque newspaper corpus which is previously indexed using swish e4 to ensure a fast search That certain words do not appear in the corpus does not mean that they are incorrect Those distractors that do appear in the corpus will be 
given preference over distractors of common usage In this step the system tries to avoid candidate distractors which the learners would reject based on their incorrect morphology LSA Specialised Dictionary The third method combines the information offered by the mo del and the entries of an encyclopaedic dictionary of Science and Technology for Basque 17 The dictionary comprises 23 000 basic concepts related to Science and Technology divided into 50 different topics 4 http swish e org 32 I Aldabe and M Maritxalar Based on the candidate distractors generated by the LSA metho d the system searches in the dictionary the lemmas of the key and the distractors If there is an appropriate entry for all of them the candidate distractors which share the topic with the key in the encyclopaedic dictionary are given preference Otherwise the candidate distractors with an entry in the dictionary take preference in the selection pro cess In addition those candidates which share any semantic characteristic cf 3 2 with the 
key have preference to be suitable distractors LSA Knowledge based Method This method is a combination of corpusbased and knowledge based approaches to measure the similarities Similarity is computed in two rounds First the system selects the candidate distractors based on LSA and then a knowledge base structure is used to refine the selection The knowledge based approach 18 uses a graph based method based on WordNet where the concepts in the Lexical Knowledge Base LKB represent the node in the graph and each relation between concepts is represented by an undirected edge5 Given an input piece of text this approach ranks the concepts of the LKB according to the relationships among all content words To do so Personalized PageRank can be used over the whole LKB graph given an input text e g a sentence the metho d extracts the list of the content nouns which have an entry in the dictionary and relates them to LKB concepts As a result of the PageRank pro cess every LKB concept receives a score Therefore the 
resulting Personalized PageRank vector can be seen as a measure of the structural relevance of LKB concepts in the presence of the input context In our case we use MCR 1 6 as the LKB and Basque WordNet as the dictionary The metho d is defined as follows Firstly the system obtains a ranked list of candidate distractors based on the LSA mo del Secondly the Personalized PageRank vector is obtained for the stem and the key Thirdly the system applies the graph based method for 20 candidate distractors in the given stem Finally the similarities among vectors computed by the dot pro duct are measured and a reordering of the candidate distractors is obtained 4 Experimental Settings A group of experts chose five articles from a web site6 that provides current and updated information on Science and Technology in Basque As selection criteria they fo cused on the length of the texts as well as the appropriateness to the learners level First of all the experts marked the blanks of the questions and then the distractors 
were automatically generated To identify the best metho d to generate distractors we designed different experiments where the system applied all the explained metho ds for each blank and text Blanks Experts who work on the generation of learning material were asked to mark between 15 and 20 suitable terms in five texts to create multiple choice 5 6 The algorithm and needed resources are publicly available at http ixa2 si ehu es ukb www zientzia net Automatic Distractor Generation for Domain Specific Texts 33 questions The blanks were manually marked because the aim of the experiment was to evaluate the quality of the distractors in a real situation When pro ceeding the experts did not follow any particular guidelines but followed the usual pro cedure7 The obtained blanks were suitable in terms of the appropriateness of the science domain and the stems In all 94 blanks were obtained As we did not give them any extra information for the marking pro cess experts marked as keys nouns verbs adjectives and adverbs 
However our study from a computational point of view aimed at generating nouns and verbs 69 14 of the obtained blanks were nouns and 15 95 verbs This shows that the idea of working with nouns and verbs makes sense in a real situation Distractors The distractors were automatically generated for each blank and metho d In the case of the nouns the four mentioned metho ds were applied and in the case of the verbs two methods were applied the LSA metho d and the LSA specialised dictionary metho d8 As the distractor generation task is completely automatic the possibility of generating distractors that are correct in the given context had to be considered That is why before testing them with learners the distractors were manually checked For each metho d we provided experts with the first four candidate distractors We had foreseen to reject the questions which had less than three appropriate distractors However in all the cases three valid distractors were obtained Just 0 95 of the distractors could be as suitable 
as the key and 3 25 were rejected as dubious For each selected text we obtained four tests corresponding to the four metho ds Moreover a fifth test was manually made by experts who created three different distractors for each blank semantically close to the keys It is important to point out that the experts did not have any information about the distractors obtained from the automatic pro cess Finally the manually built tests were compared to the automatically generated ones Schools and Learners Six different schools took part in the experiment The exercise was presented to the learners as a testing task and the teachers were not familiar with the texts until they handed out the test to their students In all 266 learners of Obligatory Secondary Education second grade participated in the evaluation They had a maximum of 30 minutes to read and complete the test The test was carried out in paper in order to avoid all noise9 249 of the learners completed the test and their results were used to analyse the items 
questions see section 5 After finishing the testing an external supervisor collected the results of the exercise in situ 7 8 9 In this step the evaluation was blind We did not apply the remaining methods because the verbs in the Basque WordNet need of manual revision We did not want to evaluate the appropriateness of any computer assisted assessment 34 I Aldabe and M Maritxalar 5 Results By means of this evaluation we intended to improve the automatic metho ds explained in section 3 The item analysis metho d was the basis of our evaluation The item analysis method reviews items qualitatively and statistically to identify problematic items The difference between both reviews is that the qualitative metho d is based on experts knowledge and that the statistical analysis is conducted after the items have been given to students This paper is fo cused on the statistical analysis We have used R free software environment10 for statistical computing and graphics of the learners results 5 1 Item Analysis and 
Distractor Evaluation The analysis of item responses in a quantitative way provides descriptions of item characteristics and test score properties among others There are two main theories to address the problem Classical Test Theory CTT and Item Response Theory IRT Both statistical theories have been already used in the evaluation of the automatic generation of distractors 3 5 In this experiment we explored item difficulty item discrimination and distractors evaluation based on CTT as 5 did However the results obtained by them and our results are not comparable since they test the MCQs separately and we test them within a text Item difficulty The difficulty of an item can be described statistically as the proportion of students who can answer the item correctly The higher the value of difficulty the easier the item Item discrimination a goo d item should be able to discriminate students with high scores from those with low scores That is an item is effective if those with high scores tend to answer it 
correctly and those with low scores tend to answer it incorrectly The point biserial correlation is the correlation between the right wrong scores that students receive on a given item and the total scores that the students receive when summing up their scores across the remaining items A large pointbiserial value indicates that students with high scores on the overall test are also answering the item correctly and that students with low scores on the overall test are answering the item incorrectly The point biserial correlation is a computationally simplified Pearson s r between the dichotomously scored item and the total score In this approach we use the corrected point biserial correlation That is the item score is excluded from the total score before computing the correlation This is important because the inclusion of the item score in the total score can artificially inflate the point biserial value due to correlation of the item score with itself There is an interaction between item discrimination and 
item difficulty It is necessary to be aware of two principles very easy or very difficult test items have little discrimination and items of moderate difficulty 60 to 80 answering 10 http www r project org Automatic Distractor Generation for Domain Specific Texts 35 correctly generally are more discriminating Item difficulty and item discrimination measures are useful only to help to identify problematic items Poor item statistics of the results should be put down to ineffective distractors Distractor evaluation to detect poor distractors the option by option responses of high scored and low scored learners groups were examined To this purpose two kind of results will be presented in the next section the number of distractors never chosen by the learners and a graphical explanation of the used distractors 5 2 Interpreting the Results of the Tests Table 1 shows the average of item difficulty and item discrimination results obtained for all the items in a text The table shows the results for the manually and 
automatically generated tests In the case of item difficulty each row presents the item difficulty index together with the standard deviation as well as the percentage of easy and difficult items In this work we have marked an item to be easy if more than 90 of the students answer it correctly On the other hand an item is defined as difficult when less than 30 of the students choose the correct answer The results shown for the manually generated test are promising near 0 5 and there is not significant differences among the automatic metho ds All of them tend to obtain better results with the second text and tend to create easy items The results of item discrimination take into account the responses of the high scoring and low scoring students The high scoring group is the top 1 3 of the class and the low scoring group comprises students with test scores in the bottom 1 3 of the class Regarding item discrimination the corrected pointbiserial index with its standard deviation as well as the percentage of items 
with negative values are shown in the table Even though all the results obtain a positive mean a value of at least 0 2 is desirable in 8 out of the 10 cases negative point biserial indexes are obtained These negative values represent the percentage of items correctly responded by a higher number of low scored students than high scored ones To identify the reasons underlying these results we study the option by option responses of high scored and low scored groups Such study led us to evaluate the distractors themselves Figure 1 shows in a graphic way the distribution of the distractors among the low scored and high scored groups The x axe represents the number of lowscored students selecting a distractor and the y axe represents the number of high scored ones In this experiment we have studied the results related to 108 distractors limiting the number of students per test to 20 Regarding the number of different distractors in the case of the manually generated distractors 83 76 85 out of the 108 distractors 
were chosen In the cases of the automatically generated distractors the results were 64 59 26 for the LSA metho d 54 50 00 for the LSA semantics morphology metho d 67 62 04 for the LSA and encyclopaedic dictionary metho d and 60 55 56 for the LSA knowledge based method 36 I Aldabe and M Maritxalar Table 1 Item Analysis Difficulty Item Difficulty Easy 0 79 Text1 Text2 LSA sem morph Text1 Text2 LSA spec dictionary Text1 Text2 LSA Knowledge based Text1 Text2 Manually generated Text1 Text2 LSA Fig 1 Distractors Evaluation is used when both methods share the point Based only on the selected distractors this last metho d gives the best results in relation to the percentage of distractors that discriminates positively among the low and high scored groups 90 00 54 out of 60 The distractors obtained by the LSA semantics morphology metho d discriminated positively in 87 04 of the cases the LSA dictionary metho d in 79 10 of the cases the LSA metho d in 76 56 of the cases and the manual metho d in 75 90 of the cases In 
a graphic way the distribution of the low right side of the graphics can be interpreted as the set of goo d distractors The distribution of the high left side of the graphics represents distractors that have to be revised because they confuse high scored students and do not confuse low scored learners The reason could be that low scored learners have not enough knowledge to be confused Looking at the results of the metho ds the LSA metho d tend to confuse more than the other metho ds 14 06 followed by the manual metho d 12 05 the LSA dictionary metho d 11 94 the LSA semantics morphology metho d 7 41 and the LSA knowledge based method 6 67 It seems there is a relation between the number of the selected distractors and the percentage of discrimination the lower the number of distractors the higher the positive discrimination However the LSA metho d do es not follow this assumption Automatic Distractor Generation for Domain Specific Texts 37 In order to improve the methods we are planning to study in more depth 
the distractors that were never chosen Moreover it is also necessary to analyse on two other aspects the domain nature and the part of speech of the keys We must not forget that experts marked the blanks without being instructed Therefore the blanks did not have to correspond with words related to the specific domain 6 Conclusions and Future Work The article presents a study about automatic distractor generation for domain specific texts The system implements different metho ds to find distractors semantically similar to the key It uses context words to compute the similarity deploying LSA We have used a balanced part of a specialised corpus to build the mo del In the near future we will make use of the whole specialised corpus to mo del it In this approach we have explored item difficulty item discrimination and distractors evaluation based on Classical Test Theory The results shown for the manually generated test were promising and there were not significant differences among the metho ds The item 
discrimination measure led us to study the option by option responses of high scored and low scored groups and we finished the study with the evaluation of the distractors Such evaluation gave us evidence to improve the metho ds regarding the domain nature and part of speech of the keys and the need to enlarge the context when applying LSA In addition we are planning to test the distractors with more learners Finally the fact that the distractors tend to confuse high scored learners but not low scored learners needs of deeper analysis In our opinion working in a specific domain may improve the quality of the distractors so in the near future we will design new experiments with test exercises independent from the domain to compare the results with the ones obtained in the current study For future work we are also planning to use data mining techniques to identify the blanks of the text Finally reliability measures should also be considered in future research Reliability tells us whether a test is likely to 
yield the same results if administered to the same group of test takers multiple times Another indication of reliability is that the test items should behave the same way with different populations of test takers Acknowledgments We would like to thank to the institution named Ikastolen Elkartea who has assigned goo d experts in order to work on the tasks of this work This research is being supported by the Basque Government SAIOTEK project S PE09UN36 and ETORTEK project IE09 262 References 1 Hoshino A Nakagawa H Assisting cloze test making with a web application In Proceedings of SITE Society for Information Technology and Teacher Eduation San Antonio U S pp 38 I Aldabe and M Maritxalar 2 Aldabe I Lopez de Lacalle M Maritxalar M Martinez E Uria L ArikIturri An Automatic Question Generator Based on Corpora and NLP Techniques In Ikeda M Ashley K D Chan T W eds ITS 2006 LNCS vol 4053 pp Summarization as Feature Selection for Document Categorization on Small Datasets Emmanuel Anguiano Laboratory of Language 
Technologies Department of Computational Sciences National Institute of Astrophysics Optics and Electronics INAOE Mexico eanguiano villasen mmontesg inaoep mx 2 Natural Language Engineering Lab ELiRF Department of Information Systems and Computation Polytechnic University of Valencia UPV Spain prosso dsic upv es 1 Abstract Most common feature selection techniques for document categorization are supervised and require lots of training data in order to accurately capture the descriptive and discriminative information from the defined categories Considering that training sets are extremely small in many classification tasks in this paper we explore the use of unsupervised extractive summarization as a feature selection technique for document categorization Our experiments using training sets of different sizes indicate that text summarization is a competitive approach for feature selection and show its appropriateness for situations having small training sets where it could clearly outperform the traditional 
information gain technique Keywords Text Categorization Text Summarization Feature Selection 1 Introduction Automatic document categorization is the task of assigning free text documents into a set of predefined categories or topics Currently most effective solutions for this task are based on the paradigm of supervised learning where a classification model is learned from a given set of labeled examples called training set 7 Within this paradigm an important process is the identification of the set of features words in the case of text categorization more useful for the classification This process known as feature selection tends to use statistical information from the training set in order to identify the features that better describe the objects of different categories and help discriminating among them Due to the use of that statistical information the larger the training set the better the feature selection Unfortunately due to the high costs associated with data labeling for many applications these 
datasets are very small Because of this situation it is of great importance to search for alternative feature selection methods specially suited to deal with small training sets In order to tackle the above problem in this paper we propose to apply unsupervised extractive summarization as a feature selection technique in other words we propose reducing the set of features by representing documents by means of a representative subset of their sentences Our proposal is supported on two facts about H Loftsson E 40 E Anguiano extractive summarization First it has demonstrated to capture the essence of texts by selecting their most important sections and consequently a subset of words adequate for their description Second it is an inherently local process where each document is reduced individually bypassing the restrictions imposed by the size of the given training set Through experiments on a collection consisting of three training sets of different sizes we show that text summarization is a competitive 
approach for feature selection and what is more relevant that it is specially appropriate for situations having small training sets Particularly in this situations the proposed approach could significantly improved the results achieved by the information gain technique The rest of this document is organized as follows Section 2 presents some related works concerning the use of text summarization in the task of document categorization Section 3 describes the experimental platform particularly it details the feature selection process and the used datasets Then Section 4 shows the results achieved by the proposed approach as well as some baseline results corresponding to the application of information gain as feature selection technique Finally Section 5 presents our conclusions and future work ideas 2 Related Work Some previous works have considered the application of text summarization in the task of document categorization Even though these works have studied different aspects of this application most of 
them have revealed directly or indirectly the potential of text summarization as a feature selection technique Some of these works have used text summarization or its underlying ideas to improve the weighting of terms and thereby the classification performance For instance Ker and Chen 2 weighted the terms by taking into account their frequency and position in the documents whereas Ko et al 3 considered a weighting scheme that rewards the terms from the phrases selected by a summarization method A more ambitious approach consists of applying text summarization with the aim of reducing the representation of documents and enhancing the construction of the classification model Examples from this approach are the works by Mihalcea and Hassan 5 and Shen et al 8 The former work is of special relevance since it showed that significant improvements can be achieved by classifying extractive summaries rather than entire documents Finally the work by Kolcz et al 4 explicitly proposes the use of summarization as a 
feature selection technique They applied different summarization Summarization as Feature Selection for Document Categorization on Small Datasets 41 selection techniques are comparable for most of the cases the former is a better option for situations restricted by the non availability of large training sets 3 Experimental Platform 3 1 Feature Selection Process Because of our interest to evaluate the effectiveness of text summarization as a feature selection technique we compared its performance against the one of a traditional supervised statistical approach Particularly to summarize the documents we used the well known HITSA directed backward graph based sentence extraction algorithm 6 The choice of this algorithm was motivated by its relevant results in text summarization as well as by its previous usage in the context of document categorization 5 On the other hand we considered the information gain IG measure as exemplar from supervised techniques 9 In a few words the feature selection was carried out as 
follows 1 Summarize each document from the training set by selecting the k of their most relevant sentences in line with the selected summarization method 2 Define the features as the set of words extracted from the summaries eliminating the stop words In contrast to this approach the common statistical feature selection process defines the features as the set of words having positive information gain IG 0 within the entire dataset That is it selects the words whose presence or absence gives the larger information for category prediction 3 2 Evaluation Datasets For the experiments we used the R8 collection 1 This collection is formed by the eight largest categories from the Reuters 21578 corpus which documents belong to only one class It contains 5189 training documents and 2075 test documents With the aim of demonstrating the appropriateness of the proposed approach for situations having small training sets we constructed two smaller collections from the original R8 corpus R8 41 and R8 10 consisting of 41 
and 10 training documents per class respectively These collections contain 328 and 80 training documents and the original 2075 test documents Details can be found in Table 1 Table 1 Documents distribution in different data sets Class R8 Training Set R8 41 Training Set R8 10 Training Set Test Set earn acq trade crude money fx interest ship grain 2701 1515 241 231 191 171 98 41 41 41 41 41 41 41 41 41 10 10 10 10 10 10 10 10 1040 661 72 112 76 73 32 9 42 E Anguiano 4 Results In the experiments we evaluated the effectiveness of feature selection by means of the classification performance Our assumption is that given a fixed test set and classification algorithm the better the feature selection the higher the classification performance In particular in all experiments we used a Support Vector Machine as classification algorithm term frequency as weighting scheme and the classification accuracy and micro averaged F1 as evaluation measures Table 2 shows two baseline results The first one corresponds to the usage 
of all words as features i e without applying any feature selection method except by the elimination of stop words whereas the second concerns the usage of only those words having positive information gain From these results it is clear that the IGbased approach is pertinent for situations having enough training data where it could improve the accuracy in 1 5 However it is also evident that it has severe limitations to deal with small training datasets For instance it only could define 20 relevant features for the R8 10 collection which represented just 1 of the whole set of words causing a decrement in the classification accuracy of around 50 Table 2 Baseline results without feature selection and using the information gain criterion R8 Features All features IG 0 Accuracy F1 Features R8 41 Accuracy F1 Features R8 10 Accuracy F1 17 336 1 691 85 25 86 51 842 857 5 404 54 78 75 42 89 782 539 2 305 20 71 71 35 57 702 0402 Table 3 and table 4 show results from the proposed method for different summary sizes 
ranging from 10 to 90 of the original sentences of the training documents The achieved results are encouraging they show that text summarization is a competitive approach for feature selection and what is more relevant that it is especially appropriate for situations having small training sets In particular for the reduced collections R8 41 and R8 10 very small summaries from 10 to 50 could outperform with statistical significance the results obtained by the application of the IG based approach IG 0 as well as those obtained using all words as features We evaluated statistical significance of the results using the z test with a confidence of the 95 Table 3 Results accuracy of proposed method using summaries of different sizes R8 Sum Size Number features Our method Top IG Number features R8 41 Our method Top IG Number features R8 10 Our method Top IG 10 20 30 40 50 60 70 80 90 8 289 9 701 11 268 12 486 13 326 14 560 15 626 16 339 17 063 87 13 88 53 89 20 87 90 87 42 86 89 86 75 86 70 86 27 85 45 85 54 85 78 
85 78 85 88 85 64 85 54 85 69 85 35 1 943 2 445 3 089 3 569 3 919 4 348 4 671 5 004 5 263 83 47 82 27 82 89 83 52 81 40 79 66 80 10 80 43 78 89 80 43 78 02 78 31 78 60 79 13 78 89 78 94 78 31 78 60 706 902 1 178 1 392 1 523 1 722 1 890 2 082 2 230 76 77 70 17 64 67 75 23 75 08 69 40 69 73 71 23 72 58 52 24 56 87 52 34 54 07 64 10 67 52 69 69 69 83 71 66 Summarization as Feature Selection for Document Categorization on Small Datasets 43 In order to have a deep understanding of the capacity of the proposed method we compared its results against those from a classifier trained with the same number of features but corresponding to the top IG values indicated in Table 4 as Top IG As can be noticed our method always obtain better results indicating that the information gain cannot be properly evaluated from small training sets Regarding this fact it is interesting to notice that for the R8 10 collection our method allowed a 7 of accuracy improvement from 71 71 to 76 77 by means of a 70 feature reduction from 2 305 
to 706 whereas for the same compression ratio the features selected by their IG value caused a 28 drop in the accuracy from 71 71 to 52 24 Table 4 F1 measure of the proposed method using summaries of different sizes R8 Sum Size Our method Top IG R8 41 Our method Top IG R8 10 Our method Top IG 10 20 30 40 50 60 70 80 90 876 886 891 877 870 864 862 861 856 846 846 848 848 848 846 845 847 843 842 834 836 842 819 798 800 803 784 817 790 789 789 791 787 786 780 781 776 709 654 766 763 683 685 693 712 572 659 618 631 700 717 716 698 703 5 Conclusions and Future Work This paper studied the application of automatic text summarization as a feature selection technique in the task of document categorization Experimental results in a collection having three training sets of different sizes indicated that summarization and information gain a statistical feature selection approach are comparable when there are enough training data such as in the original R8 collection whereas the former is a better option for situations 
restricted by the non availability of large training sets as in the cases of R8 41 and R8 10 collections This behavior is because summarization is a local process where each document is reduced individually without considering the rest of the documents while statistical techniques such as IG require lots of training data in order to accurately capture the discriminative information from the defined categories Due to this characteristic as future work we plan to examine the pertinence of summarization based feature selection into a semi supervised text classification approach It is important to mention that the success of summarization depends on the nature of the documents In this paper we evaluated the proposed method in a collection of news reports demonstrating its usefulness As future work we plan to determine its appropriateness for other kinds of documents such as web pages and emails Acknowledgments This work was done under partial support of CONACYT Project grants 83459 82050 106013 and scholarship 
271106 and the MICINN TIN200913391 C04 03 Plan I D i TEXT ENTERPRISE 2 0 research project 44 E Anguiano References 1 Cardoso Cachopo A Improving Methods for Single Label Text Categorization PhD Thesis Technical University of Lisboa Portugal October 2007 2 Ker S J Chen J N A Text Categorization Based on a Summarization Technique In ACL 2000 Workshop on Recent Advances in Natural Language Processing Honk Kong 2000 3 Ko Y Park J Seo J Improving Text Categorization Using the Importance of Sentences Information Processing and Management 40 2004 4 Kolcz A Prabakarmurthi V Jugal K Summarization as a Feature Selection for Text Categorization In Tenth International Conference on Information and Knowledge Management CIKM 2001 Atlanta GA USA 2001 5 Mihalcea R Hassan S Using the Essence of Texts to Improve Document Classification In RANLP 2005 Borovetz Bulgaria 2005 6 Mihalcea R Graph based Ranking Algorithms for Sentence Extraction Applied to Text Summarization In ACL 2004 Barcelona Spain 2004 7 Sebastiani F Machine 
Learning in Automated Text Categorization ACM Computing Survey 34 1 A Formal Ontology for a Computational Approach of Time and Aspect LaLIC Langues Logiques Informatique et Cognition University of La Sorbonne Maison de la recherche 28 rue Serpente Paris 75006 France Abstract This paper provides a linguistic semantic analysis of time and aspect in natural languages On the basis of topological concepts notions are introduced like the basic aspectual opposition between event state and process or that of time of utterance for the treatment of deictic categories that are used to analyse the semantics of grammatical tenses or more general situations This linguistic model yields a conceptualisation reused for the definition of a formal ontology of time and aspect This ontology is provided with a formal framework based on type theory that enables the processing of temporal and aspectual semantic entities and relations Keywords aspect time semantics formal ontology type theory knowledge representation 1 Introduction 
This paper addresses the problem of minimizing the distance between 1 time and aspect conceptualized from natural languages and 2 a computational mo del enabling a formal treatment of the semantics of texts Section 1 gives general information about time and aspect and intro duces a specific linguistic mo del resting on topological concepts and taking into account notions like temporal deixis or primitive aspectual values such as state pro cess or event and derived ones like resultative state see 1 Section 2 provides an original formal framework using an applicative language with Church functional types to express this linguistic ontology and then defines concepts and relations of this formal ontology of time and aspect This work intends to reach a greater expressivity compared to time notions investigated for instance within mo dal logic like with tense logic LTL or ITL if natural language semantics description is the goal to reach because it develops an interval based semantics integreting aspectual 
properties more suitable for natural languages analysis On the other hand the reasoning aspect is not considered here This paper is a knowledge representation investigation leading to a finer expressivity for applications like ontologies population or formal definition of grammatical operators H Loftsson E 46 A Arena and J P 2 An Analysis of Aspect and Temporality Time notions conceptualized from natural languages are often classified by linguistics into two main concepts that of time and that of aspect The former deals with locating situations in time with respect to the time of utterance e g deictic references like yesterday or other situations in time not related to it e g after the war The latter can be defined following 2 as the parameter that studies the different ways of viewing the internal temporal constituency of a situation Those definitions as it is going to be explained in the next sections can be given a more precise characterization adopting a theoretical point of view For the present purpose 
building an ontology of time and aspect first a list of fundamental entities that can be found in the semantics of aspect and time is drawn up and then relations over them are identified according to a specific theory that is developed here 2 1 Linguistic Concepts for a Model of Aspect and Time Discussing the different linguistic theories of aspect is not the main purpose of this paper simply a brief overview of a few typologies of aspectual semantic values will be provided then some comments are given on how aspectual values in languages can be conveyed by linguistic elements and finally the theoretical linguistic concepts adopted here are laid out Before intro ducing theoretical elements some linguistic examples of aspectual1 oppositions are considered without committing to any particular classification 1 a b c d John John John John walked was walking has walked walked to the station Dealing with aspect the classification of verbs made by the philosopher Vendler has often been taken up discussed and also 
refined He provided a set of four classes of verbs based on their distinct aspectual properties along with basic linguistic tests to determine if a verb belongs to a given class The four classes are the following activity e g run walk achievement e g see reach the top accomplishment e g run a mile build a house and state e g believe to be in the room For more details about this classification see 3 Later this aspectual typology has been refined on several points notably the semantic definitions of the aspectual classes Indeed according to different semantic features other classifications are proposed for instance in 4 5 6 Mourelatos intro duces a more fundamental aspectual distinction between event state and process from which Vendler s classes can be expressed in a hierarchical way Another ma jor refinement about the Vendler classification concerns the elements it applies to As it has been argued by Dowty Verkuyl or Mourelatos an aspectual semantic classification should not be restricted to verbs but rather 
1 Regardless of temporal information for the moment A Formal Ontology for a Computational Approach of Time and Aspect 47 to situations described by whole sentences namely verbs along with their arguments and other aspectual mo difiers like adverbials2 Consider the following examples 2 a b c d Paul Paul Paul Paul drinks drinks read a read a beer a beer book in one hour book for one hour The linguistic theory of time and aspect that is worked out here is now introduced with its formal framework The overall theory is described in 9 10 Definitions of aspectual semantic values rest on topological concepts and more precisely on topological intervals of instants open closed half open for which topological properties have a linguistic meaning This interval based semantics is in line with a model theoretic approach In other words linguistic expressions to which a logical representation will be given are given a truth value with respect to topological intervals3 of the mo del We give now information about entities or 
linguistic concepts being taken into account in the mo del Dealing with time in natural languages the notion of temporal reference system see 1 is useful to understand its semantics Different types of temporal reference systems are defined first that created by the utterance pro cess and from which other situations can be located the enunciative reference system it is said to be actualized e g now yesterday secondly that being not actualized e g the first of March or that of possible situations e g if I were rich In the mo del by hypothesis the linguistic time is defined as an organized set of temporal reference systems and each temporal reference system being a continuous and ordered set of instants They are structured by three relations the relation of identification that of differentiation for before and after relations and the breaking relation meaning that a temporal reference system is not located with respect to the enunciative reference system like with for instance the marker once upon a time 
Regarding aspect the mo del of this theory is based on three primitive semantic values those of process event and state and to each is given a conceptual content analyzed from linguistic data and cognitive considerations A situation has the value of a pro cess when it expresses an ongoing action without last 2 3 Linguistic markers being variable from one language to another For a theoretical introduction on aspect with examples taken from different languages see 2 Dependencies between aspectual values of a sentence and its constituents is known as the aspectual composition phenomena For instance see 7 for a treatment of aspect in relation with the different semantic types of nominal arguments as in example 2 a and 2 b or 8 which notably takes into account adverbials as in 2 c and 2 d Many linguists have argued about the necessity of intervals for the semantics of time Bennet Culioli Partee 48 A Arena and J P instant It is formally represented by a right open and left closed interval there is an initial but 
no final instant A situation is a state when it expresses an absence of mo difications and its semantics is represented by an open interval On the other hand a situation is an event when it refers to a mo dification between two static situations and it is represented by a closed interval with an initial and a final instant For instance a situation having the aspectual value of state is said to be realized onto an open interval true for every instant belonging to the given interval in the mo del Situations having aspectual values are given an underlying logical expression namely an applicative expression ASP where ASP is a class of aspectual operators PROC STATE EVENT applying to being a timeless4 predicative relation those operator yielding aspectualised predicative relation realized on topological intervals Up to now entities state event process temporal referential of the model have been defined now relations that can be predicated over them are going to be investigated 2 2 Linguistic Relations for a Model 
of Aspect and Time Before intro ducing temporal relations a preliminary remark is made on the notion of utterance time that is defined by the utterance pro cess and realized on a right open interval not a single instant but a process because uttering takes time and is unaccomplished see again 1 The right boundary of this pro cess is written T0 and is not the last instant of the utterance pro cess but the first instant of the non realized instants In terms of the temporal structure the bound T0 of the utterance process separates the past from the future in an asymmetric way the future having a non linear structure This is one of the reasons why linguistic time can t be conceptualized as a linear order Thus temporal relations are expressed by precedence or identification5 relation holding between a situation with an aspectual value and the utterance pro cess Consider some linguistic examples and their corresponding intervals 4 5 Without temporal or aspectual value More precisely refers to the notion of lexis 
or dictum denoting what is said Relations between topological intervals are defined formally in the next section A Formal Ontology for a Computational Approach of Time and Aspect 49 Aspectual values such as defined above are said to be primitive to the extent that they can be combined to express derived aspectual semantic values like the perfect A sentence like expresses a situation where an event and a state are in a specific configuration The bound between the event and the state is defined as a continuous cut as defined by Dedekind6 and the adjacent to the event resulting state refers to a causal relation holding between both intervals which correspond to the semantics of the perfect As defined in the mo del aspectual semantic values are not indeed independent from each other Consider some simple examples like 3 a He was washing the car b He washed the car c He has washed the car Those situations all refer to a common telic predicative relation that has to reach a final term to be true here the right 
bound of the event The clause a refers to an underlying unaccomplished process right bound open Once this pro cess is achieved right bound reached it is turned into an event the clause b From this event can be related some causal situations expressed by a perfect value as in the clause c Those aspectual properties lead to a general network of dependencies intro duced in 1 6 A continuous cut written tc For a set of instants E linearly oriented and such that for A1 and A2 two subsets of E the following conditions are verified 1 A1 A2 E 2 A1 A2 and 3 A1 A2 tc is a continuous cut of E when either tc A1 A2 or tc A1 and tc A2 exclusively and tc 50 A Arena and J P Fig 1 The core information of the ontology consists of a set of constraints between aspectual values Arrows in this network can either mean is a sort of like in the proposition an activity state is a sort of state or implies like in an event implies a resulting state or contains like in a progressive process is contained in an activity state e g the plane 
is flying vs the plane is in fly See 10 for further details and a linguistic account for those relations These theoretical linguistic developments are in line with an ontological investigation to the extent that it answers to some questions about the time nature e g analysis of intervals bounds constraints and relation between them Now are intro duced some tools to formally represent this linguistic ontology 3 A Formal Ontology of Time and Aspect First a specific framework is laid out to make possible the expression of aspectual semantics introduced in the previous section 3 1 Formal Framework According to 12 there exists in the literature of knowledge representation several meanings for the term ontology One meaning identifies an ontology to a specific conceptualization of a knowledge base here the formal semantics of time and aspect intro duced in section 1 Another meaning concerns the formal account7 given to such conceptualization which is the topic of this section The formalism that is used here is that 
of applicative systems with types Applicative formalisms have been developed along with combinatory logics by Curry who intro duced the notion of operator and the basic operation of application The notion of type here is that of functional types introduced by Church 7 For instance description logics first order logic or applicative systems like combinatory logic or lambda calculus A Formal Ontology for a Computational Approach of Time and Aspect 51 Thus the basic structure for such system rests on the fundamental distinction between operator and operand The former being applied to the later to return a result Each of those three entities having a specific functional type This notion of type is used to characterize classes of ob jects operators can be applied to The construction rule for functional types is the following 1 Primitive types are Types 2 If and are Types then F is a Type 1 F is the functional type constructor and F is the type of an operator that can be applied only to operands having the type 
and is the type of the result The application rule is the following X F Y XY 2 An ontology being often defined as a set of concepts with relations over them it is necessary to formally define the notion of concept that is used here Following Frege a concept is defined as a function f D where D is a domain of ob jects and truth values Concepts are asso ciated to unsaturated entities and ob jects to saturated entities e g the concept ishuman can be applied to the saturated ob ject John to return true Entities of the ontology of time and aspect intervals are referred to as types8 and concepts and types are related by the following equivalence9 a x iff is x a 3 The left clause of this equivalence can also be expressed by the proposition a is an instance of the type x The definition of a concept is given by writing e g for a type x is x a a has the properties inherited from the concept isX Whence if an ob ject has a given type it inherits all properties asso ciated to the concept e g John human is human John has 
bo dy John and has mind John A relation here binary has a functional type built recursively using the rule 1 In the following diagram R is the name of an arbitrary binary relation holding between two typed ob jects 8 9 The closest notion of type used in this sense can be found in the literature on ontologies in computer science under the term universal taken from philosophy See for instance 13 Types are used to express general relations that hold between classes of objects like types in categorial grammars or the TBox level in description logics Types are written with bold lower case letters and name of concepts are written with upper case 52 A Arena and J P is x a is x b R F xF yH a x Ra F yH b y Ra b H a x R FxFyH b y Fig 2 The same binary relation R is represented graphically on the left and by its applicative tree on the right This tree rests on the application rule 2 The type H corresponds to truth values Double lines in graphical representation refer to the equivalence 3 3 2 Definitions of Concepts and 
Relations in the Ontology of Time and Aspect As it has been mentioned linguistic entities identified in section 2 1 namely the different topological intervals of instants and temporal reference systems are identify to primitive types in the formal ontology Other types are intro duced for technical reasons that are explained below Table 1 This table establishes the typology of entities required in the formal ontology of time and aspect Types H ref inst intv intv topo intv topo B intv topo B intv topo B cl intv topo B op intv topo B ho Entity description Truth values Temporal reference system see section 2 1 Instant Interval Interval with topological properties Unbounded interval Bounded interval Closed bounded interval see section 2 1 Open bounded interval see section 2 1 Half open bounded interval see section 2 1 Remark 1 Having established three different types respectively for closed open or half open intervals it is possible to express polymorphic relations between specific intervals For instance a 
relation R holding between a closed and a half open interval will have the functional type R F intv topo B cl F intvtopo B op H All arguments with other types than those in the signature of the relation10 will lead to a type error 10 This means is used to express semantic constraints between aspectual values For instance two events respectively true on closed intervals cant be adjacent or meet in A Formal Ontology for a Computational Approach of Time and Aspect 53 Fig 3 Each relation in this ontology is typed For instance the relation can have the type F inst F inst H or F ref F ref H The seven relations co ad ov pr coi cot in are defined in table 2 The relation is defined from the determination operator see 14 and shares some properties with the subsumption relation An ontology being not a simple typology unstructured set of entities the next figure graphically represents specific articulations of types taken from table 1 and then a conceptual content is given definitions following the figure 3 to types 
following the equivalence 3 and to relations Definition 1 A temporal reference system type ref is a strict total ordered set T where T is a non empty set of instants is called the precedence relation and verifies the additional properties of density and continuous cut see footnote 6 is ref R R T T 4 Definition 2 An interval type intv is a non empty convex subset of a temporal reference system is intv I R is ref R I a b I a b t R a t t I 5 Definition 3 A topological interval type intv topo is an interval to which operators of the point set topology like interior boundary or closure can be applied11 11 Indeed open intervals of any totally ordered set can define a topology on this set Here there exists a topological space T O where T is the non empty set of instants and O is a topology on T consisting of all open subsets of T verifying the specific topological axioms 54 A Arena and J P Here are recalled the basic topological notions from which more specific topological intervals will be defined like the closed 
or half open intervals Given an interval I and a temporal reference system R such that I R is intv topo b ho I is intv topo b I I Int I BdL I 10 Concepts being defined the next paragraph will fo cus on relations over topological intervals defined in defintion 5 closed open and half open They can be defined from F inst F inst H and F intv F intv H being respectively the precedence relation between instants and the classical set theoretic inclusion between intervals The difference with Allen relations 15 or Van Benthem12 definitions is that the semantics of bounds is taken into consideration and related to a logicolinguistic analysis Each relation is provided with a set of admissible types for its arguments called its signature and as mentioned in remark 1 this signature is used to avoid certain undesired configurations between topological intervals semantic constraints of the ontology see figure 1 For instance given the relation ad with the signature F intv topo B cl F intv topo B op H and the following 
configurations 12 Here definitions are based on and as in period structures from 16 but periods are defined differently A Formal Ontology for a Computational Approach of Time and Aspect 55 Table 2 This table provides definitions for relations holding between topological intervals Symb Name Definition Rep co coincidence A co B A B B A ad adjacence A ad B BdR A BdL B ov overlap A ov B i i A i B BdL A BdL B pr precedence A pr B BdR A BdL B coi initial coincidence A coi B A B BdL a BdL B cot terminal coincidence A cot B A B BdR a BdR B in interiority A in B BdL B BdL A BdR B BdR A Configuration 1 will lead to a semantic type error13 because the type F intv topo B cl F intv topo B cl H is not included in the signature of the relation ad whereas configuration 2 is well typed e g value of resulting state see figure 1 This ontology with specific semantic constraints being developed it enables the definition of a specific mo del I R V where 1 I is the set of all open closed or half open intervals defined by I i is 
intv topo b ho i is intv topo b op i is intv topo b cl i 2 R co ad ov pr coi cot in the set of typed binary relations over I 3 V P R I a valuation function assigning to each predicative relation in the set P R a subset of I where it is realized 4 Conclusion The main contribution of the article lies in the establishment of the formal ontology of time and aspect section 3 as a means or a toolkit to express formally some specific semantic constraints analyzed from aspectual situations in texts References 1 13 The meeting point being contained in both intervals a proposition could have contradictory truth value at this point e g to be standing to be sitting down 56 A Arena and J P 3 Vendler Z Verbs and times The philosophical review 66 2 A Non linear Semantic Mapping Technique for Cross Language Sentence Matching Rafael E Banchs and Marta R Costa Barcelona Media Innovation Centre Barcelona Spain rafael banchs marta ruiz barcelonamedia org Abstract A non linear semantic mapping procedure is implemented for 
crosslanguage text matching at the sentence level The method relies on a non linear space reduction technique which is used for constructing semantic embeddings of multilingual sentence collections In the proposed method an independent embedding is constructed for each language in the multilingual collection and the similarities among the resulting semantic representations are used for crosslanguage matching It is shown that the proposed method outperforms other conventional cross language information retrieval methods Keywords Cross language Information Retrieval Semantic Mapping Multidimensional Scaling 1 Introduction Cross language information retrieval CLIR which is a subfield of the traditional information retrieval IR provides users with access to information that is in a different language from their queries CLIR is gaining more and more attention as the availability of information in languages different from English increases in the Internet It has become one popular research area in information 
retrieval during the last 10 years 1 Research in CLIR has been significantly encouraged by three wellknown evaluation campaigns a cross language information retrieval track at TREC the Cross Language Evaluation Forum CLEF and the NTCIR Asian Language Evaluation Recently some CLIR real applications have appeared such as the crosslanguage search by Google on 2007 and the user driven multilingual news aggregation Europe Media Monitor Given a query in a given source language the aim of CLIR is retrieving relevant documents in a target language In 2 four different strategies for matching a query with a set of documents in the context of CLIR were identified cognate matching document translation query translation and interlinguas Nowadays one of the most popular approaches is query translation However this approach is bilingual in nature and in a highly multilingual environment with for instance N languages it may be impractical as N N 1 translation directions must be accounted for On the contrary an interlingua 
based approach would only require N mappings or translations to be accounted for In this sense this latter strategy seems to be more suitable in those applications involving a large number of languages H Loftsson E 58 R E Banchs and M R Costa In this work we focus on the specific problem of cross language text matching at the sentence level In this problem a segment of text in a given source language is used as query for recovering a similar or equivalent segment of text in a different target language This task is essential to some specific applications such as parallel corpora compilation 3 and cross language plagiarism detection 4 We address the problem under consideration by means of an interlingua based CLIR system that follows a non linear semantic mapping approach similar to the one presented in 5 Semantic mapping techniques have been successfully used for concept association and related term identification tasks 6 7 We illustrate here that this kind of non linear mapping can constitute a very 
effective and valuable strategy for the problem under consideration Some other recent approaches have achieved interesting results in CLIR applications by using regression canonical correlation analysis an extension of canonical correlation analysis 8 The rest of the paper is structured as follows In section 2 the implemented interlingua based CLIR method is described In section 3 the proposed methodology is illustrated by performing cross language text matching at the sentence level on a penta lingual document collection Also within this section the performance quality of the implemented system is evaluated and compared against two conventional CLIR systems showing that the proposed approach outperforms the other two Finally in section 4 the most relevant conclusions derived from the experimental results are presented 2 Cross Language Semantic Mapping The fundamental issue behind the proposed CLIR method is the idea of semantic mapping As illustrated in 5 starting from the term document matrix 
representation of a given document collection it is possible to build a semantic representation for the collection by using the non linear projection technique known as multidimensional scaling 9 Moreover if a multilingual parallel document collection is available a semantic map can be computed independently for each language s document subset and the resulting maps will exhibit a high degree of similarity among them The observed similarities among the maps are mainly because such maps are able to capture the most prominent semantic relationships among the documents within the collection which are indeed language independent The structural similarities observed among the different semantic maps provide the possibility of placing documents from different languages into any other language generated map In this way these maps can be actually interpreted as an interlingua type of representation The general procedure for CLIR by means of semantic mapping can be summarized as follows A Non linear Semantic Mapping 
Technique for Cross Language Sentence Matching 59 Fig 1 Schematic representation of interlingua based CLIR by means of semantic mapping A linear transformation operator T for projecting documents or queries from the original high dimensional space into the designated low dimensional semantic map can be inferred from the multilingual set of anchor documents as follows M T D T M D 1 1 where D is a square matrix of size NxN being N the number of available anchor documents which contains the distances among the anchor documents in the original high dimensional space the document similarity matrix and M is the KxN matrix containing the coordinates of the anchor documents in the reduced k dimensional semantic map The matrix M which represents the coordinates for anchor documents in the computed semantic map is obtained by applying MDS to similarity matrix D More specifically the algorithmic setting for the proposed methodology considers using the cosine distance for constructing the similarity matrix D and Sammon 
s projection criterion for computing the semantic map M 10 Different from the procedure described in 5 where a monolingual projection operator was computed here we compute a cross language projection operator for which M is computed on the retrieval language and D is computed on the source language This cross language variant of the method has been proven to provide better results than the original monolingual projection operator 11 Once the projection operator has been computed any new probe document or query can be placed into the designated retrieval map by using m Td 2 60 R E Banchs and M R Costa where d represents a vector containing the distances between the probe document or query and the anchor documents in the original high dimensional space T is the projection operator defined in 1 and m is a vector containing the coordinates for the probe document or query in the low dimensional map Additionally as many maps can be generated as there are different languages in the multilingual collection we 
propose a multiple map combination approach based on a majority voting strategy According to this a retrieval map is constructed for each language in the multilingual collection Then all probe documents and queries are projected into all maps where similarities are computed and individual rankings are performed Finally a global ranking is obtained by majority voting of all individual rankings This procedure is illustrated in Figure 2 Fig 2 Majority voting strategy implemented for combining individual map rankings 3 Cross Language Sentence Matching Experiments As already mentioned in the introduction in this work we focus on the problem of cross language text matching at the sentence level In this particular task a segment of text in a given source language is used as query for recovering an equivalent segment of text in a different target language In this section the methodology described above is evaluated and compared with other two CLIR approaches another interlingua based approach which is based on 
latent semantic indexing 12 and a more conventional query translation approach 13 which considers a cascade combination of a machine translation system and a monolingual IR system 3 1 Multilingual Sentence Dataset The dataset considered for the experiments is a multilingual sentence collection that was extracted from the Spanish Constitution which is available for downloading at A Non linear Semantic Mapping Technique for Cross Language Sentence Matching 61 the Spanish government s main web portal www la moncloa es In this website all constitutional texts are available in five different languages including the four official languages of Spain Spanish Table 1 Main statistics for the overall multilingual dataset and the selected test set Overall Dataset English Number of sentences 611 Number of words 15285 Vocabulary size 2080 Average sentence length 25 01 Selected Test Set English Number of sentences 200 Number of words 4667 Vocabulary size 1136 Average sentence length 23 34 Spanish 611 14807 2516 24 23 
Spanish 200 4492 1256 22 46 Finally and for illustrative purposes one sample sentence from the multilingual collection is presented in Table 2 Table 2 A sample sentence from Spanish Constitution s multilingual sentence collection Language English Spanish 3 2 Experimental Evaluation of the Proposed Technique In this subsection the proposed methodology is illustrated by performing crosslanguage sentence matching on the Spanish Constitution s multilingual collection and its performance quality is evaluated by means of the top 1 and top 5 accuracies measured over the test subset that was described in Table 1 The specific task to be considered consists of recovering a sentence from the English version of the Spanish Constitution using as a query the same sentence in any of the four Spanish languages Spanish 62 R E Banchs and M R Costa of four hundred sentences constituted the anchor document collection that was used for constructing the maps One map was constructed for each of the five languages available in the 
collection by using multidimensional scaling The space dimension of the constructed maps was set to 350 As already reported in 5 and 11 where experiments considering a full range of reductions were presented space reductions down to dimensionalities above 75 the size of the anchor document collection provide appropriate embeddings for MDS and LSI based methods to be comparable Notice also that reducing the dimensionality down to 350 implied overall space reductions ranging from 83 in the case of English to 90 in the case of Euskera Finally following 1 and 2 transformation matrices were constructed for all constructed semantic maps and all test sentences from each language were placed into them Sentence matching was performed at each individual map by using the cosine distance as a similarity metric Table 3 summarizes results for all sentence matching exercises conducted over the five constructed maps as well as the implemented majority voting strategy and the four considered query languages Spanish Table 3 
Top 1 and Top 5 accuracies for all conducted experiments on cross language sentence matching based on semantic maps Spanish Several interesting observations can be derived from results reported in Table 3 First of all it can be seen that regardless the semantic map used for sentence matching the best scores are always obtained when Spanish is the query language This might be explained by the fact that the constitutional texts used are originally Spanish texts which have been further translated into the other four languages According to this it could be expected for Spanish sentence projections to be more coherent than other language projections across all maps and in consequence it would be reasonable to assume that best scores should be obtained in those cases where Spanish is either the query or the retrieval language On the other hand the worst results are consistently obtained for all cases in which Euskera is the query language This is explained by the morphological complexity of the language which is 
evidenced in Table 1 as it exhibits the largest vocabulary and the smallest number of running words Nevertheless surprisingly when the retrieval semantic map is derived from the Euskera anchor documents resulting scores are as A Non linear Semantic Mapping Technique for Cross Language Sentence Matching 63 good as the ones obtained from any other of the maps This verifies the high degree of language independence the generated semantic maps can provide Another interesting observation that can be derived from Table 3 is the fact that with the exception of Euskera queries the English map is the best single semantic map for sentence matching when considering top 1 matches At a first glance one may think that this must be related to English being the target language of the task under consideration Nevertheless if top 5 matches are considered best results are achieved with semantic maps constructed from Galego and Spanish anchor document collections which does not support the previous finding as well as does not 
seem to have any logical justification In this sense further research will be needed to come up with a clear understanding on these issues Finally it can be also concluded from Table 3 that the majority voting strategy implemented for combining all semantic maps is not actually providing a significant improvement on the sentence matching task under consideration Indeed it is only in two cases Spanish top 1 and 64 R E Banchs and M R Costa four Spanish languages plus English Portuguese and French However it must be advised that as the Euskera to English translation direction is not provided by this service the considered task could not be evaluated with this contrastive system for this specific language pair On the other hand the monolingual information retrieval step was implemented by using Solr which is an XML based open source search server based on the Apache Lucene search library 15 Table 4 summarizes the results obtained from the comparative evaluation between the proposed semantic map based methodology 
and the two contrastive systems Only those results corresponding to the majority voting strategy are reported for semantic maps for comparing these results to individual semantic mapping results the reader can refer to Table 3 Table 4 Comparative evaluation of the proposed method majority voting of semantic maps vs LSI based and query translation CLIR techniques Spanish As seen from the table the proposed methodology clearly outperforms the two contrastive systems in all the cases Notice however that the observed differences are small as all systems are providing high accuracy values in most of the cases Some additional experimentation has been conducted with several different subsets of the same 200 sentence test dataset and similar results have been consistently obtained This suggests that the observed differences among the methods in Table 4 although small are significant The results reported in Table 4 show that non linear semantic maps described here seem to be more suitable for cross language sentence 
matching than both the linear projections provided by the LSI based method and the language conversions provided by state of the art machine translation Also it can be verified that both contrastive systems perform the same for top 1 accuracies but query translation outperforms the LSI based approach for top 5 accuracies 4 Conclusions and Future Work A non linear semantic mapping procedure has been implemented for cross language text matching at the sentence level The proposed method relies on a non linear space reduction technique multidimensional scaling for constructing semantic embeddings of a given multilingual document collection These semantic representations can be exploited for implementing an interlingua based CLIR system In the considered CLIR task a segment of text in a given source language is used as query for recovering a similar or equivalent segment of text in a different target language The proposed method is evaluated and compared against two conventional cross language information 
retrieval methods over a penta lingual sentence collection A Non linear Semantic Mapping Technique for Cross Language Sentence Matching 65 extracted from the Spanish Constitution Results presented in this work show that the proposed methodology outperforms the other two methods on the specific task under consideration Despite the positive results the majority voting strategy that was implemented for combining the individual rankings obtained from different semantic maps does not seem to provide any significant improvement with respect to the independent use of the individual semantic maps In this sense further research will be needed to determine the best combination strategy which might include for example some optimization procedure over a linear combination of the sentence similarities computed on the different maps Additionally some other interesting problems that must be addressed in future research have been also identified References 1 Kishida K Technical issues of cross language information retrieval 
a review Information Processing and Management 41 3 66 R E Banchs and M R Costa 4 Potthast M Stein B Eiselt A Comparison of Paraphrase Acquisition Techniques on Sentential Paraphrases Houda Bouamor LIMSI CNRS Univ Paris Sud F 91403 Orsay France Abstract In this article the task of acquisition of subsentential paraphrases is discussed and several automatic techniques are presented We describe an evaluation methodology to compare these techniques and some of their combinations This methodology is applied on two corpora of sentential paraphrases obtained by multiple translations The conclusions that are drawn can be used to guide future work for improving existing techniques Keywords Paraphrase Monolingual bi phrase patterns 1 Introduction Deciding whether two text units convey the same meaning is one of the most important needs in Natural Language Processing As natural language offers many possible alternatives for expression the ability to determine that two words or phrases have equivalent meaning in context 
is required for analyzing text In question answering for instance this can be used to extract correct answers expressed with words that differ from those in the question Large scale acquisition of sets of equivalent text units is an active field of research Applications can be in text analysis for example to allow different wordings in Machine Translation evaluation 9 or in text generation for example to help writers to find more appropriate wordings 14 A number of techniques have been proposed for acquiring text units in a paraphrasing relationship defined by a reciprocal textual entailment between the two units These techniques are designed to acquire paraphrases from specific types of resources Monolingual corpora have been extensively used to test the distributional hypothesis which states that text units o ccurring in similar contexts may be paraphrases The limitation in themes or genres in a comparable corpus increases the probability of extracting accurate paraphrase pairs in context Bilingual 
parallel corpora have also been used to test the translation equivalence hypothesis which states that text units sharing translations in at least one other language may be paraphrases In contrast few works have addressed using monolingual parallel corpora made up of paraphrases at the sentential level This can be explained by the facts that very few such corpora are available and that their construction can be H Loftsson E 68 H Bouamor A Max and A Vilnat costly and difficult to design However because they asso ciate sentences which express the same meaning such corpora allows for the most reliable acquisition of paraphrase pairs Contrary to what is the case with comparable monolingual corpora or parallel bilingual corpora paraphrases can be observed directly and examples of contexts in which one may substitute one with the other can be extracted straightforwardly This work fo cusses on the acquisition of accurate paraphrases in context from monolingual parallel corpora and on combining the results obtained 
from different techniques The remainder of the paper is organised as follows In section 2 we will review the main approaches for acquiring subsentential paraphrases and then describe three particular existing techniques that can be applied on sentential paraphrases in section 3 a technique based on statistical learning of word alignments one based on the symbolic representation of linguistic variation and another based on the syntactic fusion of sentences An experimental framework for comparing and combining the outputs of these techniques will be described in section 4 Our methodology for building a suitable corpus by multiple translation will be explained as well as an existing metho dology for evaluating the performance of the various techniques Lastly we will conclude and describe our future work in section 5 2 Previous Work on Subsentential Paraphrase Acquisition The hypothesis that if two words or by extension two phrases occur in similar contexts then they may be interchangeable has been extensively 
tested This distributional hypothesis attributed to Zellig Harris was for example applied to syntactic dependency paths in the work of Lin and Pantel 13 Their results take the form of equivalence patterns with two arguments such as X asks for Y X requests Y X s request for Y X wants Y Y is requested by X Using comparable corpora where the same information probably exists under various linguistic forms increases the likelihood of finding very close contexts for subsentential units Barzilay and Lee 2 propose a multi sequence alignment algorithm that take structurally similar sentences and build a compact lattice representation that encode local variations The work by Bhagat and Ravichandran 4 describes an application of a similar technique on a very large scale The hypothesis that two words or phrases are interchangeable if they share a common translation into one or more other languages has also been extensively followed in works on subsentential paraphrase acquisition Bannard and CallisonBurch 1 describe a 
pivoting approach that can exploit bilingual parallel corpora in several languages The same technique has been applied to the acquisition of local paraphrasing patterns in Zhao et al 17 The works of Callison Burch 5 and Max 14 have shown how the monolingual context of a sentence to paraphrase can be used to improve the quality of the acquired paraphrases Another approach consists in modelling local paraphrasing identification rules The work of Jacquemin on the identification of term variants 8 which exploits Comparison of Paraphrase Acquisition Techniques on Sentential Paraphrases 69 rewriting morphosyntactic rules and descriptions of morphological and semantic lexical families can be extended to extract the various forms corresponding to input patterns from large monolingual corpora All the previous approaches can pro duce inappropriate pairs that do not correspond to paraphrastic variants in context This is largely due to the fact that the corpora that they use never explicitly encode paraphrasing 
relationships between text units For instance a paraphrase obtained by pivoting through another language may not have been observed in the context of the original phrase the phrase it is too early to could be automatically extracted for the original phrase this is not the time to by pivoting through the French phrase il est trop t ot pour an acceptable translation for some o ccurrences of the two English phrases but it would clearly not be appropriate with a context such as this is not the time to be negative Likewise extracting a phrase that appears in contexts very similar to that of an original phrase is limited by the effectiveness of the mo deling of context used for instance several occurrences of Spain defeated France and Spain lost to France should not be used as evidence for establishing a paraphrasing relationship between the two verbs 1 In contrast whenever parallel monolingual corpora aligned at the sentence level are available the task of subsentential paraphrase acquisition can be cast as one 
of word alignment between two aligned sentences 2 Previous works have exploited multiple translations which o ccur very infrequently naturally But such translations are sometimes pro duced albeit in small quantities for example as multiple reference translations for evaluating Machine Translation outputs automatically Barzilay and McKeown 3 applied the distributionality hypothesis on such parallel sentences and Pang et al 16 proposed an algorithm to align sentences by recursive fusion of their common syntactic constituants Callison Burch et al 6 describe an automatic metric that can be used to compare techniques extracting subsentential paraphrases from pairs of sentential paraphrases 3 Acquisition of Subsentential Paraphrases from Sentential Paraphrases As discussed in section 2 acquiring subsentential paraphrases is a challenging task In this work we consider the most simple scenario where sentential paraphrases are available and words phrases from one sentence can be aligned to words phrases from the 
other sentence In this section we describe several techniques and their implementation that perform the task of subsentential unit alignment and how their results can be combined in a simple way We will also describe an existing evaluation metric that will allow us to compare the performance of these techniques 1 2 As previously said the use of comparable corpora provides a promising way to alleviate this issue as limiting the corpus to for example press coverage for the same piece of news strongly increases the probability of finding very close contents We do not address here the discourse and implication issues which make aligning the full contents of two such sentences not possible in all cases 70 H Bouamor A Max and A Vilnat 3 1 Statistical Word Alignment Method Word In phrase based Statistical Machine Translation bilingual phrases are extracted from parallel corpora as the basic units for translating These biphrases are often extracted in two steps 15 first word alignments are found in both directions 
and some heuristics is then applied to symmetrize these alignments and to extract biphrases from the resulting alignment Word alignment mo dels are learnt from the full training set of parallel sentences in the training corpus more data typically resulting in improved performance Furthermore because the underlying training algorithms make the assumption that sentences in a pair form a strict correspondance sentences that are complete and somehow literal translations will make word alignment and biphrase extraction more precise 3 Note that alignment mo dels typically support alignment to Null tokens for words which could not be aligned thus representing word insertions deletions However these words can still be included in the extracted biphrases depending on the heuristics used Using such alignment mo dels for the monolingual case has already been tested but to our knowledge no work has reported using parallel monolingual corpora due to their lack of availabibility 4 For this work we collected sentential 
paraphrases to build a monolingual parallel corpus In order to increase the number of examples at the sentential level we take all pairings for paraphrases belonging to the same group if they exist to build our training corpus 5 We used the Moses system 12 for word alignments using Giza 15 and default symmetrisation Once an alignment matrix is available we extract biphrases corresponding to possible subsentential paraphrases using the following criterion 6 two phrases from each sentence constitute a paraphrase pair if all words from one phrase are aligned to at least one word from the other phrase and not to words outside it 3 2 Term Variant Identification Method Term Sentential paraphrases can use common words but also synonyms and more generally phrasal paraphrases For such pairings and under certain conditions that are met with the types of corpus that we use rules can be expressed to mo del acceptable variations Research on term variant extraction thus offers a direct solution to the problem of 
subsentential alignment for some units We have 3 4 5 This partly explains why some language pairs are harder to align than others Works based on translational equivalence such as 1 alleviate this issue by using more readily available bilingual parallel corpora However one of the main limitation of this approach which motivated works on context modeling for validating the extracted paraphrases 5 14 is that the extracted biphrases are only indirectly aligned The strong limitation of our work for using parallel monolingual corpora finds here one of its main justifications Note that this will give a clear advantage to the statistical word alignment technique over the other techniques that we will discuss which do not currently support exploiting information from other sentences or other sentence pairings Comparison of Paraphrase Acquisition Techniques on Sentential Paraphrases 71 used the Fastr system 8 which takes as input a corpus and a list of terms and outputs the indexed corpus in which terms and variants 
are recognized using sets of meta rules applying to term rules to define acceptable variations Meta rules allow us to define morphosyntactic rewriting patterns as well as morphologic and semantic lexical relationships Fastr offers a large ruleset mostly for nominal and verbal terms and large lists of morphological variants and synonyms The controlled indexing program of Fastr extracts all variants from a list of terms from a corpus We used it to find phrase alignments in both directions Given a pair of sentential paraphrases all phrases up to a given length were extracted from one of the paraphrases Those were then used by Fastr as input terms to perform controlled indexing of the corpus consisting solely of the other paraphrase Only biphrases that are found in of both directions are kept We mo dified the program configuration so that it accepts one word terms useful for synonym detection but otherwise used its default resources This consequently performs a biphrase extraction fo cussed on nominal and verbal 
terms 3 3 Alignment by Syntactic Fusion Method Synt The exploitation of the parallelism of two sentential paraphrases can be pushed further if two such paraphrases share their high level syntactic structure then it is possible to guide the alignment of their words by recursively aligning their syntactic subconstituents Pang et al 16 proposed an implementation of this idea which is illustrated on Figure 1 The two sentences of a paraphrase pair are first syntactically parsed Fusion then takes place in the following way two syntactic subtrees are recursively merged if their root category and the categories of the ordered list of their daughter subtrees match Otherwise when the categories of the daughter subtrees do not match a list of alternative derivations is created at that node In order to avoid mistakenly merging some subtree configurations the authors intro duced a lexical blocking mechanism which prevents merging two subtrees if a content that belongs to the accessible vocabulary of one daughter subtree 
of the first subtree also belongs to the accessible vocabulary of a different daughter subtree of the second subtree This prevents merging in cases such as active and passive voice sentential paraphrases where in spite of matching high level syntactic structures the agent and patient have been swapped In a last step the obtained parse forest is linearized to yield a word lattice This lattice can finally be reduced by merging edges with the same words that have common prefixes or suffixes All subpaths originating from the same start and end nodes thus represent subsentential units in an equivalence relation 6 The set of all pairs of subsentential paraphrases encoded in the lattice can be effectively extracted by a simple traversal of the lattice Note that the presented algorithm can work with any number of input sentential paraphrases as illustrated in the original article 16 6 Note that in the extreme case of two sentences whose root nodes cannot be merged the smallest pair of equivalent units is made up the 
two full sentences 72 H Bouamor A Max and A Vilnat Fig 1 Illustration of Pang et al s 16 syntactic fusion algorithm Our implementation of this technique revealed several limitations First this algorithm stops all merging whenever lexical blocking is fired which was probably motivated by the fact that the ob jective of the authors was to generate new sentential paraphrases from a set of existing sentences However because we want to extract as many accurate pairings as possible we allow merging to continue on subconstituents which should not be affected by lexical blocking We also addressed the strong dependency of the algorithm on the precision of the automatic syntactic parses used Indeed we observed many cases in our development data where legitimate merging did not take place because of incompatible parse trees resulting from locally wrong parse trees However wrong syntactic parses can sometimes produce goo d alignments if both parses can be merged in the same way as valid parses would allow Using the 
Berkeley probabilistic parser 10 the k best parses for both sentences are used and the ith parse of the first sentence and the j th parse of the second one kept are those for which the correponding lattice is the most compact before reduction The intuitive motivation for this choice is that the more compact a lattice is before reduction the more two sentences have been aligned which is a sought property given the parallelism of the input sentences and that the reduction operation should reduce as few as possible nodes that would not have been previously merged due to compatible subparses of the two sentences We chose Comparison of Paraphrase Acquisition Techniques on Sentential Paraphrases 73 a value of 5 for k thus limiting the number of merged configurations to 52 25 per sentence pair 3 4 Combination of Paraphrase Pairs from Different Techniques All the presented techniques for extracting sub sentential alignments were run independently to pro duce candidates for phrases up to 7 tokens pairs of identical 
phrases excluded Each technique makes use of its own hypotheses and resources and it can be expected that they could have different performances depending on the configurations of the merged sentences Working on efficiently combining these techniques is therefore an interesting issue In this work we started by considering the simple case of output combination by performing set unions and intersections Taking the union of the candidate pairs of two or more techniques is expected to increase the recall of found pairs relative to a reference alignment while taking the intersection is expected to increase the precision of such pairs It is of particular interest to measure how a combination of both measures would behave for such cases 4 4 1 Experimental Framework Corpus Collection In order to build development and test corpora we set up a web based data acquisition experiment Volunteer contributors were asked to translate sets of sentences into French7 The same sets of sentences were translated by several 
contributors from any of 10 languages from the Europarl corpus 11 of European parliamentary debate The web interface provided the contributors who were not for the most part professional translators with several convenience tools to help them in their task One of them allowed checking a reference translation from Europarl which was not used to build our corpus to make local improvements and corrections once an initial candidate translation was submitted This technique proved quite successful in ensuring an acceptable quality for most submitted translations All translations used in these experiments were manually checked by a unique annotator who followed the rule to remove any translation which showed too strong a bias towards the reference translation between the two versions of a translation In order to measure the similarity degree between lexical paraphrases obtained from different languages we computed the overlap coefficient which represents the lexical overlap percentage between the vocabularies of 
two sentences S1 and S2 S1 S2 1 CO min S1 S2 7 We considered important to use for our experiments a language for which all our contributors and human annotators had a native or near native command 74 H Bouamor A Max and A Vilnat Table 1 Similarity degree between paraphrases obtained from different languages All tokens Lemmas of content words en es de it pt en es de it pt en 0 90 172 0 64 69 0 59 89 0 63 84 0 62 58 0 90 172 0 65 69 0 61 89 0 66 84 0 64 58 es 0 62 57 0 63 57 0 64 51 0 57 57 0 68 57 0 68 51 de 0 58 67 0 61 53 0 59 67 0 62 53 it 0 65 50 0 66 50 pt The minimum number of pairs for a group was set to 20 in this experiment The left side of Table 1 shows the average coefficient of lexical overlap for all tokens on the selected groups of paraphrases and for different source languages Numbers shown as indices represent the number of sentential paraphrases common translations obtained from two languages For instance the 172 paraphrases obtained from English have an average of 90 common tokens In 
contrast we find that those from two different languages contain on average between 36 and 42 different tokens These values show as we expected that we obtain more lexical variation using different source languages for semantically equivalent sentences We repeated this experiment considering this time only lemmatised forms of content words Results shown on the right side of Table 1 show a similarity degree which is slightly higher than in the previous experiment varying from 57 to 68 for different languages This still shows a significant level of lexical variation in the translation pro cess at the level of the content words used In order to simulate various degrees of parallelism between two sentences in a pair we built two sub corpora from our full corpus we took a set of 50 sentences which were selected on the basis that 4 independent valid translations from English and one valid translation from German Spanish Italian and Portuguese were available 8 We therefore obtained two corpora of 50 groups of 4 
paraphrases each In each group one paraphrase is randomly selected as a reference paraphrase to which the three others will be aligned Three human annotators were then asked to manually align at the token level each of the 300 sentence pairs 2 corpora x 50 groups x 3 alignments The Yawat 7 manual word alignment tool was used to allow aligning sentences by visual selection of phrases and optional checking on word alignment matrices Each sentence pair was annotated by a single annotator as the work of Callison Burch and al 6 reports an acceptable inter annotator agreement rate on such a task 9 We nevertheless asked one of the judges to check all alignments and make the necessary mo difications to make them more uniform Reference biphrases were finally automatically extracted from the token alignment matrix by following the rule previously described at the end of section 3 1 8 9 Note that the version of the Europarl corpus that we used did not contain information about the original language for sentences It 
should be noted that their work was on news text in English and that their annotators had been provided with a detailed annotation guide Comparison of Paraphrase Acquisition Techniques on Sentential Paraphrases Source English English English 75 Contributed translation Plusieurs orateurs ont An example of a paraphrase group is shown on Figure 2 As can be seen the source language often implies an important bias for the pro duction of the contributed translation which results in part from the fact that our contributors were not professional translators One can also notice that some original translations may express slightly different content resulting from the choices of the sequence of translators involved to obtain these translations 4 2 Experimental Results In order to evaluate and compare the results of the implemented techniques for subsentential paraphrase extraction we followed the Parametric approach described in 6 the set of candidate biphrases extracted from a sentence pair is compared with a set of 
reference biphrases obtained through human annotation as described in section 4 1 by computing precision and recall values The former value corresponds to the proportion of candidates produced by a technique which are correct relative to the reference biphrases while the latter value corresponds to the proportion of reference biphrases that are extracted by a technique As we are also interested in the combination of both these values when combining candidate sets we also computed an F measure value F1 which considers recall and value as equally important We run the three techniques described in section 3 on our two subcorpora denoted en2fr and xx2fr and computed evaluation scores on their result sets and on result sets obtained for simple combinations Results are given on Table 2 It is first quite apparent that the performance of all techniques both in terms of precision and recall is highly dependent on the nature of the sentential paraphrase pairs which can be interpreted as a higher complexity for 
aligning sentence pairs pro duced from different languages If Synt is unsurprisingly very sensitive to this Word also seem to be significantly impacted when attempting alignment on less literal sentences which can be compared to the higher difficulty in aligning unrelated languages during training in Statistical Machine Translation 76 H Bouamor A Max and A Vilnat Table 2 Results obtained for each technique and some combinations of their outputs in terms of precision P and recall R of their candidate biphrases relative to reference biphrases The F1 values gives as much importance to precision and recall Word Term Synt TW TW TS TS WS WS TWS Paraphrases obtained by translating from English en2fr P 41 94 41 19 50 16 41 54 55 97 46 48 80 76 40 83 71 21 40 46 R 3578 67 07 3 07 8 77 67 66 2 48 11 26 0 58 67 83 8 02 68 41 F1 51 61 5 87 14 93 51 47 4 76 18 13 1 16 50 98 14 41 50 58 Paraphrases obtained by translating from 4 languages xx2fr P 27 05 35 98 40 46 27 08 42 98 39 46 28 57 26 91 50 15 26 90 R 2517 51 80 3 
05 8 26 52 92 1 94 11 08 0 23 53 43 6 63 54 39 F1 35 54 6 07 13 72 35 83 3 72 17 30 0 47 35 79 11 72 36 00 Looking at recall we can note a strong difference between Word on the one side and Term and Synt on the other side with the latter two proposing much fewer paraphrase pairs from the reference alignments The proportion of aligned words is unsurprisingly higher for Word as this statistical word alignment technique attempts aligning as many words as possible although aligning to a NULL token is possible under certain circumstances Nonetheless Word still achieves a reasonable precision score Note however that this technique benefited from a positive bias as it was able to exploit all sentential paraphrase pairings to build its alignment mo del and therefore could effectively make use of redundancy while the two other techniques could not take such information into account as implemented Term seems to be specialized in extracting very fo cused biphrases Synt achieves the best precision overall with a 10 
point advantage for the paraphrases obtained from one language over paraphrases obtained from 4 different languages Para 1 German En ce qui concerne les relations internationales la Fig 3 Examples of bi phrases extracted by different techniques from a pair of paraphrases produced from German and Italian sentences biphrases in bold belong to the reference set Comparison of Paraphrase Acquisition Techniques on Sentential Paraphrases 77 The various tested combinations reveal expected gains in recall for union and in precision for intersection Accordingly on the en2fr corpus maximum precision is obtained by computing intersection sets with the results of Synt and maximum recall is obtained by computing union sets with the results of Word Results are roughly similar on the xx2fr corpus with the notable exception of the TS combination which obtained a comparatively much worse performance than on the other corpus Figure 3 illustrates an example of alignment results between two paraphrases obtained from German and 
Italian whose alignment is difficult as confirmed by the low number of biphrases in the reference set Word was unable to reliably align the words and produced many incorrect biphrases Term and Synt have instead proposed only few candidates which reflects again the difficulties of matching encountered by these two techniques 5 Conclusion and Future Work In this article we have described the task of subsentential paraphrase extraction from sentential paraphrases a resource which is rare but which allows us as we argued to concentrate on the most natural scenario for observing such local paraphrases Furthermore such sentential paraphrases allow us to trivially extract contextual information e g words linked by a grammatical dependency to words in the paraphrase pair associated to paraphrase pairs that can be used to bootstrap context profiles for which the paraphrase pair is valid We have described three techniques initially developed for different purposes which operate at various levels and use different 
resources and compared them on two subcorpora representing two levels of parallelism for sentences Acceptable levels of precision and recall relative to a reference alignment were achieved and simple combinations of results yielded gains for one of the two metrics Our future work will be organized along three different lines First we want to be able to generalize the obtained subsentential paraphrases to learn paraphrasing patterns which integrate contextual information Then we plan to first independently improve each of the presented techniques and then work on efficient hybrid implementations at extraction time Finally we want to study techniques for validating paraphrases acquired on monolingual parallel corpora on much more readily available monolingual comparable corpora Acknowledgements This work was supported by a grant from LIMSI The authors wish to thank the volunteer contributors who took part in the data collection References 1 Bannard C Callison Burch C Paraphrasing with bilingual parallel 
corpora In Proceedings of ACL Ann Arbor USA 2005 2 Barzilay R Lee L Learning to paraphrase an unsupervised approach using multiple sequence alignment In Proceedings of NAACL HLT Edmonton Canada 2003 78 H Bouamor A Max and A Vilnat 3 Barzilay R McKeown K Extracting paraphrases from a parallel corpus In Proceedings of ACL Toulouse France 2001 4 Bhagat R Ravichandran D Large scale acquisition of paraphrases for learning surface patterns In Proceedings of ACL HLT Columbus USA 2008 5 Callison Burch C Syntactic constraints on paraphrases extracted from parallel corpora In Proceedings of EMNLP Hawai USA 2008 6 Callison Burch C Cohn T Lapata M Parametric An automatic evaluation metric for paraphrasing In Proceedings of COLING Manchester UK 2008 7 Germann U Yawat Yet Another Word Alignment Tool In Proceedings of the ACL 2008 HLT Demo Session Columbus Ohio pp Digital Learning for Summarizing Arabic Documents Mohamed Mahdi Boudabous1 Mohamed ANLP Research Group MIRACL Laboratory Faculty of Economic Sciences and 
Management of Sfax FSEGS B P 1088 3018 Sfax Tunisia 2 LPL Laboratory CNRS 1 Abstract We present in this paper an automatic summarization method of Arabic documents This method is based on a numerical approach which uses a semi supervised learning technique The proposed method consists of two phases The first one is the learning phase and the second is the use phase The learning phase is based on the Support Vector Machine SVM algorithm In order to evaluate our method we conducted a comparative study that involves the results generated by our system AIS Arabic Intelligent Summarizer with that realized by a human expert The obtained results are very encouraging and we plan to extend our evaluation on a larger corpus to ensure the performance of our system Keywords Automatic summarization Arabic documents Machine Learning Numerical approaches 1 Introduction In the current context we have to deal with a huge mass of electronic textual documents available through the net We need tools offering fast visualization 
of the documents so that the user can evaluate its relevance Automatic summarization provides a solution which makes it possible to extract interesting information for an advantageous reuse Indeed the summary helps the reader to decide whether the original document contains the required information or not Moreover in some cases the reader does not need to read the totality of the original document simply because the required information is in the summary 1 Automatic summarization approaches are inspired by various orientations Some approaches rely on symbolic techniques based on the analysis of the discourse and its discursive structure some others are based on numerical treatments based on statistical or even on learning 2 In addition the majority of automatic summarization systems mainly treat documents in Indo European languages such as English and French To our knowledge there are only few implementations of these methods on Arabic language such as LAKHAS 3 and Al Lakas El eli 4 Thus there is an 
increasing need to develop H Loftsson E 80 M M Boudabous M H Maaloul and L H Belguith automatic summarization systems dedicated to Arabic to handle the increasing amount of electronic documents written in Arabic 1 Thus the achievements in the field of automatic summarization are generally set out again according to the used approaches Mainly three approaches are distinguished numerical symbolic and hybrid Our contribution is in the context of numerical approach and we propose a system for the automatic summarization of Arabic documents which is based on a purely Machine learning ML technique ML technique within the framework classification is shown to be a promising way to combine automatically sentence features 5 In our method a classifier is trained to distinguish between two classes of sentences summary and non summary ones Statistical features that we consider in this work are partly from the state of art and they include cue sentencess and positional indicators 6 title keyword similarity 7 and other 
features This paper is structured around four sections Section 1 presents most related works to ours Section 2 exposes the proposed method and the summarizing workflow and Section 3 describes the implementation of our approach and the primary results Section 4 presents the conclusion and the future works 2 Related Work Three approaches have been proposed to the summarizing of documents Linguistic approaches based on a formal representation of knowledge contained in documents or on a reformulation technique Indeed these approaches are usually a formal representation of knowledge contained in documents or on reformulation techniques Numerical approaches are based on calculating a score associated for each sentence to estimate its importance relative to other sentences of the document This score is calculated by using various statistical methods probabilistic and learning Hybrid approaches combine the previous approaches to improve the quality of the summary In this paper we explore a numerical approach and 
present some examples Numerical approaches are essentially based on calculating a score associated for each sentence to estimate its importance The final summary will only keep the sentences that have the highest scores There are two main techniques statistical and learning techniques Recently various authors have explored Machine Learning techniques to summarize documents 7 This is thanks to the best performance of these techniques The learning techniques are classified into three classes The first class is the supervised learning this class is based on two phases the learning phase that use a training corpus of a very large size and the validation phase that use another corpus called validation corpus 8 The second class is the semi supervised learning that has only a learning phase this phase requires a training corpus of small size 9 10 The third one is the non supervised learning which does not require either a training corpus or a validation corpus The numerical approaches can be applied to all types of 
corpus and can operate in a big number The most important systems which are based on the numerical approaches are LAKHAS system 3 which summarizes Arabic documents in XML format CBSEAS Clustering Based Sentence Extractor for Automatic Summarization system Digital Learning for Summarizing Arabic Documents 81 11 treats the case of multi document summary Its principle is that the more redundant information are the more important they will be Our method treats the numerical approaches that have proven their effectiveness in other languages More precisely we use Machine learning techniques based on semisupervised learning this choice is justified by the fact that it allows involving a system with only a small number of labeled sentences and a large number of not labeled ones 3 Proposed Method In this section we present an overview of the proposed method and the summarizing workflows for the HTML documents 3 1 An Overview of Our Method We propose a new method for the automatic summarization of the newspaper 
articles in Arabic language It is based on a Machine learning technique More precisely it is based on the semi supervised learning technique which is composed of two phases the first one is the learning phase which allows the system to learn how to extract summary sentences We use Support Vector Machines algorithm SVM for this phase The second phase is the use phase which allows users to summarize a new document Fig 1 presents the details of the proposed method and the two phases 3 2 Summarization Workflow 3 2 1 The Learning Phase In this phase the system designer should provide the training corpus and the extraction features to perform the learning The training corpus is composed of the source documents and their summaries All the documents are initially pretreated to prepare their segmentation in titles sections paragraphs and sentences This segmentation is based on the criteria of punctuation and HTML tags After the segmentation step each sentence of the segmented document will be notified according to 
some features This step leads to the construction of a set of the vectors V corresponding to the values of the specific features to the sentence These vectors are called extraction vectors or score vectors Each vector is associated with a Boolean criterion which indicates the sentence class summary or non summary The extraction vector has the following structure V1 S1 S2 S3 Sn where Si is the score of the criterion i and n is the number of the criteria In the learning phase extraction vectors are combined to associate a score with each feature and generate rules 3 2 2 The Use Phase In this phase the user provides a HTML document as an input for the system This document is segmented and notified in order to generate a set of extraction vectors The system uses the generated rules to classify each sentence 82 M M Boudabous M H Maaloul and L H Belguith Fig 1 The principle of the proposed method 4 The AIS System The method that we proposed for automatic summarization of Arabic documents has been implemented 
through the AIS Arabic Intelligent Summarizer system In this section we present the implementation details and the preliminary results 4 1 Implementation Details Our corpus is composed of 500 Arabic documents collected from the web These documents represent newspaper articles selected according to various orientations sport economy education etc The newspaper articles are of HTML type with a UTF 8 coding The summaries of these documents are produced by three human experts Then we use the index of kappa1 to calculate the similarity between human experts and generate one summary for each document 1 http kappa chez alice fr Digital Learning for Summarizing Arabic Documents 83 After the segmentation step we use 15 features to classify each sentence Some of these features are detailed in Table 1 Table 1 Features details Features Position in the text First sentence in the section First sentence in the paragraph Range of the paragraph Tf_idf score Tf score Title keywords Indicative expressions Details Indicates the 
position of the sentence in the text Indicates if the sentence is the first in the section or not Indicates if the sentence is the first in the paragraph or not Indicates the range of paragraph that contains the sentence Calculates the tf idf of the score Calculates the Tf of the score Presents the number of title keywords in the sentence Presents the number of indicative expressions in the sentence Finally we obtain a file that contains the set of extraction vectors which constitute the input of the learning phase In the learning phase we use the SVM algorithm to learn how to classify the summary and non summary sentences At the end of the learning phase a score is associated with each feature Some features can have a score of zero The SVM algorithm generates a rule by summing scores associated with each feature The system uses the generated rules to calculate the score of each sentence If the score is positive the sentence will be considered as a summary sentence otherwise the sentence is considered as a 
non summary sentence Finally the system combines summary sentences to obtain the summary 4 2 Preliminary Results We used 60 documents of our corpus to experiment our system i e 50 documents for the learning phase and 10 documents for the evaluation phase The obtained summaries are compared to the human summaries The average measures for Precision Recall and F measure are respectively 0 992 0 991and 0 991 see Table 2 Table 2 Evaluation results Precision 0 992 Recall 0 991 F measure 0 991 Weighted Avg 5 Conclusion and Future Work In this paper we have proposed a method for automatic summarization of Arabic documents Our method is implemented by AIS system and is based on the Machine learning technique Our work focuses on a particular type of documents i e the newspaper articles in HTML format We believe that the preliminary results are very encouraging Indeed the F measure is equal to 0 991 We note that we used a small 84 M M Boudabous M H Maaloul and L H Belguith corpus for the evaluation but as perspectives 
we plan to extend the evaluation on a larger corpus We also intend to apply the proposed method for other types of documents such as XML and TXT References 1 Concept Based Representations for Ranking in Geographic Information Retrieval Maya Carrillo1 2 1 Abstract Geographic Information Retrieval GIR is a specialized Information Retrieval IR branch that deals with information related to geographical locations Traditional IR engines are perfectly able to retrieve the majority of the relevant documents for most geographical queries but they have severe difficulties generating a pertinent ranking of the retrieved results which leads to poor performance A key reason for this ranking problem has been a lack of information Therefore previous GIR research has tried to fill this gap using robust geographical resources i e a geographical ontology while other research with the same aim has used relevant feedback techniques instead This paper explores the use of Bag of Concepts BoC a representation where documents are 
considered as the union of the meanings of its terms and Holographic Reduced Representation HRR a novel representation for textual structure as re ranking mechanisms for GIR Our results reveal an improvement in mean average precision MAP when compared to the traditional vector space model even if Pseudo Relevance Feedback is employed Keywords Geographic Information Retrieval Vector Model Random Indexing Context Vectors Holographic Reduced Representation 1 Introduction Geographic Information Retrieval GIR deals with information related to geographic locations such as the names of rivers cities lakes or countries 18 The first and second authors were supported by Conacyt scholarships 208265 and 165545 respectively while the third fifth and sixth authors were partially supported by SNI Mexico This work has been also supported by Conacyt Project Grant 61335 H Loftsson E 86 M Carrillo et al Information that is related to a geographic space is called geo referenced information which is often linked to locations 
expressed as place names or phrases that suggest a geographic location For instance consider the query ETA in France Traditional IR techniques will not be able to pro duce an effective response to this query since the user information need is very general Therefore GIR systems have to interpret implicit information contained in do cuments and queries to provide an appropriate response to a query This additional information would be needed in the example to match the word France with other French cities as Paris Marseille Lyon etc Recent developments in GIR systems have demonstrated that the GIR problem is partially solved through traditional or minor variations of common IR techniques It is possible to observe that traditional IR engines are able to retrieve the ma jority of relevant documents for most geographical queries but they have severe difficulties generating a pertinent ranking of the retrieved results which leads to poor performance An important source of the ranking problem has been the lack of 
information Therefore previous research in GIR has tried to fill this gap using robust geographical resources i e a geographical ontology whilst other research has used relevance feedback techniques instead As an alternative our method suggests representing additional information incorporating concept based representations We think that concept based schemes provide important information and that they can be used as a complement to the Bag of Words representations Our goal is therefore to investigate whether combining word based and concept based representations can be used to improve GIR In particular we consider the use of two do cument representations a Bag of Concepts BoC as proposed by Sahlgren and 2 GIR Related Work Geographical Information Retrieval GIR considers the search for do cuments based not only on conceptual keywords but also on spatial information i e Concept Based Representations for Ranking in GIR 87 geographical references 18 Formally a geographic query geo query is defined by a tuple 
what relation where 19 The what part represents generic terms non geographical terms employed by the user to specify its information need which is also known as the thematic part The where term is used to specify the geographical areas of interest Finally the relation term specifies the spatial relation which connects what and where For example in query Child labor in Asia the what part would be Child labor the relation term would be in and the where part Asia GIR was evaluated at the CLEF forum 14 from 2005 to 2008 under the name of the GeoCLEF task 15 Several approaches were focused on solving the ranking problem during these years Common employed strategies are a query expansion through feedback relevance 6 9 10 b re ranking retrieved elements through adapted similarity measures 7 and c re ranking through information fusion techniques 9 10 11 These strategies have been implemented following two main paths first techniques that have paid attention to constructing and including robust geographical resources 
in the pro cess of retrieving and or ranking do cuments And second techniques that ensure that geographical queries can be treated and answered by employing very little geographical knowledge As an example of those in the first category previous research employed geographical resources in the pro cess of query expansion Here they first recognize the geographical named entities geo terms in the given geo query by employing a GeoNER1 system Afterwards they then employ a geographical ontology to search for these geo terms and retrieve some other related geographical terms The retrieved terms are then used as feedback elements to the GIR engine However a ma jor drawback with these approaches is the huge amount of work needed in order to create such ontologies for instance Wang et al in 6 employ two different geographical taxonomies Geonames2 and WorldGazetter3 to construct a geographical ontology with only two spatial relations part of and equal This leads to the fact that the amount of geographical information 
included in a general ontology is usually very small which limits it as an effective geographical resource Some other approaches that fo cus on the re ranking problem propose algorithms that consider the existence of Geo tags4 therefore the ranking function measures levels of topological space proximity or geographical closeness among the geo tags of retrieved do cuments and geo queries 7 In order to achieve this geographical resources are needed Although these strategies work well for certain type of queries in real world applications neither geo tags nor robust geographical resources are always available In contrast approaches that do not depend on any geographical resource have proposed and applied variations of the query expansion pro cess via relevance 1 2 3 4 Geographical Named Entity Recognizer Geonames geo coding web service http www geonames org WorldGazetteer http www world gazetteer com A Geo tags is a label that indicates the geographical focus of certain document or geographical query 88 M 
Carrillo et al feedback without special consideration for geographic elements 8 9 Despite this they have achieved acceptable performance results sometimes even better than those obtained employing resource based strategies There is also work fo cusing on the re ranking problem it considers the existence of several lists of retrieved documents from one or more IR engines For instance one IR engine can be configured to manage a thematic index i e non geographical terms while another IR engine is configured to manage only geographical indexes 8 9 10 11 18 Therefore the ranking problem is seen as an information fusion problem where simple strategies only apply logical operators to the lists e g AND in order to generate one final re ranked list 10 while others apply techniques based on information redundancy e g CombMNZ Round Robin or Fuzzy Borda 8 10 11 18 Recent evaluation results indicate that there is not a notable advantage of resource based strategies over metho ds that do not depend on any geographical 
resource 11 Motivated by these results our metho d do es not depend on the availability of geographical resources but we contemplate the use of different lists of ranked retrieved do cuments VSM BoC and HRR looking for improvement of the base ranker efficiency by the combination This work differs from previous efforts in that we consider in the re ranking pro cess the context information and syntactic structure contained in geo queries and retrieved do cuments This additional information is captured by BoC and HRR representations which need special vectors built by Random Indexing RI 3 Random Indexing The vector space model VSM 16 is probably the most widely known IR mo del mainly because of its conceptual simplicity and acceptable results The mo del creates a space in which both documents and queries are represented by vectors This vector space is represented by V a n x m matrix known as term do cument matrix where n is the number of different terms and m is the number of do cuments in the collection The 
VSM assumes that term vectors are pair wise orthogonal This assumption is very restrictive because the similarity between each document query pair is only determined by the terms they have in common not by the terms that are semantically similar in both There have been various extensions to the VSM One example is Latent Semantic Analysis LSA 17 a method of word co o ccurrence analysis to compute semantic vectors context vectors for words LSA applies singular value decomposition SVD to V the term document matrix in order to construct context vectors As a result the dimension of the pro duced vector space will be significantly smaller by grouping together words that mean similar things consequently the vectors that represent terms cannot be orthogonal However dimension reduction techniques such as SVD are expensive in terms of memory and processing time As an alternative there is a vector space metho dology called Random Indexing RI 3 which represents an efficient scalable and incremental method for building 
context vectors which express the distributional profile of linguistic terms Concept Based Representations for Ranking in GIR 89 RI overcomes the efficiency problems by incrementally accumulating k dimensional index vectors into a context matrix R of order n x k where k m but usually on the order of thousands This is done in two steps 1 A unique random representation known as index vector is assigned to each context either document or word consisting of a vector with a small number of non zero elements which are either 1 or 1 with equal amounts of both For example if index vectors have twenty non zero elements in a 1024 dimensional vector space they have ten 1s and ten 1s Index vectors serve as indices or labels for words or do cuments 2 Index vectors are used to pro duce context vectors by scanning through the text Every time a target word t occurs in a context c the index vector of the context ic is added to the context vector of t tc Thus the context vector of t is updated as tc ic In this way R is a 
matrix of k dimensional context vectors that are the sum of the terms contexts Notice that these steps will pro duce a standard termdo cument matrix V of order n x m if we use unary index vectors of the same dimensionality as the number of contexts Such m dimensional unary vectors would be orthogonal whereas the k dimensional random index vectors are only nearly orthogonal However Hecht Nielsen 21 stated that there are many more nearly orthogonal directions in a high dimensional space than truly orthogonal directions which means that context matrix R n x k will be an approximation of the term do cument matrix F n x m The approximation is based on the Johnson Lindenstrauss lemma 21 which states that if we project points in a vector space into a randomly selected subspace of sufficiently high dimensionality the distances between the points are approximately preserved Then the dimensionality of a given matrix V can be reduced by projecting it through a matrix P Rnxk Vnxm Pmxk 1 Random Indexing has several 
advantages 1 It is incremental which means that the context vectors can be used for similarity computations even after just a few do cuments have been pro cessed 2 It uses fixed dimensionality which means that new data do not increase the dimensionality of the vectors 3 It uses implicit dimensionality reduction since dimensionality is much lower than the number of contexts in the data k m There are works that have validated the use of RI in text pro cessing tasks for example Sahlgren Karlgren 12 demonstrated that Random Indexing can be applied to parallel texts for automatic bilingual lexicon acquisition Sahlgren 4 BoC Document Representation BoC is a recent representation scheme intro duced by Sahlgren 90 M Carrillo et al the union of the meanings of its terms This is accomplished by generating term context vectors for each term within the do cument and generating a do cument vector as the weighted sum of the term context vectors contained within that document Thus the m do cuments in a collection D are 
represented as s di j 1 hji gj i 1 m 2 where s is the number of terms in do cument di gj is the context vector of term j and hji is the weight assigned to term j in the do cument i according to the weighting scheme considered The context vectors used in BoC are generated using RI and Document Occurrence Representation DOR DOR is based on the work of Lavelli et al 13 and considers the meaning of a term as the bag of do cuments in which it o ccurs When RI is used together with DOR the term t is represented as a context vector u t k 1 bk 3 where u is the number of do cuments containing t and bk is the index vector of do cument k then the contribution of do cument k to the specification of the semantics of term t For instance the context vector for a term t which appears in the do cuments d1 1 0 1 0 and d2 1 0 0 1 would be 2 0 1 1 If the term t is encountered again in document d1 the existing index vector of d1 would be added one more time to the existing context vector to pro duce a new context vector for t of 
3 0 2 1 Context vectors generated through this pro cess are used to build do cument vectors as BoC Thus a do cument vector is the sum of the context vectors of its terms 5 HRR Document Representation In addition to BoC we explore the use of syntactic structures prepositional phrases such as in Asia to represent spatial relations and re rank the retrieved do cuments The traditional IR metho ds that include compound terms extract and include them as new VSM terms 4 5 We explore a different representation of such structures which uses a special kind of vector binding called holographic reduced representations HRRs 2 to reflect text structure and distribute syntactic information across the document representation Fishbein and Eliasmith have used the HRRs together with Random Indexing for text classification where they have shown improvement under certain circumstances having BoC as the baseline 1 It is important to mention that up to now we are not aware of other work that uses RI together with HRRs The 
Holographic Reduced Representation HRR was intro duced by Plate 2 as a metho d for representing compositional structure in distributed representations HRRs are vectors whose entries follow a normal distribution N 0 1 n Concept Based Representations for Ranking in GIR 91 They allow to express structure using a circular convolution operator to bind terms This circular convolution operator binds two vectors x x0 x1 xn 1 and y y0 y1 yn 1 to pro duce z z0 z1 zn 1 where z x y is defined as n 1 zi k 0 xk yi k i 0 to n 1 subscripts are mo dulo n 4 Circular convolution is an operator which do es not increase vector dimensionality making it excellent for representing hierarchical structures We adopt HRRs to build a text representation scheme in which spatial relations SR could be captured Therefore to define an HRR do cument representation the following steps are done a Determine the index vectors for the vocabulary by adopting the random indexing metho d as described earlier b Tag text of do cuments using a Name 
Entity Recognition System c Bind the tf idf weighted index vector of each location entity to its location role This location role is an HRR which represents a preposition i e in near around across etc extracted from the text considering the preposition preceding the lo cation entity d Add the resulting HRRs where the spatial relations are enco ded to obtain a single HRR vector e Multiply the resulting HRR by an attenuating factor f Normalize the HRR obtained so far to get the vector which represents the do cument For example when given a spatial relation R in Asia R will be represented using the index vectors r1 for Asia where r1 will be joined to its location role an HRR role1 which represents the relation in Then the in Asia vector will be R role1 r1 5 Thus given a do cument D with spatial relations in tx1 ty1 its normalized vector will be built as D role1 tx1 role1 ty1 6 where is a factor less than one intended to lower the impact of the co ded relations Queries are pro cessed and represented in a similar 
way 6 Experimental Setup We used in our experiments Lemur5 The results pro duced by the VSM configured in Lemur were taken as our baseline Our experiments were conducted using the English do cument collection for the GeoCLEF track This collection is composed of news articles taking 56 472 from the Glasgow Herald British 1995 and 113 005 from the LA Times American 1994 to total 169 477 news articles We worked with the queries of GeoCLEF 2007 and GeoCLEF 2008 a set of 50 queries from number 51 to 100 These queries are described in three parts 5 http www lemurproject org 92 M Carrillo et al a the main query or title b a brief description and c a narrative We took the title and description for all our experiments except for the query representations with HRR where we also considered the narrative statement in order to have improved relations for representation It is worth mentioning that Lemur results worsen when the narrative is included To investigate whether combining word based and concept based 
representations can be used to improve the GIR we considered two phases The aim of the first was to retrieve as many relevant do cuments as possible for a given query whereas the purpose of the second was to improve the final ranking of the retrieved do cuments by applying BoC and HRR representations Lemur was used to pro cess the 169 477 do cuments first with the queries for 2007 and then with the queries for 2008 Thereafter only the top 1000 documents ranked by the VSM were selected for each query These sub collections were pro cessed to generate the BoC representations of its do cuments and queries BoC representations were generated by first stemming all words in the sub collections using the Porter stemmer We then used Random Indexing to pro duce context vectors for the given sub collection The dimensionality of the context vectors was fixed at 4096 The index vectors were generated with 10 1s and 10 1s distributed over the 4096 dimensions This vector dimension and density were empirically determined 
These context vectors were then tf idf weighted and added up for each document and query as described earlier to pro duce BoC representations On the other hand HRRs were generated by firstly tagging all sub collections with the Named Entity Recognition System of Stanford University6 Afterwards the single word locations preceded by the preposition in were extracted This restriction was taken after analyzing the queries for each year and realizing that only about 12 of them had a different spatial relation HRRs for documents and queries were then pro duced by generating a 4096 HRR to represent the in relation The in HRR vector was then bound to the index vector of the identified locations by a Fast Fourier Transform implementation of circular convolution tf idf weighted added and multiplied by 1 6 to represent each do cument as described earlier to generate spatial relations representations Finally the evaluation of the results after re ranking the do cuments was carried out with the Mean Average Precision MAP 
7 Results We consider two experiments a The aim of the first was to prove that incorporating context information and syntactic structure for re ranking do cuments in GIR could improve precision i e to explore the use of BoC and HRR representations b The ob jective of the second was to compare our strategies against a traditional re ranking mechanism known as Pseudo Relevance Feedback PRF First Experiment Table 1 compares Lemur results with the results pro duced by adding the Lemur similarity values with its corresponding values from BoC to 6 http nlp stanford edu software CRF NER shtml Concept Based Representations for Ranking in GIR Table 1 MAP results for Geo CLEF collection 2007 2008 Lemur Lemur BoC Diff Lemur BoC HRR Diff 2007 0 1832 0 2079 13 48 0 2085 13 81 2008 0 2445 0 2619 7 12 0 2628 7 48 93 pro duce Lemur BoC which is a new list re ranked according to the new values Then the same pro cess as described above was followed but now adding LemurBoC values to HRR values to pro duce Lemur BoC HRR We only 
considered the set of supported queries that is the queries that have at least one relevant do cument 22 queries in 2007 and 24 in 2008 Notice how MAP is incremented in a constant way always at above 7 From the queries considered in 2007 1 query kept the same MAP pro duced by Lemur after adding BoC The MAP of 5 queries decreased Positively there are 16 queries improved by BoC The favorable percentages of improvement for 10 queries are observed in Table 2 above the 14 When HRRs were added to Lemur BoC only the query 64 that was not improved by BoC and in consequence not in Table 2 was affected This query had a percentage of change equal to 4 35 which was raised to 30 43 by the representation of its 5 spatial relations From the queries shown in Table 2 the unaffected queries have none or one spatial relation while the queries enhanced by adding the HRRs have on average 4 We found that HRRs improve precision when there are distinctive and specific spatial relations for example in Finland instead of in northern 
Europe Therefore when geographical information given is more precise HRRs help to achieve improved effectiveness However when the number of retrieved relevant documents Table 2 MAP for query improvement by BoC in 2007 and 2008 and their spatial relations Qry ID 52 57 58 60 61 67 69 70 72 75 Results 2008 76 80 82 84 85 86 91 93 95 96 Results 2007 Lemur Lemur BoC 0 0022 0 0038 0 204 0 2473 0 0197 0 0268 0 0022 0 0397 0 0959 0 1321 0 2569 0 2950 0 0701 0 0964 0 043 0 0509 0 4859 0 6179 0 3522 0 4580 0 44 0 4857 0 2518 0 2555 0 0005 0 0015 0 1385 0 2183 0 4554 0 4767 0 0592 0 1101 0 0625 0 1667 0 7375 0 8340 0 491 0 5320 0 2232 0 2418 Diff 72 73 21 23 36 04 1704 55 37 75 14 83 37 52 18 37 27 17 30 04 10 39 1 47 200 00 57 62 4 68 85 98 166 72 13 08 8 41 8 33 SR Lemur BoC HRR Diff additional 0 0 0038 0 00 6 0 2577 4 21 0 0 0268 0 00 1 0 0397 0 00 1 0 1318 0 23 0 0 2950 0 00 1 0 0963 0 14 0 0 0509 0 00 1 0 6179 0 00 2 0 4612 0 70 12 0 5000 2 94 1 0 2555 0 00 3 0 0018 20 00 0 0 2183 0 00 0 0 4767 0 00 2 0 1130 2 63 
1 0 1667 0 00 1 0 8340 0 00 6 0 5337 0 26 11 0 2454 1 49 94 M Carrillo et al is low with few relations to compare it is difficult to affect the ranking with the HRRs In 2008 3 queries kept the same MAP pro duced by Lemur after adding BoC The MAP of 9 queries decreased and 12 queries improved Table 2 shows 10 queries improved by BoC where favorable percentages of improvement are depicted From these 10 queries those that were improved after adding the HRRs have at least 2 spatial relations Our conclusion is that the relative small contribution to improve precision demonstrated by HRR is due to the limited amount of spatial relations appearing in the set of queries used We believe that the higher the number of spatial relations to be represented the greater the contribution of this representation We perform a paired t student test to measure the statistical significance of our MAP results The MAP differences for GeoCLEF 2007 resulted significant in a confidence interval of 95 for both Lemur BoC and Lemur BoC 
HRR however the results are below the median of the year 0 2097 by 0 57 In this year the top system at CLEF reached a MAP of 0 2859 9 However it used a very complex configuration and several external resources four Geographical Gazetteers a Feature Type Thesaurus to categorize geo terms and a Shape Toolbox a database which contains a shape file available for each country The MAP improvement for 2008 is not statistically significant Even so the MAP median of the participants in Geo CLEF 2008 was of 0 2370 15 which is 6 45 lower than that generated by our proposal This year the team at the top obtained a MAP of 0 3040 6 They used two ontologies constructed manually employing information from narratives In addition they used Wikipedia in the retrieval pro cess In contrast we do not use any complex external resource Second Experiment Finally we compare the Lemur BoC HRR results with a traditional re ranking metho d known as Pseudo Relevance Feedback PRF In order to apply this approach we used the VSM 
representing queries and documents as tf idf vectors and computing similarity with the cosine function PRF treats the n top ranked do cuments as true relevant do cuments for a given query then queries are expanded by adding the k words selected from the n top do cuments and then a second IR pro cess is done with the expanded query Table 3 presents results also for queries with relevant do cuments when the top 2 and 5 do cuments are taken to extract 5 10 and 15 words Query texts are built from title and description fields The values that improve Lemur MAP are Table 3 Difference between PRF MAP and Lemur BoC HRR MAP Lemur BoC PRF with 2 documents HRR 5 terms 10 terms 15 terms GeoCLEF 2007 0 2085 0 1925 0 1617 0 1533 Difference 8 31 28 94 36 01 GeoCLEF 2008 0 2628 0 2539 0 2596 0 2505 Difference 3 51 1 23 4 91 PRF with 5 documents 5 terms 10 terms 15 terms 0 1963 0 1703 0 1593 6 21 22 43 30 89 0 2306 0 2242 0 2101 13 96 17 22 25 08 Concept Based Representations for Ranking in GIR 95 depicted in bold and those 
obtained with our proposal in italics The difference in MAP between PRF technique and our Lemur BoC HRR proposal is about 6 21 or higher in favor of our metho d in 2007 and 1 23 or higher in 2008 8 Conclusion and Future Work In this paper we have presented two do cument representations for re ranking do cuments and improving precision for GIR RI was used to build context vectors to create BoC representations which capture context information It also defines index vectors used in the HRR representations When working with RI the appropriate selection of the values for vector length and vector density is an open research topic Our results have been compared with the VSM in its Lemur implementation They have showed that i BoC can improve the initial ranker ii HRR representation improved the ranking of queries However its utility could not be totally verified because of the lack of spatial relations to be represented ii we foresee that when more relations are added to the HRRs a better ranking is achieved It 
should be noted that in the experiments conducted only one type of spatial relation in was considered we think if more types of relations near around across far etc are added as long as they are present in the queries it could lead to improved results iii comparing our metho d against PRF pro duces higher scores for this new method Therefore the overall results demonstrate that our approach is appropriate for re ranking do cuments in GIR We will continue working with other collections where queries have not only spatial relations but other syntactic relations i e compound nouns verb sub ject which could be represented and together with the context information allow us to explore in depth the usefulness of the proposed representations as a mechanism for re ranking do cuments to improve precision References 1 Fishbein J M Eliasmith C Integrating structure and meaning A new method for encoding structure for text classification In Macdonald C Ounis I Plachouras V Ruthven I White R W eds ECIR 2008 LNCS vol 4956 
pp 96 M Carrillo et al 7 Martinis B Cardoso N Chavez M S Andrade L Silva M J The University of Lisbon at Geoclef 2006 In Working notes for the CLEF Workshop Spain 2006 8 Larson R R Cheshire at Geoclef 2008 Text and fusion approaches for GIR In Working notes for the CLEF 2008 Workshop Aarhus Denmark 2008 9 Using Machine Translation Systems to Expand a Corpus in Textual Entailment Julio J Castillo National University of Cordoba FaMAF Cordoba Argentina National Technological University FRC Cordoba Argentina Abstract This paper explores how to increase the size of Textual Entailment Corpus by using Machine Translation systems to generate additional t h pairs We also analyze the theoretical upper bound of a Corpus expanded by machine translation systems and propose how it computes the confidence of a classification translator based RTE system At the end we show an algorithm to expand the corpus size using Translator engines and we provide some results over a real RTE system Keywords textual entailment machine 
translation system double translation process 1 Introduction The Recognizing Textual Entailment RTE task is defined as a directional relationship between a pair of text fragments or sentences called the text T and the hypothesis H Thus we say that T entails H if a human reads T would infer that H is most likely true Machine learning algorithms were widely used for the task of recognizing textual entailment 1 2 3 in the past RTE Challenges Some authors 4 showed how the accuracy increases when we add more training examples and other authors holds the necessity of larger corpus 5 In any case a larger corpus enables a more detailed analysis of the problem domain and will let us build more accurate classifiers In this paper we show how a machine translation system could increase the size of a RTE Corpus and we also suggest how a translator can be used as a tool to help us with classification of new unknowns t h pairs The remainder of the paper is organized as follows Section 2 describes one approach driven by 
Machine Translation Systems and provides an analysis about the possible increasing size of the Corpus with us proposing a confidence measure for such approach whereas Section 3 shows experimental evaluation and discussion of the results Finally Section 4 summarizes the conclusions and lines for future work 2 Machine Translation Approach In this section we propose to use Machine Translation System to expand the current RTE Corpus sizes H Loftsson E 98 J J Castillo First we define double translation process as the process of starting with a String in English translating it to another language for example Spanish and backing it forward to the English language source Thus our motivation is based on the fact that we could use a double translation process to produce equivalents Texts and Hypothesis and so these new pairs can be taken as training set Also we suggest how a translator can be used as a tool to help us with classification of new t h pairs 2 1 Double Translation Process Double translation process can be 
defined as the process of starting with an S String in English translating it to a foreign language F S for example Spanish and backing it forward to the English source language F 1 S Thus the observation of that double translation process can increase the Corpus size and also can be generalized using N Translators engine It is important to note that the quality of the translation is given by the Machine Translation System and we will suppose that the sense of the sentence should not be modified by the Translator This indeed is the situation almost for the majority of the cases in our first experiments see Section 2 2 Bellow we provide a theoretical justification of the increment of the corpus size with n pairs using k translators which is O n k 2 t h pairs C q is the increased Notation C is a RTE Corpus which consists of size of the Corpus C using t h t h Tr String String t Tr t Tr t DoubleTranslationOfTheTrTranslator where t and Tr t are in English Lemma Given Tr1 Tr2 Trk translators and C a RTE Corpus 
with nIf t h pairs Tri t p Tr j h p i j i j i 1 k p 1 n then C k k 1 2 n Proof By structural induction on K As a practical result by using one translator over only one RTE dataset of 800 pairs we could obtain up to 3200 pairs in total and using two translators we could obtain a new dataset with 7200 pairs upper bound Using Machine Translation Systems to Expand a Corpus in Textual Entailment 99 2 2 Other Uses of Machine Translation in RTE Systems Machine Translation can be used as a feature in a machine learning algorithm 6 Indeed by using a MT system it is possible to reduce the complexity of some of the sentences and by this way RTE task will be easier We addressed several simple experiments using Machine Translation engines and we provide some examples below One original t h pair of RTE3 development set is T A leading human rights group on Wednesday identified Poland and Romania as the likely locations in eastern Europe of secret prisons where al Qaeda suspects are interrogated by the Central Intelligence 
Agency H CIA secret prisons were located in Eastern Europe Translated pair using Microsoft Bing Translator T A group of human rights on Wednesday had identified Poland and Romania as likely locations in Eastern Europe of secret prisons where suspects of Al Qaida are interrogated by the Central Intelligence Agency H Secret CIA prisons were in Eastern Europe Translated pair using Google Translator T A prominent human rights group on Wednesday identified Poland and Romania as the likely locations in eastern Europe of secret prisons where al Qaeda suspects are interrogated by the Central Intelligence Agency H the secret CIA prisons located in Eastern Europe Translated pair using Yahoo Babel Fish Translator T A main group of human rights Wednesday identified Poland and Rumania like the probable locations in Eastern Europe of secret prisons where the company the suspects of al Qaeda interrogate H The secret prisons of the company were located in Eastern Europe In order to move the RTE task towards more realistic 
application scenarios this year in the TAC RTE5 Challenge the texts come from a variety of sources and include typographical errors and ungrammatical sentences In this context the translation can help with this objective In the previous example eastern is a grammatical error on the original corpus but using Bing Translator this error was fixed Also we can produce some interesting variation such as al Qaeda and Al Qaida In the last pair by using BabelFish the CIA was changed for company which is an error of the translator in the context of this pair It seems an error of Babel Fish resolving acronyms Another use of Translator is to provide synonyms and expression with the same meaning An additional example is given bellow Again the following pair belongs to the RTE3 development set 100 J J Castillo The source pair 170 is entailment Yes T The man known as just the Piano Man has left the hospital and has returned home to his native Germany According to British tabloids the man after losing his job in Paris 
travelled to the UK through the Channel Tunnel H The Piano Man came from Germany Translated pair using Microsoft Bing Translator T The man known an as the Piano Man has left the hospital and has returned to his native Germany According to British tabloids man after losing her job in Paris traveled to the United Kingdom on the channel tunnel H Piano man came from Germany In this example we see how UK was translated to United Kingdom acronyms resolution and also we see interesting variations as traveled and travelled Thus it seems that by using a MT engine it is possible to improve the semantic resources of RTE Systems 2 3 Confidence of Double Translation Process The double translation process can be used in the production step when addressing machine learning algorithms or otherwise in testing stage in other systems By this way we can define for a test set ti hi pair 1 si T H RTE T H 0 otherwise In this case RTE is a result classification of a system for Recognizing Textual Entailment Then we can define 
Confidence as Confidence T H Additionally it is possible to prove that n i 1 RTE Tri T Tri H n Thus we can choose a threshold and only accept as a valid entailment a pair that outperforms this threshold Ck k 1 2 n is the upper bound for a double translation process and this bound only could be reached when the predicate Tri t p Trj hp i j i j i 1 k p 1 n holds Generally H is very simple For this reason a double translation process could not affect the Hypothesis On the other hand since T Text is complex we expect that every translator engine will return a different result 3 Experimental Evaluation and Discussion of the Results We show the following algorithm in order to obtain additional given Corpus t h pairs from a Using Machine Translation Systems to Expand a Corpus in Textual Entailment 101 1 Start with a RTE x Corpus with C n 2 For each t i hi i 1 n 3 For each Translator Tr1 Tr2 Trk If Tr j ti ti Tr j hi hi j 1 k Add Tr j ti Tr j hi to Cnew Where C new is the new Corpus obtained as the union between C 
and the new outputs pairs of the algorithm In order to test our claim over a real RTE system we performed some experiments using our RTE system 3 but without using the NER filter This RTE system is based on a machine learning approach that produces feature vectors for RTE3 RTE4 and RTE5 The chosen features quantify lexical syntactic and semantic level by matching between texts and hypothesis sentences Thus we generated a feature vector with the following components for both Text and Hypothesis Levenshtein distance a lexical distance based on Levenshtein a semantic similarity measure based on Wordnet and LCS longest common substring metric We use the following two classifiers to learn every development set Support Vector Machine and Multilayer Perceptron MLP and we choose Spanish as intermediate language First we started with RTE3 datasets applying the algorithm proposed using only one Machine Translation System Microsoft Bing Translator in order to generate additional pairs which was named as RTE3 Bing 
Secondly we split this new dataset in 200 400 600 and 800 pairs respectively Finally we tested these training sets over RTE5 development set in two way task We summarized the results in the following table Table 1 Results of two way classification task SVM Classifier Training set RTE3 55 5 RTE3 Bing 56 5 RTE3 200pairs RTE3 Bing 55 67 RTE3 400pairs RTE3 Bing 55 83 RTE3 600pairs RTE3 Bing 56 5 RTE3 RTE3 Bing 56 MLP Classifier 56 58 56 5 58 83 57 67 59 33 Interestingly a not statistical significant different was obtained between RTE3 and RTE3 Bing using SVM or MLP as learning algorithm One important point in these experiments is that adding more training set obtained with our algorithm does not decrease the performance with neither combination of learning algorithm and training sets Finally the best performance of our system was achieved with Machine Learning Algorithm with RTE3 RTE3 Bing dataset and it was obtained an interesting increase but not statistically significant of 3 33 accuracy However additional 
evidence is needed in order to support this claims but it seems promising Also it is important to note that translation web services as Google 102 J J Castillo Translator or Microsoft Bing is frequently updated Therefore the result of the translation could not be the same at different times and so we would expect better results 4 Conclusions and Future Work In this work we propose the use of Machine Translation systems as a way to increase the corpus sizes We also show the maximum size which can be yield and we present an algorithm to increase training sets We concluded that for our algorithm Microsoft and Google MT are more useful that Yahoo Babelfish MT and also we note that these results are strong dependent of the RTE system architecture However further analysis is required to determine the impact of Machine Translation in Textual Entailment Systems by using others RTE systems Future work will be oriented to explore more deeply how Machine Translation could improve the accuracy of the RTE Systems and to 
test over different datasets and RTE systems available Finally we will test the double translation process but passing through Spanish Portuguese Dutch and Russian as intermediate language and assessing the improvement that they can yield References 1 Marneffe M MacCartney B Grenager T Cer D Rafferty A Manning C Learning to distinguish valid textual entailments In RTE2 Challenge Italy 2006 2 Zanzotto F Pennacchiotti M Moschitti A Shallow Semantics in Fast Textual Entailment Rule Learners In RTE3 Prague 2007 3 Castillo J Recognizing Textual Entailment Experiments with Machine Learning Algorithms and RTE Corpora In Cicling 2010 Iai Romania 2009 4 Inkpen D Kipp D Nastase V Machine Learning Experiments for Textual Entailment In RTE2 Challenge Venice Italy 2006 5 Newman E Stokes N Dunnion J Carthy J UCD IIRG Approach to the Textual Entailment Challenge In PASCAL Proc of the First Challenge Workshop Recognizing Textual Entailment 2005 6 Agichtein E Askew W Liu Y Combining Lexical Syntactic and Semantic Evidence 
for Textual Entailment Classification In TAC 2008 Gaithersburg Maryland USA 2008 7 Bentivogli L Dagan I Dang H Giampiccolo D Magnini B The Fifth PASCAL Recognizing Textual Entailment Challenge In Proceedings of Textual Analysis Conference NIST Maryland USA 2009 8 Dolan B Quirk C Brockett C Unsupervised construction of large paraphrase corpora exploiting massively parallel news sources In COLING 2004 Proceedings of the 20th International Conference on Computational Linguistics Association for Computational Linguistics Morristown NJ USA p 350 2004 9 Castillo J A Machine Learning Approach for Recognizing Textual Entailment of the Spanish In North American Chapter of ACL 2010 10 Vanderwende L Dolan W B What syntax can contribute in entailment task Springer Heidelberg 2006 11 Dagan I Dolan B Magnini B Roth D Recognizing textual entailment Rational evaluation and approaches Natural Language Engineering 15 4 Frames in Formal Semantics Robin Cooper Department of Philosophy Linguistics and Theory of Science 
University of Gothenburg Box 200 S 405 30 1 Introduction In his classic paper on frame semantics Fillmore 12 says Frame semantics comes out of traditions of empirical semantics rather than formal semantics It is most akin to ethnographic semantics the work of the anthropologist who moves into an alien culture and asks such questions as What categories of experience are enco ded by the members of this speech community through the linguistic choices that they make when they talk A frame semantics outlook is not or is not necessarily incompatible with work and results in formal semantics but it differs importantly from formal semantics in emphasizing the continuities rather than the discontinuities between language and experience The ideas I will be presenting in this paper represent not so much a genuine theory of empirical semantics as a set of warnings about the kinds of problems such a theory will have to deal with If we wish we can think of the remarks I make as pre formal rather than non formalist I claim 
to be listing and as well as I can to be describing phenomena H Loftsson E 104 R Cooper which must be well understoo d and carefully described before serious formal theorizing about them can become possible In this paper we will make a connection between formal semantics and frame semantics by importing into our semantic analysis ob jects which are related to the frames of FrameNet 1 Our way of doing this will be different from for example 1 An important part of our proposal will be that we intro duce semantic objects corresponding to frames and that these ob jects can serve as the arguments to predicates We will use record types as defined in TTR type theory with records 2 3 5 13 to characterize our frames The advantage of records is that they are ob jects with a structure like attribute value matrices as used in linguistics Labels corresponding to attributes in records allow us to access and keep track of parameters defined within semantic ob jects This is in marked contrast to classical model theoretic 
semantics where semantic ob jects are either atoms or unstructured sets and functions We will first give a brief intuitive introduction to TTR and show how it can be used to represent frames Sect 2 We will then show how we propose to represent the contents of verbs in a compositional semantics Sect 3 The use of frames here leads us naturally from the Priorean tense operators used by Montague to the Reichenbachian account of tense 22 preferred by most linguists working on tense and aspect which involves what we will think of as parameters for speech time event time and reference time The use of frames also leads us to a particular view of Partee s puzzle about temperature and price first discussed in 16 PTQ reprinted as Chap 8 of 17 We will discuss this in Sect 4 Our solution to this puzzle relates to Fernando s 9 11 theory of events as strings of frames which we discuss in Sect 5 Finally Sect 6 we will consider how our proposal can be used to talk about how agents can mo dify word meaning by adjusting the 
parameters of word contents This relates to a view of word meaning as being in a constant state of flux as we adapt words to describe new situations and concepts In Sect 7 we draw some conclusions 2 Using TTR to Represent Frames Consider the frame Ambient temperature defined in the Berkeley FrameNet2 by The Temperature in a certain environment determined by Time and Place is specified Its core frame elements are given in 1 1 Attribute The temperature feature of the weather Degree A mo difier expressing the deviation of the Temperature from the norm Place The Place where it is a certain Temperature Temperature A quantity or other characterization of the Temperature of the environment Time The Time during which an ambient environment has a particular Temperature 1 2 http framenet icsi berkeley edu Accessed 25th Oct 2009 Frames in Formal Semantics 105 To make things of a manageable size we will not include all the frame elements in our representation of this frame We have also changed the names of the frame 
elements to suit our own purposes We will say that an ambient temperature frame is a record of type 2 x e time 2 e lo cation ctemp at in Ind Time Loc temp at in e time e lo cation x We will call this type AmbTemp It is a set of four fields each consisting of a label to the left of the colon and a type to the right of the colon A record of type AmbTemp will meet the following two conditions 3 A TTR Approach to Verbs in Compositional Semantics Consider an intransitive verb such as run The simplest way to think of this is as corresponding to a predicate of individuals Thus 3 would represent the type of events or situations where the individual a runs 3 run a However as anybody who has thought about tense and aspect knows we need to get time into the picture somewhere If you look up run on FrameNet3 you will find that on one of its readings it is asso ciated with the frame Self motion Like many other frames in FrameNet this has a frame element Time which in 3 Accessed 1st April 2010 106 R Cooper this frame is 
explained as The time when the motion occurs This is what Reichenbach 22 called more generally event time and we will use the label etime We will add an additional argument for a time to the predicate and create a frame type 4 4 4 e time TimeInt crun run a e time For the type 4 to be non empty it is required that there be some time interval at which a runs We use TimeInt as an abbreviation for the type of time intervals 5 start Time 5 end Time c start end No constraints are placed on when that time interval in 4 should be Thus this frame type corresponds to a tenseless proposition something that is not available in the Priorean setup 18 19 that Montague employs where logical formulae without a tense operator correspond to a present tense interpretation In order to be able to add tense to this we need to relate the event time to another time interval normally the time which Reichenbach calls the speech time 5 A past tense type anchored to a time interval is represented in 6 6 e time TimeInt ctns e time end 
start This requires that the end of the event time interval has to precede that start of the speech time interval In order for a past tense sentence a ran to be true we would need to find an ob ject of both types 4 and 6 This is equivalent to requiring that there is an ob ject in the result of merging the two types given in 7 e time TimeInt 7 ctns e time end start crun run a e time Suppose that we have an utterance u that is a speech event of type 8 phon a ran 8 s time TimeInt cutt uttered phon s time 4 5 Of course we are ignoring many other frame elements which occur in FrameNet s Self motion which could be added to obtain a more detailed semantic analysis Uses of historic present tense provide examples where the tense is anchored to a time other than the speech time Frames in Formal Semantics 107 where a ran is the type of strings of an utterance of a concatenated with an utterance of ran Then we can say that the speech time interval in 7 is u s time That is the past tense constraint requires that the 
event happened before the start of the speech event In a complete treatment both the type of the speech event 8 and the content 7 would be packeted together in a single sign type together with more information about syntax HPSG style see 4 for a preliminary indication of how this would look 7 is a type which is the content of an utterance of the sentence a ran In order to obtain the content of the verb ran we need to create a function which abstracts over the individual a Because frames will play an important role as arguments to predicates below we will not abstract over individuals but rather over frames containing individuals The content of the verb ran will be 9 e time TimeInt 9 r x Ind ctns e time end start crun run r x e time 4 The Puzzle about Temperature and Prices Montague 16 intro duces a puzzle presented to him by Barbara Partee From the premises the temperature is ninety and the temperature rises the conclusion ninety rises would appear to follow by normal principles of logic yet there are o 
ccasions on which both premises are true but none on which the conclusion is Exactly similar remarks can be made substituting price for temperature Montague s solution to this puzzle in 16 was to analyze temperature price and rise not as predicates of individuals as one might expect but as predicates of individual concepts For Montague individual concepts were mo delled as functions from possible worlds and times to individuals To say that rise holds of an individual concept do es not entail that rise holds of the individual that the concepts finds at a given world and time Our strategy is closely related to Montague s However instead of using individual concepts we will use frames By interpreting rises as a predicate of frames for example of type AmbTemp as given in 2 we obtain a solution to this puzzle e time TimeInt 10 r x Ind ctns e time crun rise r e time Note that a crucial difference between 9 and 10 is that the first argument to the predicate rise is the complete frame r rather than the value of the 
x field which is used for run Thus it will not follow that the value of the x field i e 90 in Montague s example is rising While there is a difference in the type of 108 R Cooper the argument to the predicates a record as opposed to an individual the type of the complete verb content is the same x Ind RecType that is a function from records of type x Ind to record types This ability to use different types internally but still have the same overall type for the content of the word is convenient for compositional semantics But now the question arises what can it mean for a frame to rise 5 Fernando s String Theory of Events In an important series of papers including 8 9 10 11 Fernando intro duces a finite state approach to event analysis where events can be seen as strings of punctual observations corresponding to the kind of sampling we are familiar with from audio technology and digitization pro cessing in speech recognition When talking about the intuition behind this analysis Fernando sometimes refers to 
strings of frames in a movie e g in 10 But in many cases what he is calling a movie frame can also be seen as a frame in the sense of this paper as well Thus an event of a rise in temperature could be seen as a concatenation of two temperature frames that is an ob ject of type AmbTemp AmbTemp We have seen a concatenation type previously in our characterization of a phonology type in 8 That is because phonological events are also to be seen as event strings in Fernando s sense 11 shows a type of event for a rise in temperature using the temperature frame AmbTemp in 2 e time TimeInt x Ind start e time e time start Time e location Loc c temp at in start e time start e location start x temp at in x Ind end e time e time end Time e location start e location Loc c temp at in end e time end e location end x temp at in event start end AmbTemp AmbTemp cincr start x end x 11 Here we make use of manifest fields 7 such as 12 e time e time start Time which restrict the type in the field to be a singleton type of the 
unique ob ject represented after the equality sign Thus 12 is syntactic sugar for 13 e time Time e time start This uses a singleton type represented by Time e time start If some ob ject a is of type T a T then Ta is a type such that b Ta iff b a That is we Frames in Formal Semantics 109 restrict the type to be the type of a unique particular ob ject It should also be noted that path names such as start e time always begin at the root of the record type rather than the most local record type in which they occur 11 is then the type of events where there is a rise in ambient temperature An event e of this type will be of type rise e start e e time In fact we will make the stronger requirement that if r AmbTemp and i TimeInt then e rise r i iff e 11 e start r and e e time i 6 Word Meaning in Flux For all 11 is based on a very much simplified version of FrameNet s Ambient temperature it represents a quite detailed account of the lexical meaning of rise in respect of ambient temperature detailed enough in fact to 
make it inappropriate for rise with other kinds of sub ject arguments Consider price The type of a price rising event could be represented by 14 e time TimeInt x Ind e time e time start Time e location Loc start commodity Ind cprice of at in price of at in start commodity start e time start e location start x 14 x Ind e time e time end Time e location start e location Loc end commodity start commodity Ind cprice of at in price of at in end commodity end e time end e location end x event start end Price Price cincr start x end x 14 is similar to 11 but crucially different A price rising event is not surprisingly a string of price frames rather than ambient temperature frames The type of price frames Price is given in 15 x e time 15 e lo cation commo dity cprice of at in Ind Time Loc Ind price of at in commo dity e time e lo cation x If you look up the noun price in FrameNet6 you find that it belongs to the frame Commerce scenario which includes frame elements for goods corresponding to our commo dity and 
money corresponding to our x field If you compare the 6 Accessed 8th April 2010 110 R Cooper FrameNet frames Ambient temperature and Commerce scenario they may not initially appear to have very much in common However extracting out just those frame elements or roles that are relevant for the analysis of the lexical meaning of rise shows a degree of correspondence They are nevertheless not the same Apart from the obvious difference that the predicate in the constraint field that relates the various roles involves temperature in the one and price in the other price crucially involves the role for commo dity since this has to be held constant across the start and end frames We cannot claim that a price is rising if we check the price of tomato es in the start frame and the price of oranges in the end frame This corresponds to a situation which is familiar to us from work on the Generative Lexicon 20 21 where the arguments to words representing functions influence the precise meaning of those words For example 
fast means something different in fast car and fast road although of course the two meanings are related There are two important questions that arise when we study this kind of data 7 http en wikipedia org wiki Risen _ video _game accessed 4th February 2010 Frames in Formal Semantics 111 The type of the rising event described here could be something like 17 e time TimeInt x Ind e time e time start Time start e lo cation Loc ime cat at start x start e lo cation start e t x start x Ind 17 e time e time end Time end e lo cation Loc cat at end x end e lo cation end e time event start end Position Position cincr height start e lo cation height end e lo cation This relies on a frame type Position given in 18 x Ind e time Time 18 e lo cation Loc cat at x e lo cation e time 18 is perhaps most closely related to FrameNet s Locative relation 17 is structurally different from the examples we have seen previously Here the content of the x field the fo cus of the frame which in the case of the verb rise will correspond 
to the sub ject of the sentence is held constant in the string of frames in the event whereas in the case of rising temperatures and prices it was the fo cus that changed value Here it is the height of the location which increases whereas in the previous examples it was important to hold the location constant 8 This makes it difficult to see how we could give a single type which is general enough to include both varieties and still be specific enough to characterize the meaning of rise It appears more intuitive and informative to show how the variants relate to each other in the way that we have done The second question we had concerned whether there is a fixed set of possible meanings available to speakers of a language or whether speakers create appropriate meanings on the fly based on their previous experience Consider the examples in 19 19 a Mastercard rises b China rises 8 We have used height start end e location in 17 to represent the height of the location since we have chosen to treat Loc the type of 
spatial location as a basic type However in a more detailed treatment Loc should itself be treated as a frame type with fields for three coordinates one of them being height so we would be able to refer to the height of a location l as l height 112 R Cooper While speakers of English can get an idea of the content of the examples in 19 when stripped from their context they can only guess at what the exact content might be It feels like a pretty creative pro cess Seeing the examples in context as in 20 reveals a lot 9 20 a Visa Up on Q1 Beat Forecast Mastercard Rises in Sympathy By Tiernan Ray Shares of Visa V and Mastercard MA are both climbing in the aftermarket reversing declines during the regular session after Visa this afternoon reported fiscal Q1 sales and profit ahead of estimates and forecast 2010 sales growth ahead of estimates raising enthusiasm for its cousin Mastercard b The rise of China will undoubtedly be one of the great dramas of the twenty first century China s extraordinary economic growth 
and active diplomacy are already transforming East Asia and future decades will see even greater increases in Chinese power and influence But exactly how this drama will play out is an open question Will China overthrow the existing order or become a part of it And what if anything can the United States do to maintain its position as China rises It seems like the precise nature of the frames relevant for the interpretation of rises in these examples is being extracted from the surrounding text by a technique related to automated techniques of relation extraction in natural language pro cessing 7 Conclusion We have suggested that a notion of frame can be of use in an approach to formal semantics dealing with hard empirical questions of lexical semantics and linguistic processing The important aspect of our analysis is that we have semantic ob jects corresponding to frames and allow these to be arguments to predicates We have illustrated this with an old puzzle from formal semantics the Partee puzzle 
concerning the rising of temperature Our solution is very similar in strategy to that originally proposed by Montague It differs in that we use frames where Montague used individual concepts The additional detail of the lexical semantic analysis obtained by using frames comes at a cost however It has as a consequence that there is not obviously a single meaning or even a small set of meanings asso ciated with rise Rather rise means something slightly different for temperatures and prices ob jects rising in 9 http blogs barrons com stockstowatchtoday 2010 02 03 visa up on q1beat forecast mastercard moves in sympathy mod rss_BOLBlog accessed 4th February 2010 http www foreignaffairs com articles 63042 g john ikenberry the rise of china and the future of the west accessed 4th February 2010 Frames in Formal Semantics 113 location not to mention countries as in China rises This spread of meanings seems to be important if we are to draw the kinds of detailed inferences that speakers of a language are able to draw 
from these examples We have argued that there is no fixed set of meanings but rather that speakers of a language create meanings on the fly for the purposes of interpretation in connection with a given speech or reading event This idea is related to the notion of meaning potential discussed for example in 15 and a great deal of other literature While we have made no precise proposal for how speakers go about creating new situation specific meanings in this paper we believe that the kinds of structured semantic ob jects such as frames that we are proposing in this paper will facilitate an account of this Our record types comprise a collection of fields which can be used to correspond to frame elements New meanings can be constructed from old ones by adding subtracting or modifying such fields thus providing possibilities for change that are not so obviously available in traditional possible world semantics based on functions from possible worlds and times to denotations Acknowledgements This research was 
supported in part by VR project 20091569 Semantic analysis of interaction and coordination in dialogue SAICD and Swedish Tercentenary Foundation Project P2007 0717 Semantic Coordination in Dialogue SemCoord I am grateful to Jonathan Ginzburg and Staffan Larsson for discussion Previous versions of this material have been presented at the seminar of the Centre for Language Technology in Gothenburg the linguistics seminar of the Department of Philosophy Linguistics and Theory of Science at the University of Gothenburg and the Grammar Festival organized by the Department of Swedish at the University of Gothenburg I am grateful to the audiences on all these o ccasions for useful discussions and improvements References 1 Bos J Nissim M Combining Discourse Representation Theory with FrameNet In Favretti R R ed Frames Corpora and Knowledge Representation pp 114 R Cooper 7 Coquand T Pollack R Takeyama M A logical framework with dependently typed records Fundamenta Informaticae XX Clustering E Mails for the Swedish 
Social Insurance Hercules Dalianis1 Magnus Rosell1 2 and Eriks Sneiders1 Department of Computer and Systems Science DSV Stockholm University Forum 100 164 40 Kista Sweden 2 KTH CSC 100 44 Stockholm Sweden hercules dsv su se rosell csc kth se eriks dsv su se 1 Abstract We need to analyse a large number of e mails sent by the citizens to the customer services department of a governmental organisation based in Sweden To carry out this analysis we clustered a large number of e mails with the aim of automatic e mail answering One issue that came up was whether we should use the whole e mail including the thread or just the original query for the clustering In this paper we describe this investigation Our results show that only the query and the answering part should be used but not necessarily the whole e mail thread The results clearly show that the original question contains more useful information than only the answer although a combination is even better Using the full e mail thread does not downgrade the 
result Keywords E government query answering e mail threads Swedish clustering 1 Introduction In Sweden the public authorities have been in the lead to implement E government This includes communication with the citizens through various electronic channels One such channel is to put important information on their web sites Citizens often do not find the information they are seeking however and initiate communication in one of several ways such as telephone calls e mails chat lines etc The Swedish Social Insurance Agency1 SSIA receives more than 10 000 emails from citizens each week These are answered manually by handling officers Many of the e mails from the public are very similar Therefore a lot would be gained if these re occurring questions could be answered automatically or semiautomatically To accomplish this first the common questions must be identified The e mails are either sent directly to an available address or via a web form on the agency s web site When a citizen uses the web form he she also 
has to assign 1 www forsakringskassan se H Loftsson E 116 H Dalianis M Rosell and E Sneiders a category to it such as parental benefit 2 Previous Research An e mail consists of a header including sender and receiver addresses subject matter etc and body text The body text may also be divided into several zones of different kinds of content such as sender zones author greeting signoff quoted conversation zones reply forward and boilerplate zones signatures advertising disclaimer attachment 2 Previous work on clustering of e mails has discussed the inclusion of different parts of the e mails but has not tried different parts of the body In 3 using a combination of the header and body gives better results than using only the body In 4 the authors let the user weight the importance of the parts to cc from subject date body Whereas previous research was aimed at personal inboxes we study e mails sent to a whole organisation 3 Text Sets and Preprocessing We received about 9 000 e mails from the SSIA Around 4 000 
of these were either sent directly without the use of the web form or assigned a miscellaneous category other questions Clustering E Mails for the Swedish Social Insurance Agency 117 3 1 Extracting Parts of the E Mail Thread The e mails we obtained were actually complete e mail threads as they had developed up until the moment they were extracted at the SSIA The number of items in a thread varied from one to 40 although 96 2 percent of all threads where up to four components long The principle of separating thread components was empirically obtained by working on a large number of e mails The system iteratively cuts off the top message It first looks for several successive lines that start with If these are found then everything above these lines is the top message Otherwise the system looks for a typical message separator line such as abc doc com wrote Original message etc in several languages Swedish English Norwegian with a certain level of wording freedom If this does not help it looks for an array of 
lines that start with From To Date Subject in different languages This method is based on heuristics but works comparably well For our clustering experiments we created four sets of texts Using a few simple rules we removed the e mail headers and characters indicating quotation citation of previous messages in the thread We were not allowed to use the headers due to the sensitive nature of these e mails The results for each of the different sets were tokenised and lemmatised using the Swedish grammar checking program Granska 5 The resulting texts still contained a lot of non word character sequences coming from signatures advertisements disclaimers etc To try to remove them we have used several simple methods We removed words shorter than three characters and longer than 20 since this only removes a few interesting words and captures some of the nonwords Further we removed all words only appearing in only one e mail see appendix in 6 since they did not contribute to the similarity between e mails We also 
used a common stoplist of Swedish words 3 3 Statistics for the Preprocessed Text Sets Table 1 gives some statistics for the extracted and preprocessed text sets the number of texts and lemmas as well as the average number of different lemmas per text and the average number of texts in which each lemma occurs 4 Clustering For each text set we constructed an ordinary term document matrix with tf idfweights We defined similarity between texts as the cosine measure 118 H Dalianis M Rosell and E Sneiders We have used the K Means algorithm see for instance 7 as it is simple fast and therefore suitable for interactive exploration In the end we want the handling officers to use clustering as a tool to obtain an overview of the trends in the questions and to indentify common questions this in an interactive manner as described in 8 5 Evaluation Since internal clustering quality measures are based on the representation we can not use them to compare results based on different representations i e our text sets External 
quality measures compare the clustering with a categorisation We have the categorisation made by the citizens It may not be ideal but at least it groups questions with similar content We want clusterings to compare well with this categorisation although we do not expect them to be very similar We prefer a clustering to be more similar rather than less similar however There are many external quality measures We prefer information theory based measures as these take the whole distribution of texts over categories and clusters into account For this reason we use the Normalised Mutual Information NMI between the clustering and the categorisation see 9 Table 1 Clustering results for four different text sets based on the original question only the first answer only both first question and answer and the full e mail thread The first four measures describe the text sets after preprocessing The last measure is the average clustering result in NMI Normalised Mutual Information of 20 K Means clusterings to nine 
clusters compared with the categorisation Standard deviations are shown in parenthesis Text Set Measure Question Answer Question and Answer Thread Number of Texts 4 652 4 681 4 839 4 841 Number of Lemmas 2 929 2 055 3 956 4 398 Lemmas Text 12 2 9 2 19 5 23 2 Texts Lemma 19 3 21 0 23 9 25 5 NMI 0 28 0 03 0 14 0 02 0 40 0 03 0 38 0 04 6 Experiments and Discussion In Table 1 we report average results in NMI for nine to 20 clusterings of the different text sets with the standard deviation shown in parenthesis In order for two results to be considered different they as a rule of thumb they need not overlap with their standard deviations We choose nine clusters as the categorisation has nine categories The tendencies we describe are similar for other numbers of clusters The result clearly shows that the textual information in the question Question is better than what is in the answer Answer The result gets even better Clustering E Mails for the Swedish Social Insurance Agency 119 however if we also include the 
answer Question and Answer The result for the entire e mail thread Thread is the same as for Question and Answer As shown in Table 1 the Normalised Mutual Information NMI for the query and answering part is 0 12 units higher than for only the query alone It is not surprising that the result is better for the set of questions than for the set of answers as the categories are chosen by the citizens who also formulated the questions The answers are often shorter than the questions see the statistics in Table 1 use a more formal language and do not necessarily include the same terms as their corresponding question This makes the answers harder to group Combined with the question however the answer does give more information than using only the question for the clustering algorithm to work with as similar questions tend to be answered in similar ways By the same reasoning the result when using the entire thread is used should be even better The questions that require more responses follow up questions with 
answers however are probably more complicated and therefore harder to group The categorisation of the first question might not even be suitable for the entire thread as it may well include new questions regarding other matters As the full thread Thread contains most information and it performs equally well with the questions and answers Question and Answer we will use it in our further work 7 Conclusions and Future Work We have compared clusterings of e mails sent to the SSIA based on different parts of the e mail thread texts The results clearly show that the original question contains more useful information than only the answer although a combination is even better Using the full e mail thread does not downgrade the result We plan to involve the handling officers in our next investigation We will let them explore clusterings of the e mails and interview them to learn whether an approach like this is actually useful and if it can provide insights help to find common questions and formulate standard answers 
Acknowledgements We would like to thank Anne Lie Karlsson at References 1 Knutsson O Pargman T Dalianis H Rosell M Sneiders E Increasing the efficiency and quality of e mail communication in e Governmnent using language technology In Proc of IFIP e Government Conference 2010 EGOV 2010 Lausanne Switzerland August 29 September 2 2010 to be published 2 Lampert A Dale R Paris C Segmenting email message text into zones In Proc of the 2009 Conference on Empirical Methods in Natural Language Processing EMNLP 2009 2009 120 H Dalianis M Rosell and E Sneiders 3 Huang Y Govindaraju D Mitchell T M de Carvalho V R Cohen W W Inferring ongoing activities of workstation users by clustering email In OpenMaTrEx A Free Open Source Marker Driven Example Based Machine Translation System Sandipan Dandapat1 Mikel L Forcada1 2 Declan Groves1 3 Sergio Penkale1 John Tinsley1 and Andy Way1 Centre for Next Generation Localisation School of Computing Dublin City University Glasnevin Dublin 9 Ireland 2 Departament de Llenguatges i 
Sistemes Inform atics Universitat d Alacant E 03071 Alacant Spain 3 1 Abstract We describe OpenMaTrEx a free open source examplebased machine translation EBMT system based on the marker hypothesis comprising a marker driven chunker a collection of chunk aligners and two engines one based on a simple proof of concept monotone EBMT recombinator and a Moses based statistical decoder OpenMaTrEx is a free open source release of the basic components of MaTrEx the Dublin City University machine translation system Keywords example based machine translation corpus based machine translation free open source software 1 Introduction We describe OpenMaTrEx a free open source FOS example based machine translation EBMT system based on the marker hypothesis 1 It comprises a marker driven chunker a collection of chunk aligners and two engines one based on the simple proof of concept monotone recombinator previously released as Marclator 1 and a Moses based deco der 2 OpenMaTrEx is a FOS version of the basic components of 
MaTrEx the Dublin City University machine translation MT system 3 4 Most of the co de in OpenMaTrEx is written in Java although there are many important tasks that are performed in a variety of scripting languages A preliminary version 0 71 has been released for download from http www openmatrex org on 2nd June 2010 under a FOS licence 2 The architecture of OpenMaTrEx is the same as that of a baseline MaTrEx system 3 4 as MaTrEx it can wrap around the Moses statistical MT 1 2 http www openmatrex org marclator GNU GPL version 3 http www gnu org licenses gpl html H Loftsson E 122 S Dandapat et al deco der using a hybrid translation table containing marker based chunks as well as statistically extracted phrase 3 pairs OpenMaTrEx has been released as a FOS package so that MaTrEx components which have successfully been used 5 6 7 may be combined with components from other FOS machine translation FOSMT toolkits such as Cunei4 8 Apertium5 9 etc 6 Indeed using components released in OpenMaTrEx researchers have 
previously used statistical mo dels to rerank the results of recombination 10 used aligned marker based chunks in an alternative deco der which uses a memory based classifier 11 combined the marker based chunkers with rule based components 12 and used the chunker to filter out Moses phrases for linguistic motivations 13 The rest of the paper is organized as follows Section 2 describes the principles of training and translation in OpenMaTrEx section 3 describes the EBMT specific components in OpenMaTrEx section 4 describes its software requirements and briefly explains how to install and run the available components A sample experiment performed on a standard task with OpenMaTrEx is described in section 5 and results are compared to those obtained with a a standard statistical machine translation SMT system Concluding remarks are made in section 6 2 OpenMaTrEx Training and Translation Training with OpenMaTrEx may be performed in two different mo des In MaTrEx mode 1 Each example sentence in the sentence 
aligned source text and its counterpart in the target training text are divided in subsentential segments using a marker based chunker Chunks may optionally be tagged according to their initial marker word to further guide the alignment pro cess 2 A complete 3 4 5 6 7 In statistical MT the term phrase is stretched to refer to any contiguous sequence of words http www cunei org http www apertium org For a longer list of FOSMT systems visit http fosmt info http www fjoch com GIZA html OpenMaTrEx Free Open Source EBMT System 123 Translation may be performed as training in two ways 3 EBMT Specific Components Chunker The main chunker in OpenMaTrEx is based on the marker hypothesis 1 which states that the syntax of a language is marked at the surface level by a set of marker closed category words or morphemes The chunker in OpenMaTrEx deals with left marking languages a chunk starts at a marker word and must contain at least one non marker word Punctuation is also used to delimit chunks Version 0 71 provides 
marker files for Catalan Czech English Portuguese Spanish Irish French and Italian Marker files specify one marker word or punctuation in each line its surface form its category and optionally its subcategory A typical marker word file contains a few hundred entries Chunk aligners There are a number of different chunk aligners available in OpenMaTrEx The default aligner aligns chunks using a regular Levenshtein edit distance with a combination of costs specified in a configuration file optionally allowing jumps or block movements 3 The default combination uses two costs a probability cost based on word translation probabilities as calculated by using GIZA and Moses see training step 2 in section 2 and a cognate cost based on a combination of the Levenshtein distance the longest common subsequence ratio and the Dice co efficient As in 3 equal weights are used as a default for all component costs specified Translation table merging To run the system in MaTrEx mode markerbased chunk pairs are merged with phrase 
pairs from alternative resources here Moses phrases Firstly each chunk pair is assigned a word alignment based on the refined GIZA alignments for example please show me por favor 124 S Dandapat et al then carried out from step 6 scoring which calculates the required scores for all feature functions including the reordering mo del based on the combined counts A binary feature distinguishing EBMT chunks from SMT chunks may be added for subsequent MERT optimization as was done in 16 4 Technical Details Required software OpenMaTrEx requires the installation of the following software GIZA Moses IRSTLM 17 and a set of auxiliary scripts for corpus prepro cessing8 and evaluation mteval 9 Refer to the INSTALL file that comes with the distribution for details Installing OpenMaTrEx itself OpenMaTrEx may easily be built simply by invoking ant or an equivalent tool on the build xml provided The resulting OpenMaTrEx jar contains all the relevant classes some of which will be invoked using a shell OpenMaTrEx see below 
Running A shell OpenMaTrEx has options to initialise the training development and testing sets to call the chunker and the aligner to train a target language mo del with IRSTLM to run GIZA and Moses training jobs to merge marker based chunk pairs with Moses phrase pairs to run MERT optimization jobs and to execute the deco ders Future versions will contain higherlevel ready made options for the most common training and translation jobs For detailed instructions on how to perform complete training and translation jobs in both MaTrEx and Marclator mo de see the README file Test files will be provided in the examples directory of the OpenMaTrEx package 5 A Sample Experiment To show how OpenMaTrEx can be used to improve baseline SMT results we report on a simple experiment using 200 000 randomly selected sentences from the 8 9 http homepages inf ed ac uk jschroe1 how to scripts tgz We currently use version 11b from ftp jaguar ncsl nist gov mt resources OpenMaTrEx Free Open Source EBMT System 125 Table 1 A sample 
experiment using 200 000 randomly selected sentences from the Spanish English fraction of Europarl as provided for the Third Workshop on SMT WMT08 Testing was performed on the 2 000 sentence test set provided by WMT08 System Baseline Moses Simple merging Feature based merging BLEU 30 59 30 42 30 75 NIST EBMT pairs 7 5171 27 60 7 5156 29 53 7 5269 33 55 nicely with the number of marker based chunks actually used during translation It would be interesting to pursue a more detailed study of the actual differences in the translations produced when using more linguistically motivated chunk pairs 6 Concluding Remarks and Future Work We have presented OpenMaTrEx a FOS EBMT system including a markerdriven chunker with marker word files for a few languages chunk aligners a simple monotone recombinator and a wrapper around Moses so that it can be used as a deco der for a merged translation table containing Moses phrases and marker based chunk pairs OpenMaTrEx releases the basic components of MaTrEx the Dublin City 
University machine translation system under a FOS license to make them available to researchers and developers of MT systems As for future work version 1 0 will contain among other improvements a better set of marker files improved installing and running pro cedures with extensive training and testing options and improved do cumentation further versions are expected to free open source additional MaTrEx components Acknowledgements The original MaTrEx co de on which OpenMaTrEx is based was developed among others by S Armstrong Y Graham N Gough D Groves H Hassan Y Ma B Mellebeek N Stroppa J Tinsley and A Way We specially thank Y Graham and Y Ma for their advice P Pecina helped with Czech markers and Jim O Regan with Irish markers M L Forcada s sabbatical stay at Dublin City University is supported by Science Foundation Ireland SFI through ETS Walton Award 07 W 1 I1802 and by the Universitat d Alacant Spain Support from SFI through grant 07 CE I1142 is acknowledged References 1 Green T The necessity of syntax 
markers two experiments with artificial languages Journal of Verbal Learning and Behavior 18 126 S Dandapat et al 3 Stroppa N Way A MaTrEx DCU machine translation system for IWSLT 2006 In Proceedings of IWSLT 2006 pp Head Finders Inspection An Unsupervised Optimization Approach Grupo de Procesamiento de Lenguaje Natural Universidad Nacional de 1 2 Abstract Head finder algorithms are used by supervised parsers during their training phase to transform phrase structure trees into dependency ones For the same phrase structure tree different head finders produce different dependency trees Head finders usually have been inspired on linguistic bases and they have been used by parsers as such In this paper we present an optimization set up that tries to produce a head finder algorithm that is optimal for parsing We also present a series of experiments with random head finders We conclude that although we obtain some statistically significant improvements using the optimal head finder the experiments with random head 
finders show that random changes in head finder algorithms do not impact dramatically the performance of parsers Keywords Syntactic Parsing Head Finders Genetic Algorithms Bilexical Grammar 1 Introduction Head finder algorithms are used by supervised syntactic parsers to transform phrase structure trees into dependency ones The transformation is carried out by selecting a word as the head in every constituent Head finder algorithms are based on a set of head finder rules which provides instructions on how to find the head for every type of constituent For every internal node of a tree the head finder rules specify which children of the node contains the head word The first set of head rules based on linguistic principles was introduced in 1 and it is used by many state of the art statistical parsers like 2 3 4 5 with only minimal changes The standard set of head finder rules was handcrafted and consequently not optimized for parsing therefore there might exist different sets of head finder rules that can 
improve parsing performance In this paper we investigate their role in parsing and we experiment with two different state of the art parsers We present an optimization algorithm that improves the standard set of head finders one rule at the time with the goal of finding an optimal set of rules Even though our optimization algorithm produces statistically significant improvements they hardly obtain a better performance In order to better understand why our optimization algorithm cannot produce bigger improvements we test the stability of the search space We test this by generating different head finders we generate head finders that always select the right most and left H Loftsson E 128 M A most subtrees as the trees containing the headword We also generate 137 random sets of rules and we test head finders that are not consistent that is head finders whose set of rules change during the same training session Our optimization procedure aims at finding the best possible set of rules that improves parsing 
performance Our procedure is defined as an optimization problem and as such it defines the quality measure that it has to optimize its search space and the strategy it should follow to find the optimal set of rules among all possible solutions The search space is the possible sets of rules our procedure optimizes one rule in the set at a time A new set of rules is then created by replacing an original rule in the standard set with its optimized rule The quality measure for a rule set is computed in a serie of steps First the training material is transformed from phrase structure trees into dependency ones using the rule to be evaluated second a bilexical grammar 6 is induced from the dependency tree bank and finally the quality of the bilexical grammar is evaluated The quality of the grammar is given by the perplexity PP and missed samples MS found in the automata of the grammar as explained in Section 3 3 Finally the strategy for traversing the search space is implemented by means of Genetic Algorithms Once 
we obtain an optimized set of rules we proceed to evaluate its impact in two parsers Collins s parser 5 by means of Bikel s implementation 4 and the Stanford parser 7 These two parsers have their source code available and their head finder algorithms are rather easy to modify We considered experimenting also with the Maltparser 8 but its performance is hard to evaluate when its head finder is modified Our experiments show that the parsing performance of the two parsers is insensitive to variations in head finders They also show that among all possible head finders our optimization procedure is capable of finding improvements Our experiments also show that in the presence of inconsistent head finder rules parsers performance drops 1 6 and 0 9 for Bikel s and for Stanford respectively Our experimental results with random head finders show that modifications in the rule for VP produced the biggest impact in the performance of the two parsers More interestingly our experiments show that inconsistent head finders 
are more stable than random deterministic head finder We argue that this is the case because the variance on the structures the later produce is considerably bigger with respect to the former Our experiments also show that Stanford parser performance is more stable with respect to variations in the head finder rules than Bikel s All in all our experiments show that even though it is possible to find some new set of rules that improves parsers performance head finding algorithms do not have a decisive impact on the performance of these two state of the art syntactic parsers this also indicates that the reason for their performance lies beyond the procedure that is used to obtain dependencies The rest of the paper is organized as follows Section 2 explains head finding algorithms Section 3 presents the quality measure used in our optimization algorithm while Section 4 discusses the search space and the strategy to traverse it Section 5 presents how random rules are generated Section 6 presents the results of 
our experiments and Section 7 introduces related work Finally Section 8 concludes the paper Head Finders Inspection An Unsupervised Optimization Approach 129 2 Head Finding Algorithms For each internal node of a phrase structure tree the head finder HF determines which of its subtrees contains the head word of the constituent The procedure of transforming a phrase structure into a dependency one starts in the root of the tree and moves downwards up to the tree preterminals The HF has as a parameter a set of head finder rules R R contains one rule for each possible grammatical category Formally let R be rgc1 rgck where rgci is the head finder rule associated with the grammatical category gci the set gc1 gck is the set of all grammatical category tags like S VP ADJP ADVP SBAR for the Peen Tree Bank 9 PTB S NP VP NP g Tl1 Tlk Tr1 Trs Fig 2 A simple phrase structure tree NNP NNP VBZ Ms Haag plays NNP Elianti Fig 1 Sentence 2 from Section 2 of the PTB The nodes where the head of each constituent is searched for 
is marked in boldface A head finding rule rgc is a vector of pairs d1 t1 dkn tkn where ti is a non terminal and dm lef t right is a search direction We also use the notation of direction vector to refer to the vector d1 dki which is the projection of the first component of the head finding rule vector Similarly the tags vector t1 tki is the projection of the second component We refer to a head finder rule as one vector of pairs or as a pair of vectors For example l TO l VBD l VBN l MD l VBZ l VB l VBG l VBP l VP l ADJP l NN l NNS l NP is the rule that is associated with tag VP in the standard set of head finder rules It is important to highlight that our definition of head finder rule is a simplification of the standard head finder rule In the standard definition of rules for tags NP and NX sets of non terminals are used instead of simple non terminals Our definition excludes this situation because otherwise the size of the search space makes the optimization procedure unfeasible In order to show how the 
head finder algorithm works we introduce a few auxiliary functions root T returns the root node of the phrase structure tree T children T g returns the list of children of node g in T and subtreeList T returns the list of subtrees of T ordered from left to right For example if T is the tree in Figure 2 then root T g children T g root Ti1 root Tlk root Tr1 root Trs and subtreeList T Tl1 Tlk Tr1 Trs Using this definition we formally define in Figure 3 the algorithm HFR that transforms a phrase structure tree into a dependency one where R is a set of head finder rules 130 1 2 3 4 5 6 7 8 9 10 11 12 13 M A Consider the tree in Figure 1 Suppose that the head finder rule for tag VP is l TO l VBD l VBN l MD l VBZ l VB l VBG l VBP l VP l ADJP l NN l NNS l NP When the head finder algorithm reaches the node VP it looks from left to right a tag TO since it cannot find such tag it looks from left to right a tag VBD it keeps changing what it is looking for until it looks from left to right for a tag VBZ Once it has found 
it it marks that subtree as head and it recursively inspects all subtrees 3 A Quality Measure Based in Bilexical Grammars This section introduces the quality measure used in our optimization procedure Our procedure is based on the optimization of a quality measure q defined over a set of head finder rules In order to compute q for a given set of rules we proceed as follows We transform Sections 01 22 of PTB into dependency structures Using the resulting dependency tree bank we build a bilexical grammar and finally we compute a quality measure on this grammar The measure over the bilexical grammar is a formula that takes into account PP and MS of the set of automata that define the bilexical grammar 3 1 Bilexical Grammars Bilexical grammars are a formalism in which lexical items such as verbs and their arguments can have idiosyncratic selective influences on each other We define a bilexical grammar B as a 3 tuple Ro rc wC lc cC where Head Finders Inspection An Unsupervised Optimization Approach 131 0 
Researchers NN 1 can MD 2 apply VB 3 for IN 4 permission NN 5 to 6 use TO VB 7 the DT 8 probes NN 9 for IN 10 brain NN 11 studies NN 12 dot DOT Fig 4 Tree extracted from the PTB file wsj 0297 mrg and transformed to a dependency tree node are ordered in a sequence with respect to each other and the node itself so that each node may have both left children that precede it and right children that follow it A dependency tree T is grammatical if for every tag token c that appears in the tree lc accepts the possibly empty sequence of c s left children from right to left and rc accepts the sequence of c s right children from left to right 3 2 Induction of Bilexical Grammars Bilexical grammars can be induced from a dependency tree bank by inducing two automata for each tag in C The induction of Bilexical Grammars is carried out in a supervised fashion Our training material comes from transforming Sections Table 1 Bags of left and right dependents extracted from dependency tree in Figure 4 Left dependents are to be 
read from right to left All displayed sets are singletons Word 0 1 2 3 4 i i i Tlef Tright t NN NN NN MD MD NN MD VB DOTSYB VB VB VB IN IN IN IN NN NN NN NN TO 132 M A 3 3 Quality Measure for Grammars The measure q of a set of head finder rules is defined as a measure of the grammar that is built from using the head finder rule to transform the PTB into dependencies The measure is then defined over the automata that defined the grammars The measure over bilexical grammars contains two components The first one called test sample perplexity PP is the per symbol log likelihood of strings belonging to a test sample according to the distribution defined by the automaton The minimal perplexity PP 1 is reached when the next symbol is always predicted with probability 1 while PP corresponds to uniformly guessing from an alphabet of size The second component is given by the number of missed samples MS A missed sample is a string in the test sample that the automaton fails to accept One of such instance suffices to 
have PP undefined Since an undefined value of PP only witnesses the presence of at least one MS we count the number of MS separately and compute PP without considering MS The test sample that is used to compute PP and MS comes from all trees in sections 00 01 of the PTB These trees are transformed to dependency ones by using HFRc where Rc is the candidate set of rules Better values of M S and P P for a grammar mean that its automata capture better the regular language of dependents by producing most strings in the automata target languages with fewer levels of perplexity The quality measure of grammar is then the mean of PP s and MS s for all automata in the grammar 4 Building and Traversing the Search Space This section introduces the search space and the strategy to traverse it in our optimization procedure The search space consists of different sets of head rules The standard set of rules contains 26 rules The longest is the one associated with ADJP it contains 18 entries Finding a new set of rules means 
that we should find a new set of 26 vectors For each candidate rule we have to transform the PTB build the bilexical grammar and compute PP and MS for all automata It takes us 1 2 minutes to evaluate one candidate set of rules In principle all possible head rules can be candidate rules but then the search space would be huge and it would be computationally unfeasible to traverse it In order to avoid such search space we run a series of experiments where we optimize one rule at a time For example one of our experiments is to optimize the rule associated with VB Our search space contains all possible set of rules where all rules exept the one associated to VB are as in the standard set of head finder rules To optimize one rule we traverse the search space with Genetic Algorithms Genetic Algorithms need for their implementation 1 Definition of individuals each individual codifies one candidate of the head finder rule that is being optimize 2 A fitness function defined over individuals the quality measure is 
computed by constructing a set of head finder rules by adding the candidate rule to the standard set of rules building a bilexical grammatical and evaluating it as described in Section 3 3 Finally 3 A strategy for evolution we apply two different operations to individuals namely crossover and mutation crossover gets 0 95 probability of being applied while mutation gets 0 05 We select individuals using the roulette wheel strategy 11 In our experiments Head Finders Inspection An Unsupervised Optimization Approach 133 in each generation there is a population of 50 individuals we let the population evolve for 100 generations The mutation function is easily defined by computing a random permutation of the rule tags vector and a random sample of its direction vector The crossover operation is defined as follows Let g1 gi gn and h1 hi hn be the tag vectors of two different individuals and let i be random number between 1 and n The crossover produces two new individuals The tag vector of one of the individuals is 
defined as follows sub g1 gn hi 1 hn 5 Stability of Head Finders As it is shown in Section 6 our optimization method only improves the performance of Bikel s parser The reason for the lack of improvement of Stanford parser can be either because our optimization method is ill defined or because the parser is indifferent to the set of head finders rules However using no head finder that is non dependency grammars performance never reaches beyond 75 So heads and dependencies based on heads are an important element in parsing performance In this section we present experiments that try to shed light on this issue We tested 1 randomly generated head finder rules 2 head finders whose rules were reversed 3 head finders that always choose right or left and 4 inconsistent head finders Random Head Finders We experiment generating several sets of head finder rules A random set of head finder rules is created by replacing one rule in the standard set of head finder rules by one random rule A random rule is created by 
randomly permuting the elements of both the tags vector and the direction vector Experimental results for this head finder are shown in Table 3 A In Figure 5 the first row shows the head rule defined in the standard set for category S The second row shows a random permutation of this rule The last row shows a reverse permutation of the original one The reverse permutation of a rule is obtained by reversing the order of its tag vector and leaving its direction vector unchanged In this example the rule presented in the second row is calculated using the permutation 7 1 4 8 2 3 6 5 for the tags vector and its random direction vector was l r r l l l l l We generate 119 rules and consequently 119 different head finder rule sets Each of these sets differs in one rule from the standard set In this way we show the impact of each rule in the overall parsing performance We also experiment with a set of rules were all of its rules were randomly generated We test 7 of such random sets for each parser Experimental 
results for this head finder are shown in Table 4 134 M A Original S l TO l IN l VP l S l SBAR l ADJP l UCP l NP sampled rule S l UCP r TO r S l NP l IN l VP l ADJP l SBAR reverse rule S l NP l UCP l ADJP l SBAR l S l VP l IN l TO Fig 5 The first row shows the original Collins s head rule for S The second row shows a random permutation of the original rule The last row is the reverse of the original rule Reverse Rules There are 26 rules in the standard set of head finder rules We generate 26 new sets by changing one rule at a time by its reverse The reverse of a rule is constructed by reading it from left to right The impact of reverse rules in parsing performance is shown in Table 3 B Left Most and Right Most Head Finders We define two special algorithms for finding heads The always leftmost and always rightmost algorithm chooses for each internal node the leftmost and rightmost subtree respectively These are special cases of head finder algorithms that cannot be expressed with a set of rules In order to 
implement these algorithms we modified both parser implementations The results are shown in Table 2 B Non deterministic Head Finders All previous experiments were based on deterministic head finders every time they are used to transform a given phrase structure tree they transform into the same dependency tree We implemented a non deterministic head finder algorithm this algorithm flips a coin every time it has to decided where the head is When this head finder is used to transform a phrase structure into a dependency tree it produces different dependency trees for every time it is called We report results for 7 of these experiments for each parser they can be seen in Table 4 6 Experimental Results In this section we show the results of all our experiments In all experiments we used Sections 02 21 of the PTB for training and Section 23 for testing The optimization algorithm use Sections 00 01 for computing the quality measure defined on automata Our experiments aim to analyze the variation in performance by 
changing one or more head rules in the standard set of head rules The rules that are modified by our experiments correspond to tags WHADJP CONJP WHNP SINV QP RRC S ADVP NAC SBAR VP SQ ADJP WHPP SBARQ PP WHADVP Table 2 A shows the performance of the set of rules produced by the optimization procedure Each row displays labeled precision labeled recall significance level pval and harmonic mean F1 The baseline row reports the performance of both parsers using the standard set of rules pval was computed against the baseline We consider a result as statistically significant if its significance level pval is below 0 05 Performance value were computed using the evalb script significance values were measured using Bikel s Randomized Parsing Evaluation Comparator script The table shows that the performance in the Stanford parser using our optimized head finder set of rules is below the baseline however this decrease in performance is not statistically significant Head Finders Inspection An Unsupervised Optimization 
Approach 135 Table 2 A The result of the experiments corresponding to the optimized head finder The upper part shows evaluation in Bikel s parser while the bottom with Stanford parser B First column shows the F1 when all worst performing rules reported in Table 4 A are put together Second and third columns show average F1 for the always right most and always left most head finders Num Bikel Baseline optimal head L R L P pval R pval P F1 88 53 88 63 88 583 88 72 88 85 0 006 0 002 88 785 85 742 0 131 85 727 Parser F1 worst choice F1 Right Most F1 Left Most Bikel 82 486 83 024 85 102 Stanford 84 092 84 206 85 566 Stanford Baseline 85 26 86 23 optimal head 85 24 86 22 0 098 A B The best set of head rules was obtained by combining all rules that our optimization method produced Table 4 shows the results of the random head rule generation The Table contains one row per each rule that was permuted We consider 17 different rules for each we build 7 17 new random rules Each row shows the maximal the minimal and the 
average F1 measure we obtained Table 3 B shows F1 measure for the 17 rules that were obtained by reversing one of the rules in the standard set at a time From Tables 3 A and B we can see that the rule defined for tag VP has the greatest impact on the performance of both parsers The first column of Table 2 B shows the results for the head finder that is built by using the 17 rules with the worst performance in experiments in Table 4 The second and the third columns show the results for the head finder algorithms that choose always the leftmost and always the rightmost respectively Table 3 A Parsing results obtained by replacing one rule in the standard set by a random rule Each row shows the average maximal and minimal impact in the F1 measure for each parser B Experiments result for each Head finder built with the reverse of head rule One column for each parser rule tag WHADJP CONJP WHNP SINV QP RRC S ADVP NAC SBAR VP SQ ADJP WHPP SBARQ PP WHADVP avg 88 589 88 586 88 586 88 485 88 538 88 588 88 092 88 586 88 
594 88 195 87 330 88 571 88 616 88 583 88 583 88 617 88 607 Bikel max 88 596 88 601 88 596 88 608 88 604 88 595 88 569 88 615 88 607 88 653 88 471 88 592 88 698 88 583 88 583 88 668 88 706 min 88 583 88 582 88 577 88 009 88 474 88 583 87 458 88 564 88 581 88 013 85 870 88 562 88 566 88 583 88 583 88 583 88 583 avg 85 739 85 739 85 739 85 749 85 747 85 739 85 689 85 740 85 743 85 734 85 247 85 739 85 727 85 739 85 739 85 740 85 739 Stanford max min 85 739 85 739 85 739 85 739 85 739 85 739 85 772 85 730 85 759 85 732 85 739 85 739 85 726 85 652 85 743 85 739 85 743 85 743 85 739 85 733 85 612 84 918 85 739 85 739 85 739 85 718 85 739 85 739 85 739 85 739 85 740 85 739 85 739 85 739 Gram tag WHADJP SBAR CONJP VP WHNP SQ SINV ADJP QP WHPP RRC SBARQ S PP ADVP WHADVP NAC B F1 88 596 87 990 88 600 85 820 88 596 88 562 88 465 88 626 88 490 88 583 88 595 88 583 88 178 88 583 88 613 88 593 88 603 S F1 85 739 85 733 85 739 84 249 85 739 85 739 85 758 85 726 85 748 85 739 85 739 85 739 85 670 85 739 85 739 85 739 85 
743 A B 136 M A Table 4 Experiments result of random choice of rules for each experiment we show the impact in the F1 measure for the average maximal and minimal Rand No Det Random Det Parser avg max min avg max min Bikel 86 976 87 166 86 754 86 001 87 974 83 857 Stanford 84 810 84 997 84 691 84 805 85 625 84 360 Table 4 shows results for the non deterministic and deterministic head finders The set of rules are obtained by changing rules for the 17 tags considered in our work We run 7 tests for deterministic and 7 tests for non deterministic head finders In both cases we calculate the average the maximum and the minimum obtained for the measure F1 The results show that the non deterministic head finder is more stable because the variation between the minimum and the maximum results is lower A priori this is a surprising result because the dependency trees used to induce the grammar during the training phase have percolated inconsistent heads We think that the non deterministic head finders are more stable 
because in average they make more correct choices In contrast if deterministic head finders contain an erroneous rule all the resulting dependency trees are wrong This fact is also supported by the results reported in Table 2 B It shows that using the head finder built out of the worst performing rules is the one with the worst performance in both parsers The performance drops nearly 6 and 1 9 for Bikel and Stanford respectively The right most head finder decline is the next considering the performance downfall 7 Related Work Similar work has been published in 12 and an improved version can be found in the Bikel s thesis 4 In this work the authors tried to induce head rules by means of defining a generative model that starts with an initial set of rules and uses an EM like algorithm to produce a new set of rules that maximize the likelihood They used different sets of rules as seeds for the EM but the approach only shows improvement when the standard set of rules is used In contrast to our approach none of 
their improvements were statistically significant They also show that when the seed is a set of random rules the overall performance decreases In a different approach 13 the authors present different unsupervised algorithms for head assignments used in their Lexicalized Tree Substitution Grammars They study different types of algorithms based on entropy minimization familiarity maximization and several variants of these algorithms Their results shows that using the head finder they induced they obtain an improvement of 4 over a PCFG parser using an standards head assignments In our work we don t use lexicalized grammars Our approach is based on improvements to a given rule set as opposed to theirs where they use unsupervised methods to find assigments for heads 8 Conclusions In our approach we aim at generating dependency trees that improve the performance of the statistical parser To do so we vary the head rules that are used while transforming Head Finders Inspection An Unsupervised Optimization Approach 
137 constituent trees into dependency ones Besides finding some new rules which hardly improve parsers performance we found that variations in head finding algorithms do not have a decisive impact on the syntactic parsers performance to the extent that an aleatory translation of the phrase structure trees into dependency ones can be used without damaging considerably the parsing performance However removing head finding altogether produces a 10 decrease in performance considerably higher than the 1 9 and 6 decreases in performance produced by the worst possible head finders Therefore head finders are crucial for the performance of dependency parsers but their variations are not References 1 Magerman D M Natural language parsing as statistical pattern recognition Ph D thesis Stanford University 1994 2 Charniak E A maximum entropy inspired parser In NAACL 2000 2000 3 Klein D Manning C Accurate unlexicalized parsing In Proc 41st ACL 2003 4 Bikel D On the Parameter Space of Generative Lexicalized Statistical 
Parsing Models PhD thesis University of Pennsylvania 2004 5 Collins M Three generative lexicalized models for statistical parsing In ACL 1997 1997 6 Eisner J Bilexical grammars and a cubictime probabilistic parser In Proceedings of IWPT04 1994 7 Klein D Manning C Distributional phrase structure induction In CoNLL 2001 2001 8 Nivre J A A Maltparser A language independent system for data driven dependency parsing In Natural Language Engineering pp Estimating the Birth and Death Years of Authors of Undated Documents Using Undated Citations Yaakov HaCohen Kerner1 and Dror Mughaz2 1 1 Dept of Computer Science Jerusalem College of Technology 91160 Jerusalem Israel 2 Dept of Computer Science Bar Ilan University 52900 Ramat Gan Israel kerner jct ac il myghaz cs biu ac il Abstract Precious historical treasures might be hidden between the lines of a text There are many implicit details which can be extracted from a text particularly if one has access to an entire corpus of texts pertaining to the given subject One of 
these details is the identification of the era in which the author of the given document s lived For rabbinic documents written in Hebrew and Aramaic which are almost without exception undated and do not contain any bibliographic section this problem is extremely important The aim of this novel research is to find in which years an author was born and died based on his documents and the documents of other authors whose birth and death years are known who refer to the author under discussion or are mentioned by him Such estimates can help determine the time frame in which certain documents were written and in some cases identify an anonymous author In the framework of this research we formulate various kinds of iron clad heuristic and greedy constraints defining the birth and death years of an author based on citations referring to him or mentioned by him Experiments applied on a corpus containing texts composed by rabbinic authors show reasonable results Keywords Citation analysis Hebrew Hebrew Aramaic 
documents knowledge discovery time analysis undated citations undated documents 1 Introduction Citations are a defining feature of many kinds of documents e g academic legal and religious Authors cite previous works which are related in some way to their own work or to their discussion Citations included in documents are important information resources of interest to researchers Therefore automatic extraction and analysis of citations from documents are of great importance Recent developments e g computerized corpora and search engines enable accurate extraction of citations As a result citation analysis has an increased importance A citation is a brief reference in the body of the text to a source of published information A reference includes bibliographic details about a source that is mentioned in a citation The reference is found at end of a document in a reference list Citations are presented in agreed typographical formats Different disciplines have different conventions citation in footnotes citations 
with numbers e g 1 or mixed symbols such as Cohen98 or Cohen 1998 Harvard style citations H Loftsson E Estimating the Birth and Death Years of Authors of Undated Documents 139 Garfield 2 was the first to propose automatic production of citation indexes extraction and analysis of citations from corpora of academic papers Powley and Dale 5 develop techniques to extract from a given academic paper a list of citations and for each citation the corresponding reference in the reference list They find each instance of a citation in the body of the paper parse it into a set of author names and years and find the segment of text from the references which contains the corresponding reference Teufel et al 8 use extracted citations and their context for automatic classification of citations to their citation function the author s reason for citing a given paper Some research has been done concerning the improvement of retrieval performance using terms Ritchie et al 6 show that document indexing based on combinations of 
terms used by citing documents and terms from the document itself give better retrieval performance than standard indexing of the document terms alone In 7 Ritchie et al investigate how to select text from around the citations in order to extract good index terms in order to improve retrieval effectiveness Citations are a defining feature not just of academic papers but also and even more of rabbinic responsa answers written in response to Jewish legal questions authored by rabbinic scholars Citations included in rabbinic literature are more complex to define and to extract than citations in academic papers written in English because 1 In contrast to academic papers there is no reference list that appears at the end of a responsa 2 There is an interaction with the complex morphology of Hebrew and Aramaic For example citations can be presented with different types of prefixes e g and when and when in and in and when in included in the citation word s 3 Natural language processing in Hebrew and Aramaic has 
been relatively little studied 4 Many citations in Hebrew Aramaic documents are ambiguous For instance a a book titled magen avot was composed by four different Jewish authors and b The abbreviation m b relates to two different Jewish authors and has also other meanings which are not authors names and 5 At least 30 different syntactic styles are used to present citations This number is higher than the number of citation patterns used in academic papers written in English e g see 5 Each specific document written by a specific author can be referred to in at least 30 general possible citation syntactic styles Furthermore each citation pattern can be expanded to many other specific citations by replacing the name of the author and or his book responsa by each one of their other names e g different spellings full names short names first names surnames and nicknames with without title and abbreviations The citation recognition in this research is done by comparing each word to a list of 298 known authors and many 
of their books responsa This list contains 19 506 specific citations that relate to names nick names and abbreviations of these authors and their writings Basic known citations were collected and all other citations were produced from them based on an automatic extension process using regular expressions 140 Y HaCohen Kerner and D Mughaz Hebrew Aramaic documents in general and Hebrew Aramaic responsa in principle present various interesting text mining problems Firstly Hebrew is richer in its morphology forms than English According to linguistic estimates Hebrew has 70 000 000 valid inflected forms while English has only 1 000 000 1 In Hebrew there are up to seven thousand declensions for one stem while in English there are only a few declensions Secondly these kinds of documents include a high rate of abbreviations about 20 while more than one third of them about 8 are ambiguous 4 A previous research that works on corpora which contain responsa referring to Jewish law written in Hebrew Aramaic dealt with 
text classification 3 In this research HaCohen Kerner et al investigate whether the use of stylistic feature sets and or name based feature sets is appropriate for classification of documents to the ethnic group of their authors and or periods of time when the documents were written and or places where the documents were written In addition HaCohen Kerner et al 4 have experience with the processing of such texts from the viewpoint of disambiguation of ambiguous abbreviations The current research is a continuation of this long term research interest In this research we present a novel model that estimates the birth and death years of a given author using undated citations of other authors whose birth and death years are known who refer to him or mentioned by him The documents are undated nontime stamped and mentions of years or historical events in the documents are very rare The estimations are based on various constraints of different degree of certainty iron clad heuristic and greedy constraints The 
constraints are based on general citations without cue words and citations with cue words such as father son rabbi teacher student friend and late of blessed memory This paper is organized as follows Section 2 presents various constraints of different degree of certainty iron clad heuristic and greedy constraints that are used to estimate the birth and death years of responsa authors Section 3 describes the model Section 4 introduces the tested dataset the results of the experiments and their analysis Section 5 summarizes concludes and proposes future directions 2 Citation Based Constraints This section presents the citation based constraints formulated for the estimation of the birth and death years of an author X based on his documents and on other authors Yi documents who mention X or one of his documents We assume that the death years for those who died and birth years of all authors are known excluding those of the investigated author Below are given some notions and constants that are used Estimating 
the Birth and Death Years of Authors of Undated Documents 141 Various types of citations exist general citations without cue words and citations with cue words such as father son rabbi teacher student friend and late of blessed memory Another classification of the discussed citations is to those referring to living authors and those referring to dead authors In contrast to academic papers responsa include much more citations to dead authors than to living authors We will introduce citation based constraints of different degrees of certainty ironclad I heuristic H and greedy G Iron clad constraints are absolutely true without any exception Heuristic constraints are almost always true Exceptions can occur when the heuristic estimates for MIN MAX and MIN_FATHER are incorrect Greedy constraints are rather reasonable constraints for responsa authors However sometimes wrong estimates can be drawn while using these constraints Each constraint will be numbered and its degree of certainty will be presented in 
brackets 2 1 Iron clad and Heuristic Constraints First of all we present two general heuristic constraints based on authors that cite X which are based on regular citations i e without mentioning special cue words e g friend son father and rabbi General constraint based on authors that were cited by X D X MAX B Yi MIN 1 H X must be alive when he cited Yi so we can use the earliest possible age of publishing of the latest born author Y as a lower estimate for X s death year General constraint based on authors that cite X B X MIN D Yi MIN 2 H All Yi must have been alive when they cited X and X must have been old enough to publish Therefore we can use the earliest death year amongst such authors Yi as an upper estimate of X s earliest possible publication age and thus his birth year Posthumous citation constraints Posthumous constraints estimate the birth and death years of an author X based on citations of authors who refer to X as late of blessed memory or on citations of X who mentions other authors as late 
Figure 1 describes possible situations where various kinds of authors Yi i 1 2 3 refer to X as late The lines depict authors life spans where the left edges represent the birth years and the right edges represent death years In this case as all Yi refer to X as late we know that all Yi died after X and some of the Yi might be still alive but we do not know when they were born in relation to X s birth Y1 was born before X s birth Y2 was born after X s birth but before X s death and Y3 was born after X s death X Y1 Y2 Y3 time axix Fig 1 Citations mentioning X as late 142 Y HaCohen Kerner and D Mughaz D X MIN D Yi 3 I However we know that X must have been dead when Yi cited him as late so we can use the earliest born such Y s death year as an upper estimate for X s death year Like all authors dead authors of course have to comply to constraint 2 as well Let us now look at the cases where the author X we are studying refers to other authors Yi as late Figure 2 describes possible situations where X refers to 
various kinds of authors Yi i 1 2 3 as late All Yi died before X s death or maybe X is still alive Y1 died before X s birth Y2 was born before X s birth and died when X was still alive and Y3 was born after X s birth and died when X was still alive Y1 Y2 Y3 time axix Fig 2 Citations by X who mentions others as late X D X MAX D Yi 4 I X must be alive after the death of all Yi who were cited as late by him Therefore we can use the death year of the latest born such Y as a lower estimate for X s death year B X MAX D Yi MAX 5 H X was probably born after the death year of the latest dying person who X wrote about Therefore we can use the death year of the latest born such Y minus his maximal life period as a lower estimate for X s born year Contemporary citation constraints Contemporary citation constraints calculate the upper and lower bounds of the birth year of an author X based only on citations of known authors who refer to X as their friend student rabbi This means there must have been at least some period 
in time when both were alive and intellectually active Figure 3 describes possible situations where various kinds of authors Yi refer to X as their friend student rabbi Y1 was born before X s birth and died before X s death Y2 was born before X s birth and died after X s death Y3 was born after X s birth and died before X s death and Y4 was born after X s birth and died after X s death Like all authors contemporary authors of course have to comply to constraints 1 and 2 as well Y1 Y2 Y3 Y4 time axix X Fig 3 Citations by authors who refer to X as their Friend Student Rabbi Estimating the Birth and Death Years of Authors of Undated Documents 143 B X MIN B Yi MAX MIN 6 H All Yi must have been alive when X was alive and all of them must have been old enough to publish Therefore X could not be born MAX MIN years before the earliest birth year amongst all authors Yi D X MAX D Yi MAX MIN 7 H Again all Yi must have been alive when X was alive and all of them must have been old enough to publish Thus X could not be 
alive MAX MIN years after the latest death year amongst all authors Yi Intellectual son father based constraints Son based constraints calculate the upper and lower bounds of the birth and death years of an author X based only on citations of only one known author who refers to X as his son According to rabbinic conventions X can be either a truly son i e a biological son or an intellectual son i e a student Figure 4 describes five possible situations Yi i 1 2 3 refer to X as their truly son In all these cases Yi were born before X s birth Y1 died before X s birth maximum 9 months before X s birth Y2 died before X s death and Y3 died after X s death Y1 is not a possible father in the discussed context since in this case Y1 cannot refer to his son who was born only after Y1 s death However in Jewish rabbinic documents it is possible that an author Yi e g Y4 or Y5 will call his student X a son meaning an intellectual son although X is not his truly son In such a case Yi the father can be born even after X s 
birth X Y1 Y2 Y3 Y4 Y5 time axix Fig 4 Citations by authors who refer to X as their son When taking into account situations such as an intellectual son X towards Y4 or Y5 all son based constraints are expressed by the friend student rabbi based constraints 6 7 If a biological bond i e a truly son can be absolutely identified than a unique constraint can be formulated Father based constraints calculate the upper and lower bounds of the birth and death years of an author X based only on citations of known authors who refer to X as their father Also here according to rabbinic conventions X can be either a truly father i e a biological father or an intellectual father i e a rabbi or a teacher Therefore all father based constraints are expressed by the friend student rabbi based constraints 6 7 144 Y HaCohen Kerner and D Mughaz 2 2 Greedy Constraints We also formulate and apply greedy constraints These bounds are sensible in many cases but which can nevertheless sometimes lead to wrong estimates It is important 
to mention that the greedy constraints are applied in combination with the iron clad and heuristic constraints This is because in many cases some of the greedy constraints are not applied because lack of explicit citations citations with cue words In such cases we use the estimations that are products of the iron clad and heuristic constraints Greedy constraint based on authors who are mentioned by X B X MAX B Yi 8 G Most of the citations in our research domain relate to dead authors Thus most of the citations mentioned by X relate to dead authors That is most of Yi were born before X s birth and died before X s death Therefore a greedy assumption will be that X was born no earlier than the birth of latest author mentioned by X Greedy constraint based on authors who refer to X D X MIN D Yi 9 G As mentioned above most of the citations mentioned by Yi relate to X as dead Therefore most of Yi die after X s death Therefore a greedy assumption will be that X died no later than the death of the earliest author who 
refers to X Refinement of constraints 8 9 are presented by constraints 10 13 Constraints 10 11 are due to X citing Yi and Constraints 12 13 are due to Yi citing X Greedy constraint for defining the birth year based only on authors who were cited by X B X MAX D Yi 10 G When taking into account only citations that are cited by X most of the citations relate to dead authors That is most of Yi died before X s birth Therefore a greedy assumption will be that X was born no earlier than the death of the latest author mentioned by X Greedy constraint for defining the birth year based only on authors who are mentioned by X as a friend B X MIN B Yi 11 G When taking into account only citations that are mentioned by X which relate to contemporary authors a greedy constraint can be that X was born no later than the birth of the earliest author mentioned by X as a friend Greedy constraint for defining the death year of X based only on authors who cited X as late D X MIN B Yi 12 G Estimating the Birth and Death Years of 
Authors of Undated Documents 145 When taking into account only citations that are mentioned by Yi who relate to X as late a greedy assumption can be that X died no later than the birth of the earliest author who cited X as late Greedy constraint for defining the death year of X based only on authors who cited X as a friend D X MAX D Yi 13 G When taking into account only citations that are mentioned by Yi who cited X as a friend all Yi must have been alive when X was alive and all of them must have been old enough to publish Therefore a greedy assumption will be that X died no earlier than the death of the latest author who cited X as a friend We do not present greedy constraints regarding son and father because they can be intellectual son and father and not truly relatives 3 The Model The main steps of the model are presented below Most of these steps were processed automatically except for steps 2 and 3 that were processed semi automatically 1 Cleaning the texts Since the responsa may have undergone some 
editing we must make sure to ignore possible effects of differences in the texts resulting from variant editing practices Therefore we eliminate all orthographic variations 2 Normalizing the citations in the texts For each author we normalize all kinds of citations that refer to him e g various variants and spellings of his name books documents and their nicknames and abbreviations For each author we collect all citation syntactic styles referred to him and then replace them to a unique string 3 Building indexes e g authors citations to late friend student rabbis son father and calculating the frequencies of each item 4 Citation identification into various categories of citations including self citations 5 Performing various combinations of iron clad and heuristic constraints on the one hand and greedy constraints on the other hand to estimate the birth and death years for each tested author 6 Calculating averages and std deviations for the best iron clad and heuristic version and the best greedy version 4 
Experimental Results The examined dataset includes 3 488 responsa1 authored by 12 Jewish rabbinic scholars two of whom are still alive All these authors lived in the last 130 years and were very productive regarding the number of documents and citations that were written by them On average there are about 291 documents for each scholar These responsa were written in the last 100 years The total number of words is about 6 887 351 words average per documents is 1 975 words This corpus includes citations to 298 1 Contained in the Global Jewish Database The Responsa Project at Bar Ilan University Http www biu ac il ICJI Responsa 146 Y HaCohen Kerner and D Mughaz authors including the 12 investigated authors The dataset before the normalization step step 2 in section 4 includes 106 923 citations i e mentions of other works which are about 8 910 citations in average for each author and about 31 citations for each document 19 506 of these citations are different Since this dataset represents a special corpus 
containing responsa authored by 12 authors who lived in the last 130 years the incoming posthumous citations count is always 0 This special situation enables us to correct death ages which are higher than the current year That is if the upper bound of D X is greater than the current year then we change it to the current year If the investigated authors died a few hundreds years ago then the upper bounds would probably been much worse The situation with these authors also means that we did not apply average posthumous constraints greedy rules 8 10 for the birth year and greedy rules 9 12 for the death year In a different corpus situation where all authors are roughly from the same period these greedy rules help but not here where many ancient authors are cited i e some of the lower bounds can be hundreds years ago and if we use them than the estimation for B X will be too low and therefore very bad Several characteristics of this dataset are presented below On average each author cites 8 910 citations while 
only about 10 of them are posthumous citations and about 6 of them are contemporary citations About 99 8 of the citations are implicit i e they are not accompanied with cue words that identify whether the citations are posthumous or contemporary The average number of citations to each author is 88 including self citations and 33 excluding self citations That is most of the citations 62 5 are self citations Among the explicit citations those with cue words the average number of posthumous citations 10 25 is about twice greater than the average number of contemporary citations 5 67 That is about two thirds of the explicit citations are posthumous On average for each author there are much more outgoing citations 8 910 than incoming citations 88 in general and more outgoing contemporary citations 6 than incoming contemporary citations 4 Table 1 compares the ground truth about the birth and death years on the one hand to the best iron clad and heuristic version and on the other hand to the best greedy version 
Since this is a novel problem it is difficult to evaluate the results in the sense that although we can compare how close the system guess is to the actual birth death years what we cannot do is assess how close is close i e there is no real notion of what a good result is Currently we use the notion difference which is defined as the estimated value minus the ground truth value Some of the estimates for birth and death years are not integer values This finding is due to the use of average functions in certain versions e g two last sub rows in tables 2 and 3 Table 1 shows that the best experimental results have been achieved by the best greedy version which was better than the best iron clad and heuristic version as follows 1 Its average birth year and death year differences 13 04 and 15 54 respectively are better than those of the best iron clad and heuristic version 22 and 22 67 respectively 2 The absolute differences of 12 out of 24 estimates were less or equal to 6 5 years versus only 5 such estimates of 
the best iron clad and heuristic version and 3 The standard deviation of the birth year s greedy estimate is less than its comparable Estimating the Birth and Death Years of Authors of Undated Documents 147 iron clad and heuristic standard deviation This indicates that the results of the best greedy version are steadier Indeed the best greedy version was better than the best iron clad and heuristic version only in 14 out of 24 estimates of birth and death years Therefore these results are still not enough significant Table 2 presents the experimental results using the various iron clad and heuristic constraints only section 2 1 The minimal average birth year and death year differences 22 and 22 67 respectively have been achieved by the version of the average late based constraints constraints 3 6 This result was obtained using the average Table 1 Experimental results using various groups of constraints Author X 1 2 3 4 5 6 7 8 9 10 11 12 Name of X in Hebrew Ground truth Best iron clad heuristic version Birth 
year 1879 1885 1888 5 1862 5 1888 5 1887 1857 5 1913 5 1833 5 1915 5 1916 1906 5 Death year 1971 5 1959 5 1981 1952 1981 1958 5 1950 1988 1923 1980 5 1981 1981 Ave Std dev Differences for best iron clad heuristic version Birth Death year year 38 34 5 26 29 5 31 5 29 17 5 1 22 0 5 15 30 5 40 5 13 18 5 2 56 5 46 5 14 5 21 5 29 2 14 3 5 22 17 15 22 67 13 28 Best greedy version Birth year 1899 5 1910 1894 1884 1874 5 1880 5 1885 1889 1889 1874 5 1920 1890 5 Death year 1953 1989 1953 1959 5 1958 1995 1980 5 1959 1971 5 1950 2009 1989 Ave Std dev Differences for best greedy version Birth Death year year 17 5 53 1 0 26 57 4 6 5 14 5 1 21 5 6 13 17 5 27 6 1 2 5 26 5 9 6 1 19 5 6 13 04 9 32 15 54 20 00 Birth year 1917 1911 1920 1880 1889 1902 1898 1895 1890 1901 1914 1910 Death year 2006 1989 Alive 1953 1959 1989 1963 1986 1969 1959 Alive 1995 Table 2 Experimental results using different groups of constraints Group of cons Upper and lower bounds Average of absolute differences in years Birth year Death year 35 83 38 
67 43 42 26 33 43 42 55 83 43 42 26 33 75 75 55 83 22 00 22 67 37 58 33 67 37 58 38 25 87 58 33 67 87 58 38 25 45 08 29 79 Cons 1 2 Posthumous citation cons cons 2 3 cons 2 5 cons 3 4 Contemporary cons 1 2 4 5 cons B X D X B X D X B X D X B X D X B X D X B X ave B X B X D X ave D X D X B X D X B X D X B X D X B X D X B X ave B X B X D X ave D X D X 148 Y HaCohen Kerner and D Mughaz of the upper and the lower bounds of the birth year as estimate for the birth year and the average of the upper and the lower bounds of the death year as estimate for the death year This version is better than the version that contains the two most simple constraints 1 2 which do not take into consideration any cue words This finding indicates that the posthumous and contemporary constraints do contribute to the estimates The result achieved by the best iron clad version was successful also because an important correction that was done by us concerning the iron clad constraints dealing with the estimation of D X That is if the 
upper bound of D X is greater than the current year then we change it to the current year If the investigated authors died a few hundreds years ago then the upper bounds would probably been much worse In general the results achieved by the contemporary friend constraints were worse than those achieved by the late constraints That might be due to the fact that there more posthumous citations than contemporary citations Table 3 Experimental results using different versions of the greedy constraints Group of cons Average of absolute differences in years Birth year Death year 13 42 17 30 43 42 26 30 37 58 33 67 13 04 15 54 Cons 8 9 Posthumous cons 10 12 Contemporary cons 1 2 4 5 Ave friend cons B X ave 10 11 D X ave 12 13 Table 3 presents the results achieved by the different versions of the greedy constraints section 2 2 The minimal averages of absolute differences in years for the birth and death years 13 04 and 15 54 respectively have been achieved by the greedy version of the average friend based constraints 
constraints 10 13 5 Summary Conclusions and Future Work To the best of our knowledge we are the first to investigate the estimation of the birth and death years of the authors using undated citations referring to them or written by them This investigation was performed on a special case of documents i e responsa where special writing rules are applied The estimation was based on the author s documents and documents of other authors whose birth and death years are known who refer to the discussed author or are mentioned by him To do so we formulate various kinds of iron clad heuristic and greedy constraints The best estimates have been achieved using the version of the average contemporary greedy constraints Regarding the estimation of the birth and death years of an author X it is important to point that citations mentioned by X or referring to X are more suitable to estimate the birth and death writing years of X rather than his real birth and death years This model might be applied with suitable changes to 
similar research problems that might be relevant for some historical legal or religious document collections Usually such documents include citations to previous documents of the same kind Estimating the Birth and Death Years of Authors of Undated Documents 149 We plan to improve the estimation of the birth and death years of authors by 1 Combining and testing new combinations of iron clad heuristic and greedy constraints 2 Improving existing constraints and or formulating new constraints e g statistical based constraints 3 Defining and applying heuristic constraints that take into account various details included in the responsa e g dates in case that they appear events names of people concepts special words and collocations that can be dated 4 Conducting additional experiments using many more responsa written by more authors is supposed to improve the estimates 5 Checking why the iron clad heuristic and greedy constraints tend to produce more positive differences and 6 Testing how much of an improvement we 
got from a correction of the upper bound of D x and how much we will at some point use it for a corpus with long dead authors Definition and application of additional kinds of constraints is planned 1 Constraints that are based on historical events mentioned in the documents and 2 Threegeneration constraints i e constraints that relate to biological or preceding relations e g grand son and grand student Another interesting future research is the disambiguation of ambiguous citations Acknowledgements The authors thank Simone Teufel for reviewing drafts of this article and offering many helpful comments and three anonymous reviewers for their reviews References 1 Choueka Y Conley E S Dagan I A Comprehensive Bilingual Word Alignment System Application to Disparate Languages Hebrew English In Veronis J ed Parallel Text Processing pp Using Temporal Cues for Segmenting Texts into Events Ludovic Jean Louis Romaric CEA LIST Vision and Content Engineering Laboratory Fontenay aux Roses F 92265 France ludovic jean 
louis romaric besancon olivier ferret cea fr Abstract One of the early application of Information Extraction motivated by the needs for intelligence tools is the detection of events in news articles But this detection may be difficult when news articles mention several occurrences of events of the same kind which is often done for comparison purposes We propose in this article new approaches to segment the text of news articles in units relative to only one event in order to help the identification of relevant information associated with the main event of the news We present two approaches that use statistical machine learning models HMM and CRF exploiting temporal information extracted from the texts as a basis for this segmentation The evaluation of these approaches in the domain of seismic events show that with a robust and generic approach we can achieve results at least as good as results obtained with a specialized heuristic approach Keywords Information extraction text segmentation temporal cues 1 
Introduction The detection of events has always been a major issue of Information Extraction It was already addressed in the Message Understanding Conferences MUC 6 and is still a subject of interest in more recent evaluations such as ACE Automatic Content Extraction 4 This detection is the trigger of the extraction process which aims at filling the slots of a template defining the typical information associated with a certain type of events In the domain of terrorist attacks for instance such process identifies the type of an attack bombing etc and extracts slot information such as its date its location or its target The content of these slots are typically named entities whereas events appear as a direct relation between named entities for example the Date September 1989 Date Hurricane Hugo Hurricane Hurricane as a verb or a verbal noun Hurricane Hurricane Hugo Hurricane struck Location South Carolina Location in Date 1989 Date or can extend beyond the scope of a sentence Most of the work about slot 
filling in Information Extraction focuses on the first two cases mainly because the identification of a relation between the mention of an event and a named entity often use lexico syntactic patterns or syntactic relations However as it was already underlined in 11 and later analyzed more H Loftsson E Using Temporal Cues for Segmenting Texts into Events 151 precisely in 22 a significant number of such relations can be identified only at the discourse level This fact was taken into account in some existing works mainly through coreference resolution 23 or by acquiring and using domainknowledge for guiding the slot filling process 21 8 In this article we tackle this issue through the means of discourse segmentation More specifically we propose to segment texts according to the events they refer to in order to narrow the span of text to explore for linking a named entity to an event mention As time is an important feature for discriminating events as illustrated by 18 we chose to perform this segmentation by 
relying on temporal cues The extraction of temporal information from texts has been widely studied in different fields of Natural Language Processing since this kind of information is useful for many applications In the field of Information Extraction temporal information is for instance use to find the ordering of events 5 16 or to identify their overlapping 18 In our work the dates associated with event mentions are useful for segmenting texts into events However each mention of an event doesn t necessarily appear with a date in the same sentence as it is illustrated by the following example 1 Hurricane Hurricane Hugo Hurricane in Date 1989 Date was a Level class 4 Level storm 2 Hurricane Hurricane Hugo Hurricane caused Damages 7 billion damage Damages in the Caribbean Sea and South Carolina In sentence 1 the date 1989 is linked to the event mention through syntactic dependencies whereas without named entity coreference resolution linking a date to the event mention in sentence 2 is not possible The 
problem of associating a date to an event is addressed for instance in 5 which proposes a set of heuristics for assigning time stamps to the events of a text by relying on three temporal cues in each sentence the presence of a date and the tenses of verbs more globally the date of the document In 7 each sentence is not assigned a date but a date associated with an event is propagated to an undated event according to the relations between them e g a cause consequence relation In relation to this problem 15 proposes a segmentation model that views texts as sequences of situations These situations are defined through three types of entities temporal entities locations and persons Moreover 15 distinguishes between texts with a simple structure and texts with a complex structure The first ones are centered on one event only that is considered from one viewpoint All entities in this case tend to contribute to the definition of this event The second type of texts refer to several events among which a main event can 
be distinguished together with subordinate events In this article we focus more particularly on the segmentation into events of texts with a complex structure We present the general principles of our work in Section 2 while Section 3 is dedicated to the way we apply these principles with machine learning techniques Finally we present in Section 4 the results of the evaluation of the method on French news articles in the seismic domain 152 L Jean Louis R 2 Principles and Objectives Event extraction as presented in this article takes place in a wider context of technology watch in which users are mainly interested in the most recent events In this context our goal is to synthesize from news articles the information about such recent events into a kind of dashboard1 However news articles often refer to several comparable events generally for pointing out the similarities and differences between a recent event and older ones In our case we are not interested in these older events and we consider them as a source 
of noise for extracting information about the main event of a news article We adopted a two step strategy to tackle this problem Fig 1 Segmentation of a text into events The work of this article mainly focuses on the segmentation of texts into events Following this perspective we adopted a representation of texts turned towards events a text is viewed as a sequence of sentences in which each sentence is characterized by the presence or the absence of an event2 In addition we also focus in this work on the identification of the named entities associated with the main event of a news article We have therefore decided not to differentiate one secondary event from another Thus we propose to classify sentences according to the following three categories 1 2 This approach is a little bit different from most works in the information extraction field in which information is searched for all the events of a certain type The hypothesis one sentence one event is a simplification but it is globally not too simplistic in 
our application domains Using Temporal Cues for Segmenting Texts into Events 153 3 Approaches for Segmenting Text into Events In this work we treat the problem of segmenting texts into events as a classification problem each sentence of the text must be associated with an event type As we also consider that the sequence of sentences contain valuable information for this classification and we want classify all sentences a graphical model of sequence annotation is particularly suited We describe in this section two standard sequence classification models for this task Hidden Markov Model HMM and Conditional Random Fields CRF 3 1 Text Preprocessing The segmentation itself uses a sentence representation composed of linguistic cues extracted from the text We therefore perform a linguistic analysis of the text consisting in tokenization sentence boundary detection Part Of Speech tagging verb tense analysis and named entity recognition This linguistic processing pipeline is implemented using the linguistic analyzer 
LIMA 1 3 2 HMM Model Hidden Markov Model 19 is a sequence classifier widely used in NLP for example named entity recognition POS tagging etc and has also been applied on text segmentation in particular for topic segmentation 24 HMM are stochastic models used to find an underlying sequence of hidden states from the sequence 154 L Jean Louis R of observable data In our case we want to find the sequence of events associated with a given text considered as a sentence sequence We assume that the segmentation is a Markov process which means that the state associated with the current sentence only depends on previous state we propose to use the temporal information verb tenses as observation and the event categories as hidden states The transition matrices are obtained from a corpus of manually annotated texts supervised learning An illustration of the HMM model used in our approach is presented in Figure 2 Fig 2 Illustration of text segmentation using HMM A constraint of the HMM is that for a given observation the 
sequence of states does not originally take into account dependencies between the current state and previous observations and does not allow to easily integrate several criteria without multiplying the number of observation and the amount of training data To avoid this constraint we also tested a CRF model presented in the following section 3 3 CRF Model Since they were introduced in 2001 Conditional Random Fields 13 have been widely and successfully used in several NLP fields In text segmentation and classification 9 have obtained good results applying CRF models to classify sentences contained in scientific abstracts into four categories objective methods results conclusions The main difference between HMM and CRF is that HMM aim at maximizing the joint probability P x y between a sequence of observations x and a sequence of state y whereas CRF compute conditional probability P y x in order to associate a sequence of states with the observations The advantage of the conditional approach is that it allows 
to represent the sequence of observation as a vector whose components are related to features attached to the observation These features allow to integrate more linguistic expert knowledge in the models A more formal definition of CRF is P y x 1 Z x exp F y x Using Temporal Cues for Segmenting Texts into Events 155 F y x i f y x i Z x y exp F y x where F y x is a vector whose components are the values of the features for the input sequence and is a vector of weights associated with the features and Z x is a normalizing factor which depends on all possible sequences of states To apply CRF to text segmentation we use the following features 4 Evaluation We present in this section the results obtained by applying statistical learning algorithms for segmenting texts into events The evaluation process is composed of two stages first an intrinsic evaluation of the segmentation approach are the identified segments correct and second a final evaluation on the intended application i e the impact of text segmentation 
on the extraction of the entities associated with the main event We compare the statistical based approaches to a domain specific heuristic HeurSeg and a paragraph based heuristic ParaSeg HeurSeg is used in an existing application dedicated to information extraction for seismic events It is mainly based on the presence and the value of dates different date values correspond to different text segments the main event segment being the one containing the most recent date and between two distinct dates the boundaries of the segment are set according to the presence of sentence and paragraph breaks and the presence of domain specific entities between the two dates ParaSeg determines event categories at a paragraph level a sentence is associated with the main event category if it belongs to the first two paragraphs otherwise the sub event category is chosen We notice that HMM and CRF use different features for their classification decisions HMM decision is only based on the sequence of verb tenses whereas CRF 
decision uses a richer set of features described in section 3 3 Therefore we also use a Maximum Entropy model MaxEnt 20 as comparative model for the CRF with the same set of features in order to confirm the interest of the information given by the sequence for the segmentation 156 L Jean Louis R From an implementation point of view we used a set of Python scripts together with several reference toolkit NLTK3 CRF 4 and MaxEnt5 respectively for HMM CRF and MaxEnt models 4 1 Corpus In order to evaluate our text segmentation approach we used a corpus of 501 French news articles concerning earthquake events The articles were collected between February 2008 and early September 2008 partly from the French Agence France Presse newswire 1 3 of the corpus and partly from Google News 2 3 of the corpus These articles deal with 142 distinct major earthquake events The corpus contains both articles with a simple structure only one event and articles with a complex structure several events 252 articles 50 report at least 
one secondary event The corpus has been manually annotated for named entities by domain analysts only for entities associated with the article main event6 Table 1 reports the categories of named entities associated with an earthquake event together with their distribution in the corpus The table shows that the distribution of named entities is not homogeneous many location names are found 947 whereas only few geographical coordinates Geo are present in the articles 30 As a consequence the overall performance of the latter category will be strongly influenced by a matched missed entity while the former entity category will not In order to evaluate our text segmentation approach we annotated a subset of the corpus composed of 140 articles with event segmentation information Most of the selected articles in the subset contain a main and a secondary event some short articles might not contain secondary event information Table 2 shows the distribution of event categories in the annotated subset The most 
represented event category is Main Event 70 which is consistent with the factual aspect of news articles The Secondary Event includes without distinction all earthquake events that are comparable to the Main Event notice that among the selected news the real number of distinct secondary events evoked in the article can go up to 4 Finally the annotated subset has on average 1 7 distinct secondary events mentioned per article 4 2 Intrinsic Evaluation of the Segmentation of Texts into Events This section presents the results in terms of precision and recall percentage denoted P and R for different text segmentation approaches These results where obtained through 5 fold cross validation on the annotated subset 140 news articles using 4 5 for training and the remaining 1 5 for testing We 3 4 5 6 http www nltk org http crfpp sourceforge net http webdocs cs ualberta ca lindek downloads htm Annotators could annotate several entities of the same type if they thought the entities were equally satisfactory as pieces of 
information related to the article main event for instance for location entities annotators could annotate both a city and a country name Using Temporal Cues for Segmenting Texts into Events 157 Table 1 Distribution of named entities in the annotated corpus 3306 named entities for 501 articles Entity type Event type Location Date Time Magnitude Damages Geo Number 499 947 470 345 484 531 30 Nature type of event earthquake tsunami place where the event occurred date when the event occurred time when the event occurred magnitude damages caused by the event geographical coordinates of the event Table 2 Distribution of the event categories in the annotated corpus 1659 annotated segments in 140 news articles Event type Number Percentage of annotated segments Main Event 1168 70 Secondary Event 287 17 Background 213 13 report in Table 3 the segmentation results for statistical and heuristic based approaches We first notice that both ParaSeg and HeurSeg are outperformed for the precision on Main Event by assigning 
all sentences to the most frequent category see Table 2 Results for ParaSeg and HeurSeg are comparable in terms of precision but ParaSeg achieves a poor score on recall which is explained by the fact that most articles contain short paragraphs and few sentences are associated with main event in this case Concerning the statistical segmentation models the HMM results prove that the unique verb tense criterion is not sufficient to discriminate all event categories while the main event category is correctly identified 83 0 recall and 93 6 precision secondary event and background categories are poorly identified and they can be useful for a general purpose Information Extraction task Results also demonstrate that performances for secondary event and background categories are improved with CRF and MaxEnt models better results with CRF which confirms the importance of considering the successive sentence categories while segmenting the text into events 4 3 Evaluation of Text Segmentation for Information Extraction 
The goal of the text segmentation presented here is to delimit text segments that refer to a single event category in order to link the relevant entities to the main event linking is made within a text segment In order to evaluate the impact of the segmentation on this linking task we used a simple heuristic for linking the entities to the main event based on the hypothesis that pieces of information contained in the articles are organized according to their importance in the newscast the most relevant pieces of information which are generally associated 158 L Jean Louis R Event type Main Event Secondary Event Background Table 4 Results for the entity to main event linking Entity type Damages Date Event type Geo Location Magnitude Time All NoSeg R P 83 5 77 9 38 4 35 9 82 1 81 6 86 7 96 3 41 0 40 9 93 5 93 0 61 0 51 2 66 6 63 5 ParaSeg R P 62 1 61 0 34 3 32 6 78 7 78 2 44 8 50 0 39 2 39 1 72 7 73 2 25 3 22 5 53 1 51 8 HeurSeg R P 76 3 74 4 69 3 65 0 79 3 78 8 66 7 74 1 56 0 55 9 86 2 85 9 56 4 49 2 71 0 68 6 
HMM R P 69 8 65 1 48 9 45 6 59 1 58 8 86 7 96 3 61 2 61 1 66 7 66 2 78 8 71 5 63 4 61 1 CRF Gold Std R P R P 80 2 75 3 76 7 73 5 64 4 60 1 89 5 86 9 76 7 76 2 85 6 85 6 83 3 92 6 100 0 100 0 57 4 57 3 86 4 86 4 86 7 86 1 93 4 93 4 63 4 55 5 92 2 91 3 71 7 68 8 87 5 86 3 with the main event generally appear before the subordinate pieces of information associated with either a secondary event or the background Following this idea we use the following linking heuristic for each entity category select as most relevant entity the first entity found in the segment Table 4 summarizes the results of the entity linking task while using heuristicbased and learning based approaches for the segmentation into events We also report as baseline the results of the entity linking without event segmentation NoSeg and with the reference segmentation on the 140 annotated articles Gold Std NoSeg achieves fairly good results for the entity linking process even higher than the HMM 2 7 F1 Measure These results are significantly 
improved by HeurSeg 6 2 F1 Measure compared to NoSeg which demonstrates the interest of segmentation for the entity linking task Even if there are variations depending on the type of entities the CRF model gives results equivalent to those obtained with the heuristic approach and even slightly better which is quite satisfactory since the CRF model is a generic method while the heuristic is explicitly dependent on the domain Finally the best results obtained by Gold Std also show that there is still room for improvement using a smarter entity linking strategy 5 Related Work The work we have presented in this article focuses on text segmentation with two main characteristics this segmentation is performed according to the event Using Temporal Cues for Segmenting Texts into Events 159 type of sentences and relies on temporal cues The use of temporal cues for discourse segmentation was mainly explored by linguistic and psycho linguistic works through the study of the role of clause initial temporal adverbials as 
segmentation markers From the psycholinguistic viewpoint 2 showed that clauseinitial temporal adverbials are correlated with topic shifts whereas from a more linguistic viewpoint 10 uncovered a more complex situation where the role of clause initial temporal adverbials is text type dependent Our use of temporal cues for text segmentation is both more crude and more extensive we mainly rely on the sequence of grammatical tenses and we use other temporal cues such as the presence of dates or temporal adverbials as features for identifying more accurately secondary event and background sentences Segmenting texts according to their events was also addressed by some few works In 12 this segmentation was mainly based the identification in texts of the components of a type of events defined by a priori domain knowledge However two typical discourse structures of the news articles they considered were also taken into account for this segmentation one is made of a sequence of different events while the other one is 
structured as in our case round a main event with several mentions with references to minor events 3 also heavily based its segmentation of texts on the identification the components of a priori template but tested several discourse inspired heuristics for assigning a clause to an event as for instance favoring the event of the most recently assigned clause Finally the closest work to ours from this viewpoint is 17 which tagged sentences with four event labels new event continuing event back reference to an earlier event no reference to an event These labels are close to our three event types except that we don t distinguish the introduction of a new event from its continuation The model used in 17 is a probabilistic Finite State Automaton relying on the MDI algorithm for its learning part While this work aims at modelling the discourse structure of texts from the viewpoint of events it differs from ours in that it directly models sequences of event labels and don t exploit temporal information 17 showed 
that its segmentation had a positive impact on its final task i e grouping sentences in news articles that refer to the same event but didn t report results for only segmentation 6 Conclusion and Future Work The aim of our work is to segment texts into events in order to make easier the linking of relevant entities to the main event of a news article In our approach we addressed this segmentation problem as a sequence classification task where the goal is to determine the type of event associated with each sentence We proposed and evaluated three models a HMM model which uses as single decision criterion the succession of tenses in a text a MaxEnt model which integrates additional temporal cues temporal expressions dates into its decision and a CRF model which integrates the same features as MaxEnt in addition to the dependencies between the successive event types While conducting comparative experiments of the models on a corpus of news articles concerning earthquake 160 L Jean Louis R events we have shown 
that the CRF model provides the best results for the segmentation of texts into events In addition we tested the impact of segmenting the texts into events on the identification of the entities related to the main event and we showed that CRF which is a learning based model achieves equivalent results and even slightly better than those obtained with a heuristic based approach by using a much more generic approach Regarding the generalization of the approach we have obtained encouraging results for initial tests by applying the models on an English corpus of news articles using the models learned from French Concerning the application domain we used a corpus of news articles related to the earthquake field in which information is quite structured Nevertheless we believe our approach can provide reasonable results in other areas which we are planning to experiment in the near future Finally a detailed error analysis on the test corpus showed that the process of linking the entities to the main event is a 
major source of error as we are currently using a simplistic heuristic We will therefore focus our future research on this issue by using both entity density criteria and linguistic criteria e g explicit syntactic dependencies between entities References 1 Using Temporal Cues for Segmenting Texts into Events 161 10 Ho Dac L M Enriching the Adjective Domain in the Japanese WordNet Kyoko Kanzaki1 Francis Bond2 Takayuki Kuribayashi1 and Hitoshi Isahara1 1 National Institute of Information and Communications Technology 3 5 Hikaridai Seikacho Sorakugun Kyoto 619 0289 Japan kanzaki kuribayashi isahara nict go jp 2 Nanyang Technology University 14 Nanyang Drive Singapore 637332 bond ieee org Abstract We released Japanese WordNet Version 1 0 in March 2010 and are continuing to enrich the Japanese WordNet in several directions The current version of the Japanese WordNet is a kind of translation of Princeton WordNet 3 0 and we used WordNets of multiple languages in order to disambiguate Japanese translations Although 
the structure is based on Princeton WordNet 3 0 some information is still insufficient in our Japanese WordNet For example the adjective domain lacks semantic relations such as antonyms attributes and so on As part of our ongoing work to enrich the Japanese WordNet we are investigating attribute nouns for which adjectives express values Keywords WordNet Adjective Japanese 1 Japanese WordNet We are building a Japanese version of WordNet and released version 1 0 in March 2010 The WordNet project at Princeton has been a resounding success creating a resource that is widely used in research and has been emulated in many languages 7 In order for a lexical resource to be widely adopted it must be both accessible and usable The Princeton WordNet is accessible because it is released under a nonrestrictive license and it is usable because it has not just precise information but also reasonable coverage especially of common words There have been several initiatives to create a Japanese WordNet but none of them has yet 
produced something that is both accessible and usable The large amount of previous work shows the great interest and value of producing a Japanese WordNet We therefore decided to construct one as follows 1 First automatically translate the Princeton WordNet into Japanese All relations e g hypernyms and meronyms come from Princeton WordNet 3 0 Second the most frequent 20 000 synsets are manually checked Third the synsets are linked to a corpus Fourth the data are released under an open license The Japanese WordNet is based on the structure of the English WordNet Japanese near synonyms are added to the existing English synsets For example one of the English synsets consisting of seal has the explanation any of numerous marine H Loftsson E Enriching the Adjective Domain in the Japanese WordNet 163 mammals that come on shore to breed chiefly of cold regions and has the following azarashi and azarashi Japanese words associated with it A statistical summary of the contents of WordNet 3 0 and Japanese WordNet 1 0 
is shown in Table 1 Table 1 Statistics for WordNet 3 0 and Japanese WordNet 1 0 from websites 9 and 10 Unique strings Synsets Senses word sense pair in English synset word pair in Japanese WordNet 3 0 155 287 117 659 206 941 Japanese WordNet 1 0 92 241 56 741 157 398 2 Creating the Japanese WordNet Our approach to building the Japanese WordNet is the standard expansion approach translate WordNet synsets to another language and take over the structure 8 We did this both to keep a compatible structure with WordNet and because we had access to a variety of resources that would make the task easier Our main innovation is that we are using WordNets in multiple languages to disambiguate the Japanese translations thus providing more reliable estimates The English Japanese lexicon has two translations for bat i e koumori mammal and batto club However because there is no way of distinguishing between them we get a mixture of meanings with koumori bat n 1 and batto bat n 5 Chiropteran bat n 1 is not in any of the 
English Japanese lexicons we used and bat n 5 has no synonyms Therefore using only the English WordNet as source and English Japanese lexicons there is no way to disambiguate them However both synsets are also in the French WordNet bat n 1 is chauve souris and bat n 5 is batte gourdin These are not ambiguous in the same way chauvesouris goes only to koumori and batte only to batto Thus if we can match through two languages the mapping is much more likely to be the correct sense The actual algorithm we used was as follows For each synset in WordNet 3 0 164 K Kanzaki et al being linked in multiple languages Thus we built and released Japanese WordNet 1 0 but many improvements remain to be made 3 Adjective Domain in WordNet 3 0 and Japanese WordNet As for the adjective domain basic adjectives were translated in our Japanese WordNet Table 2 shows a statistical comparison of the adjective domain between WordNet 3 0 and Japanese WordNet However the adjective domain in the Japanese WordNet is still insufficient 
compared to WordNet 3 0 Table 2 A statistical comparison of the adjective domain WordNet 3 0 Japanese WordNet Unique strings 21 479 4 494 Synsets 18 156 8 915 Word sense pairs 30 002 17 679 According to WordNet 3 0 adjectives are arranged in clusters containing head synsets and satellite synsets Each cluster is organized around antonymous pairs and occasionally antonymous triplets The antonymous pairs or triplets are indicated in the head synsets of a cluster Most head synsets have one or more satellite synsets each of which represents a concept that is similar in meaning to the concept represented by the head synset In the Japanese WordNet head synsets and satellite synsets are not clearly distinguished As shown in the following examples adjectives in WordNet 3 0 have several semantic pointers such as antonym attribute similar to derivationally related form also see and pertainym by which adjectives are related with other synsets i e not only other adjectives but also other parts of speech like nouns and 
verbs In the Japanese WordNet they have not been created yet These useful relations between synsets or words across a part of speech should be introduced in the Japanese WordNet Examples 1 refers to a pointer and refers to a synset Pointer antonym 01661289 a good right ripe 4 Enriching the Adjective Domain in the Japanese In WordNet 3 0 through the pointer attribute an adjective is linked to a noun for which an adjective expresses its value Such nouns called attributive nouns are linked to their hypernyms via a pointer inherited hypernym As an example hypernyms for abundant are shown below The hypernyms quantity amount Enriching the Adjective Domain in the Japanese WordNet 165 magnitude property attribute abstraction and entity can be identified via the pointer attribute and inherited hypernym Example 2 An example of inherited hypernym for abundant teeburu ga yasui nedan ni natta table cheap price became The price of a table became a cheap price b teeburu ga yasuku natta table cheap in adverbial form became 
A table became cheap In the above examples the meaning of yasuku naru become cheap is equal to yasui nedan ni naru become a cheap low price that is nedan price can be omitted when the adnominal usage of yasui 166 K Kanzaki et al cheap in adnominal form changes to the adverbial form yasuku cheap in adverbial form In this case nedan price is a meaning implied by yasuku in adverbial usage cheap 5 Future Direction As shown in the above sections both the English and Japanese WordNet are not sufficient in the adjective domain e g an attribute concept of cheap does not exist There are several methods for finding categories of words 2 3 4 5 6 These methods showed good results for generating hypernym concepts mainly from nouns and verbs We have to develop a method useful for finding relations between attribute concepts and adjectives As a future direction we will contribute to the development of the English and Japanese WordNet by considering what are attribute concepts for adjectives Acknowledgment We are grateful 
for discussion and resources from the Kyoto Project which is co funded by References 1 Bond F Isahara H Kanzaki K Uchimoto K Boot strapping a WordNet using multiple existing WordNets In 6th International Conference on Language Resources and Evaluation LREC 2008 pp Comparing SMT Methods for Automatic Generation of Pronunciation Variants Panagiota Karanasou and Lori Lamel Spoken Language Processing Group LIMSI CNRS 91403 Orsay France pkaran lamel limsi fr Abstract Multiple pronunciation dictionaries are often used by automatic speech recognition systems in order to account for different speaking styles In this paper two methods based on statistical machine translation SMT are used to generate multiple pronunciations from the canonical pronunciation of a word In the first method a machine translation tool is used to perform phoneme to phoneme p2p conversion and derive variants from a given canonical pronunciation The second method is based on a pivot method proposed for the paraphrase extraction task The two 
methods are compared under different training conditions which allow single or multiple pronunciations in the training set and their performance is evaluated in terms of recall and precision measures Keywords P2P conversion pronunciation lexicon SMT Moses pivot paraphrasing 1 Introduction Pronunciation variation is one of the factors that influences the performance of an automatic speech recognition ASR system especially for spontaneous speech Predicting pronunciation variations that is alternative pronunciations observed for a linguistically identical word is a complicated problem and depends on a number of factors such as the linguistic origin of the speaker the speaker s education and so cio economic level the speaking style and conversational context and the relationship between interlocutors The construction of a goo d pronunciation dictionary is thus critical to ensure acceptable ASR performance 8 Moreover the number of pronunciation variants that need to be included in a dictionary depends on the 
system configuration 1 A variety of methods have been proposed in order to obtain pronunciation variants from the canonical pronunciations baseforms of words and can be broadly grouped into data based and knowledge based metho ds Knowledgebased metho ds using phonological rules 3 17 require specific linguistic skills are not language independent and do not always capture the irregularities in natural languages By contrast the data driven approaches are based on the idea that given enough examples it should be possible to predict the pronunciation of unseen words in the grapheme to phoneme task or generate multiple H Loftsson E 168 P Karanasou and L Lamel pronunciations for improved speech recognition In this paper the latter task of generation of pronunciation variations is addressed using data based metho ds Other data based metho ds proposed in the literature for the mo deling of pronunciation variations include the use of neural networks 4 confusion tables 14 and decision trees 16 or automatic generation 
of rules for phoneme to phoneme conversion 6 All these methods predict a phoneme for each input symbol using the input symbol and its context as features but ignore any structure in the output The metho ds proposed in this paper take advantage of both the input and the output context and can predict variable length phoneme sequences The two methods presented in this paper aim to automatically generate pronunciation variants of words for which a canonical pronunciation is available The first metho d is based on the simple use of Moses 7 a publicly available phrasebased statistical machine translation tool as a phoneme to phoneme converter to generate an n best list of pronunciation variants The second metho d is based on a paraphrase metho d that uses bilingual parallel corpora and is founded on the idea that paraphrases in one language can be identified using a phrase in another language as a pivot In the case of multiple pronunciation generation sequences of modified phonemes found in pronunciation variants 
are identified using a sequence of graphemes in the corresponding word as a pivot The paper is organized as follows Section 2 describes the two metho ds used in this study Section 3 describes the experimental framework and details about the corpora used and the training conditions applied Section 4 presents the evaluation results of the automatic generation of multiple pronunciations in terms of precision and recall Conclusions and some discussions for future work are reported in Section 5 2 Phoneme to Phoneme Conversion This section presents the two proposed methods in detail and compares their strengths and weaknesses pointing out their utility in different situations Both metho ds aim to produce pronunciation variants of the initial canonical phonemic transcription Since the canonical pronunciations are not explicitly indicated in the master lexicon see Section 3 1 the longest one is taken as the canonical form since the reduced forms often correspond to variants found in conversational speech The first 
method generates pronunciation variants using only the phonemic transcriptions of words while the second metho d makes use of both the orthographic and phonemic transcriptions and thereby permits to the system to also benefit from the information provided by the orthographic transcription of a word As we will see later in the results analysis this last characteristic of the second metho d is particularly useful under certain training conditions 2 1 Moses as a Phoneme to Phoneme Converter Moses has already been proposed for the grapheme to phoneme g2p conversion 5 9 task A pronunciation dictionary is used in the place of an aligned bilingual text corpora The orthographic transcription is considered as the source Comparing SMT Methods for Automatic Generation 169 language and the pronunciation as the target language In the case that the pronunciation dictionary has a reasonable coverage of the language of interest this metho d can be successfully used for g2p conversion because it has all the desired 
properties of a g2p system To predict a phoneme from a grapheme it takes into account the local context of the input word from a phrase based mo del and allows sub strings of graphemes to generate phonemes The phoneme sequence information is mo deled by a phoneme n gram language mo del that corresponds to the target language model in machine translation More technical details on the Moses components will be given in the Section 3 2 These properties are also desired for a p2p converter which has moreover higher potential for capturing pronunciation variation phenomena in languages like English where orthography and pronunciation generally have a looser relationship than in other languages A second direction explored in this work is based on the idea of seeing the use of SMT tools with a monolingual corpora for paraphrase generation 11 as being analogous to generating pronunciation variants These similar approaches to two distinct problems led us to the idea of trying to use Moses for p2p conversion In this 
case the source language and the target language are aligned phonemic transcriptions As the source language we define the canonical pronunciation the longest one1 and as target language itself and or its variants depending on their existence or not in the different versions of the training set as presented in the Section 3 1 2 2 Pivot Paraphrasing Approach This metho d is based on the one presented in 2 Paraphrases are alternative ways of conveying the same information We can easily see the analogy with multiple pronunciations of the same word The multiple pronunciations are alternative phonemic expressions of the same orthographic information In 2 a bilingual parallel corpus is used to show how paraphrases in one language can be identified using a phrase in another language as a pivot In the problem of automatic generation of pronunciation variants a corpus of wordpronunciation pairs is used as the analogy of the aligned bilingual corpus instead of the pronunciation pronunciation aligned corpus in the 
previous metho d The idea is to define a paraphrase pronunciation variant probability that allows paraphrases pronunciation variant sequences extracted from a bilingual parallel corpus to be ranked using translation probabilities and then rerank the generated pronunciation variants taking the contextual information into account The translation table that is used is extracted by Moses In 2 the authors look at the English translation of foreign language phrases find all occurrences of those foreign phrases and then look back to determine to what other English phrases they correspond The other English phrases are seen as potential paraphrases In the pronunciation generation case we look at all the entries in the translation table find the sequences of graphemes to which a sequence of phonemes is translated and then look back to what other sequences of phonemes the particular sequence of graphemes is translated 1 Most of the variants reflect reduced pronunciations found in casual speech 170 P Karanasou and L 
Lamel Phrase alignments in a parallel corpus are used as pivots between English pronunciation paraphrases These two way alignments are found using recent phrase based approaches to statistical machine translation In the following definitions f is a graphemic sequence and e1 and e2 are phonemic sequences The paraphrase probability p e2 e1 is assigned in terms of the translation mo del probabilities p f e1 and p e2 f Since e1 can be translated as multiple foreign language phrases graphemic sequences we sum over f e 2 arg max p e2 e1 e2 e1 e2 e1 1 2 arg max p f e1 p e2 f f This returns the single best paraphrase e 2 irrespective of the context in which e1 appears Since the best paraphrase may depend on information about the sentence that e1 appears in the paraphrase probability can be extended to include the sentence S e 2 arg max p e2 e1 S 3 e2 e1 This allows the candidate paraphrases to be ranked based on additional contextual information in the sentence S A simple language mo del probability is included 
which can additionally rank e2 based on the probability of the sentence formed by substituting e2 for e1 in S The language mo del is trained on the correct pronunciations of the training set For the reranking based on the language model we use the SRI toolkit 13 Finally some more pruning is done on the reranked list keeping the maximum of ten five or one pronunciation variants per canonical pronunciation without changing the order of the elements of the reranked list An example of a paraphrase pattern in the pronunciation dictionary is2 discounted dIskWntxd discounted dIskWnxd discountenance dIskWnNxns discountenance dIskWntNxns The alternative pronunciations differ only in the part that can be realized as either nt or n while the rest remains the same 3 3 1 Experiments Corpus The LIMSI Master American English dictionary serves as basis of this work It is a pronunciation dictionary with 187975 word entries excluding words starting with numbers with on average 1 2 pronunciations per word The pronunciations 
are represented using a set of 45 phones 8 each phone corresponding to a 2 The phone set used is given in Table 1 of 8 Comparing SMT Methods for Automatic Generation 171 single character The dictionary has been created with extensive manual supervision Each dictionary entry has the orthographic transcription of a word and its pronunciations one or more 18 of the words are asso ciated with multiple pronunciations The ma jority of words have only one pronunciation leaving it to the acoustic mo del to represent the observed variants in the training set that are due to allophonic differences Moreover since the dictionary is mostly manually constructed it is certainly incomplete with respect to coverage of pronunciation variants particularly for uncommon words The pronunciations of words of foreign origin mostly proper names may also be incomplete since their pronunciation depends highly on the speaker s knowledge of the language of origin This means that some of the automatically generated variants are likely to 
be correct or plausible even if they are not in the current version of the Master dictionary Case distinction is eliminated since in general it do es not influence the word s pronunciation the main exceptions being the few acronyms which have a spoken and spelled form Some symbols in the graphemic form are not pronounced such as the hyphen in compound words The dictionary contains a mix of common words acronyms and proper names It should be noted that these last categories are difficult cases for g2p or p2p converters and particular effort has been made to pronounce proper names in text to speech synthesis technology 12 The corpus is randomly split into a training a development dev and a test set The dev set is necessary for the optimisation of the weights of Moses mo del as will be later explained tuning and the test set is used for the evaluation of the system This division is based on dictionary entries so that all the pronunciations of a given word will be in the same set If not we would have the paradox 
of training the system with certain pronunciations and asking it to generate only the different pronunciations of the same word found in the test set The dev set has 9000 entries and the test set 16000 entries The original dictionary entries of training dev and test sets were transformed to have one graphemic transcription pronunciation pair per entry as opposed to one entry corresponding to the graphemic transcription of a word with all its pronunciation variants This is to have a format that resembles the aligned parallel texts used for training machine translation mo dels After transformation the dev and test sets have 11196 and 19782 distinct entries All the results are calculated for the same test set so that their comparison is legitimate Three different training conditions are compared for the two p2p systems 1 Train on the entire dictionary Words may have one or more pronunciations tr set 2 Train only on words with two or more pronunciation variants All words have multiple pronunciations tr set m 3 
Train on the entire dictionary using only the longest canonical pronunciation to have one pronunciation per word tr set l At this point a further preparation of the training set for each metho d is required For the metho d where Moses is used as a p2p converter a monolingual 172 P Karanasou and L Lamel parallel corpus is needed meaning that both the source language and the target language will have phonemes as elements The source language is always formed by the canonical pronunciation segmented into phonemes The target language is formed of the corresponding pronunciations depending on the training condition For the pivot metho d the training set is used as a parallel corpus with one graphemic transcription pronunciation pair per line with spaces separating characters in order to use Moses as in a g2p task to generate a translation table that will be used to extract paraphrased sequences Each word is a source sentence with each grapheme being an element of the source sentence and each pronunciation is a 
target sentence with each phoneme forming an element of the target sentence Table 1 gives an overview of the data sets used with the number of entries distinct pairs and the average number of pronunciations word in the three training conditions after prepro cessing Table 1 Training conditions Training set Number of entries Average number prons word tr set 201423 1 2 tr set m 67769 2 3 tr set l 162974 1 0 It can be seen in Table 1 that there are large differences for the three training conditions For tr set m the number of entries diminishes to one third of the original dictionary However the number of pronunciations per word almost doubles In this case the extra information given by the canonical pronunciations of words with only one pronunciation is lost but we allow the systems to change the frequency relationship between the phrases of the canonical pronunciations and the phrases found in pronunciation variants and see how this influences the generation of pronunciation variants which is our main interest 
in this work In the third training condition only the canonical pronunciation of each word is kept in the training data This allows us to see if pronunciation variants can be generated even under limited training conditions For example this condition corresponds to generating variants from the output of a rule based g2p system which if originally developed for speech synthesis may not mo del pronunciation variants or to enriching a dictionary with limited pronunciation variants 3 2 System The system that is used to train the mo dels in both metho ds is based on Moses In the first proposed metho d Moses is used as a p2p converter in an one stage pro cedure Besides the phrase translation table a phoneme based 5 gram language mo del is built on the pronunciations in the training set using the SRI toolkit 13 Moses also calculates a distortion mo del but our dictionary do es not include a sufficient number of metathesis cases so the monotonic deco ding Comparing SMT Methods for Automatic Generation 173 does not 
change the final results Finally the combination of all components is fully optimized with a minimum error training step tuning on the dev set The tuning strategy we used was the standard Moses training framework based on the maximization of the BLEU score 10 The optimized weights generated by tuning are added to the configuration file Moses can also provide an n best translation list This list gives the n best translations of a source string with the distortion the translation and the language mo del weights as well as an overall score for each translation As stated earlier we keep only the 1 5 or 10 best translations i e pronunciation variants per canonical pronunciation Some pronunciations have fewer possible variants in which case all variants are taken In the pivot metho d generating pronunciation variants is a four stage pro cedure Moses is used in the first stage for g2p conversion and extraction of the translation table In the second stage the paraphrased pairs with their probabilities are extracted 
from the canonical pronunciations of the test set as previously described The 10 best paraphrases for each input phonemic sequence are extracted with a maximum length of 3 for the extracted paraphrases In the third stage the paraphrases are substituted in the canonical pronunciations of the test set for all their o ccurrences with all the possible combinations only in the first o ccurrence only in the second o ccurrence in the first and in the second o ccurrence etc limiting to 3 the maximum number of o ccurrences of the same paraphrase in a canonical pronunciation In the fourth and final stage the generated list of pronunciation variants is reranked based on the context The context is expressed by the same phoneme based 5 gram language mo del used in the first method The SRI toolkit is used to rerank the multiple pronunciation n best list modifying its probabilities As for the first metho d the 1 5 or 10 best variants are kept for each canonical pronunciation in the ordered list 4 Evaluation Different 
measures have been proposed to evaluate the predictions of pronunciation variants derived from the original canonical form The most frequently used are precision and recall first introduced in information retrieval 15 The canonical pronunciations xi of the test set can have one or more variants yi yi is a set Moreover our systems can generate one or more variants f xi f xi is also a set Thus the recall that corresponds to a couple yi f xi is the number of correct generated variants for a canonical pronunciation in the test set divided by the number of correct variants given in the test set for this canonical pronunciation f xi yi 4 ri yi The precision is the number of correct generated variants divided by the number of generated variants f xi yi 5 pi f xi 174 P Karanasou and L Lamel The total recall is the mean value of the recall of each example r 1 n n ri i 1 6 Analogously the total precision is the mean value of the precision of each example We refer to the previous definitions as micro recall and micro 
precision respectively If the examples are normalized by the number of expected variants correct variants in the reference the total recall becomes r n i 1 ri yi n i 1 yi 7 In this last case the macro recall is defined Macro precision is defined analogously The macro measures give more weight to the examples with multiple variants while the micro measures consider all the examples equally weighted It is important to do the evaluation on a pair level and not just consider the error rate of generated pronunciations to avoid counting as correct a generated pronunciation that do es not correspond to the canonical pronunciation it is associated with in the reference There is always the possibility that our system will generate a pronunciation out of a baseform i e canonical pronunciation that is not a variant of this baseform but however is a correct variant of another baseform This is counted as a false generation Another thing that should be noted is that we prune the canonical pronunciation pronunciation 
variant pairs that do not include a new variant This is important in order to improve the precision because while the pivot metho d generates only pronunciation variants when Moses is used as p2p converter it often outputs the canonical pronunciation that was used as input because it learns from the training data that the most probable pronunciation corresponding to a given canonical pronunciation is usually itself This depends a lot on the training conditions and makes this metho d inappropriate in certain cases To control the precision of our systems an upper limit is put to the number of n best variants that are kept in the hypotheses The 1 5 and 10 best variants per canonical pronunciation are generated consecutively The n best list is limited to 10 because preliminary studies showed that larger n only slightly improves recall while severely degrades precision There is quite a bit of overgeneration since in the 19k pronunciation pronunciation pairs of the test set there are only 4k pairs with 
pronunciation variants This could not be avoided with a random selection of the test set from the original dictionary where only 18 of words have variants as already stated However there is the possibility that some of the generated variants which are not in the reference and therefore counted as errors could be considered acceptable by a human judge Evaluating the system by an automatic measure cannot take into account the potential lack of coverage of the reference dictionary The two systems Moses as phoneme to phoneme converter m p2p and the pivot paraphrasing method p p2p were tested for the 3 training conditions presented earlier The results using the two proposed evaluation metrics are Comparing SMT Methods for Automatic Generation 175 Table 2 Results using Moses as phoneme to phoneme converter for the 3 training conditions Training set Measure tr set Micro recall Macro recall tr set m Micro recall Macro recall tr set l Micro recall Macro recall 1 best 0 20 0 19 0 21 0 Table 3 Results using the pivot 
paraphrasing method for the 3 training conditions Training set Measure tr set Micro recall Macro recall tr set m Micro recall Macro recall tr set l Micro recall Macro recall 1 best 0 29 0 26 0 25 0 22 0 09 0 09 5 best 10 best 0 60 0 70 0 56 0 66 0 56 0 70 0 53 0 66 0 26 0 38 0 24 0 35 shown in Tables 2 and 3 respectively We only present recall measures in the tables because this is what is of most interest in the particular task It is more important to cover possible pronunciations than to have too many since other metho ds can be applied to reduce the overgeneration alignment with audio manual selection use of pronunciation probabilities etc The best value that both precision and recall can obtain is 1 However the best value of precision is often further limited depending upon the number of elements of the n best list and the overgeneration that cannot be avoided As can be expected for both methods the number of correctly generated variants increases with the size of the n best list This is normal not only 
because the number of hypothesis increases with the size of the n best list but also because there are canonical pronunciations in the test set that have more than one variant approximately 1 6 of the part of the test set with multiple pronunciations which cannot be captured when an insufficient number of pronunciations is generated It is also interesting to compare the results of the first and the second training conditions whole dictionary vs keeping only entries with multiple variants for the two metho ds In the second case the amount of training data is only one third of the original training set However the results are more or less the same for both training conditions This may be because the information that the mo del is using to learn how to generate variants is mostly captured by the multiple pronunciations in the training set and less by the fewer variations observed in the canonical pronunciations of one pronunciation words What the mo del is learning in this case is fo cused on the relationship 
between the canonical pronunciation and other variants and therefore has effectively more relevant information and it does not get watered down by the self pro duction This may compensate for the reduced amount of data 176 P Karanasou and L Lamel A comparative analysis of the two methods can also be made In the first whole training set and in the second only entries with variants training conditions using Moses as a p2p converter gives better results in terms of the generation of pronunciation variants for both micro and macro measures when the 5 best and the 10 best variants are kept However when only the 1 best generated pronunciation is kept the pivot metho d gives better results This is due to the generation of canonical pronunciations by Moses when used as p2p converter which are subsequently removed from the results because they already exist in the input The number of variants generated by Moses p2p when only the 1 best is kept are quite limited This is why while the recall is lower than that of the 
pivot method the precision is higher It can be seen that the results change when the training set is limited to the canonical pronunciations only the third condition In this case the pivot metho d manages to pro duce some results while for Moses the mo del fails to generate any variants this is why the corresponding columns are left empty in Table 2 and all the variants that are in the 5 best or the 10 best lists are false These results warrant a bit more discussion The results are promising for the pivot metho d because even when the training dictionary has few or no pronunciation variants the pivot metho d can still be used to generate some alternative pronunciations This can be explained by the fact that the pivot metho d uses also the graphemic information Even if no variants are included in the training set it can still find graphemic sequences of words that correspond to different phonemic sequences and consider these phonemic sequences as possible mo difications of pronunciations For example in the 
training set the word autoroute is pronounced ctorut and the word shouting is pronounced sWtIG These words have the graphemic sequence out in common which can be used as a pivot between the phonemic sequences ut and Wt These phonemic sequences become a paraphrased pair that generates correctly the variants rWts and ruts of the word routes found in the test set This illustrates the difficulty of generating pronunciations in English because the correspondence between orthographic forms and canonical pronunciations do es not follow strict rules which would prevent the pivot metho d from finding mo dified phonemic sequences corresponding to the same graphemic sequence This is not the case when Moses is used as phoneme to phoneme converter When no variants are given to the system it do es not have any additional information in order to be trained for the task of generating multiple pronunciations It is like trying to train an SMT system without a target language It can just learn to align the phonemic sequences 
with themselves which is fine for the g2p task but is not applicable to the generation of variants In this case is it wrong to use Moses for this task as it is obvious that it has nothing to learn from the training data 5 Conclusions This paper has reported on applying two data based approaches inspired from research in statistical machine translation to the problem of generating pronunciation variants One of the ob jectives of this work was to compare these Comparing SMT Methods for Automatic Generation 177 two approaches to mo deling pronunciation variations The approaches differ in the way that information about pronunciation variation is obtained The approach using Moses as phoneme to phoneme converter takes into account only the information provided by the phonemic transcriptions The pivot metho d uses information from both the phonemic and the orthographic transcriptions When the full dictionary that contains words with one or more pronunciations is used for training the Moses based metho d gives 
better results than the pivot based one This is also the case when training is carried out on only entries with multiple pronunciations However when the training dictionary do es not contain any pronunciation variants the Moses based metho d cannot be used while pivot can still learn to generate variants This is an advantage of the pivot metho d and could be useful for languages without well developed multiplepronunciation dictionaries This arises from the use of information provided by the orthographic transcription by the pivot metho d An interesting follow up study is to use the pivot metho d to propose variants as a post pro cessing step to a g2p system Another case to study is the influence of the p2p converter on the results of a g2p converter if their output n best lists are combined We have started some preliminary experiments in these directions with promising results In future work we will evaluate the proposed metho ds at generating pronunciations and variants for proper names which are the most 
difficult cases to handle These also account for the ma jority of words that need to be added to a dictionary once a reasonably sized one is available for the given language Another important outstanding issue concerns the proper way to evaluate the ability of a system to generate pronunciation variants In this work recall and precision have been used however other measures such as phoneme accuracy can also be applied In this case it may be appropriate to have phone class dependent penalties with certain confusions being more important than others In order to improve the precision the n best lists need to be more heavily pruned One direction to explore is using audio data to remove pronunciations however this can only apply to words found in the audio data The ultimate test of course is how the variants affect the accuracy of a speechto text transcription system Acknowledgments This work is partly realized as part of the Quaero Programme funded by OSEO French State agency for innovation and by the ANR EdyLex 
project References 1 Adda Decker M Lamel L Pronunciation variants across system configuration language and speaking style Speech Communication 29 178 P Karanasou and L Lamel 4 Fukada T Yoshimura T Sagisaka Y Automatic generation of multiple pronunciations based on neural networks Speech Communication 27 1 Automatic Learning of Discourse Relations in Swedish Using Cue Phrases Stefan Karlsson and Pierre Nugues Lund University Lund Institute of Technology Department of Computer Science Box 118 S 221 00 Lund Sweden stefan karlsson 342 student lth se Pierre Nugues cs lth se Abstract This paper describes experiments to extract discourse relations holding between two text spans in Swedish We considered three relation types cause explanation evidence CEV contrast and elaboration and we extracted word pairs eliciting these relations We determined a list of Swedish cue phrases marking explicitly the relations and we learned the word pairs automatically from a corpus of 60 million words We evaluated the method by 
building two way classifiers and we obtained the results Contrast vs Other 67 9 CEV vs Other 57 7 and Elaboration vs Other 52 2 The conclusion is that this technique possibly with improvements or modifications seems usable to capture discourse relations in Swedish Keywords rhetorical relations discourse relations cue phrases 1 Introduction Rhetorical relations and the Rhetorical structure theory 1 form a framework to describe and interpret the organization of a text In this theory relations consist of annotated links tying two text spans as for example the clauses in the sentence Malaria H Loftsson E 180 S Karlsson and P Nugues Initially the newspaper was published once a week but in Dec 1850 it was transformed into a daily 2 Uggleupplagan vol 3 1157 Rhetorical relations can be associated with certain cue words or phrases such as 2 A Statistical Model Some word pairs are frequent in contrasts hypothetically for example week and daily as in the example above and other pairs in explanations i e exists and 
develops Instead of extracting relations with manual rules we can try to derive automatically sets of words involved in specific relations from corpora Marcu and Echihabi proposed an unsupervised method 5 to train Using the Automatic Learning of Discourse Relations in Swedish Using Cue Phrases 181 3 3 1 Experimental Setup Extraction of Text Spans We considered three discourse relations cause explanation evidence CEV contrast and elaboration We compiled a Swedish corpus using texts from the Runeberg project 45 million words and the European Parliament proceedings 6 16 million words a multilingual corpus where we used the Swedish source parts We then inspected the corpus manually and incrementally built the extraction patterns shown in Table 1 Table 1 Swedish extraction patterns used in the experiments BOS indicates the beginning of the sentence and EOS the end of the sentence Contrast BOS BOS BOS BOS men EOS ehuru EOS Cause Explanation Evidence BOS BOS BOS BOS BOS BOS BOS BOS BOS BOS BOS Elaboration BOS 
vilket EOS BOS hvilket EOS The Nordisk Familjebok encyclopedia from the end of the 19th century and the beginning of the 20th century represents a large part of the corpus This explains why we had to use words like ty because and ehuru in spite of as markers that do not belong to present day Swedish The corpus was randomly divided into a training set 90 and a test set 10 To improve training we used only verbs and nouns 5 We tagged the corpus words with their part of speech using the Granska tagger 7 We kept the nouns and the verbs and discarded the rest of the words including the markers from the patterns 182 S Karlsson and P Nugues Finally we compiled the training examples 130 796 contrasts 37 319 CEV and 43 387 elaborations and a test set of 14 643 contrasts 4 107 CEV and 4 976 elaborations all extracted using the patterns in Table 1 3 2 Evaluation Methods For the evaluation we built binary classifiers to distinguish where cardinal is the number of entries in the table We found that a lambda of 0 05 seemed 
to maximize the accuracy of the classifiers In a similar experiment 9 used the value of 0 25 3 3 Results Table 2 shows the accuracy of the classifiers A result of 67 9 in the Contrast vs Other condition is in the same range as the results obtained for English 5 which reported between 60 and 70 for most relations The results for Elaboration vs Other that reached 52 2 were significantly lower however Table 2 The accuracy of each classifier In each case the baseline is 50 Relation Accuracy Contrast vs Other 67 9 CEV vs Other 57 7 Elaboration vs Other 52 2 Automatic Learning of Discourse Relations in Swedish Using Cue Phrases 183 4 Conclusions Results around 60 clearly indicates that the classifier is better than a random assignment of text spans to each class The result with elaboration is not completely satisfying though which first of all can be accounted to the fact that we only used 43 387 training examples As perspectives some simple improvements could be made Since there is no intrinsic order in contrast 
relations the table could be made commutative However we did not consider it a critical point since there were more than 130 000 training examples of contrasts The most critical point though is to find the best set of cues phrases for each discourse relation The corpora used in this experiment was quite small for the task and we had to use many cue phrases at one time With a larger training set we could determine which phrases contribute most to the model without introducing noise for example by comparing results obtained by including or excluding training examples from a particular extraction pattern Not only the size of the corpora limits the performance of this technique The example words that indicate a contrast i e week and daily in the example in Sect 1 can possibly stand in other types of discourse relations Such overlapping word pairs will dim the statistical accuracy of the model no matter the size of the corpora This is a major limitation of the general approach taken and can only be dealt with by 
introducing other types of classification information to distinguish between the rhetorical relations In English possibly WordNet 10 11 or FrameNet 12 could be used to figure out which word pairs indicate a particular relation To sum up we presented evidence of a feasible technique for the automatic extraction of discourse relations in Swedish Marcu and Echihabi showed 5 that using this technique as a complement to extracting cue phrase marked sentences can increase the number of correctly classified contrasts from 26 to 77 Further investigations are however necessary to evaluate more accurately the applicability of this algorithm in Swedish Acknowledgments Lars Aronsson provided most of the corpora used from the Runeberg project Jonas References 1 Mann W C Thompson S A Rhetorical structure theory A theory of text organization Technical Report RS 87 190 Information Sciences Institute 1987 2 Meijer B ed Nordisk familjebok Uggleupplagan edn Nordisk familjeboks 184 S Karlsson and P Nugues 4 Corston Oliver S 
Computing Representations of the Structure of Written Discourse PhD thesis University of California Santa Barbara 1998 5 Marcu D Echihabi A An unsupervised approach to recognizing discourse relations In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics ACL 2002 Philadelphia pp The Representation of Diatheses in the Valency Lexicon of Czech Verbs Charles University in Prague Institute of Formal and Applied Linguistics kettnerova lopatkova ufal mff cuni cz Abstract In the present paper we deal with diatheses in Czech from a lexicographic point of view We propose a method of their description in the valency lexicon of Czech verbs VALLEX We distinguish grammatical and semantic diatheses as two typologically different changes in verbal valency structure In case of grammatical diatheses these changes are regular enough to be described by formal syntactic rules In contrast the changes in valency structure of verbs associated with semantic diatheses vary even within one type of 
diathesis Thus for the latter type we propose to set separate valency frames corresponding to their members and to capture the changes in verbal valency structure by lexical rules based on an adequate lexical semantic representation of verb meaning Keywords valency lexicon grammatical and semantic diatheses changes in valency structure of verbs lexical semantic representation of verbs 1 Introduction Valency behavior of verbs is so heterogenous that it cannot be described by general syntactic rules Instead it must be captured in the form of lexical entries separately for each verb Prototypically a single meaning of a verb corresponds to a single valency frame However in many cases semantically close uses of verbs can be syntactically structured in different ways See the following examples 1 a Peter smeared butter on bread The research reported in this paper was carried out under the project of MSMT CR No MSM002162083 It was supported by the grant No LC536 and partially by the grant No GA P406 2010 0875 H 
Loftsson E 186 V to be contraintuitive Thus we propose to describe the changes in the valency structure of semantically related uses of verbs by means of formal syntactic and lexical rules First let us fo cus on the pairs in 1a 2a and 1b 2b respectively The changes in the valency structure of the verb to smear are expressed by grammatical means we refer to the relation between these uses of the verb as a grammatical diathesis In contrast the changes in the valency structure of the verb to smear in pairs 1a 1b and 2a 2b respectively are expressed by lexical semantic means We refer to the relation between such uses of verbs as a semantic diathesis The representation of grammatical and semantic diatheses is proposed here for the valency lexicon VALLEX which aims at the explicit description of valency behavior of Czech verbs 1 This lexicon takes the Functional Generative Description henceforth FGD as its theoretical background 7 In FGD valency is related primarily to the tectogrammatical layer i e the layer of 
linguistically structured meaning The valency characteristics are enco ded in a form of a valency frame which is mo deled as a sequence of frame slots corresponding to valency complementations of a verb labeled by rather coarse grained tectogrammatical roles as Actor Patient Effect Direction etc 5 In addition possible morphemic forms are specified for each valency complementation For our purposes we enhance FGD i with the concept of lexical conceptual structures 6 representing lexical semantic properties of verbs and ii with the open set of labels for situational participants as Agent Recipient Filler Surface etc The paper is structured as follows In Section 2 we define the notions situation and perspective which play a crucial role in the characteristics of diatheses Then on the basis of the correspondence between situational participants valency complementations and surface syntactic positions we distinguish two types of diatheses grammatical diatheses Section 3 and semantic diatheses Section 4 In Section 
3 1 and 4 1 the representation of grammatical and semantic diatheses in the valency lexicon is proposed respectively Conclusion and an outlook for future work is presented in Section 5 2 Situation vs Perspective The members of both types of diatheses are usually characterized as constructions denoting the same situation though each time from a different perspective Thus the concepts situation and perspective play a key role in the characteristics of diatheses First let us fo cus on the concept of a situation The term do es not refer to a real life situation it is rather a situation mo deled by language i e a linguistic situation The linguistic situation related to an event represents a set of facts and entities i e participants linked in a unified structure Thus an analysis of a particular situation denoted by the verb must involve not only the specification 1 http ufal mff cuni cz vallex 2 5 The Representation of Diatheses in the Valency Lexicon of Czech Verbs 187 of the relevant number of its participants 
but also the description of the relations between them see e g 3 For example the situation portrayed by the uses of the verb to smear in examples 1 and 2 consists of three participants labeled as Agent Cover and Surface and it may be informally described as an Agent covers a Surface of an ob ject with a Cover We refer to this part of the verbal meaning as a situational meaning and to its components as situational participants Situational meaning represents an abstract mo del of situation which has not yet been linguistically structured Sentences expressing the same situational meaning can be usually structured in several ways i e different situational participants can occupy the syntactically prominent positions of sub ject and direct ob ject This results in different perspectives from which the situation is viewed see e g 2 In case of the verb to smear the situation can be viewed from the perspective of Agent i e Peter as in 1a and 1b from the perspective of Cover i e butter as in 2a or from the perspective 
of Surface i e bread as in 2b The different perspectives in these sentences are manifested by grammatical and lexical semantic means As a result we distinguish two typologically different changes in the verbal valency structure Grammatical diatheses refer to the relation between the uses of verbs characterized by the differences in the mapping of valency complementations and surface syntactic positions as in 1a 2a and 1b 2b These differences are based on grammatical means Section 3 In contrast semantic diatheses refer to the relation between the uses of verbs characteristic of the different correspondence between situational participants and valency complementations as in 1a 1b and 2a 2b These differences are expressed by lexical semantic means i e by the change of lexical unit Section 4 3 Grammatical Diatheses Sentences related by a grammatical diathesis express the same situational meaning however they are characterized by different perspective which results from the changes in the mapping between valency 
complementations and surface syntactic positions Further within this type the linking of situational participants and valency complementations remains unchanged This can be illustrated by the pairs of examples 1a 2a and 1b 2b the first one repeated here as 3 3 a Peter smeared butter on bread 188 V 1 The use of the marked member of grammatical diatheses is conditioned by the grammatical meaning of the verb represented by a specific verbal grammateme in FGD 4 In Czech verbal forms of these marked members typically consist either of auxiliaries and non finite form of lexical verb or they have a reflexive form 2 The marked member of grammatical diatheses is prototypically connected with the shift of some of situational participants from the prominent surface syntactic position of sub ject to a less prominent syntactic position 3 The correspondence between situational participants and valency complementations remains unchanged thus the set of situational participants is directly enco ded in the valency frame by a 
sequence of valency complementations It implies that the number of valency complementations and their type are preserved and the changes in the valency frame are limited only to the changes in morphemic forms of the valency complementations The asymmetry between the mapping of the situational participants and the surface syntactic positions corresponding to the verb to smear in example 3 is illustrated in Figure 1 Fig 1 The changes in the mapping of the valency complementations and the syntactic positions of the verb to smear associated with the passive grammatical diathesis 3 1 Representation of Grammatical Diatheses In this section we propose the representation of grammatical diatheses in the valency lexicon VALLEX The changes in valency structure asso ciated with grammatical diatheses are regular enough to be described by general syntactic rules these rules are stored in the grammar component of the lexicon Then it is sufficient to indicate the applicability of a certain rule in a special attribute 
attached to each relevant lexical unit of a verb in the data component of the lexicon Let us demonstrate our approach on the example of the recipient diathesis The marked construction as in 5 is characterized by the verbal form consisting of the auxiliary dostat to get and the past participle of a lexical verb The structural condition on the recipient diathesis is the presence of a valency complementation expressed by the dative case in the valency frame corresponding The Representation of Diatheses in the Valency Lexicon of Czech Verbs 189 to the situational participant Recipient The following verbs satisfy this condition doporu cit to recommend nahradit to recompense na Table 1 Recip r rule for the recipient diathesis Recip r verbal grammateme valency frame Unmarked Recip 0 ACTnom ADDRdat Marked Recip 1 ACTod gen ADDRnom Note 1 2 3 190 V 2 The shift of the valency complementation ACT from the sub ject position into the adverbial position is manifested by the change of its morphemic form from the nominative 
into the prepositional group od genitive 3 The shift of the valency complementation ADDR from the indirect ob ject position into the prominent sub ject position is expressed by the change of its morphemic form from the dative into the nominative 4 Every valency complementation that is not listed in the rule is preserved For example if we apply the rule Recip r to the valency frame describing the unmarked use of the verb p rid elit to allocate example 4 we derive the valency frame corresponding to the verb in the marked construction of the recipient diathesis example 5 as follows ACTnom ADDRdat PATacc Recip r ACTod gen ADDRnom PATacc Other grammatical diatheses may be described in the same way Passive diathesis Zam estnanci informovali 4 Semantic Diatheses Sentences related by a semantic diathesis express the same situational meaning similarly as grammatical diatheses However in case of semantic diatheses the different perspective is reflected by the changes in the mapping of situational participants and 
valency complementations This can be illustrated by examples 1a 1b and 2a 2b the first one repeated here as 6 6 a Peter smeared butter on bread The Representation of Diatheses in the Valency Lexicon of Czech Verbs 191 In contrast to grammatical diatheses semantic diatheses are not connected with changes of grammatical categories of verbs They are rather related to a small number of well delimited semantic classes of verbs which share certain facets of meaning The members of semantic diatheses satisfy the following criteria 1 Semantic diatheses are expressed by lexical semantic means i e by different lexical units The members of semantic diatheses do not differ from each other in a specific grammatical meaning of a verb instead they differ in structuring situational participants into a valency frame 2 Semantic diatheses are characterized by shifts of some of situational participants from the prominent surface syntactic position of ob ject or sub ject to a less prominent syntactic position 3 The changes in a 
verbal valency structure arisen from the changes in the correspondence between situational participants and valency complementations may affect the number of valency complementations their type as well as their morphemic form s The asymmetry between the correspondence of the situational participants and the valency complementations of the verb to smear in example 6 is illustrated in Figure 2 Fig 2 The changes in the mapping of the situational participants and the valency complementations of the verb to smear associated with the semantic diathesis 4 1 Representation of Semantic Diatheses In this section we propose the representation of semantic diatheses in the valency lexicon VALLEX As the members of semantic diatheses differ in the correspondence between situational participants and valency complementations an appropriate lexical semantic representation of the situational meaning of the verb is necessary for their adequate description For this purpose we adopt the lexical conceptual structures proposed in 6 
Furthermore contrary to grammatical diatheses the changes in the verbal valency structure associated with semantic diatheses vary even within a single type of diathesis It follows that they cannot be described by general syntactic 192 V rules For these reasons we propose to specify separate lexical units corresponding to the individual members of semantic diatheses in the data component of the lexicon these lexical units are interlinked by a relevant type of semantic diathesis In the grammar component the changes in the verbal valency structure are represented by lexical rules Our approach can be explained on the example of the locative semantic diathesis see below Let us mention some other types of Czech semantic diatheses that may be described in the same way Material Product diathesis Na rezal The Representation of Diatheses in the Valency Lexicon of Czech Verbs 193 i In the data component the members of semantic diathesis are represented by separate lexical units which are interlinked by a relevant type 
of semantic diathesis ii In the grammar component the changes in valency structure of verbs are captured by a lexical rule determining the changes in the mapping of situational participants and valency complementations Locative Diathesis of Verbs Indicating Creating Co occurrence Let us demonstrate our approach on the example of the verb nalo zit to load With respect to its semantic properties we determine the following three situational participants Agent Filler and Container The situational meaning of the verb may be informally described as an Agent fills a Container with a Filler This meaning is syntactically structured in two ways as in 7 and 8 above Both sentences 7 and 8 express the change of location of the situational participant Filler caused by the participant Agent In comparison with the variant 7 variant 8 is semantically more 194 V the LCS b the same correspondence between the variables and the labels is preserved as in LCS a With respect to the complexity we consider the variant corresponding 
to the LCS a as unmarked and the variant characterized by the LCS b as the marked one Table 2 The possible mapping of the situational participants and the valency complementations of the verbs expressing creating co occurrence specifying the relation inside x Agent ACT ACT ACT y Filler z Container examples PAT DIR nalo zit seno na Considering the mapping between the valency complementations and the situational participants represented by the variables in the LCS a and LCS b Table 2 we formulate the lexical rule Lo c r1 for the locative diathesis The rule Loc r1 can be applied also to other verbs expressing creating co o ccurrence specifying the relation inside e g nato cit to draw nasypat to pour doplnit to add as well as to those verbs specifying the relation outwards see below inside Filler Container outwards Cover Surface LCS a LCS c PAT DIR Loc r1 Loc r1 LCS b LCS d EFF PAT Commentary on the Loc r1 On the left side of the rule the valency complementations of the unmarked member of the diathesis are given 
i e the situational participant mapped onto PAT in the valency frame of the verb represented by the LCS a and LCS c below is changed into EFF in the frame corresponding to the LCS b and LCS d If EFF is not present in the valency frame then this participant is not linguistically structured see in Table 2 The situational participant mapped onto the valency complementation DIR in the valency frame of the unmarked member of the diathesis corresponds to the valency complementation PAT in the frame of the marked member The rule Loc r1 holds also for the changes in the mapping of situational participants and valency complementations of the verbs indicating creating coo ccurrence with the relation outwards e g nat The Representation of Diatheses in the Valency Lexicon of Czech Verbs 195 PAT Surface barvou EFF Cover 10 Petr ACT Agent nat rel zed Eng Peter ACT smeared the wall PAT Surface with paint EFF Cover c x ACT SMEAR CAUSE BECOME y ON z d x CAUSE BECOME z SMEARED x ACT SMEAR CAUSE BECOME y ON z BY MEANS OF 
However in case of the verbs expressing creating co o ccurrence with the relation outwards another set of labels for the situational participants is asso ciated with the variables in the LCS c and LCS d x Agent y Cover and z Surface Despite the different set of labels the changes in the mapping of the situational participants and the valency complementations are described by the same rule Loc r1 as the changes of the verbs indicating the relation inside Locative Diathesis of Verbs Indicating Destroying Co occurrence For the description of the changes in the correspondence between situational participants and valency complementations of the verbs expressing the event destroying co o ccurrence we formulate the lexical rule Loc r2 We demonstrate cistit to clean see 11 and 12 this rule on the example of the verb o 11 Jana ACT Agent o cistila outwards Cover Surface inside Filler Container LCS e PAT DIR Loc r2 Loc r2 LCS f ORIG PAT 196 V Commentary on the Loc r2 On the left side of the rule the set of valency 
complementations representing the unmarked member of the diathesis is given The situational participant Cover or Filler mapped onto PAT in the valency frame represented by the LCS e is mapped onto ORIG in the frame represented by the LCS f If ORIG is not present in the marked frame then this participant is not expressed The situational participant Surface or Container corresponding to the valency complementation DIR in the valency frame of the unmarked member of the diathesis described by the LCS e is mapped onto the valency complementation PAT in the frame of the marked use represented by the LCS f 5 Conclusion and Future Work We have distinguished two types of relations between semantically close uses of verbs which are syntactically structured in different ways grammatical and semantic diatheses We have proposed their representation in the valency lexicon VALLEX The changes in a verbal valency structure asso ciated with grammatical diatheses are described by formal syntactic rules which determine regular 
changes in morphemic form s of complementations Thus both verbal uses may be represented by a single lexical unit with ascribed information on applicability of individual formal syntactic rule for a relevant grammatical diathesis In contrast the changes typical of semantic diatheses are represented by lexical rules which formally describe the changes in the mapping of situational participants and valency complementations It implies that both verbal uses are represented by separate lexical units interlinked by a relevant type of semantic diathesis In the future we intend to represent typologically different changes in valency structure of verbs in the lexicon in a similar way References 1 Anderson S R On the Role of Deep Structure in Semantic Interpretation Foundations of Language 7 Symbolic Classification Methods for Patient Discharge Summaries Encoding into ICD Laurent Kevers and Julia Medori CENTAL Abstract This paper addresses the issue of semi automatic patient discharge summaries encoding into medical 
classifications such as ICD 9CM The methods detailed in this paper focus on symbolic approaches which allow the processing of unannotated corpora without any machine learning The first method is based on the morphological analysis MA of medical terms extracted with hand crafted linguistic resources The second one ELP relies on the automatic extraction of variants of ICD 9CM code labels Each method was evaluated on a set of 19 692 discharge summaries in French from a General Internal Medicine unit Depending on the number of suggested classes the MA method resulted in a maximal F measure of 28 00 and a highest recall of 46 13 The best Fmeasure for the second method was 29 43 while the maximal recall was 52 74 Both methods were then combined The best recall increased to 60 21 and the maximal F measure reached 31 64 Keywords Patient discharge summaries ICD 9 CM classification symbolic approach 1 Introduction This paper presents work done in collaboration with one of the ma jor hospitals in Brussels les Cliniques 
universitaires Saint Luc The Belgian government requires hospitals to report their activity through the enco ding of patient s stays into ICD 9 CM1 Co des from this classification symbolise diagnoses procedures aggravating factors such as allergies smoking but also elements in the patient s past history that may influence his her current health status The encoding task is generally done by professional co ders Their work consists in going through the patient s medical record and translating into ICD 9 CM co des every activity that o ccurred during the patient s stay The main source of information co ders use is patient discharge summaries PDS that physicians write after each patient s stay These do cuments are written in free text in the form of a letter addressed to the patient s GP or external care professionals The enco ding task is a tedious and demanding task that requires very specialized 1 International Classification of Diseases Ninth revision Clinical Modification H Loftsson E 198 L Kevers and J 
Medori skills Consequently many hospitals try to reduce the amount of work involved by trying to partly automate this pro cess Our goal is to build a tool that would help co ders by providing them with a set of most likely co des Indeed the full automation of the task is difficult to achieve as the PDS seldom contains all the necessary information2 The automation of the enco ding process can be considered as analogous to a classification problem It involves classifying PDS into a nomenclature Here ICD 9 CM codes are considered as classes There are two main approaches to this classification problem symbolic and statistical metho ds Statistical approaches are based on machine learning metho ds and require a large amount of annotated data for training which makes it difficult for this type of approach to face a change of nomenclature Saint Luc will adopt ICD 10 in the near future and to classify certain very specific co des for which we have only sparse data available In this paper the aim is therefore to 
develop a symbolic approach to the encoding task Symbolic approaches involve linguistic knowledge are therefore usually more language dependent and require more time for development However in one of our metho ds metho d 2 we will see that it is possible to partly generate the linguistic resources automatically Two metho ds are described metho d 1 is based on the morphological analysis of medical language metho d 2 relies on the automatic extraction of variants of ICD 9 CM co de labels Metho d 2 is thus limited to the vocabulary used in the resource whereas metho d 1 uses a wider range of vocabulary They should therefore complement each other After a brief description of related work section 2 and evaluation data section 3 sections 4 and 5 will detail the two metho ds and their respective results Then their combination will be discussed in section 6 before considering ways to improve the performance of the system in section 7 2 Related Work Since the early 1990s many scientists have looked into the possible 
automation of the enco ding pro cess 1 2 3 As mentioned above there are two main approaches to the enco ding task knowledge based e g MedLEE 4 and machine learning e g Auto co der 5 Both approaches scored highly in the Computational Medicine Challenge in 2007 6 among the best three systems two combined a statistic and a symbolic approach 7 and only one relied only on a symbolic approach 8 All these studies were developed on English language Pereira et al 9 built a fully symbolic system for French relying on a linguisticbased indexing system into the French version of the MeSH classification and then mapping it to ICD 10 What is noteworthy is that most systems even when choosing a statistical approach still rely on a linguistic component The results of most of these studies are promising Autocoder achieved very high precision for two thirds of the assigned co des but the do cuments are often more structured 2 Additional information can be found in the full medical record but to avoid having to deal with the 
variety of formats in the record we decided to focus our work on PDS as sources of information Symbolic Classification Methods for PDS Encoding into ICD 199 than our PDS e g diagnoses clearly marked And they are also often limited with regard to the number of co des involved or to the types of do cuments 3 Data The ICD 9 CM is a hierarchical nomenclature comprising 15 688 codes which are 4 or 5 characters long The first three characters represent a general category of diagnoses and the next one or two digits specify the exact diagnosis Fig 1 The ICD 9 CM is divided into 1 135 general categories In the perspective of a coding help our study will classify according to these categories letting the co der choose the right co de into the hierarchy within each suggested category Code 001 0010 0011 0019 Label Cholera Cholera due to Vibrio cholerae Cholera due to Vibrio cholerae el tor Cholera unspecified Fig 1 Hierarchical structure of ICD9 CM Our evaluation data consists in 19 692 PDS in French taken from the 
General Internal Medicine unit Patients in this unit suffer from very diverse diseases A wider range of co des is therefore used 6 029 different co des dispatched into 895 categories The PDS in our corpus were assigned 150 116 co des 137 336 categories which makes an average of 7 6 co des 7 categories per document Note that 27 241 out of 895 of the categories were used less than 6 times These manually assigned co des are used as a gold standard for our evaluation In order to broaden the scope of our linguistic resources we used the UMLS3 as a source of variants for the ICD 9 CM co de labels The UMLS metathesaurus unifies and integrates into one unique resource many medical nomenclatures from different languages giving to each concept a unique concept identifier CUI Using this CUI we were able to extract from this metathesaurus different wordings for the ICD 9 CM co de labels These synonyms were then added to the original co de labels as illustrated in Fig 2 Class French label 061 Dengue Dengues Fi evre 
dengue Infection par le virus de la dengue English label Dengue Dengues Dengue fever Infection by the dengue virus Source ICD 9 CM UMLS UMLS UMLS Fig 2 Definition of class 061 with terms from ICD 9 CM and UMLS 3 Unified Medical Language System http www nlm nih gov research umls 200 L Kevers and J Medori 4 Classification Method 1 Morphological Analysis MA This metho d follows a two part structure to re create the work of human co ders who first read the PDS while highlighting information that needs to be enco ded and then assign codes to these expressions Therefore the first mo dule extraction module section 4 1 already presented in 10 aims at restricting the information to be processed by selecting informative sequences of text The second mo dule encoding module section 4 2 is a new one which translates the extracted phrases into co des 4 1 Extraction Module This mo dule selects in PDS sequences of text that convey information that needs to be encoded diagnoses pro cedures allergies etc This mo dule is based 
on specialized dictionaries and transducers built with Unitex4 11 The specialized dictionaries were built partly automatically and completed manually The main dictionaries are the dictionaries of diagnoses and pro cedures These two dictionaries were automatically constructed using the French nomenclatures comprised in the UMLS as described in section 3 As cataloguing all the different ways a physician may mention a diagnosis is difficult these dictionaries still needed to be completed manually All the dictionaries were then automatically inflected To compensate the lack of exhaustivity of the dictionaries we needed to use more flexible linguistic resources to detect diagnoses and informative data We therefore used finite state transducers5 Transducers are represented as graphs see Fig 3 Each path of the graph represents a recognized sequence These hand crafted transducers aim at marking up the phrases that mention a diagnosis or a pro cedure by adding XML tags to the original text In Fig 3 the graph 
recognizes different ways of mentioning fractures fracture and sprains foulures as well as the bo dy part affected with a link to the anatomical dictionary in subgraph d localisation It then outputs the XML tags MALINDET indicating that the phrase is a diagnosis Fig 3 Transducer for fractures 4 5 A corpus processing tool http www igm univ mlv fr unitex A transducer is not only able to recognize a sequence of elements but also to produce a related output Symbolic Classification Methods for PDS Encoding into ICD 201 When diseases or pro cedures are not in the dictionaries graphs allow us to locate these terms thanks to clues in the text the context in which the term appears and its morphology Contexts like The patient presents with shows signs of indicate that what follows may be a disease or a symptom and will therefore be extracted The morphological clues consist mainly of word endings like pathy indicating a disease or therapy for a pro cedure Detecting contexts is also very important as it influences the 
way the pathology will be encoded a negated diagnosis will not be enco ded or past history illnesses are co ded differently This extraction mo dule was evaluated in 10 on a corpus of 220 PDS 66 6 of phrases highlighted manually by the professional co ders were also extracted by the system and 85 5 of the extracted phrases were diagnoses More work has been done on the system since then but it has not yet been re evaluated 4 2 Encoding Module Once the terms containing information to be enco ded are extracted they need to be matched to an ICD 9 CM co de This classification metho d aims at taking advantage of the fact that medical language has a rich morphology When comparing the original terms used in the PDS and the corresponding co de label we observed that many terms looked morphologically close We therefore developed a methodology very similar to a bag of words approach where the extracted terms and the ICD co de labels were matched according to the morphemes composing them The breaking down into morphemes 
was done using 6 7 202 L Kevers and J Medori Fibroscopie bronchique PDS Bronchoscopie par fibre optique ICD 9 CM fibrbronch scopie scopie bronchfibre ique optique Fig 4 Example of bronchoscopy label Cj S Ti Cj NTi Cj NTi NCj where NTi Cj is the number of common words between Ti and Cj NTi and NCj are the number of words on each side The resulting bag of word of each extracted phrase is then compared to all the co de labels and their synonyms To assign co des to the entire PDS the maximum similarity value for each co de is kept Score Cj max S Ti Cj All the co des are ordered according to their similarity value The first co des in this list are then considered as the most likely codes to be assigned to the document The score for each parent category is the maximum score of its children co des The output list is then ranked according to this parent category score This list can be returned as it is or shortened using a thresholding function 14 4 3 Results The enco ding evaluation was conducted on 19 692 do 
cuments The measures Recall Precision and F measure F1 were macro averaged8 For a do cument R gives the proportion of manually assigned classes retrieved in the suggested list and P the proportion of goo d classes as defined by the gold standard into this list The best results are reported on table 1 Depending on the will to promote recall or on the contrary precision we can tune the thresholding function acceptance level and then compute intermediate results Of course a higher recall level always comes with a higher number of suggested categories Table 1 Evaluation of MA method Recall R Precision P F measure F1 Nb classes Threshold Best Recall 46 13 14 70 21 10 20 No Best F measure 34 52 27 34 28 00 8 6 Yes 5 Method 2 Extended Lexical Patterns ELP This method previously described in 14 and used in 15 on parliament documents is based on the insight that well described classes9 can be sufficient to find a significant intersection with the text vocabulary Class labels are extracted from 8 9 Computed for each 
document and then averaged Class defined by a descriptive set of words and or compound expressions Symbolic Classification Methods for PDS Encoding into ICD 203 existing terminological resources e g thesauri nomenclatures For this paper the ICD 9 CM enhanced with the UMLS synonyms was used section 3 Our approach attaches value to compound expressions because of their high descriptive power They are often used to refer to complex concepts or ob jects and as a result are goo d items to contribute to class definition The extended lexical patterns ELP method consists in two steps First the automatic transformation of a class definition resource into a term extraction resource Section 5 1 It detects in each text a list of expressions considered as interesting for class inference Then the class assignment Section 5 2 step uses this result for classification The first step is performed once while the second must be repeated for each new do cument 5 1 Extraction Resource Building The original nomenclature is 
automatically converted to finite state transducers compatible with Unitex They are made of a lexical elements from the original class label b other more generic items like grammatical co des10 or meta labels11 The aim is to increase the coverage with generic elements while preserving the goo d precision induced by the lexical units The transducer output is the class related to the recognized sequence First for each category all IDC 9 CM labels and their UMLS synonyms are gathered The complex expressions are then automatically cleaned and split Some recurrent and non informative parts like sans autre 10 11 Example N for nouns A for adjectives V for verbs Example TOKEN recognizes any token 204 L Kevers and J Medori The second step is stopwords pro cessing They are not removed but replaced with meta labels like TOKEN 12 This substitution improves the coverage to similar expressions where small vocabulary variations o ccur Original form Atteinte d un nerf Injury to nerves Modified pattern Atteinte TOKEN nerf 
Example of recognized forms Atteinte du de d un des nerf Stemming is also used to broaden the recognition Snowball13 16 provided stems used to build regular expressions14 The patterns are then able to recognize expressions with words beginning with these stems Grammatical variations are therefore covered gender number agreement noun form adjectival form swap etc Note that we also remove accents and decapitalize letters Original form Oed eme de membre The next step aims at allowing more important variations additional words may appear e g adjectives but also other signs comas parentheses etc To do this we allow any token to be inserted between two elements of the transducer This is achieved with a special insert subgraph which contains a TOKEN tag Figure 6 shows the final transducer automatically generated for class 061 Fig 6 Final transducer for class 061 All transducers are gathered into a main transducer which also contains some additional elements to locate negative contexts and to avoid the assignment of 
classes to expressions such as absence d infection no infection 5 2 Class Inference Once the extraction resource is available it is applied to each new do cument The result of this operation is a list of expressions15 see Fig 7 Each item of the list comes with a class identifier and in the case of a negative context the minus tag is added 12 13 14 15 Note that a TOKEN tag is not allowed on its own or at the begin or at the end of a pattern An implementation of the Porter algorithm http snowball tartarus org The signs and determine the regular expression and specify the anchor at the beginning of the string An expression can of course occur more than once and can also be linked to more than one class In this last case the expression participate to the score of each class Symbolic Classification Methods for PDS Encoding into ICD zona 053 oesophagite moderee aspecifique 947 extremement douloureux 729 infection a mycobacterie 031 gastroscopie Z44 fond de oeil Z16 acide E96 pas de atteinte du nerf 957 anemie 
normochrome normocytaire 285 zona 053 sequellaires apicales droite tuberculose 137 hyperthyroidie 242 intestin grele Z45 goitre 706 tuberculose V12 goitre 240 205 Fig 7 List of recognized expressions from one text For each expression a weight based on its frequency is computed16 The frequency measure is multiplied by 2 in the case of a multi word expression A final weight for each represented category is then obtained after the addition of all related expression weights This list can be returned as it is or shortened by a thresholding function 14 5 3 Results The evaluation was conducted similarly as for the first metho d section 4 3 The best results are reported on table 2 As in section 4 3 intermediate results can be computed depending on the interest to promote recall or precision Table 2 Evaluation of ELP method Recall R Precision P F measure F1 Nb classes Threshold Best Recall 52 74 20 69 27 37 19 6 No Best F measure 37 97 30 30 29 43 9 8 Yes 6 Combination of Symbolic Methods The two metho ds detailed 
here can be used on their own However their combination may improve the classification results In order to test that hypothesis we implemented the same mixing metho dology as in 15 This pro cess consists in the merging of the resulting lists of classes by computing their weighted17 union Mix1 or intersection Mix2 before a possible thresholding To be comparable the weights were normalized values between 0 and 1 The list of classes can also be thresholded before their union Mix3 or intersection Mix4 These last two approaches are based on the lists of classes returned by each metho d to maximize F1 see sections 4 3 and 5 3 18 16 17 18 The use of a TF IDF did not bring significant improvement as in previous work 14 An hyperparameter balances the importance of each method merged method1 1 method2 with 0 1 and steps of 0 1 In these cases the use of the hyperparameter has no influence on the results because thresholding comes before balancing 206 L Kevers and J Medori 6 1 Results Combined metho ds results are 
presented in table 319 This approach clearly improves both recall and f measure Generally the union combination tends to improve the recall while the intersection combination or the thresholding pro cess tend to increase precision In our experiments the best results were always reached with the union of unshortened lists Mix1 For F1 maximization thresholding has to be done afterwards Table 3 Evaluation of symbolic methods combination Recall R Precision P F measure F1 Nb classes Threshold 1 Mix1 Threshold Method1 Method2 Best R 60 21 13 20 20 86 30 5 No Any 37 13 33 12 31 64 8 1 Yes 0 3 0 7 Best F1 Mix2 Threshold Method1 Method2 Best R 38 66 29 28 30 52 9 1 No Any 34 73 34 55 31 50 7 Yes 0 3 0 7 Best F1 Mix3 Threshold Method1 Threshold Method2 Best F1 43 28 20 59 27 90 14 7 Yes N A Mix4 Threshold Method1 Threshold Method2 Best F1 24 07 37 95 29 46 4 4 Yes N A The best recall increases to 60 21 Mix1 compared with 46 13 14 08 for metho d 1 and 52 74 7 47 for method 2 As for metho d 1 and metho d 2 the best 
recall is reached when returning all classes from the merged lists that is 30 5 classes on average 10 additional categories Among the co des retrieved by the combination metho d 64 21 were returned by both metho ds and the remaining 35 79 by one method or the other From this result we can conclude that the two metho ds complement each other well At 31 64 Mix1 the best f measure improvement is not as clear cut but it nevertheless outperforms metho d 1 28 00 3 64 and method 2 29 43 2 21 This result is mainly due to the increase of precision for both methods 5 78 for method 1 and 2 82 for metho d 2 while the recall remains basically the same for metho d 2 0 84 and improves for method 1 2 61 This greater precision reduces the number of suggested categories to 8 1 previously 8 6 for metho d 1 and 9 8 for metho d 2 without lowering the recall level The Mix2 approach which only keeps the intersection of both metho ds result lists has lower performance The highest recall is limited by the proportion of common co des 
returned by both metho ds 64 21 and is therefore lower than for original metho ds The maximization of the f measure gives a similar result 31 50 as for Mix1 and an increase with regard to metho d 1 and 2 Again the improvement comes mostly from the precision 7 2 for method 1 and 4 24 for method 2 However this increase is greater than in Mix1 due to the filter effect of the intersection and the number of suggested categories dropped to 7 19 As a consequence of their implementation thresholding before merging we only look at f measure maximization for Mix3 and Mix4 approaches Symbolic Classification Methods for PDS Encoding into ICD 207 For the last two combinations regarding to the original metho ds Mix3 turns out to be less efficient and Mix4 shows only very little improvements Finally a brief look at the rare co des i e those that are used less than 6 times in our test corpus shows that 35 of their o ccurrences 212 out of 603 were covered by the unshortened list resulting of the union of both metho ds 7 
Discussion and Future Work The reported results have to be put into perspective First the evaluation was conducted on manual indexing As shown by several studies in particular 17 for medical indexing the maximum inter annotator agreement is often situated at approximately 70 Therefore the evaluation of an automatic classification method compared with manual annotation cannot reach the maximal recall and precision Secondly we saw that most of the information that needs to be enco ded is present in the PDS However an internal study in iSaint Luc showed that 15 to 20 of the codes assigned by the coders cannot be inferred from the PDS The MA metho d relies on the extraction of phrases indicating diagnoses and pro cedures As indicated in section 4 66 of the phrases highlighted by professional coders were extracted by the system This means that 34 of the phrases were not found and therefore no co de could be inferred from them This lowered the maximum recall value accordingly More time should be dedicated to the 
development of the graphs used in this mo dule Regarding the ELP method the automatic transformation of the basis resource into the extraction transducer can be improved further The phrases used to build the transducers have to be as short as possible to promote recall and as unambiguous as possible to promote precision while some category labels are very complex Rule based automatic enumeration parsing i e splitting turns out to be more difficult to adapt than the thesaurus used in a previous experiment 14 because of the various possible syntactic compositions This could be explained by the fact that thesaurus items are well designated concepts whereas nomenclature items can be viewed as indications to guide the category choice For both methods a better precision may be reached by weighting more efficiently extracted phrases according to the part of the do cument in which they o ccur intro duction conclusion past history and current illness description Finally it will be interesting to conduct an evaluation 
of the help effectively provided by the tool to the co ders As a conclusion we can outline that our symbolic metho ds prove their usefulness in the context of unannotated corpora where it is difficult to apply machine learning approaches Moreover we think they can successfully be mixed with learning algorithms when a training set is available Acknowledgements This work was partly supported by the CAPADIS and STRATEGO projects CAPADIS is funded by the government of the BrusselsCapital Region Belgium ISRIB STRATEGO WIST 2 project 616442 is funded by the government of Walloon Region Belgium 208 L Kevers and J Medori References 1 Ananiadou S McNaught J Introduction to text mining in biology In Text Mining for Biology and Biomedicine pp User Tailored Document Ralf Klabunde and Alexander Kornrumpf Ruhr Abstract In order to satisfy the informational demands of different users generated texts should be tailored to the respective user types Document planning may benefit from a formal modeling of the participating 
agents the generation system and the user within the framework of game theory We show how rhetorical structures map to speaker strategies and how a user model may be represented as a domain theory containing different hypotheses for the listener strategies Based on this we present an algorithm which simultaneously performs the tasks of message selection and document structuring Keywords natural language generation document planning game theory 1 Introduction During natural language generation NLG document planning is the first step in transforming non linguistic informational units database entries or concepts into a coherent text Document planning comprises two subtasks the selection of the information to be conveyed from the underlying data or knowledge base and merging the selected informational units to textual units Typically the latter subtask is performed by establishing rhetorical relations 6 between semantic representations for corresponding text spans The result is a tree structure for the entire 
do cument plan Obviously the variable information requirements of users result in different do cument plans with respect to content and structure We aim at a general game theoretic mo del of do cument planning where different contents for various user types are determined by differing assumption costs and utilities for the players involved The close link between Gricean pragmatics and game theory is well known since 5 but only recently game theoretic mo deling of information exchange became a flourishing area in pragmatics 7 Document planning is driven by pragmatic demands since goo d guidelines for selecting the information to be conveyed are the Gricean maxims ensuring effective communication 10 Since almost every game theoretic model of communication is an attempt to formalize the Gricean ideas we achieve a precise reformulation of some of the Gricean ideas for do cument planning H Loftsson E 210 R Klabunde and A Kornrumpf 2 A Game Theoretic Algorithm for Document Planning Our starting point is a normal 
form game N A u where N is a finite set of players A We start with the specification of the actions A of the normal form game N A u Speaker actions Despite the well known weaknesses of an analysis based on rhetorical structures we act on the assumption that rhetorical relations reflect the basic actions of S in order to select the messages and to structure the do cument Speaker actions play a ma jor role in the discourse planning algorithm presented in subsection 2 3 This algorithm do es not only realize h but g as well because we map our data to message types From the total amount of messages we receive only those messages will be taken into consideration that are linked by rhetorical relations Listener actions L s action set is closely linked to the preconditions and effects of the rhetorical relations used Listener actions are responsible for the update of the information state in the user model Ls interpretation task is to find an explanation for the information on the basis of his own beliefs Since 
interpretation is equivalent to finding an explanation why the information conveyed might be true User Tailored Document 211 Definition 2 Candidate hypothesis Be TA a domain theory and P the set of predicate symbols within TA Then H p P p does not occur in a head clause of TA is the set of abducibles for TA and H P H the set of non abducibles h H is called a candidate hypothesis Definition 3 Abduction Be TA a domain theory and H a set of observations to be explained Find a consistent explanation formula F with F F H In other words abduction means given a set of nonabducibles find a formula F which consists solely of abducibles and explains Since there is usually more than one explanation F which explains external criteria are needed to determine the best explanation This can be done by assigning costs to each candidate hypothesis h Abduction then becomes an optimization problem usually called weighted abduction Definition 4 Weighted or cost based abduction Be TA an abduction problem Minimize hF costs h 
subject to F F H Given a generated do cument plan the action set of L comprises all updates of his information state which explain why the document plan might be true 2 2 The Utility Function The central concept for every game theoretic mo del is the utility function All aspects we have mentioned so far lead to plausible utilities or payoffs for S and L respectively The utility function is not only the core concept the exact fixing of the payoffs is also the most problematic decision for every game While in economic scenarios the payoffs are often asso ciated with monetary values in our approach the payoffs represent the cognitive burden of the agents which is hard to quantify However the exact numerical utilities are not the crucial factor but the relation between the different payoffs Therefore we do not postulate that the exact numerical utilities bear any deep semantics The differences between the payoffs determine the preferences of the players and their best response to the actions of the other agent 
Let us assume that S has a set of do cument plans at its disposal which express the same data in different manners Furthermore L knows a set of hypotheses AL H which offer possible explanations for the data H may be computed from L s domain theory TL Communication between S and L requires that S chooses a rhetorical relation aS AS and conveys the relation and its arguments to L who in turn chooses a hypothesis aL AL as an interpretation The utility function provides a basis for the agents to make their choices Payoffs for L Cost based abduction actually requires only very little reformulation to fit into the framework of game theory since the notion of costs i e negative utility is already accounted for in that concept For reasons of computational feasibility in our approach L may only adhere to a single hypothesis which does not have to match all of the facts Hence in addition to costs we 212 R Klabunde and A Kornrumpf need a metric of how goo d a hypothesis fits into the observed facts We call the 
selection of a hypothesis on this basis naive abduction L s utility may be formulated in a similar way Definition 5 Naive abduction Be TA costs a cost based abduction problem Find h H such that h H matchTA h costs h matchTA h costs h Definition 6 Listener utility Be aS m aL h a strategy profile with the interpretation as given above Let TL be the domain theory of L and m the propositions covered by aS The utility of L is defined as follows uL m h p hp q matchTA p q 1 if q p matchTA p q 2 User Tailored Document 213 The utility function for S has to find a balance between the goal of leading L to a hypothesis that accounts for d and minimizing the complexity of the communicated document plan m 2 3 A Multi iteration Game Algorithm for Discourse Planning We are now able to achieve text plans with our game theoretic concepts As already mentioned our task is to compute h g d The algorithm given in Table 1 determines the relevant subset of all possible messages and the structure of the do cument plan simultaneously 
The algorithm combines only those messages by means of rhetorical relations that form together with a listener s update mechanisms a Nash equilibrium Table 1 A game theoretic document planning algorithm 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Pool all messages derived from d N S L AL H u uS uS R nil repeat AS speaker actions rhet relations which may link pairs of elements in Pool Pool m h pure strategy equilibrium of N A u if m Pool then Rm else E constituents of aS Pool Pool E Pool Pool m end if until R nil return R There are some important differences between this algorithm and the algorithm presented in 8 p 108 Therefore we will explain this algorithm in some detail Line 1 Instead of preselecting messages by means of heuristics the pool is initialized with all messages known to S Lines 214 R Klabunde and A Kornrumpf Lines 9 10 If S is in an equilibrium and realizes an existing rhetorical relation instead of constructing a new one the pool does not change Lines 3 Summary and Outlook To sum up we provided 
generic formal notations for the interlocutors their tasks action sets and the utility functions All definitions are grounded in wellknown theoretical frameworks and game theory allowed the formulation of the interplay of the relevant representations and processes rooted in these theories In addition to developing the formal mo del of do cument planning mentioned above we applied that model to the generation of do cument plans for different users of performance data The generated texts explain the output of a heart rate monitor HRM worn by a runner during his training for amateur atheletes and beginners For reasons of space we cannot go into the details 4 will provide a comprehensive overview References 1 Console L Dupre D Torasso P On the relationship between abduction and deduction Journal of Logic and Computation 1 5 Anaphora Resolution with Real Preprocessing Manfred Klenner Don Tuggener Angela Fahrni and Rico Sennrich Institute of Computational Linguistics Binzmuehlestrasse 14 CH 8050 Zurich klenner 
tuggener sennrich cl uzh ch angela fahrni swissonline ch Abstract In this paper we focus on anaphora resolution for German a highly inflected language which also allows for closed form compounds i e compounds without spaces Especially we describe a system that only uses real preprocessing components e g a dependency parser a two level morphological analyser etc We trace the performance drop occurring under these conditions back to underspecification and ambiguity at the morphological level A demanding subtask of anaphora resolution are the so called bridging anaphora a special variant of nominal anaphora where the heads of the coreferent noun phrases do not match We experiment with two different resources in order to find out how to cope best with this problem Keywords Anaphora Resolution Coreference Resolution Bridging Anaphora 1 Introduction Anaphora resolution is a resource intensive task In order to find out whether a noun phrase is an antecedent of another subsequent noun phrase the anaphor information 
from various preprocessing components are to be combined A morphological analyser is needed for number person and gender determination a tagger is required to deliver part of speech tags a parser to find grammatical functions and the embedding depth of noun phrases and finally semantic information is necessary to tackle the most difficult task namely bridging anaphora Bridging anaphora are nominal anaphora where the heads of the noun phrases do not match Take the following sequence Iceland is an interesting place to visit The land of ice and fire is famous for Here Iceland and land of ice and fire are coreferent In order to establish this coreference link the least a system has to know is that Iceland is a land Lexical resources such a WordNet or its German counterpart GermaNet do comprise this kind of information although not exhaustively The proper determination of coreference depends on the quality of these resources and the preprocessing units using them Thus a poor performance of a system for anaphora 
resolution can have multiple causes and often it is hard to tell which component or resource is to blame Therefore it is tempting to reduce this kind of noise to its minimum and to create idealised H Loftsson E 216 M Klenner et al conditions under which one can easily fix failures Instead of using a parser one could use a treebank and if the treebank also has morphological annotations why not use it as well This way one ends up with a system that expects perfect prepro cessing and whose empirical results no longer indicate its usefulness for real world applications This kind of simplifications are often made by current approaches to anaphora resolution One of the most unrealistic and simplifying idealisations is to use true mentions instead of all noun phrases True mentions are those markables that are according to a coreference gold standard part of a coreference chain The ma jority of noun phrases in a text however are not in a coreference set The determination whether a NP is anaphoric i e a true mention 
or not is a demanding problem the so called anaphoricity classification problem There are a few systems that incorporate anaphoricity classification the ma jority of systems leaves this as an implicit task to the anaphora resolution component Separate anaphoricity classification has not proven to be more successful than its implicit counterpart Anaphoricity determination of markables is a non trival task and cutting it away makes a system an artificial one We are not saying that experiments under idealised conditions are totally in vain We are just arguing that it doesn t help a lot to tune a system on the basis of gold standard information if one intends to switch to a real world system One never foresees the amount of noise that is introduced by real components In this article we intro duce a system for anaphora resolution for German that uses only real preprocessing components Gertwol a morphological analyser Pro3Gres a dependency parser GermaNet a German wordnet and Wortschatz Leipzig a lexical resource 
generated by statistical means As most approaches we cast anaphora resolution as pairwise classification we use TiMBL Daelemans et al 2004 as a machine learning tool Our system is filter based that is candidate pairs that do not fulfil linguistic filter criteria are sorted out We give empirical results and discuss the reason for the drop of performance from an idealised setting to a real world setting Also different filters have been investigated to determine the usefulness of lexical resources for the task of resolving bridging anaphora for German 2 Filter Based Pairwise Classification Approaches to pairwise classification of anaphora resolution differ among others in their pair generation mo dule Some systems generate every pair independent of the distance between two markables the noun phrases that might stand in a coreference relation Under a linguistic point of view this only makes sense for nominal anaphora A pronoun at the end of a text could hardly refer back to a noun phrase at the beginning of a 
text without further intervening chain links Moreover the problem with such an approach is the vast amount of negative instances it pro Anaphora Resolution with Real Preprocessing 217 antecedent of an anaphor is reached We use a fixed window of three sentences for pronominal anaphora and bridging anaphora while for named entities there is no restriction Each pair additionally must pass all applicable filters Filters depend on the part of speech of the antecedent anaphor candidate For instance personal pronouns must agree in person number and gender with its antecedent head whether this is a pronoun or a noun After morphological analysis we often have underspecified information at hand only For instance German ihr can be plural without gender restriction their or singular feminine her If no information is available e g for unknown nouns we take a disjunction of all allowed values Possessive pronouns only unify in person and gender e g Sie liebt ihre 218 M Klenner et al Here is the list of our features 
Salience of a grammatical function is estimated on the basis of the training set in the following way the number of cases a grammatical function realises a true mention divided by the total number of true mentions it is the conditional probability of a grammatical function given an anaphor The function sub ject is the most salient function followed by direct ob ject It has been noticed that the local perspective of pairwise classification yields problems Take the following markable chain Hillary Clinton she Angela Merkel she is compatible with Hillary Clinton Angela Merkel is compatible with she but Merkel and Clinton are incompatible Since transitivity is outside the scope of a pairwise classifier it might well classify both compatible pairs as positive without noticing that this leads to an implicit contradiction setting Clinton and Merkel to be coreferent In a former paper we have argued that coreference clustering based on the so called Balas order coupled with intensional constraints to ensure 
consistency of coreference sets performs best in order to remedy these problems Klenner and Ailloud 2009 In this paper we concentrate on the performance drop of the baseline system under the conditions of real preprocessing components We do not discuss problems of coreference clustering 3 Real Preprocessing Tools Fortunately good NLP tools are available for a number of languages For German a two level morphology program called Gertwol a fast and well performing partof speech tagger the TreeTagger Schmid 1994 and a fast and state of the art dependency parser the Pro3Gres parser are the components of our systems Additionally we have developed a named entity recognition based on pattern matching and Wikipedia entries It is evident that the quality of prepro cessing determines the quality of the rest namely the decision made by linguistic filters and the classification carried out by the machine learning classifier 3 1 Morphology with Gertwol We use Gertwol a commercial system based on two level morphology 
Gertwol is fast and also carries out noun decomposition which is rather useful since in Anaphora Resolution with Real Preprocessing 219 German compounds are realised as single wordforms closed form compounds e g Computerexperte computer expert Compounds which are quite frequent in German might become very complex but often the head of the compound is sufficient to semantically classify the whole compound via GermaNet For instance Netzwerkcomputerexperte expert for network computers is an expert and thus is animate The other important task of Gertwol is to determine number person and gender information of a word Unfortunately ambiguity rate is high since e g some personal pronouns are highly ambiguous For instance the German pronoun sie she might be singular feminine or plural without gender restriction The pronoun ich does not impose any gender restrictions and moreover often refers in reported speech to a speaker which is referred to in the text by a noun phrase in third person 3 2 Named Entity Recognition 
Our Named Entity Recognition NER is pattern based but also makes use of extensive resources We have a large list of international first names 53 000 where the gender of each name is given From Wikipedia we have extracted all multiword article names e g Berliner Sparkasse a credit institute from Berlin and if available their categories e g Treptower Park has Parkanlage in Berlin Bezirk Treptow Pro3GresDe is a hybrid dependency parser for German that is based on the English Pro3Gres parser cf Schneider 2008 It combines a hand written grammar and a statistical disambiguation module trained on part of the 1 For a full discussion of Pro3GresDe see Sennrich et al 2009 220 M Klenner et al The parser give access to the following features e g grammatical function depth of embedding subclause information 4 Empirical Evaluation We have carried out two series of experiments The first one is concerned with the costs of real prepro cessing compared to the use of gold standard information e g tree bank instead of parser We 
incrementally fix the reasons for the performance drop The second experiments are devoted to bridging anaphora and the impact of two main lexical resources for German GermaNet a German WordNet and Wortschatz Leipzig a statistically derived thesaurus 4 1 The Price of Real Preprocessing From the two pro cessing steps of coreference resolution pairwise classification and subsequent clustering only the first is of interest here It is the baseline performance drop that we are interested in This degradation occurs before clustering and it cannot be compensated by clustering operations The performance drop is measured in terms of save gold standard versus noisy real world components morphological functional and syntactic information The gold standard information stems from the Table 1 Performance Drop gold standard info morphological functional subclause real 61 49 59 01 58 20 58 01 68 55 69 78 69 12 70 89 55 73 51 12 50 56 49 01 F measure Precision Recall Anaphora Resolution with Real Preprocessing 221 anaphoric 
relations Our experiments have however shown that it is better to restrict the search than to generate each and every pair performance drops to a great extent the larger the window Finally the local perspective of pairwise classification do es not allow to take boundness restrictions into account For instance we know that third person personal pronouns and possessive pronouns as well are anaphoric i e must be bound there are only very few exceptions There is however no way to tell the learner this kind of prior knowledge Fortunately this shortcoming can be compensated at the subsequent clustering step where these markables can be forced to be bound to the best available anaphor Let us see how the performance is like if we take gold standard information especially perfect morphology perfect syntax and perfect functional information The f measure value is 61 49 about 3 5 above the real world setting Precision drops slightly 68 55 but recall significantly increases to 55 73 The reason for performance increase 
is the increase in recall How can we explain this Let us first see how the different gold standard resources contribute to this increase If we turn grammatical functions from parser given to gold standard given the increase on the baseline is small f measure raises from 58 01 to 58 20 Our dependency parser is goo d enough to almost perfectly replace gold standard information The same is true with syntactic information concerning the depth of embedding and subclause detection Here as well only a small increase occurs the f measure is 59 01 But if we add perfect morphology an increase of 3 5 pushes the results to the final 61 49 The reason for the increase in recall and f measure is our filter based metho d Only those pairs are generated that pass the filter If the morphology is noisy pairs erroneously might pass the filter and others pairs erroneously do not pass the filter The first one spoils precision the second hampers recall We were quite surprised that the replacement of syntactic and functional 
information by real components was not the problem Morphology is responsible for the drop 4 2 Filtering for Resolution of Bridging Anaphora In this section we show that using different morpho syntactic distance based and semantic filters derived from real resources the task of resolving bridging anaphora in a pairwise manner is far from being accomplished with satisfying results Filtering aims at reducing the number of negative instances but this has been hardly investigated regarding the ceiling or performance upper bound it pro duces The upper bound values given in Tab 2 indicate how many false negatives a filter produces i e how many real positives it filters out 2 We have further investigated these upper bounds see Tab 3 and found that they are either very low when using very restrictive strict filters or that the filters do not eliminate enough negative instances when used in a relaxed lax mo de Throughout our experiments we use the CEAF scorer presented in Luo 2005 2 We get a slight reduction in 
precision when using no filters because of a string matching issue when filtering out string matching multiword items 222 M Klenner et al Table 2 Upper Bound of the Morpho syntactic and Distance Filters Filter no filter diff regens anaphor definite number agreement all morphosynt filters dist limit 3 all Recall Precision F measure Pairs Reduction Positives 100 00 98 60 99 21 4869822 4924 0 10 99 87 98 53 99 11 4864018 0 10 4915 0 10 100 00 98 60 99 21 4401565 9 62 4913 0 11 93 91 94 64 94 00 3480538 28 53 4622 0 13 93 78 94 57 93 90 3110842 36 12 4602 0 15 68 36 80 54 72 46 818588 83 19 1697 0 21 63 31 76 82 67 81 520735 89 30 1579 0 30 We can see from Tab 2 that the morpho syntactic filters which perform well in resolving pronominal anaphora give good upper bounds but do not reduce the amount of negative instances sufficiently Subclause exclusion here diff regens determined through verb dependency which establishes a kind of c command in our dependency framework is not really that relevant for resolving 
bridging anaphora as antecedents are often not in the same sentence Perhaps surprising is the fact that 9 positive instances get deleted by this filter Such errors o ccur with real prepro cessing as parsing is not perfect A simple definiteness filter anaphor definite that checks if a candidate anaphor has an indefinite determiner German ein i e a or an or its morphological variants reduces the training instances by almost 10 without reducing the upper bound Number agreement filtering shows that there are 302 positive instances that do not agree in number Still this filter cuts down the number of instances by almost 30 The often used distance filter with a sentence window of 3 pro duces an acceptable upper bound and reduces the instance size by 83 91 This is still not enough however looking at the percentage of positives 0 21 For filtering based on semantic information we use Wortschatz Leipzig and GermaNet We apply head extraction and decomposition to composite nouns based on Gertwol morphological analysis 
in the case they are not found directly in the lexical resources For 54 593 83 1 of the 65 703 markables synonyms can be found in Wortschatz Leipzig WSL for 60 985 92 8 we can make a often ambiguous GermaNet GN classification The synonymy filter WSL checks if a mention is in the synonymy list of the other one or if they share a common synonym The GN filter checks if both mentions are in the same GN class if the class is ambiguous we check all and let the pair be generated if we find a match We investigate the upper bounds of the semantic filters in two ways see Tab 3 If for a mention no information has been derived we let it pass the filter lax or we delete it strict There are huge differences between the upper bounds and the percentages of positive instances between lax and strict filtering This suggests that although for quite a large number of markables semantic information can be retrieved it do es not allow us to use it for hard filtering without a significant drop in the upper bound ceiling This gets 
obvious when we combine the strict semantic Anaphora Resolution with Real Preprocessing Table 3 Upper Bound of the Semantic Filters Filter WSL strict WSL lax GN strict GN lax all filt lax all filt strict Recall Precision F measure Pairs 37 00 55 71 42 34 112921 72 94 83 35 76 38 1679610 57 98 73 52 63 21 1030441 81 36 89 03 84 04 1694157 36 86 53 97 41 93 97593 15 1 28 41 18 32 8953 Reduction 97 86 65 51 78 84 65 21 98 00 99 81 Positives 1590 1 41 3300 0 20 3283 0 32 4385 0 26 1013 1 04 441 4 93 223 constraints with the morpho syntactic and distance filters in all filt strict It is the only filter that generates a fairly reasonable percentage of positives but drops the upper bound immensely As one would expect a synonymy based filter WSL is more strict than a semantic class based constraint GN The trade off between the percentage of positives and the reduction of the upper bound holds equally for both of the semantic filters the more positives the lower the upper bound Filtering is a key element to 
successful resolution of bridging anaphora However our experiments show that filters based on morphological information and syntactical constraints do not sufficiently reduce the amount of negative instances in order to train a reasonable classifier The distance constraint is a goo d filter to tune the trade off between recall and precision although the distance values might be highly dependent on the test domain and genre On the other hand using the lexical resources for filtering based on semantic constraints heavily suffers from sparseness leading to a considerably lower upper bound These findings seem to suggest that pairwise classification is not the best technique for resolving bridging anaphora given a real anaphora resolution scenario We are currently carrying out experiments with an incremental approach where pairwise classification is done only between the last mentions of already established coreference sets and the anaphor candidate We hope to show that by recasting the problem of coreference 
resolution as an incremental clustering problem the issue of resolving bridging anaphora becomes less important because true mentions linked through a bridging relation can be merged by a pronoun between them 5 Related Work The work of Soon et al 2001 is a prototypical and often reimplemented machine learning approach in the paradigm of pair wise classification Our system has a similar baseline architecture and our features do overlap to a great extent Work on coreference resolution for German is rare most of it uses the coreference annotated treebank 224 M Klenner et al took this finding seriously and have tried to use Wikipedia to complement GermaNet we map Wikipedia multiword items via Wikipedia categories to GermaNet classes We also have experimented with a statistically derived lexical resource the Wortschatz Leipzig Hinrichs et al 2005 intro duce anaphora resolution only pronouns on the basis of a former version of the 6 Conclusion In this paper we have discussed the intricacies of anaphora resolution 
based on real prepro cessing components Our system makes extensive use of non statistical resources rule based dependency parsing a German wordnet Wikipedia twolevel morphology but at the same time is based on a state of the art machine learning approach We have traced the performance drop that o ccurs under this conditions back to its origin It is the morphology of German that yields the problem Although German counts as a highly inflected language underspecification and ambiguity prevail and are the main cause of degrading performance We have also evaluated the usefulness of two resources GermaNet and Wortschatz Leipzig Our experiments suggest that filtering for pairwise classification is not a successful technique if bridging anaphora are concerned Other metho ds for finding proper antecedent anaphor candidates are needed here Our initial experiments with an incremental model are promising our future work will proceed in this direction Acknowledgements Our project is funded by the Swiss National Science 
Foundation grant 105211 118108 References Daelemans et al 2004 Daelemans W Zavrel J van der Sloot K van den Bosch A TiMBL Tilburg Memory Based Learner 2004 Hamp and Feldweg 1997 Hamp B Feldweg H GermaNet a Lexical Semantic Net for German In Proc of ACL Workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications 1997 Anaphora Resolution with Real Preprocessing 225 Hinrichs et al 2005 Hinrichs E Filippova K Wunsch H A Data driven Approach to Pronominal Anaphora Resolution in German In Proc of RANLP Borovets Bulgaria 2005 Klenner and Ailloud 2008 Klenner M Ailloud E Enhancing Coreference Clustering In Johansson C ed Proc of the Second Workshop on Anaphora Resolution WAR II Bergen Norway NEALT Proceedings Series vol 2 2008 Klenner and Ailloud 2009 Klenner M Ailloud E Optimization in Coreference Resolution Is Not Needed A Nearly Optimal Zero One ILP Algorithm with Intensional Constraints In Proc of the EACL 2009 Luo 2005 Luo X On coreference resolution performance 
metrics In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing Association for Computational Linguistics Morristown NJ USA pp Automatic Construction of a Morphological Dictionary of Multi Word Units Cvetana Krstev1 Ranka 2 Faculty of Philology University of Belgrade Faculty of Mining and Geology University of Belgrade 3 Faculty of Mathematics University of Belgrade 1 Abstract The development of a comprehensive morphological dictionary of multi word units for Serbian is a very demanding task due to the complexity of Serbian morphology Manual production of such a dictionary proved to be extremely time consuming In this paper we present a procedure that automatically produces dictionary lemmas for a given list of multi word units To accomplish this task the procedure relies on data in e dictionaries of Serbian simple words which are already well developed We also offer an evaluation of the proposed procedure on several different sets of data Finally we 
discuss some implementation issues and present how the same procedure is used for other languages Keywords electronic dictionary Serbian morphology inflection multi word units noun phrases query expansion 1 Introduction We have been developing morphological electronic dictionaries of Serbian for natural language pro cessing for many years now Our e dictionaries follow the metho dology and format known as DELAS DELAF which is presented for French in 1 1 Serbian e dictionaries of simple forms have reached a considerable size they have a total of 122 000 entries 2 Although we are continually enlarging our e dictionaries of simple words we have taken a further step towards tackling the problem of multi word units MWUs In an intro duction in 3 supported by comprehensive references Savary states that MWUs are hard to define controversial linguistic ob jects Nevertheless it seems that many authors agree that MWUs a are composed of two or more graphical words b show some degree of morphological syntactic 
distributional or semantic non compositionality and c have unique and constant references A deeper discussion of the nature of MWUs is beyond the scope of our paper When dealing with MWUs we are taking a pragmatic approach and consider MWUs to be sequences of graphical units that for some reason have to be described and pro cessed as a unit 1 A comprehensive bibliography on e dictionaries for other languages developed using the same methodology is given at http www igm univ mlv fr unitex H Loftsson E Automatic Construction of a Morphological Dictionary of MWUs 227 For some pro ductive classes of MWUs like different types of numerals and named entities time and duration measures and currencies we have developed finite state transducers FSTs that rely on morphological e dictionaries of simple words to model these MWUs correctly 4 When applied to a text in automatic text analysis these FSTs asso ciate recognized MWUs with lemmas as well as with appropriate grammatical categories Both the asso ciated lemmas and 
grammatical categories are in the same format as the one provided by dictionaries of simple words Some other MWUs that are idiosyncratic in nature ask for a different description Namely these MWUs cannot be described by FSTs they have to be listed in an e dictionary in a similar way and for similar reasons as simple words That means that some regular form or a lemma has to be listed in a DELAC dictionary together with some additional information that would enable the generation of all inflected forms that is entries for a DELACF dictionary Several questions arise here a what should this regular form listed in a dictionary of lemmas be b what additional information is necessary and c how the generation of all forms is to be performed When trying to find the answers to these questions one has to keep in mind that the graphical units composing a MWU are themselves simple words that have their own inflectional behavior However simple words that represent components of a MWU do not inflect freely they have to 
conform to some combining rules In the case of Serbian these rules can be rather complex since simple words constituting a MWU like adjectives and nouns can inflect in several grammatical categories For instance civilni vojni rok civil military service is a multi word noun that inherits its gender from the constituent noun rok masculine in this case and it inflects for case but it does not inflect for number although the simple word rok do es The adjectives civilni and vojni agree with the noun rok in number case gender and animacy but the comparative and indefinite forms of these adjectives are not used in this MWU It is clear from this example that the combining rules for Serbian MWUs are by no means trivial After considering several options we concluded that Multiflex a finite state tool for MWUs developed by A Savary 5 suits our needs best This tool supports finite state transducers that can mo del a number of combining conditions As for the inflection of constituent simple words it relies completely on 
FSTs for the inflection of simple words This way the generation of all inflected forms with Multiflex is performed with two types of FSTs for inflection of simple words and for inflection of MWUs To that end a lemma for each simple word constituent that inflects in a MWU has to be provided as well as values of all grammatical categories of forms appearing in the MWU lemma its regular or dictionary form For the given example the entry in DELAC dictionary is civilni civilni A2 adms1g vojni vojni A2 adms1g rok rok N81u ms1q nc axaxn1 The information given in this entry allows automatic pro duction of all 26 inflected forms for the DELACF dictionary as for example the entry for the singular dative case civilnom vojnom roku civilni vojni rok N ms3q 228 C Krstev et al Although the Multiflex approach is theoretically well founded easy to understand and apply and it successfully solves many problems of MWU inflection for Serbian it is obvious from the given example that the pro duction of a single DELAC entry is a 
very tedious task As a matter of fact we initially produced only 30 DELAC entries from scratch Realizing that additional information within DELAC entries everything between the parenthesis in most cases already exists in dictionaries of simple words DELA we decided to develop a mo dule for our lexical resources management tool LeXimir an enhancement of its predecessor WS4LR 6 that would help in obtaining this information However due to homography of forms and homonymy of simple word entries the developer of a DELAC entry still needs to choose between several options offered by a dictionary look up provided by this tool For the above example the choice had to be made three times for the information about forms like adms1g for the first component and once for the simple word entry because there are two entries for rok in the Serbian DELAS one for service and one for ro ck music Following this approach only 3195 DELAC entries were pro duced in the past three years which we found very ineffective In a 
comprehensive analysis of several tools for MWU inflection description 3 the author mentions only one system FASTR that supports automated MWU lexicon creation 7 Since this system is based on an approach very different from DELA metho dology we developed our own pro cedure for automatic construction of DELAC entries from a given list of MWUs For an item such as civilni vojni rok this pro cedure produces the aforementioned MWU lemma However the pro duced list of lemmas has to be manually checked and some decisions still have to be made even when the pro cedure offers only one candidate MWU lemma Thus we have to stress that our dictionaries remain handcrafted resources as per categorization in 8 in terms of the information they offer and the metho dology used to produce them without statistical engineering 2 Analysis of Initial Data We based the development of the automated pro cedure for DELAC construction on the initial dictionary of MWUs which contained 3195 lemmas covering different part of speech as 
presented in Table 1 nouns adjectives adverbs conjunctions prepositions interjections As only MWU nouns and adjectives inflect they have an inflectional class co de assigned to them in DELAC Each inflectional class is asso ciated with one inflectional transducer as described in 9 that controls the pro duction of all inflected forms The forms lemmas ratio in Table 1 shows that a direct pro duction of DELACF dictionary is out of the question for Serbian though it may seem possible for some other languages For instance for English this ratio for MWU nouns is 1 90 whereas for French it is 1 29 The calculation is performed on the basis of dictionaries described in 10 and 11 that are part of the standard distribution of Unitex 12 a corpus processing system based on the finite state technology Some inflectional transducers are frequently needed like nc axn which is used for MWUs consisting of an adjective followed by a noun in our case 1208 Automatic Construction of a Morphological Dictionary of MWUs Table 1 
Initial content of the Serbian morphological dictionary of MWUs POS Nouns Adjectives Other lemmas 2571 207 415 forms 49792 21045 forms lemmas 19 4 101 7 Inflectional classes super classes 67 16 28 9 229 MWUs while some apply to a single MWU In order to design our automated procedure we grouped all inflectional transducers into equivalence classes or super classes a super class consists of all transducers that use the same form of a MWU lemma that is the same information for the pro duction of inflectional forms This is also reflected in the convention we used for naming the inflectional transducers A stands for an adjective constituent N stands for a noun constituent X stands for a constituent that do es not inflect including a separator while some additional digits and letters may be added to differentiate transducers This is illustrated in Table 2 by six inflectional transducers all belonging to one super class nxn and used for the inflection of MWUs consisting of a noun followed by another noun where both 
nouns inflect and must agree in basic grammatical categories It should be noted that MWUs sharing the same inflectional class or superclass do not necessarily have the same syntactic structure and vice versa For instance film u boji color movie and bolest ludih krava mad cows disease share the same class nc n4x although the first MWU consists of a noun followed by a prepositional phrase and the second of a noun followed by a phrase in the genitive However from the inflectional point of view they both behave in the same way only the first component inflects whereas the second and the third do not On the other hand predsednik dr zave president of the state and advokat odbrane attorney of the defense both consist of a noun followed by a component in the genitive case but their inflectional behavior is different in the first MWU the second component can change in number whereas in the second MWU it does not inflect For that reason they belong to two different classes and superclasses nc nxn4 and nc n4x 
respectively In order to formulate our strategy for the production of MWU lemmas we analyzed the data in the initial dictionary looking for useful information We identified the additional information assigned to components of MWUs belonging to a particular inflectional class and vice versa we identified inflectional classes asso ciated with the same additional information For instance the class nc nxn2m explained in Table 2 is asso ciated with only one combination of grammatical categories characterized by the fact that the components differ in gender On the other hand some combinations of grammatical categories like ms1q ms1q two masculine inanimate nouns in the nominative singular were associated in DELAC with three classes nxn nxn2 and nxn3 see Table 2 the first one being the most frequent 230 C Krstev et al Table 2 The inflectional classes for MWUs belonging to the super class nxn Infl class nc nxn nc nxnf nc nxn3 nc nxn2 nc nxn4 Example Translation Explanation lekar aku ser obstetrician Both components 
inflect and agree in case and number kit ubica killer whale The gender of the second component changes in plural kamenfoundation The separating hyphen can be omitted temeljac stone Kongo CongoNeither of the components inflects for Brazavil Brazzaville number predsednik president The first component inflects for case dr zave of the state and number the second may or may not inflect for number Kne zevina Principality The second component does not necesMonako of Monaco sarily inflect nc nxn2m On the basis of this information we got the general idea which combinations of atomic data can we expect to be extracted from dictionaries of simple words and how they combine with inflectional classes However the obtained information was incomplete For components that do not inflect for a certain MWU there was no additional information in the MWU lemma For such components this information was subsequently searched in dictionaries of simple words It was thus possible to establish that in naliv pero ink pen the first 
component was not in the dictionary of simple words in bruto plata gross income the first component was an adjective that do es not inflect in mini suknja mini skirt the first component was a prefix All three examples belong to the same super class 2xn There are still cases when no additional information whether extracted from MWU lemmas themselves or subsequently from electronic dictionaries can help in deciding in favor of one inflectional class over other possibilities For instance this is the case for krem karamel caramel cream and kamen temeljac foundation stone both components in these MWUs are masculine inanimate nouns in the nominative singular but in the first one karamel is the head and the first component do es not inflect super class 2xn while in the second example kamen is the head and both components inflect super class nxn 3 Description of the Strategy The purpose of the analysis performed in the first step was not to pro duce a strategy for construction of MWU lemmas automatically it was 
rather used as a reference during the manual pro duction of a strategy in the form of XML documents The schema of these do cuments is presented in Figure 1 Our strategy consists of two XML do cuments one for MWU nouns and the other for MWU adjectives Each XML do cument consists of a sequence of rules that are grouped by the number of components in MWUs Each rule states the conditions that a MWU and its components have to satisfy in order to be placed in a particular inflectional class and to have a particular MWU lemma assigned to them In order to make our strategy easier to produce and maintain conditions for some Automatic Construction of a Morphological Dictionary of MWUs 231 Fig 1 The XML Schema for a strategy document rules were grouped into general and specific conditions where specific conditions are simply additions to general conditions One rule will illustrate this Rule ID 2 CFLX NC_AXN3 CflxGroup AXN RuleGenCond Word ID 1 POS A Flex true Case 1 Anim a Gen g Word ID 2 POS N Flex true Case 1 Anim a 
Gen g RuleGenCond RuleSpecCond ID 1 Example Ajfelova kula Word ID 1 Num s Cond PRE Word ID 2 Num s RuleSpecCond RuleSpecCond ID 2 Example poljski radovi Word ID 1 Case 1 Num p Word ID 2 Case 1 Num p RuleSpecCond RuleSpecCond ID 3 Example elektronsko poslovanje Word ID 1 Case 1 Num s Word ID 2 Case 1 Num s SinSem VN Coll HumColl RuleSpecCond Rule This rule classifies a MWU in the inflectional class nc axn3 asso ciated with adjective noun MWUs that do not inflect for number super class axn if the following conditions are satisfied a General conditions the first component is an adjective the second is a noun both components are listed in the nominative case they agree in gender and animacy whichever they are b Additional specific conditions 232 C Krstev et al Table 3 Overview of rules for nouns and adjectives with 2 to 7 components Components 2 3 4 5 6 7 Total Rules for nouns general special 26 25 14 6 4 1 76 59 85 50 54 29 9 286 Rules for adjectives general special 5 8 5 2 9 8 5 2 20 24 4 Analysis and 
Evaluation of the Strategy We have performed the analysis and evaluation of our strategy in two steps In the first step we applied the strategy to the same data that we used to pro duce Automatic Construction of a Morphological Dictionary of MWUs 233 it that is our initial DELAC dictionary In the second step we applied it to several lists of MWUs that we have collected from various sources After applying our strategy to the initial DELAC dictionary containing 2571 nouns and 207 adjectives the obtained results were manually validated When assessing the success of the strategy we adhered to the following a If for a given MWU one of the rules pro duced the correct lemma and assigned the correct inflectional class we considered the strategy as successful b If for a given MWU none of the rules satisfied a but one of the rules pro duced the correct lemma although the assigned inflectional class was not correct whereas the assigned super class was correct we considered the strategy as partially successful c If for 
a given MWU none of the rules satisfied a or b we considered that the strategy failed for that MWU d For each lemma where either a or b applied we also determined the rank of the accepted solution the higher on the list of offered solutions the more favorable the accepted solution Table 4 gives percentages of successfully pro duced noun and adjectives entries case a partially successful results case b and failures case c and also indicates the rank of these results rank 0 means no solution offered A failure means that either no solution was offered or none of the solutions was classified as a or b In either case the reason was that the strategy failed to cover a particular MWU structure in 52 cases or 20 of all failures or a MWU component that inflects was not in the dictionary of simple words in 255 cases or 80 of all failures The latter case o ccurred frequently due to MWUs representing proper names where components are often not words in Serbian e g Dar es Salam or due to the fact that some words are used 
only in MWUs like domali in domali prst next to little ring finger and are thus not listed in dictionaries of simple words The lowest rank of a successful result was 10 for nouns and 3 for adjectives In the second step we applied our strategy for nouns to several different lists of MWUs We have not applied our strategy for adjectives in this step simply because we have not collected enough new adjectives Our list of new MWU nouns came from several different sources the official list of MWU names of settlements in Serbia 236 MWUs extracted from a log file of a Serbian professional journal that deals with economic issues 162 from Verne s novel Around Table 4 Evaluation of the strategy applied to the initial DELAC dictionary Rank 0 1 2 3 4 10 Total Nouns c 5 50 6 28 0 08 Adjectives c 2 93 37 56 36 59 2 93 15 61 4 39 a 63 74 6 56 2 22 0 90 b Total 5 50 82 71 7 26 3 43 1 10 a b Total 2 93 53 17 40 98 2 93 0 00 100 00 12 69 0 62 1 21 0 20 73 42 14 72 11 87 100 00 77 07 20 00 2 93 234 C Krstev et al the world in 80 
days 114 from the explanatory dictionary of Serbian under the letter R 604 As the analysis in the first step indicated that MWU proper names pro duce in general worse results we decided to separate these lists in two groups After removing those already in DELAC we got a list of MWU toponyms 206 and a list of MWU common nouns 784 Table 5 shows that in the case of common nouns for only 3 62 items on the list 28 items no satisfactory solution a or b was offered For toponyms this percent is much higher 38 61 accounting for 78 items on the list In the case of toponyms all cases of failure are due to the fact that components of toponyms were not simple words in Serbian e g Feja in Kriva Feja The lowest rank of a successful result was 6 for common nouns and 2 for toponyms Table 5 Evaluation of the strategy for nouns applied on a list of MWU common nouns and compound toponyms Rank 0 1 2 3 4 6 Total Common nouns b c Total 10 08 0 26 0 13 1 68 1 94 1 68 92 12 4 78 1 42 0 00 100 00 Toponyms c 24 75 13 86 a 80 23 4 39 1 
29 a 48 02 8 91 1 00 b Total 24 75 65 35 8 91 0 00 1 00 3 47 85 92 10 47 3 62 57 92 3 47 38 61 100 00 Row 2 in Table 6 shows that less rules were used for common nouns in the second step than for nouns in the initial DELAC in the first step This is probably due to the fact that while building our initial DELAC and the inflectional classes we tried to find various structurally different examples Row 3 shows that for all subsets except adjectives the number of rules that were not used is greater than the number of used rules namely many rules were added to the strategy upon analogy with other rules As the number of rules does not affect the effectiveness of the pro cedure except the pro cessing time to a small degree we believe that unused rules should not be removed because they may prove useful in the future Indeed the subset common nouns used seven rules that were not used for the initial DELAC nouns while toponyms used one new rule The rules used most frequently for nouns row 5 are rules pertaining to 
adjective noun MWUs which are also the most frequent in the dictionary The rules that failed each time they were used row 7 are obvious candidates for deletion from the strategy Rules that were more unsuccessful than successful row 8 should probably also be reconsidered as for instance by reinforcing the conditions if possible The average number of solutions per item is rather low row 9 ranging from 1 4 for the common nouns to 3 7 for toponyms In some cases much larger sets of solutions were offered The leader is Velika Plana a small town in Serbia with 52 solutions offered Such a high number of solutions o ccurred due to the homography of both components However even in this case the correct solution Automatic Construction of a Morphological Dictionary of MWUs 235 Table 6 Strategy rules performance data for subsets N initial DELAC nouns A initial DELAC adjectives CN additional common nouns T additional toponyms N 1 2 3 4 5 6 7 8 9 10 11 12 13 A CN T Items 2571 207 784 206 Rules applied 85 19 56 33 Rules not 
applied 201 5 230 253 Applications of rules 4083 434 1060 769 Most frequently used rule nc axn ac 2x2 nc axn nc axn3 number of times applied 1499 108 589 144 Absolutely successful rules 26 9 19 1 Absolutely unsuccessful rules 9 1 16 26 Rules more unsuccessful than successful 36 6 23 29 Solutions item 1 6 2 1 1 4 3 7 Maximum solutions per item 38 6 19 52 80 64 54 77 91 85 68 43 Success 1st solution Success 2nd solution 7 58 42 21 4 73 11 84 Success 3rd solution 3 62 3 02 1 44 0 0 had a high rank namely the second solution for Velika Plana was correct It may seem reasonable to exclude some dictionaries of simple words from this procedure for instance dictionaries of personal names in order to alleviate similar problems However that might not be such a good idea these very dictionaries successfully pro cessed many items among them three very specific names of small towns in Serbia named after famous Serbian po ets and politicians Aleksa 5 Implementation and Usefulness Our pro cedure for automatic pro duction of 
DELAC entries is a mo dule of the LeXimir tool 6 which is written in C and operates on the NET platform It supports development maintenance and exploitation of various resources edictionaries wordnets and aligned texts A user of this tool need not use all of these resources or even possess them but those that exist are visible in all mo dules and can be exploited in a useful way The e dictionaries of simple and MWU words that we develop using LeXimir are used primarily within the Unitex system As Unitex is open source software distributed under the terms of LGPL we easily incorporated its mo dules in LeXimir for many tasks that involve manipulation of e dictionaries including dictionary look up used in the mo dule for automated pro duction of DELAC 236 C Krstev et al entries To manipulate the strategy in the form of XML do cuments our tool relies on W3C standard languages XQuery and XSLT supported by NET The user interface of the mo dule for automatic pro duction of DELAC lemmas is very friendly A user can 
choose files with lists of MWUs and a strategy and results are presented in a form of a table in which the user has only to check the correct solutions upon which a list of DELAC entries is produced Various debugging tools and preference selections are at the user s disposal It has already been shown that LeXimir can be used for languages other than Serbian and English Our new mo dule for pro duction of DELAC entries can also be successfully applied without any mo dification to other languages that have dictionaries of simple words in DELA thus directly supported by Unitex naturally corresponding XML do cuments representing the strategy for a particular language need to be created However the system can be easily mo dified to support other formats of simple words dictionaries because only the dictionary look up mo dule has to be changed The experiment is already in progress for Polish for which Multiflex is used for inflection of MWUs but simple word dictionaries are handled in a different database 
environment 13 Another tool WS4QE shortened for Work Station for Query Expansion was developed on basis of LeXimir and it enables expansion of queries submitted to the Google search engine 6 Integrated lexical resources enable mo difications of user queries for both monolingual and multi lingual search The main feature of WS4QE is that it enables inflection of simple words and MWUs supplied as keywords to Google Again Unitex and Multiflex are used for inflection However WS4QE goes one step further for free phrases supplied as key words having a structure covered by a MWU inflectional class the tool uses our strategy and acts upon the first offered solution which is the correct one in most cases 6 Conclusions The results that we have presented justify the efforts invested in designing our pro cedure because it allows for massive pro duction of DELAC entries We have already prepared a list of 25 000 MWUs extracted from the Serbian explanatory dictionary that we hope to be able to pro cess in a few months One 
important thing that remains to be done is the addition of semantic and or domain markers to MWU lemmas which have so far been systematically added only for proper names following the approach suggested in 14 We are considering several solutions including one proposed in 15 We envisage further development of our procedure We would like to allow MWUs to be components of other MWUs and components of free phrases as well This would keep the number of possible structures low and consequently reduce the number of inflectional classes and the number of rules in the strategy For instance a lemma for a MWU from the beginning of this article civilni vojni rok could in this case be civilni civilni A2 adms1g vojni rok vojni rok nc axn ms1q nc axn Automatic Construction of a Morphological Dictionary of MWUs 237 That is its second component could be a MWU itself vojni rok military service and not a simple word and it would remain in the most frequent adjectivenoun class This approach is already implemented in Multiflex 
but not in Unitex However DELAC entries that are already pro duced need not be revised References 1 Courtois B Silberztein M Collocation Extraction in Turkish Texts Using Statistical Methods Senem Kumova Metin1 and Bahar Karao glan2 Izmir University of Economics Engineering and Computer Science Faculty Izmir Turkey senem kumova ieu edu tr Ege University International Computing Institute Izmir Turkey bahar karaoglan ege edu tr 1 2 Abstract Collocation is the combination of words in which words appear together more often than by chance Since collocations are blocks of meaning they play an important role in natural language processing applications word sense disambiguation part of speech tagging machine translation etc In this study a corpus of Turkish is subjected to the following statistical techniques frequency of occurrence mutual information and hypothesis tests We have utilized both stemmed and surface form of corpus to explore the effect of stemming in collocation extraction The techniques are evaluated 
by recall and precision measures Chi square hypothesis test and mutual information methods have produced better results compared to other methods on Turkish corpus In addition we have found that a stemmed corpus facilitates discrimination between successful and unsuccessful collocation extraction methods Keywords Collocation collocation extraction 1 Introduction Collocations are conventional word combinations that co o ccur together so recurrently that they may not be regarded as random combination of words The term collocation has been first intro duced by an English linguist J R Firth in the book Modes of Meaning 1 in which he states that a word can be understoo d by the company it keeps and gives some examples to illustrate the notion of collocations In his further study he states Collocations of a given word are statements of the habitual or customary places of that word Later Sinclair a student of Firth defined collocation as the o ccurrence of two or more words within a short space of each other in a 
text 2 In contrast Hoey 3 gives a more statistical definition stating that a collocation is the appearance of two or more lexical items together with a probability that cannot be interpreted as random In Oxford Collocation Dictionary collocation is defined as the co o ccurrence of words to pro duce natural sounding speech and writing Since collocations are arbitrary and indefinite they have an important effect on meaning in text and speech As a result extracting of collocations supports a H Loftsson E Collocation Extraction in Turkish Texts Using Statistical Methods 239 wide range of natural language processing applications such as natural language generation machine translation word sense disambiguation part of speech tagging information retrieval computational lexicography corpus linguistic search and in some social studies through language 4 5 In order to serve for this wide range of applications many different metho ds of collocation exploration can be found in the literature which can be categorized as 
statistical and rule based metho ds Rule based methods depend especially on part of speech tagging information On the other hand statistical metho ds frequency measure mutual information 6 hypothesis testing etc are based on some kind of frequency measure to extract collocations in a given corpus Smadja s Xtract 7 and the techniques of Kita et al 8 and Shimohata et al 9 are also examples of methods known by the names of researchers In this study we have applied some statistical techniques to extract collocations in Turkish and compared the results using recall and precision measures We have utilized both stemmed and surface formed corpora to examine the effect of extensive agglutination in Turkish Although there are many studies on different languages including English Spanish Russian Chinese French to the best of our knowledge there is no corresponding study on Turkish in this concept We believe that our results may further open research in the field of agglutinative languages especially Turkish In section 
2 the term collocation is presented In section 3 we have given previous work on Turkish collocations In section 4 collocation extraction techniques which are implemented in the study are briefly described In section 5 experimental setup which clarifies utilized corpora the base set and evaluation metho d is given Section 6 involves the implementation results Finally section 7 deals with the discussion of the above study 2 Collocation As it is evident from different definitions of collocation in recent works there are no known rules for the formation of collocations Although researchers do not have a total consensus on either the definition of collocation or the rules by which they are created common features collected from different studies may be listed and defined as in below Collocations are recurrent Of all properties which discriminates collocations from other word combinations recurrence is the easiest property to measure As a result almost all extraction techniques depend on some kind of frequency 
measure 4 6 7 10 11 Collocations are arbitrary and language specific There are no known rules that define which words collocate and how a word chooses a particular word or words from millions of different words in language to create a collocation For example strong is a common collocation with coffee in English But there is no clear explanation for the preference of this word instead of powerful Also collocations may change in different languages depending on the so cial or 240 S Kumova Metin and B Karao glan cultural behaviors of native speakers In Turkish strong coffee is called sert kahve the exact translation of the words to English gives hard coffee Collocations create a unit block in language In natural language processing applications considering sense or meaning integrity a unit block may be defined as a single word or a combination of words that has an individual meaning sense and may be regarded as a sentence or a constituent of a sentence Especially in applications such as word sense 
disambiguation part of speech tagging or machine translation detection of units is an important prepro cessing step that affects the whole performance of the proposed system For example the collocation lady killer means a man exceptionally attractive to women rather than one who kills them So for collocations the meaning of the whole is not the meaning of the parts Collocations are domain dependent There are many different domain specific collocations especially in particular sports medicine or science Smadja 7 gave the domain of sailing as an example Word combinations a dry suit or a wet suit do es not mean a suit that is literally dry or wet they are special types of suit which sailors use but these meanings are not obvious for even native speakers Since the definition of collocation is still a controversial issue in our study we assumed the following word combinations as collocations 3 Collocation Extraction in Turkish Turkish is a highly pro ductive language through extensive agglutination with a rich 
set of derivational and inflectional suffixes In a theoretic manner it is possible to derive millions of different word forms from just a particular lexeme in Turkish As a result computational linguistic applications have a high level of complexity of time and space In addition to this high level of complexity in applications language mo dels may need mo difications for surface formed corpus and stemmed corpus Collocation Extraction in Turkish Texts Using Statistical Methods 241 Recent work on Turkish collocations involves commonly linguistic studies discussing the importance of collocation notion in translation and second language education or examining the collocativity of a particular word in written Turkish texts 12 13 14 In the area of computational linguistics Oflazer et al 15 propose a rulebased multiword expression processor The pro cessor extracts multiword expressions in a morphologically analyzed corpus in which parts of speech and inflections are all tagged Multiword expressions are categorized 
in four different forms in the study by Oflazer et al lexicalized collocations semi lexicalized collocations non lexicalized collocations and multiword named entities Depending on the certain morphological patterns multiword collocations are retrieved by querying about 1100 rules 4 Some Collocation Extraction Techniques There are various statistical and rule based techniques for collocation extraction We have applied common statistical techniques to avoid the time and space complexity of the prepro cessing steps needed in rule based techniques In the following subsections utilized techniques frequency of o ccurrence mutual information and some of hypothesis tests t test log likelihood chi square will be briefly described 4 1 Frequency of Occurrence The frequency of o ccurrence metho d is the simplest and earliest approach on collocation extraction In this technique the frequency of bigrams or frequency of words that co o ccur in a given window designates whether the word combination is a collocation or not 
Combinations are ranked by the frequency to create a candidate list and the most frequent combinations are accepted as collocations Although some of the frequent candidates are collocations others are pairs of function words 5 To discard these frequent function word pairs filtering e g part of speech tagging is recommended in many existing studies 16 4 2 Mutual Information In information theory mutual information is defined as the quantity that measures the mutual dependence of the two variables In collocation extraction instead of two random variables the definition is mo dified for values of random variables Thus a new measure point wise mutual information is intro duced 6 10 If we write x and y for the first and the second word respectively point wise mutual information for them is given by I x y log2 P x y P x If the words x and y are independent of each other the probability of the words coming together must be equal to the multiplication of their own probabilities 242 S Kumova Metin and B Karao glan P 
x y P x To decide whether a word combination is a collocation it is necessary to prove that the joint o ccurrence of the words is more than coincidence The common approach showing the dependence between words is testing the hypothesis of independence Hypothesis testing methods attempt to reject the null hypothesis that states that words in combination are independent of each other The different metho ds testing null hypothesis used in literature are described as follows 4 4 Dunning s Log Likelihood Test Log likelihoo d metho d is a hypothesis testing approach presented by Dunning 11 In collocation discovery two alternative explanations for the o ccurrence frequency of bigram w1w2 is examined Hypothesis 1 P w2 w1 p P c2 c12 N c1 c1 c2 c12 are respectively the number of occurrences of w1 w2 and w1w2 N is the total number of words in the corpus Assuming a binomial distribution b k n x xk 1 x n k log likelihood ratio is then as follows log log b c12 c1 p b c2 c12 N c1 p L H 1 log L H 2 b c12 c1 p1 b c2 c12 N c1 
p2 2 logL c12 c1 p logL c2 c12 N c1 p logL c12 c1 p1 logL c2 c12 N c1 p2 where L k n x xk 1 x n k Mood 1974 has shown that 2log has an asymptotically 2 distribution So if the calculated values 2log are less than Collocation Extraction in Turkish Texts Using Statistical Methods 243 2 value at given level of significance the null hypothesis of independence is accepted otherwise the hypothesis that states w1w2 is a collocation is accepted As a result the log likelihoo d method pro duces a statistic that tells how much more likely one hypothesis is than the other the higher the number the closer the candidate is to being a collocations The metho d is applied to all word combinations in the corpus and a ranked list is generated to extract collocations 4 5 The t Test In the t test null hypothesis states that sample is drawn from a normal distribution with mean The test looks at the differences between expected and observed means scaled by variance of the data As a result if the observed mean differs from expected 
mean null hypothesis is rejected The t statistics is computed as s2 N where f w2 N f w1w2 P w1w2 N The null hypothesis is P w1w2 P w1 2 test is a hypothesis testing technique presented by Pearson The technique does not require normally distributed probabilities as in t test The test is applied to 2x2 tables to compare observed frequencies with expected frequencies 244 S Kumova Metin and B Karao glan Table 1 2x2 table showing frequencies for words beyaz and saray w1 beyaz w2 saray 8 beyaz saray w2 saray 15820 e g beyaz to examine whether the null hypothesis of independence can be rejected The expected frequency representing independence are calculated and if the observed frequencies differ from expected frequency null hypothesis is rejected Table 1 includes the 2x2 frequency table of the words beyaz white saray palace The couple beyaz saray refers to The White House in English The null hypothesis in 2 test may be stated as beyaz and saray are independent The 2 statistic sums differences between observed Oij 
and expected values Eij in all cells of the table and scales the differences by the magnitude of the expected as follows Oij Eij 2 4 2 Eij where i is row and j is column index in the table The expected frequency of each cell is computed from the totals of rows and columns converted into proportions The 2 value is calculated for all word combinations in corpus and a ranked list is generated The combinations having higher values are accepted as being collocations 5 5 1 Experimental Setup The Base Data Set Collocation extraction is a corpus based application As a result ideally a collocation tagged corpus is required Due to the great sizes of corpora however it is impossible to tag manually all collocations in a corpus In many studies researchers extract a base set from corpora and implement the metho ds on this set The base set may be constructed in many different ways It may be constructed from a specific word combination considering part of speeches For example adjective noun pairs in the corpora may be 
selected to create a base set as in the study of Evert and Krenn 17 Or the set may be retrieved from a dictionary 18 In our study we have used a different approach to generate the base set We have retrieved all bigrams in the corpus excluding those across sentence boundaries Afterwards five techniques defined in previous chapter are applied to generate a ranked list of bigrams In the ranked lists candidates having higher scores are assumed to be collocations We selected the first best 200 candidates in each list to create the base set and tagged the set manually Thus it not only became Collocation Extraction in Turkish Texts Using Statistical Methods 245 possible to compare all metho ds based on the same data set but also all preprocessing steps to retrieve collocation candidates in the corpus were eliminated This study utilized the Bilkent corpus compiled as Bilkent University for computational linguistic studies 19 The corpus consists of articles from popular newspapers over an interval of several years It 
has been morphologically analyzed by a finite state machine sentence boundaries and stemmed forms of words have been tagged automatically 19 We have applied collocation extraction techniques to both the stemmed and surface form of corpus to examine the effect of stemming in Turkish The corpus has about 719665 words and 48268 sentences Since collocations are defined as frequently occurring word combinations we have eliminated word combinations o ccurring less than four times in the corpus before the application of statistical extraction techniques 5 2 Evaluation Method Evaluation of extraction techniques defined in previous chapters is performed by recall and precision which are frequently used as performance measures in information retrieval For collocation extraction recall may be defined as the fraction of the collocations in the corpus or the base set that is successfully retrieved Precision is the fraction of true collocations in the retrieved list of collocation candidates Taking to be collocations 
extracted from base set and to be the number of true collocations in the base set recall and precision may be defined as r p While presenting the precision and recall values we have applied the approach of Evert and Krenn 17 In the approach instead of computing the measures for only a single proportion of candidate list for example just for the whole set or just for the first N candidates recall and precision are computed for N highest ranked candidates where N may vary from 1 to the total number of candidates N 1 2 3 base data set This approach prevents misleading conclusions being drawn from a single value of N We have plotted graphs of precision and recall for whole base set 6 Results Through agglutination in Turkish a stem can occur in many different forms due to many possible different inflections All words in collocations are prone to agglutination especially those in final position As a result in the corpus the same collocation may occur with different surface forms and this variation reduces the 
total frequency of the collocation to the extent that collocation may completely lose recurrence property For example maliye bakanli gi is a collocation that 246 S Kumova Metin and B Karao glan can be translated as ministry of commerce to English However this collocation may occur in the forms such as maliye bakanli gina to the ministry of commerce or maliye bakanli ginin of the ministry of commerce maliye bakanli ginda in the ministry of commerce maliye bakanli giyla with the ministry of commerce Therefore the frequency of o ccurrence is widely spread across different forms This property induced us to expect that collocation extraction may give better results for stemmed corpora in which different word forms are all merged to one stem The implementation of extraction techniques on the Bilkent corpus has returned a larger base set for stemmed corpus 661 bigrams compared to surface formed corpus 507 bigrams as expected Base sets involve 53 5 and 49 8 true collocations respectively for surface and stemmed 
forms of the Bilkent corpus The proportions give us the baseline for the precision graphics As a result if one particular metho d gives lower values than the baseline for a particular N value it is said to be even worse than random selection Figures 1 and 2 show precision graphs of surface and stemmed Bilkent corpus respectively The horizontal axis in the graphs presents the percentage of base set completed In the graphs three important results are pointed out Firstly it is noteworthy that 2 and mutual information metho ds give consistently higher precision for both stem and surface form lists In contrast log likelihoo d t test and frequency measure metho ds perform even worse than the assumed baseline which is random selection Secondly in the stemmed corpus as expected precision values are higher and the gap between 2 and mutual information metho ds with the others are more apparent Finally it can be seen from the figures that the stemmed corpus gives a clearer indication of which metho ds are successful 
and which are not and the degree of difference between them Figures 3 and 4 show that 2 and mutual information metho ds reach higher scores of recall earlier compared to other metho ds supporting precision results Correlatively 2 and mutual information metho ds generate higher scores for true collocations and extract them earlier than other metho ds 1 0 9 0 8 0 7 Precision 0 6 0 5 0 4 0 3 0 2 0 1 0 0 20 40 Frequency of Occurence Mutual Information Log likelihood The t test Chi square test 60 80 100 Fig 1 Precision graph for Bilkent corpus surface form Collocation Extraction in Turkish Texts Using Statistical Methods 247 1 0 9 0 8 0 7 Precision 0 6 0 5 0 4 0 3 0 2 0 1 0 0 0 2 0 4 Frequency of occurence Mutual Information Log likelihood The t test Chi square test 0 6 0 8 1 Fig 2 Precision graph for Bilkent corpus stemmed form 1 0 9 0 8 0 7 Recall 0 6 0 5 0 4 0 3 0 2 0 1 0 0 20 40 60 80 100 Frequency of Occurence Mutual Information Log likelihood The t test Chi square test Fig 3 Recall graph for Bilkent corpus 
surface form 1 0 9 0 8 0 7 Recall 0 6 0 5 0 4 0 3 0 2 0 1 0 0 20 40 60 80 100 Frequency of occurence Mutual Information Log likelihood The t test Chi square test Fig 4 Recall graph for Bilkent corpus stemmed form 248 S Kumova Metin and B Karao glan 7 Discussion In this study some statistical methods were applied to Turkish corpus to retrieve collocations 2 and mutual information methods generated higher precision and recall values compare to other techniques Methods are both utilized on stemmed and surface form of corpus to explore the effect of agglutination in collocation extraction It is seen that the stemmed corpus generated results that are more effective in discriminating between successful and unsuccessful metho ds In a further work we hope to be able to improve metho ds of analysis in the light of the results of this study References 1 Firth J R Modes of Meaning Papers in Linguistics 1934 51 Oxford University Press Oxford 1957 2 Sinclair J M Corpus Concordance Collocation Oxford University Press 
Oxford 1991 3 Hoey M Patterns of Lexis in Text Oxford University Press Oxford 1991 4 Bisht R K Dhami H S Neeraj Tiwari N An evaluation of different statistical techniques of collocation extraction using a probability measure to word combinations Journal of Quantitative Linguistics 13 Collocation Extraction in Turkish Texts Using Statistical Methods 249 16 Justeson J S Katz S M Principled Disambiguation Discriminating Adjective Senses with Modified Nouns Computational Linguistics 21 1 1995 17 Evert S Krenn B Methods for the qualitative evaluation of lexical association measures In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics Toulouse France pp Towards the Design and Evaluation of ROILA A Speech Recognition Friendly Artificial Language Omar Mubin Christoph Bartneck and Lo e Feijs Department of Industrial Design Eindhoven University of Technology Den Dolech 2 5612 AZ Eindhoven The Netherlands o mubin c bartneck l m g feijs tue nl Abstract In our research we argue for the 
benefits that an artificially designed language that we call ROILA could provide to improve the accuracy of speech recognition given that it is constructed on speech recognition friendly principles We also contemplate the trade off effect of users investing some effort in learning such a language Initially we present the design and evaluation of the vocabulary of ROILA and subsequently we describe the ROILA grammar and the method by which we rationally chose grammar rules Our evaluation results indicated that the vocabulary of ROILA significantly outperformed English whereas we could not yet replicate similar trends while evaluating the grammar Keywords Artificial Languages Automatic Speech Recognition Sphinx 4 1 Introduction Recent research in speech recognition is gradually progressing towards altering the medium of communication in a bid to improve the quality of speech interaction As stated in 12 constraining language is a plausible metho d of improving recognition accuracy In 15 the user experience of 
an artificially constrained language was evaluated within a movie information dialog interface and it was concluded that 74 of the users found the constrained language interface to be more satisfactory than natural language interface The limitations prevailing in current automatic speech recognition technology for natural languages is an obstacle behind the unanimous acceptance of Speech Interaction Generally in speech interfaces the fo cus is on using natural language it may be time to explore a different balance in the form of a new language The field of handwriting recognition has followed a similar road map The first recognition systems for handheld devices such as Apple s Newton were nearly unusable Palm solved the problem by inventing a simplified alphabet called Graffiti which was easy to learn for users and easy to recognize for the device Using the same analogy we aim to design a Speech Recognition Friendly Artificial Language ROILA where an artificial language as defined by the Oxford Encyclopedia 
is a language deliberately invented or constructed In linguistics there are numerous artificial H Loftsson E Towards the Design and Evaluation of ROILA 251 languages for e g Esperanto Interlingua whose goal is easier communication amongst users however there has been little or no attempt to optimize a spoken artificial language for speech recognition In summary our research is constructed on the basis of two main goals Firstly the artificial language should be optimized for efficient automatic speech recognition and secondly there should be an attempt to make it learnable for a user two possibly contradictory requirements for e g users would prefer shorter words but shorter words would be harder to recognize 2 Vocabulary Design In order to obtain a group of phonemes that could be used to generate the vocabulary of ROILA we conducted a phonological overview of natural languages 10 Extending from our goal of designing a language that is easy to learn for humans we extracted a set of the most common phonemes 
present in the ma jor 13 natural languages of the world based on number of speakers We used the UCLA Phonological Segment Inventory Database UPSID 11 The database provides an inventory of the phonemes of 451 languages of the world We generated a list of phonemes that are found in 5 or more ma jor languages This resulted in a total of 23 phonemes Certain other constraints were employed to reduce this list further diphthongs were excluded and phonemes that had ambiguous behavior across languages were ignored Therefore the final set of 16 phonemes that we wished to use for our artificial language was in ArpaBet notation AE B EH F IH JH K L M N AA P S T AH W As a starting point for the first version of the vocabulary of ROILA we choose the artificial language Toki Pona 6 which caters for the expression of very simple concepts by just 115 words Therefore this number formed the size of the ROILA vocabulary In order to maintain a balance between our two research goals we set the word length to 4 5 and 6 characters 
with each word having 2 or 3 syllables rendering the following word types CVCV VCVC VCCV CVCVC VCVCV CVCVCV VCCVCV VCVCCV where V refers to a vowel and C to a consonant from our pool of 16 phonemes The 8 word types were simple extensions of words existing in Toki Pona based on the assumption that such words would be easy to learn and pronounce To define the scalable representation of the words we utilized a genetic algorithm that would converge to a vocabulary of words that would have the lowest confusion amongst them and in theory be ideal for speech recognition The genetic algorithm randomly initialized a vocabulary of N words for P vocabularies where each word was any one of the 8 afore mentioned word types The algorithm was then run for G generations with mutation and cross over being the two primary offspring generating techniques The fitness function was determined from data available in the form of a confusion matrix from 7 where the matrix provided the conditional probability of recognizing a phoneme 
pj by a speech recognizer when phoneme pi was said instead Therefore the confusion between any two words was determined by computing the probabilistic string edit distance as suggested 252 O Mubin C Bartneck and L Feijs in 1 The first ROILA vocabulary was generated by running the algorithm for P G 200 In order to have a benchmark of English words to compare against we set the English vocabulary as the meanings of the 115 Toki Pona words 2 1 Vocabulary Evaluation In order to evaluate ROILA 16 6 female voluntary participants were asked to record samples of every word from both English and ROILA The recordings were then passed offline through the Sphinx 4 4 speech recognizer Participants had various native languages but all were graduate students and hence had reasonable command over English Recordings were carried out using a high quality microphone Sphinx was tuned such that it was able to recognize ROILA by means of a phonetic dictionary however an acoustic mo del for English was used In addition we did not 
carry out any training on the acoustic mo del for ROILA One of the researchers conducted rounds of ROILA recordings until we had a pool of recordings that rendered a recognition accuracy of 100 These sample recordings of every word would be played out before participants recorded each ROILA word This was done to ensure that the native language of participants would not affect their ROILA articulations The experiment was carried out as a 2 condition within sub ject design where the language type English ROILA was the main independent variable The dependent variable was the total number of errors in recognition Words from both English and ROILA were randomly presented and the order of recording English or ROILA first was also controlled between participants We carried out a repeated measure ANOVA which revealed that language type did not have an effect F 1 9 0 758 p 0 41 Both ROILA and English performed equally in terms of accuracy 67 61 and 67 66 respectively Without any training data such accuracy is 
expected from Sphinx on test data 14 To judge if ROILA word structure had an effect on recognition accuracy we executed an analysis in which the type of word was the independent variable This factor had 2 levels namely CV or non CV type where CV type words were CVCV CVCVC and CVCVCV The ANOVA analysis revealed a nearly significant trend F 1 113 3 6 p 0 06 CV type words performed better on recognition on average 4 19 participants got such words wrong as compared to non CV type words where 5 75 participants got them wrong Therefore for our second iteration of the evaluation we generated a new vocabulary that comprised of CV type words only The genetic algorithm was run with the parameters G P 200 We had 11 4 female from the earlier 16 participants carry out recordings of the new vo cabulary using the same setup and pro cedure We did not have them record the English words again Participants would once again hear sample pronunciations The REMANOVA revealed that the new ROILA vocabulary significantly outperformed 
English F 1 10 4 86 p 0 05 see Figure 1 The accuracy for the 11 participants was English 65 11 and ROILA CV 71 11 This vocabulary was hence declared as the first ROILA vocabulary Towards the Design and Evaluation of ROILA 253 Fig 1 Average Errors Bar Chart for English and ROILA CV 3 Grammar Design In conjunction with conducting a phonological overview of artificial languages we also carried out a morphological overview of artificial languages individually and also in contrast to ma jor natural languages of the world 10 This aided us in identifying grammar features which were popular in both sets of languages We determined several grammatical categories based on properties defined in various linguistic encyclopedias 5 Gender numbering tense and aspect are some examples However within each category there were a number of options that we could choose from for e g should we have gender How many tenses should we have In order to make our choice we carried out a rationale decision making pro cess by utilizing the 
Questions Options and Criteria QOC technique 8 For this purpose we defined the following important criteria for every grammatical property Learnability defines whether the grammatical marking in question would be easy to learn or not Expected recognition accuracy defines the effect the grammatical marking would have on the anticipated word error rate given that the more constrained a grammar lower perplexity is the better it would be for recognition 9 Vocabulary size describes the effect the grammatical marking would have on increasing or decreasing the vocabulary size Expressive Ability of the language defines whether using the grammatical marking in question would actually enable speakers to express more concepts then they would have been unable to do so otherwise Efficiency simply relates the grammatical marking to how many words would be required to communicate any solitary meaning Acknowledgement within Natural and Artificial Languages states the popularity of the particular grammatical marking amongst 
each type of languages Appropriate weights were assigned to the criteria based on importance for e g learnability and expected recognition accuracy were assigned higher weights with recognition accuracy being given twice as much weight as learnability The total sum of the weights was 1 254 O Mubin C Bartneck and L Feijs Table 1 ROILA Example Sentences ROILA Sentence pito fosit bubas pito fosit jifi bubas pito fosit jifo bubas English Translation I am walking to the house I walked to the house I will walk to the house Literal Meaning I walk house I walk past tense marker house I walk future tense marker house All possibilities of each grammatical category were listed and every category was then ranked across the criteria by giving a number between 1 and 3 with 3 being the best fit The category which yielded the highest output was then chosen to be as the grammar category of choice After filling in a matrix we concluded firstly that the ROILA grammar would be of isolating type Affixes would not be added as 
this might alter the word structure hereby reducing their efficiency for speech recognition Therefore grammatical categories in ROILA would be represented by word markers see Table 1 At the end we arrived at the following properties Gender male female on the level of pronouns only and not nouns Numbering singular plural on the level of nouns Person references first second third on the level of pronouns Tense past present future and word order would be SVO 3 1 Grammar Evaluation In order to evaluate the grammar in terms of recognition we formulated some sample sentences N 30 based on a hypothetical interaction scenario for a dialog system These sentences were evaluated against their English semantic counterpart Sphinx 4 Language Models were created using the Sphinx Knowledge Base tool 13 An identical setup was followed as done in the evaluation of the vocabulary except that participants would now record sentences and not isolated words Participants would once again hear a sample voice as a guide of how to 
pronounce sentences The dependent variable was word accuracy a common metric to evaluate continuous speech recognition 3 with the independent variable yet again language type In the initial evaluation we conducted recording sessions with 8 participants However we were unable to achieve significant results in favor of ROILA as indicated by the REMANOVA results F 1 7 1 97 p 0 21 4 Discussion and Conclusion Our results revealed some interesting insights Firstly we were able to achieve improved speech recognition accuracy as compared to English for a relatively larger vocabulary Similar endeavors have only been carried out for a vocabulary size of 10 2 Secondly we quantitatively illustrated that CV type words perform better in recognition co articulation of CV syllables could be one explanation for that We must keep in mind several implications to our results Firstly participants recorded words without any training in ROILA whereas they were already Towards the Design and Evaluation of ROILA 255 acquainted with 
English Potentially by training participants in ROILA the accuracy could be further improved This effect was observed to be more pronounced when participants had to speak ROILA sentences which could explain the insignificant difference between ROILA and English in terms of the accuracy of speech recognition The acoustic mo dels of Sphinx are trained with dictation training data and from what we observed the ROILA sentence articulations of participants did not fall within the domain of dictation speech There were pauses between words and pronunciations were not smooth which could have been caused by the inexperience of the participants in ROILA In the future we aim to conduct more evaluation sessions of ROILA sentences after carrying out training with additional participants It may also be observed that the acoustic mo del of Sphinx was primarily designed for English yet our ROILA accuracy in the second vocabulary iteration were significantly better as compared to English a promising result indeed What we 
would also like to determine in the future is the magnitude of the difference between ROILA and English This could be accomplished by using the same English acoustic mo del for another natural language and comparing the differences in recognition accuracy between the three languages English ROILA and the second natural language We acknowledge the trade off factor of humans having to invest some energy in learning a new language like ROILA even though in various steps of the design pro cess we have tried to accommo date the aspect of human learnability and intro duce language features which were conducive to learnability In summary by designing an artificial language we are faced with the effort a user has to put in learning the language Nevertheless we wish to explore the benefits that an artificial language could provide if its designed such that it is speech recognition friendly This factor might end up outweighing the price a user has to pay in learning the language and would ultimately motivate and 
encourage them to learn it Another criticism that might be levied on ROILA is that many artificial languages were created already but not many people ended up speaking them Where our approach is different is that we aim to deploy and implement our artificial language in machines and once certain machines can speak the new language it could encourage humans to speak it as well In the future we aim to train participants in ROILA and evaluate it by deploying it in an interaction context We acknowledge that a meaningful so cietal application of our language would provide an extra gain in addition to recognition performance We aim to explore applications for children medical tasks or care robots1 Acknowledgements We would like to thank the reviewers for their helpful comments and feedback to revise the paper References 1 Amir A Efrat A Srinivasan S Advances in phonetic word spotting In The Tenth International Conference on Information and Knowledge Management pp 1 To know more about ROILA and its latest 
developments please visit http roila org 256 O Mubin C Bartneck and L Feijs 2 Arsoy E Arslan L A universal human machine speech interaction language for robust speech recognition applications In Sojka P Kope cek I Pala K eds TSD 2004 LNCS LNAI vol 3206 pp Time Expressions Ontology for Information Seeking Dialogues in the Public Transport Domain Agnieszka Mykowiecka Institute of Computer Science Polish Academy of Sciences J K Ordona 21 01 237 Warsaw Poland agn ipipan waw pl Abstract The paper presents an ontology of natural language temporal expressions which occur in dialogues led by users of a public transport call center It was elaborated on the basis of analysis of 500 transliterated dialogues and contains a multihierarchy of concepts representing semantics of time referring fragments of interlocutors utterances The paper contains also an analysis of frequencies of all types of time relevant expressions within the analyzed data Keywords temporal expressions in Polish time ontology 1 Introduction Time 
expressions occur in dialogues of nearly every type and for many of them resolving time references is crucial for successful communication One of the domains for which temporal relations are extremely important is information about public transport connections In this domain a description of time together with space and transportation lines organization issues are the most important areas of interest As representing time points or intervals and inferring about their interdependencies is crucial for natural language understanding no wonder that there are a lot of time models and temporal logics describing ways of operating on time points or H Loftsson E 258 A Mykowiecka a day and also imprecise notions like later However the annotation guidelines concern English lexical triggers and their usage for recognizing boarders and types of temporal phrases in texts in another natural language is limited One of the first complex solutions of the problem of linguistic temporal expressions interpretation was proposed 
within the Verbmobil project for the task of appointment scheduling 2 In recent years a lot of different ideas were explored in all the domains enumerated above 4 The research presented here aimed at collecting and organizing linguistic material concerning temporal linguistic constructions in Polish The motivation of this work was the fact that although in general time expressions in Polish are similar to English their exact formulations have to be described on the basis of real data The constructed set of concepts was used as a starting point for creating machine learning models for solving a problem of automatic recognition of temporal concepts in the chosen type of Polish dialogues In an experiment described in 7 of semantical labeling using CRF model for 15 types of time related concepts occurring in the test set the F measure ranged from 0 5 for time_rate_rel to 0 97 for at_hour and 1 0 for before_hour only concept names without values were evaluated The data which is our source of knowledge about the 
way people talk about time in the context of using public transport is a set of Polish dialogues collected at the Warsaw Transport Authority call Time Expressions Ontology for Information Seeking Dialogues 259 2 Temporal Expressions Ontology The ontology presented in the paper is a newly defined OWL resource There are two reasons for this solution First although a lot of effort has been put into making ontologies reusable using already defined ones like 8 is still very difficult and all problems connected with their full understanding projection into a chosen subdomain and expansion issues described for example in 9 still exist The second reason was connected with our 260 A Mykowiecka Fig 1 Upper part of the class hierarchy of all defined subclasses is given The second case in which a lot of class instances differing in values of a datatype property can occur is marked by the The fourth column contains the number of different natural language phrases which occurred within the corpus Their examples together 
with English translations for cases in which a name of a class is not sufficient for understanding the phrase are given in the last column As it is not easy to clearly linearise the multidimensional division the subsections which try to represent it are not always quite precise For most classes their exact placement in the hierarchy is given in Figure 1 The numbers given support the statement that in this domain relative temporal expressions are very frequent For example for about 2300 time concept occurrences there are 448 occurrences of TIME_REL objects which represent relative time of a day in intervals On the other hand the phrases used to represent them are not very diverse 41 type of phrases The most frequent notion of this class is Later 170 occurrences were expressed by only 6 types of phrases There are also quite a lot of partial expressions not only those giving hours without giving minutes but there are also a lot of expressions in which only the minute part is given 261 Such an expression can 
either mean an additional specification of an exact hour which was already given before or address every hour in a given time period The next observation is the fact that there are some domain related language expressions which are not likely to be found in general resources e g godziny szczytu peak hours Time Expressions Ontology for Information Seeking Dialogues Table 1 Temporal concepts statistics name TIME_DAY_LEVEL Points DATE DATE_ABS DATE_REL TIME_OF_WEEK_WRK HOLIDAY WORKINGDAY Intervals TIME_YEARPERIOD_ABS DATE_ABS_BEG DATE_ABS_END TIME_HOUR_LEVEL Points AFTER_HOUR AROUND_HOUR AT_HOUR AT_HOUR_MINPART AT_HOUR_PART BEFORE_HOUR IN_X_HOURS IN_X_MINUTES Intervals DAYTIME_PERIOD_BEG DAYTIME_PERIOD_END DAYTIME_PERIOD_SPAN TIME_DAYPERIOD _AFTERNOON _DAY _EVENING _MORNING _NIGHT _OFFPEAK _PEAKHOURS _WHOLEDAY TIME_REL _AtThisTime _Earlier _Earliest _Now _Early _Later _TheSame _Tomorrow _Today _TooEarly _TooLate Span TIME_SPAN TIME_SPAN_D TIME_SPAN_H RIDE_DURATION_H RIDE_DURATION_M RIDE_DURATION_REL WALK_
DURATION Rates RATE_HOUR_ABS RATE_HOUR_REL _Freq _LessFreq _MoreFreq _NoFreq occurr different different examples subtypes phrases objects 261 18 14 4 62 30 32 30 22 2 2 2 8 2 2 2 1 1 9 2 8 1 11 10 1 20 4 16 pierwszy wrzenia 1 09 dzie urodzin birthday w dni nierobocze non working days w dni robocze working days 11 10 od pierwszego kwietnia from 1 04 1 do 31 sierpnia to 31 08 33 58 757 261 38 31 3 16 50 25 18 79 2 3 4 36 12 1 11 5 448 12 23 5 95 2 170 6 69 45 6 45 12 3 9 7 169 11 5 69 28 10 3 3 12 19 26 420 62 23 13 1 8 21 17 17 8 8 1 1 1 1 1 1 1 1 11 11 1 1 1 1 1 1 1 1 1 1 1 2 2 2 8 2 40 2 5 13 4 5 1 1 1 1 23 57 514 74 24 15 3 12 27 21 18 22 2 2 4 6 1 1 3 2 41 3 7 2 4 1 6 5 4 4 2 2 po dwudziestej after 20 okolo dziesitej around ten dziesita osiem eight past ten zero zero pitnacie po fifteen after dwunasta 12 00 przed dziesit before 10 za jak godzink za jakie 2 8 2 92 5 5 28 12 4 2 1 6 okolo tydzie around a week dziesi minut ten minutes mniej wiecej godzin around an hour maksimum czternacie minut dlugo long 
dwie trzy minuty minuta drogi dwie co godzin raz na 262 A Mykowiecka 3 Further Work In the paper the ontology for representing Polish phrases referring to temporal expressions used in one chosen domain was presented Two different directions of further usage of this resource are planned First the ontology will be utilised while implementing an algorithm for resolving time references in the original domain of public transport information Second the experiment with expanding this resource with time related notions which occur in a different domain of medical clinical notes is planned The next possible subject for further research could be a detailed comparison of type of phrases used in Polish dialogues with phrases used in another natural language in a similar context There is also planned to establish a method for automatic conversion of the elaborated annotation into TIMEX labels References 1 Ahn D van Rantwijk J de Rijke M A cascaded machine learning approach to interpreting temporal expressions In HLT 
NAACL ACL pp Reliability of the Manual Segmentation of Pauses in Natural Speech Raoul Oehmen Kim Kirsner and Nicolas Fay University of Western Australia Stirling Hwy Crawley 6008 Perth Western Australia raoul oehmen graduate uwa edu au Abstract Recent innovations regarding analysis of pauses in natural speech have necessitated the segmentation of increasingly small pause durations from the speech stream 1 Identifying pauses and pause durations relies on human judgement However the reliability of these judgements has yet to be established This study investigated the reliability of multiple segmentations of four speech files Results suggest that while inter analyst reliability is moderate intra analyst reliability was high Furthermore inter analyst variation appears to be related to the signal to noise ratio of the speech files A further analysis of the segmentation of one speech file demonstrated that a lack of reliability was associated with certain non speech vocalizations suggesting that reliability could 
potentially be increased with more precise guidelines for analysts Keywords Pauses Fluency Distributional Fitting Reliability 1 Introduction Traditional analyses of pause durations in spontaneous speech have applied arithmetic means to real time pause frequency distributions 2 similar to those shown on the left in Figure 1 However significant skew in these distributions makes this problematic 3 More recently the frequency of durations of pauses has been more accurately viewed in log time where pauses form two normal distributions 1 4 as shown on the right in Figure 1 These distributions can be mathematically modelled and descriptive statistics obtained They have been dubbed the short pause SP and long pause LP distributions and are thought to represent pauses related to articulatory and cognitive processes respectively 1 in line with traditional accounts 2 However prior to modelling pause frequency distributions pauses must first be identified from the speech stream For more than forty years a widespread 
disinterest in short articulatory pauses due in part to poor recording and analysis resolutions has led to the use of a variety of minimum pause duration thresholds the smallest pauses considered to be real Numerous different thresholds used across different studies have made comparisons difficult as changes in threshold affects almost every descriptive statistic 5 The pervasive 250msec threshold employed by GoldmanEisler 2 to exclude articulatory pauses was perhaps inaccurate as numerous pauses between 130 and 250msec have been shown to have both cognitive and expressive H Loftsson E 264 R Oehmen K Kirsner and N Fay Fig 1 Pause frequency distributions in both real time left and log time right for a typical speaker Note log time frequency distribution includes bimodal two distributional fit functions 6 Furthermore the junction between articulatory and cognitive pauses has been shown to be highly variable between individuals 1 Technological and theoretical advances have allowed more recent approaches to 
embrace articulatory pauses via the use of ultra short minimum pause thresholds no minimum 7 10 msec 4 and 20msec 1 This raises the additional question of the accuracy of such fine judgements by analysts in discerning vocalisations from brief silences Such judgements are necessary regardless of whether pauses are segmented from speech entirely by hand or whether a semi automated procedure is used e g 8 and 9 Semi automated procedures typically consider speech to be any time when an amplitude contour rises above some threshold usually set manually to the level of ambient noise 10 Thus manual and semi automated procedures differ only with regards to whether analysts pick a single best fit threshold or decide upon thresholds on a pause by pause basis While semi automated systems have an advantage in terms of consistency 6 their performance may be degraded under conditions of low signal to noise ratios caused by a range of factors including recording conditions stress and aphasia Furthermore while automated 
systems are reliant purely on the amplitude contour modern analysis equipment puts many more measures at the disposal of the manual analyst with none of the resolution problems inherent in early studies 2 Albeit with a few exceptions 4 11 reliability statistics have seldom been reported and no detailed analysis of variations between segmentations regardless of measurement techniques has been conducted to date This study will determine both Inter analyst and Intra analyst reliability in addition to investigating the causes of any observed variation 2 Method 2 1 Stimuli The stimuli to be analysed consisted of four spontaneous monologues elicited via a stimulus question averaging 1 86 minutes and containing on average 162 discernable pauses Each contained a different English first language adult speaker 3 females Reliability of the Manual Segmentation of Pauses in Natural Speech 265 and 1 male and each discussed different topics Files MB SK CN were recorded on dynamic unidirectional lapel microphones into 
portable digital recorders File RO was recorded using a head mounted dynamic unidirectional microphone in an acoustically controlled laboratory with a signal to noise S N ratio of 53 86 Files SK CN were recorded in a quiet laboratory setting and had S N ratios of 20 36 and 16 36 respectively while File MB was recorded in a home setting and had a S N ratio of 5 90 All files were recorded at a sampling frequency of 44100 Hz 2 2 Segmentation Four experienced pause analysts two qualified Speech Therapists one Linguist and one Psychologist segmented each of the sound files In addition each analyst segmented one of the four files a second time after an interval of roughly one week This provided five segmentations of each sound file and 20 segmentations in total Segmentation was conducted in PRAAT 12 via the procedure described in 13 This physical approach ignores linguistic considerations entirely requiring analysts only to place boundaries at those points in the file where a period of silence transitions into a 
period of speech and vice versa Thus each pause consists of two boundaries acknowledging the on off sequence of vocalisations that comprise speech Sounds that were clearly not associated with a speech act such as non stylistic coughs and audible movements of the muscles of articulation were excluded Analysis was conducted primarily via the visual modality making use of spectrographic information including amplitude and fundamental frequency contours but also by listening to the speech The minimum pause duration threshold employed in the present study was 10msec while the viewing window was set at 0 6 seconds 2 3 Distributional Analysis Pauses were modelled in an analogous fashion to that reported in 1 Pause durations were converted to logarithms and grouped into a series of bins forming pause frequency distributions for each speaker separately A Expectancy Maximisation algorithm 14 was then applied to each distribution fitting the best two distribution model to the data in an attempt to minimise log 
likelihood A number of statistics can then be derived from the modelled distributions including the means standard deviations and pause rates per minute of speech of the long and short pause distributions 3 Results Distributional statistics for each segmentation were obtained following application of the Expectancy Maximisation algorithm 14 Measures of variation were calculated for Inter analyst reliability between the different segmentations of the same file by different analysts by calculating the standard deviation of the four segmentations of each file for the Long and Short Pause distributional means Similarly measures of Intra analyst variation were calculated as the standard deviation of the repeat segmentations of one file by each analyst The measures are presented in Table 1 below and show intra analyst variability to be lower than Inter analyst variability Furthermore the data indicate that variation increases with signal to noise ratio This point is seen in Figure 2 where larger shaded areas 
representing variation between segmentations 266 R Oehmen K Kirsner and N Fay are associated with those files with lower signal to noise ratios This is further demonstrated by a Pearson correlation coefficient of r 0 60 between the variability in short pause means and signal to noise ratio albeit with only 4 data points Table 1 Signal to noise ratio and Inter Intra analyst distributional statistic variation for the short and long pause distribution mean of four speech files in log File MB Signal Noise Ratio Inter Intra SP Mean Variation LP Mean Variation SP Mean Variation LP Mean Variation 5 90 0 50 0 32 0 18 0 05 File CN 16 36 0 16 0 09 0 01 0 04 File SK 20 36 0 49 0 13 0 02 0 03 File RO 53 86 0 17 0 07 0 03 0 00 Fig 2 Long pause mean plotted against short pause mean for all segmentations of four speech files inter analyst reliability Shaded areas represent the spread of distributional parameters between analysts segmentation of the same file Despite File RO being the highest quality recording variation 
still remained between analysts A more detailed analysis of the placement of boundaries junctions between speech and pause between segmentations of this file was therefore conducted Two important analyses were conducted the reliability with which analysts find a particular junction agreement and the variability in msec of the positioning of boundaries between analysts Variation between positioning of analysts boundaries was calculated by taking the absolute root mean square difference as used in 4 For the present file Reliability of the Manual Segmentation of Pauses in Natural Speech 267 this was 5 83msec for all boundaries located by two or more analysts For voice offset boundaries speech changing to pause those followed by a short pause had significantly less variation compared to those followed by a long pause absolute root mean square difference of 4 41msec and 12 63 respectively t 61 2 65 p 0 01 With regards to agreement out of a total of 471 boundaries discovered by one or more analysts 66 03 of 
boundaries were located by all four analysts while 76 92 of boundaries were located by at least 3 of the 4 analysts Only 14 32 and 8 76 of boundaries were located by one or two analysts In an attempt to determine more specifically the cases of low agreement all speech surrounding boundaries was coded into phonemic categories Table 2 below shows the proportions of some of these events for both speech onset and speech offset boundaries Boundaries located by one or two analysts appear to be characterised by a disproportionately high percentage associated with preparatory movements for articulation e g sounds emitted by opening the mouth prior to vocalisation as well as boundaries associated with fricatives Similarly there appears to be a disproportionately low number of boundaries associated with plosives that were detected by only one analyst Table 2 Percentage of boundaries comprising each phonemic category at varying agreement levels separated by whether the boundary signifies the onset or offset of speech 
Category Nasal Plosive Fricative Long Vowel Short Vowel Prep Artic Onset of Speech after pause 1 4 2 4 3 4 4 4 0 00 4 76 0 00 1 96 12 12 33 33 42 31 41 18 24 24 19 05 15 38 18 30 9 09 4 76 3 85 7 19 12 12 9 52 7 69 22 88 21 21 14 28 19 23 1 31 Offset of Speech before pause 1 4 2 4 3 4 4 4 14 71 5 00 12 00 23 37 11 76 25 00 28 00 23 38 23 53 10 00 8 00 16 23 8 88 35 00 12 00 11 69 11 76 0 00 16 00 12 34 25 71 5 00 12 00 2 60 4 Discussion Results suggest that while intra analyst reliability is high inter analyst reliability is only moderate with sizable differences between segmentations that can be linked to the signal to noise ratio of the files With regards to intra analyst segmentations the data suggest that whatever reasoning or judgement is being used it is being applied consistently However in inter analyst segmentations an increase in variability at low S N ratios could be due to a decrease in the discriminability of the signal This would bring differences in analysts criterion or their propensity to 
say the signal was present to the fore in line with traditional Signal Detection Theory accounts 15 While these findings suggest that attention should be paid to the quality of recordings as well as using only a single analyst where possible it is clearly not feasible to insist on laboratory quality recordings Instead future research could investigate possible increases in accuracy associated with the use of additional variables e g pulse rates and formant frequency as well as changing spectrogram viewing settings A further detailed analysis of the positioning of boundaries in a single file revealed greater variation related to the positioning of boundaries proceeding long pauses pauses occurring after the completion of a motor plan compared to proceeding short 268 R Oehmen K Kirsner and N Fay pauses pauses occurring within a motor plan One possible explanation is that the occlusion of vocalisation within a motor plan is likely to be under stricter temporal constraints due to the following speech than at the 
completion of a motor plan This may lead to a more defined boundary and make analysis more precise Nonetheless variation in the placement of boundaries does not account for the extent of the variation between analysts statistics Variation is more likely to be due to imperfect agreement on the presence or absence of boundaries In particular it appears that disagreements are frequently caused by misclassification of non speech artefacts such as a preparatory articulation and audible breathing events as speech Such problems can likely be resolved with improved segmentation guidelines for analysts References 1 Kirsner K Dunn J Hird K Parkin T Clark C Time for a pause Speech Science Technology Melbourne 2002 2 Goldman Eisler F Psycholinguistics Experiments in spontaneous speech Academic Press London 1968 3 Quinting G Hesitation phenomena in adult aphasic and normal speech Mouton The Hague 1971 4 Rosen K M Kent R D Duffy J R Lognormal distribution of pause length in ataxic dysarthia Clinical Linguistics and 
Phonetics 17 Large Scale Language Modeling with Random Forests for Mandarin Chinese Speech to Text Ilya Oparin Lori Lamel and Jean Luc Gauvain LIMSI CNRS Spoken Language Processing Group B P 133 91403 Orsay cedex France oparin lamel gauvain limsi fr http www limsi fr Abstract In this work the random forest language modeling approach is applied with the aim of improving the performance of the LIMSI highly competitive Mandarin Chinese speech to text system The experimental setup is that of the GALE Phase 4 evaluation This setup is characterized by a large amount of available language model training data over 3 2 billion segmented words A conventional unpruned 4 gram language model with a vocabulary of 56K words serves as a baseline that is challenging to improve upon However moderate perplexity and CER improvements over this model were obtained with a random forest language model Different random forest training strategies were explored so as to attain the maximal gain in performance and Forest of Random 
Forest language modeling scheme is introduced Keywords language modeling random forest speech to text ASR STT Mandarin Chinese 1 Introduction The task of language mo deling is to create language mo dels LM that are able to capture the regularities of a natural language A language mo del is an inherent part of any modern speech to text STT system Language mo dels are used in STT systems along with acoustic mo dels AM and a pronunciation dictionary that links the them to perform large vocabulary continuous speech recognition The basis of modern language mo dels is the word N gram approach The omnipresent assumption of N grams is that the o ccurrence of a word can be predicted according to its immediate in case of bigrams or short range left context Word N grams appear extremely efficient in practice Frankly speaking despite the fact that it has already been many decades since they were introduced into the recognition field N grams are still the most common framework for language mo deling since their modeling 
capacity is hard to beat This work aims to improve over a baseline N gram mo del by using random forest LMs The peculiarity of this work is that the brute force N gram LM is trained on very large amounts of text data over 3 billion of word tokens H Loftsson E 270 I Oparin L Lamel and J L Gauvain without any pruning and cut off This model is thus robust and very challenging to improve upon The baseline recognition system is a competitive state of theart Mandarin STT system that was developed at LIMSI for and submitted to the GALE Phase 4 evaluation Language mo dels are usually evaluated by means of word error rate WER and perplexity Due to peculiarities of Mandarin Chinese it is common practice to measure speech recognition performance in terms of the character error rate CER rather than the traditional WER However WER or CER do not give an opportunity to compare LMs directly since WER CER results also include the impacts of both the acoustic mo del and the deco der Thus if the systems to be compared differ 
either in the acoustic component or in the deco der no LM comparison can be made The perplexity is widely used in speech recognition community to evaluate the LM independently from the full system P P 2H PM w1 w2 wm m 1 1 is a per word entropy PM is the probability assigned to a string of Where H words from a test corpus by a LM and m is the length of a string in words In the next section random forest approach RF to language mo deling is intro duced The data and experimental setup are described in Section 3 Perplexity and speech recognition accuracy results are given in Section 4 with conclusions and directions for future work presented in Section 5 2 2 1 Random Forest Language Models Decision Trees The decision tree DT mechanism for estimating probabilities of words following each other has long been known as an alternative to the N gram approach for language mo deling in STT 1 Several studies showed that stand alone DTs do not outperform traditional smoothed N gram mo dels 2 However with recent advances 
in language mo deling that extend the use of decision trees to that of random forests this research direction has reentered the research spotlight With the help of DTs it is possible to cluster together similar histories i e possible previous words to the one being predicted at the leaves of a tree Each leaf forms an equivalence class of histories that share the same probability distribution over words to predict Usually binary DTs are implemented in which sets of possible histories are split at every node with a yes no question If the predictor i e position in N gram history we ask questions about is the previous word a question looks like Is the previous word in the set S or S The data i e N grams corresponding to yes answers are propagated through one branch going out of a node the no data is passed along the other branch Actually a conventional N gram model can be represented as a special case of a tree mo del For example a bigram mo del may represented by a DT in which the yes set consists of one 
individual word at each node Ideally during the training phase all possible predictors and questions should be tried at each node to split the Large Scale Language Modeling with Random Forests 271 data and the best predictor question pair should be picked and stored for that node However in real life greedy algorithms have to be used A DT is constructed in a way to reduce the uncertainty about the event being predicted Thus entropy can naturally be used as the goo dness measure One should measure entropy for training data M in a node before split then w w C w S log C w S C S 2 A random forest is a collection of decision trees that include randomization in the tree growing algorithm The underlying assumption is that while one DT do es not generalize well to unseen data a set of randomized DTs interpolated 272 I Oparin L Lamel and J L Gauvain together might and actually should perform better Greedy algorithms are used at the stage of DT construction for choosing the best questions to split the data We also do 
not take into account questions asked at other nodes when searching for the one to be asked at a given node As a result trees are only locally but not globally optimal with respect to training data Randomized trees where the randomization is intro duced during tree construction i e finding the best questions to ask at each node are not locally optimal but the collection of them may be and actually is closer to the global optimum and thus these provide better results Different randomization schemes may be used to randomize DTs in order to form a RF The most commonly used metho ds are random predictor selection to ask questions about and random initialization of greedy algorithms used to find the best question in a node It should also be noted that the RF approach is also a promising framework to incorporate different sources of information into a language mo del 4 6 The RF models were shown to consistently outperform word based N gram mo dels for relatively small scale tasks e g the Wall Street Journal 
portion of Penn Treebank 2 4 5 7 reported improvements in recognition performance with random forest LMs trained on limited data that take account of morphological features for inflectional languages Improvements in recognition rate after rescoring N best lists generated with a conventional N gram mo del with a RF mo del were also reported for Mandarin Chinese for the GALE task with LMs trained on about 700 million word tokens of data 5 8 The setup used here is similar to the latter However our experiments are characterized by significantly larger training data size and a lower CER for the recognition baseline 3 3 1 Experiments Chinese Mandarin STT System The GALE Phase 4 Mandarin Chinese setup was used for the current experiments The system is a highly competitive one with a language mo del trained on large amounts of Mandarin Chinese data thus providing the system with robust linguistic estimates This makes improving upon the performance attained with this system a very challenging task Recognition 
vocabulary In written Chinese words are not separated by white spaces The natural solution is thus to make use of character based LMs or perform word segmentation as a pre pro cessing step The former was shown to be inferior to the latter 9 so the segmentation approach was taken in this work Due to ambiguity word segmentation in Chinese is not a trivial task as even native speakers of Chinese may disagree in certain cases 10 Several approaches to automatic word segmentation in Chinese exist 11 In this work we make use of the longest match algorithm based on the 56052 word vocabulary used in LIMSI STT systems developed for previous GALE Mandarin Chinese evaluations This is a simple greedy algorithm that tries to match the longest possible word according to the vocabulary adds a space after it and shifts to the Large Scale Language Modeling with Random Forests 273 next character after the space to search for another word The segmented files are used to train word based LMs The vocabulary also includes all 
individual Chinese characters This way we avoid the problem of having out of vo cabulary words in a text after the segmentation Decoding The speech recognizer is a development version of the LIMSI STT system used in the AGILE participation in the GALE 09 evaluation Word recognition has one deco ding chain with three passes The first pass generates a word lattice with cross word position dependent gender independent acoustic mo dels followed by consensus decoding with 4 gram and pronunciation probabilities 12 13 Unsupervised acoustic model AM adaptation is performed for each segment cluster using the CMLLR Constrained Maximum Likelihoo d Linear Regression and MLLR 14 techniques prior to the next deco ding pass The first deco ding pass is done with an MLP PLP f0 acoustic mo del the second uses a PLP F0 based mo del and the third pass also uses an MLP PLP f0 acoustic mo del All AMs are tied state left to right context dependent CD HMMs with Gaussian mixtures The triphone based CD phone mo dels are word 
independent but position dependent The tied states are obtained by means of a decision tree The mo dels all use speaker adaptive SAT and Maximum Mutual Information Estimation MMIE training They are trained on 1400 hours of manually transcribed broadcast news and broadcast conversation data distributed by LDC for use in the GALE program using both standard PLP and concatenated MLP PLP features For the PLP mo dels a maximum likelihoo d linear transform MLLT is also used The model sets cover about 49k phone contexts with 11 5k tied states and 32 Gaussians per state Silence is mo deled by a single state with 2048 Gaussians Initially speaker independent mo dels are trained on all of the available data and serve priors for Maximum a Posteriori MAP estimation of gender specific mo dels LM training data The LM training data consists of 48 different text sources in Mandarin Chinese These sources are collected by different institutions and are diverse in size genre and internal structure The data includes transcripts 
of broadcast news and broadcast conversations newspaper texts text collected from the web etc The description of the data available for the GALE Phase 3 evaluation can be found e g in 15 For the Phase 4 evaluation new text corpora of Mandarin Chinese became available Some corpora are entirely new some are the extended versions of the ones that existed before The new data were added to that used for the previous evaluations to train the language mo dels The total amount of new data is 590 32M words after segmentation It represents a 22 5 increase in the data available for training as compared to the data available for the previous evaluations 2 6G words resulting in a corpus with 3 2 billion word tokens Baseline Language Model The baseline LM is a word based 4 gram LM Individual LMs are first built for each of the 48 corpora These mo dels are smoothed 274 I Oparin L Lamel and J L Gauvain Table 1 Perplexity on different GALE evaluation sets Set Phase 3 LM Phase 4 LM dev07 181 184 eval07 206 206 dev08 194 192 
dev07 eval07 dev08 193 194 dev09 234 211 dev09s 230 207 according to unmo dified interpolated Kneser Ney discount scheme 16 No cutoffs and pruning is imposed thus allowing the LMs to take account of all possible information These individual mo dels are subsequently linearly interpolated together with the interpolation weights tuned on dev09 data It should be noted that the dev data is never used it for RF tuning e g as held out data for controlling the DT growing pro cess This is done in order to keep the same test conditions and to not intro duce any biases As the number of individual mo dels is 48 one mo del is trained for each available corpora this small number of parameters do es not result is bias towards this data This is supported by the comparison with the previous baseline 4 gram mo del developed for the GALE Phase 3 evaluation That LM was tuned on the dev07 eval07 dev08 subset As can be seen from Table 1 current Phase 4 LM attains the same perplexity results on dev07 eval07 dev08 data even though 
it was not tuned on these and it performs significantly better on dev09 and dev09s data The GALE Phase 4 dev09 sets were used in this study to evaluate the performance of different mo dels A subset of dev09 called dev09s was also defined for this evaluation It constitutes about half of dev09 data The baseline N gram Phase 4 LM perplexity is 211 on dev09 and 207 on dev09s sets In the system submitted to Phase 4 evaluation word lattices generated with the baseline LM are subsequently rescored with the Neural Network language model NNLM 17 However in this study the NNLM was not applied in order to assess the improvement with RFLMs over N gram mo dels 3 2 Training of RF Models The SRILM compatible RF toolkit was used in these experiments with random forests 18 Growing DTs for large training data and large vocabularies is very computationally expensive We found it infeasible to do straightforward RF training for the 3 2 billion words data used to train the baseline N gram mo del Thus in our experiments we tried 
different strategies to train RF mo dels that would result in building RFLMs in reasonable time benefit from all the available data and improve the baseline at the same time An important feature of tree based models is that after an RF is grown i e the structure of constituent randomized DTs are established it is possible to Large Scale Language Modeling with Random Forests Table 2 Data chosen to estimate RF probabilities corpus word sampling rate sampled word bcm bnm data 19 42M 1 0 19 42M ng 315 52M 0 01 3 15M giga xin 366 96M 0 008 2 93M ibm sina 279 87M 0 01 2 79M giga cns 76 73M 0 03 2 28M 275 pour larger amounts of data down to the leaves The structure of a DT is a set of nodes and leaves together with questions that are assigned to each node A question for a binary word based DT is a position in history it is asked about e g 1 for an immediate left neighbor and words constituting yes no sets for this position Thus after a DT is grown we can propagate data down to the leaves if a particular N gram 
contains a word from a yes set at a specific position it is propagated along one branch if it contains a word from the no set the N gram is pushed along the other If a N gram contains a word that is in neither set the background N gram LM is used to bailout to estimate the probability Thus any N gram either ends up in the leaf or gets its probability from the backoff LM Pouring larger amounts of training data down to the leaves make probability estimations more robust since they are based on larger data RF on restricted data Decision tree training may be performed on restricted data This is the first experiment we ran before addressing the problem of making use of all available data The crucial point is thus to choose the training and heldout data that is likely to be representative of the test data For the GALE Mandarin Chinese task this is broadcast news Mandarin bnm and broadcast conversations Mandarin bcm transcribed data as it constitutes the target type of data in the evaluations The training data was 
chosen to contain all available bnm and bcm transcriptions except for the recent bcm and bnm data released during Phase 4 The latter was chosen as the heldout data used as a stopping criterion during the DT training phase After the structures of constituent DTs are defined the training data together with additional data is poured down to the leaves to get more robust probability estimates The additional data was taken from the remaining top four according to the interpolation weights text sources These text sources are quite large and thus were downsampled The resulting size corresponds to the weights inferred during interpolation of separate source N gram LMs to form the baseline N gram mo del see Table 2 Heldout data is usually used as a stopping criterion during the DT training phase In the SRILM compatible RFLM toolkit 18 the DTs are actually fully grown on the basis of the training data and then pruned according to gains on heldout data However in 8 it was shown that shallow RFs that contain DTs of 
limited depth have performance close to the RFs consisting of fully grown DTs We thus first compare the performance of RFs consisting of fully grown 276 I Oparin L Lamel and J L Gauvain and shallow DTs Another issue that needs evaluation is the number of DTs to form an RF Usually 100 or 50 randomized DTs are sufficient to train a RF The perplexity results for different RF configurations are presented in Table 3 The numbers 50 and 100 correspond to the number of randomized DTs that constitute a RF The second and third columns correspond to the performance of RFs as stand alone models while the last two columns show the perplexity when the RFs are interpolated with the baseline 4 gram LM As can be seen from this table while the RFs with DTs of maximum 1000 nodes appear to be too shallow the ones with 10000 nodes perform close to the fully grown and subsequently pruned trees There is also no really significant difference between RFs consisting of 50 and 100 trees trained on restricted data RF trained on 
different sources As already mentioned the baseline 4gram LM is obtained as result of interpolation of many sub LMs each being trained on one of 48 available Mandarin Chinese corpora The interpolation weights are tuned on dev09 data Applying the same strategy to RF construction seems a natural thing to do A problem arose in that the corpora are very large containing hundreds of millions words which was found infeasible to train RFs straightforwardly Thus a different strategy was utilized for these corpora Individual RFs are not trained for large corpora The DTs trained on restricted data are used instead The data of large corpora is poured down these trees Thus tree structures are the same as for the restricted data RF but the probability distributions in the leaves are estimated on the data from specific large corpora There are a total of 48 sources used to train Mandarin Chinese mo dels It was found to be feasible to train specific RFs for 34 of the sources Randomized DTs or to be more precise their 
structures in terms of nodes and questions asked in the nodes trained on restricted data see section 3 2 were used to pour down the counts for the larger 14 corpora Training RFs for specific sources consumes a lot of computational time and puts high demand on memory usage Training about 50 full grown RFs consisting of hundred DTs may keep busy a modern computational cluster with couple of dozens nodes for months The performance of shallow trees with maximum 10000 nodes was shown to be close to that of the full grown trees on restricted data At the same time such trees are much faster to train As a result we trained shallow DTs with a maximum of 10000 nodes for each individual corpora Another decision that was taken on the basis of these results is that 50 randomized DTs are basically enough to form a RF The final RF is obtained by interpolation of RFs corresponding to different sources We call such an RF a Forest of Random Forests FRF Table 3 Dev set perplexity for different RF configurations on dev09 set DT 
depth 50 DTs 100 DTs 50 DTs interp 100 DTs interp fully grown 279 4 276 1 206 8 206 4 10000 nodes 299 1 295 7 207 9 207 7 1000 nodes 358 1 356 1 210 7 210 7 Large Scale Language Modeling with Random Forests 277 4 Results Decision tree probabilities were discounted according to mo dified Kneser Ney scheme and used together with a corresponding Kneser Ney smoothed 4 gram LM The N gram LM is used as a backoff mo del This backoff mo del is trained on restricted data see section 3 2 A total of 50 randomized DTs form the random forest As already mentioned the perplexity of the dev09 and dev09s data sets are respectively 211 and 207 with the baseline 4 gram LM For dev09s set the perplexity with the RF trained on restricted data is 293 which is higher than that with the best interpolated N gram LM trained on all available data When these two are interpolated together the perplexity of this data set decreases to 201 corresponding to a 3 relative improvement However we are mostly interested in checking the results on 
dev09 since this set was used to tune the N gram mo dels for individual corpora in order to form the baseline N gram LM The dev09 perplexity with different RFLMs are given in Table 4 The random forests were trained on different sources as described in Section 3 2 The RF type RF corresponds to the RF trained on restricted data as described in Section 3 2 As for the dev09s setup the RF on its own performs worse than the N gram LM but a small gain in perplexity is observed when these mo dels are interpolated According to results obtained with RFLMs using smaller setups one would expect a perplexity reduction over N gram baseline with standalone RF mo dels if trained on the same data However in these restricted data experiments we by definition use much less data to train a RF and also do not make use of interpolation of LMs trained on the different data sources The FRF in Table 4 stands for the Forest of Random Forests that takes account of all available data with interpolation weights tuned on dev09 The stand 
alone perplexity of the FRF is 10 better than the perplexity of the RF However no further improvement after the interpolation with the baseline Ngram mo del was observed The RF minimum and maximum perplexities for individual corpora RFs are 302 and 414 The perplexities with the different individual N gram mo dels range from 485 to 2614 The perplexity distribution for individual corpora RFLMs are thus much flatter This must be due to the fact a backoff N gram LM plays significant role in RF probability estimation Another explanation of this fact is the shallow nature of randomized DTs that form RFs with the limitation of 10000 nodes that was imposed on the mo dels This makes individual mo dels smoother and as a result the interpolation of these mo dels is actually less promising as compared to that of individual N gram LM mo dels Having observed the interpolation weights for different RFs in FRF we found them considerably different from the weights of N gram mo dels that form the final N gram LM In FRF the 
RFs for the smallest corpora obtained unexpectedly large weights We presume the reason is as follows The mo dified KneserNey N gram LM trained on restricted data is used as a backoff mo del for the RF The Kneser Ney discount coefficients for 4 grams are rather high especially for the singleton 4 grams discount1 0 873236 discount2 1 476549 discount3 278 I Oparin L Lamel and J L Gauvain Table 4 Perplexity for differently trained RFLMs on dev09 set RF type Stand alone ppl Interpolated ppl RF 299 207 FRF 268 208 FRF with N gram weights 283 210 FRF without small corpora 279 208 RF modified KN discounts 334 208 FRF modified KN discounts 282 208 1 364567 Thus a lot of probability mass is taken from RF estimates and transferred to the N gram mo del that is used as a backoff mo del This effect is more severe in case an RF is trained on very small corpora in our setup there are four corpora that are several order of magnitude smaller than the others and thus contain many singleton 4 grams The smallest corpora got as 
much as 0 3 of the total weight which looks rather strange The backoff N gram LM thus contributes most to the final probability estimation At the same time the N gram backoff mo del is trained on restricted data performs well with the perplexity of 309 on dev09 Consequently at the stage of weight optimization for different RFs these mo dels may be given prominence This helps to reduce the perplexity of the stand alone FRF but not in interpolation with the baseline N gram LM In order to compensate for this effect we tried two experiments First we did not try to optimize weights for individual RFs but rather took the interpolation weights that were calculated to build the baseline N gram LM from N gram LMs corresponding to each of 48 corpora The results given in the row in FRF with N gram weights of Table 4 show that this approach do es not lead to an improvement Another possibility to compensate for the effect of unwanted backoff mo del prominence is eliminating small corpora RFs from the final interpolation 
We thus eliminated 4 of the 48 corpora The weights of the top corpora among the remaining 44 become less peaky but the results shown in the FRF without small corpora row in Table 4 give the impression this do es not solve the problem We also tried to hand edit the mo dified Kneser Ney discount co efficients for order 4 making them equal to 0 1 and then re estimate the individual RF mo dels This way we rely more on probability estimates for 4 grams provided by the RF mo dels and pass less probability mass to the backoff N gram LM The results for such mo dels are shown in the last two rows of Table 4 The lattices generated by the Mandarin GALE Phase 4 LIMSI STT system were rescored with the best RFLM the first one from the Table 4 The LM Table 5 CER of RF on dev09s set RF weight 0 00 0 10 0 15 0 20 0 25 0 30 0 50 1 00 CER 9 81 9 77 9 76 9 72 9 75 9 75 9 80 10 41 Large Scale Language Modeling with Random Forests 279 to generate these lattices is the baseline 4 gram LM described earlier As we already mentioned 
the lattices were not rescored with the neural network LM These results are presented in Table 5 The RF weight row corresponds to the weights given to the RFLM Small but significant improvement in CER over the baseline N gram mo del is observed with the RFLM 5 Conclusion and Future Work Improving over a robust state of the art STT system trained on large amounts of data is a very challenging task Many of the approaches that perform well on small and medium size tasks do not scale well to experiments on large data In this paper we presented results using random forest language mo dels to improve upon a well tuned competitive speech to text system for Mandarin Chinese Improvements both in perplexity and CER were observed However these improvements are significantly less impressive than those reported for smaller scale tasks This lessor degree of improvement can be expected for large scale tasks One can argue that the RF approach can actually be regarded as a sophisticated smoothing technique At the same time a 
baseline 4 gram LM with a comparatively small vocabulary of 56K words is trained on the very large corpus containing 3 2 billion words that makes the estimates provided by this mo del robust and rather reluctant to adding new ways of enhancements The results presented here are still preliminary Due to the very large size of training data and high computational demands imposed by a random forest LM several simplifications were made at the stage of RF construction E g the number of nodes was forced to be not much than 10000 while in fully grown trees it can be more than a million for very large corpora DT structures were not individually trained but only the corresponding counts were poured down the nodes of the trees trained on restricted data etc These simplifications may result in losing much of the potential gain that can be attained with RFLMs for example in the forest of random forests scenario Thus the ma jor direction of future work is performing efficient straightforward training of RF language mo 
dels on the same amounts of data available for N gram LM training Acknowledgments This work has been partially supported by OSEO under the Quaero program and by the GALE program Any opinions findings or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding organizations References 1 Bahl L R Brown P F de Souza P V Mercer R L A Tree Based Statistical Language Model for Natural Language Speech Recognition CSL 37 280 I Oparin L Lamel and J L Gauvain 3 Navratil J Jin Q Andrews W Campbell J P Phonetic Speaker Recognition Using Maximum Likelihood Binary Decision Tree Models In Proc of ICASSP 2003 Hon Kong pp Design and Evaluation of an Agreement Error Detection System Testing the Effect of Ambiguity Parser and Corpus Type Maite Oronoz Arantza IXA NLP Group University of the Basque Country maite oronoz a diazdeilarraza koldo gojenola ehu es http ixa si ehu es Abstract We present a system for the detection of agreement errors in Basque a language 
with agglutinative morphology and free order of the main sentence constituents Due to their complexity agreement errors are one of the most frequent error types found in written texts As the constituents concerning agreement can appear in any order in the sentence we have implemented a system that makes use of dependency trees of the sentence which abstract over specific constituent orders We have used Saroi a tool that obtains the analysis trees that fulfill a set of restrictions described by means of declarative rules This tool is applied to the output of two dependency analyzers MaltIxa data driven and EDGK rule based The system has been evaluated on two corpora a group of texts containing errors and another one composed of correct texts As a secondary result we have also estimated a measure of the impact of syntactic ambiguity on the quality of the results Keywords Grammar error ambiguity parsing 1 Introduction Detection of grammatical errors is a relevant area of study in computer assisted language 
learning and grammar checking This paper presents the implementation and evaluation of a system for the detection of agreement errors in Basque regarded as one of the most frequent kinds of error 1 iaz de Ilarraza et Referring to the pro cess of detecting grammatical errors H Loftsson E 282 M Oronoz A After the analysis of dependencies the system will make use of Saroi 3 a tool that given a set of dependency trees obtains those that fulfill the set of restrictions described by means of declarative rules Although the tool is useful for several types of tree inspection processes in this work we will use it for the detection of ungrammatical structures Saroi will be applied to the outputs of two dependency analyzers EDGK a knowledge based dependency parser 4 and MaltIxa 5 a data driven parser based on Maltparser a freely available and state of the art parser 6 For the evaluation of the system texts containing errors and correct texts from the Basque Dependency Treebank 7 will be used We are also concerned about 
the impact of morphosyntactic ambiguity in the quality of our system A lot of error detection has been carried out on English for which this kind of ambiguity is less of an issue but in morphologically rich languages a deep analysis of the influence of ambiguity in error detection is in our opinion fundamental Among the three main types of ambiguity that can be relevant to grammatical error treatment morphological syntactic and semantic our study will concentrate on measuring the effect of morphological and syntactic ambiguity in the results leaving aside semantic ambiguity The remainder of this paper is organized as follows After this intro duction section 2 relates our work to similar systems Section 3 comments on general aspects of agreement errors in Basque Section 4 will describe the linguistic resources used for the analysis of incorrect texts corpora and the main computational tools two dependency analyzers and Saroi a tool for tree inspection Section 5 will present the experiments performed and the 
main results obtained We conclude the paper in section 6 with our main contributions 2 A Bird s Eye View of Error Detection Techniques Approaches to grammatical error detection correction are difficult to compare due to mainly the following reasons i most of them concentrate on one error type and ii the lack of large available error corpora Choosing the more appropiate technique to the problem of error detection is not a trivial decision Empirical and knowledge based approaches can be used for this purpose Empirical approaches are suitable for error types related to the omission replacement or addition of elements For example Tetreault and Cho dorow 8 use machine learning techniques to detect errors involving prepositions in non native English speakers A deeply studied area using machine learning techniques is that of context sensitive spelling correction 9 where the ob jective is to detect errors due to word confusion e g to too Bigert and Knutsson 10 prove that precision is significantly improved when 
unsupervised metho ds are combined with linguistic information Regarding knowledge based methods many types of local syntactic errors have been detected by means of tools based on finite state automata or transducers such as Constraint Grammar CG 11 The Xerox Finite State Tool 12 or ad hoc systems Systems based on finite state techniques usually define error patterns enco ded in the form of rules which are applied to the analyzed texts Design and Evaluation of an Agreement Error Detection System 283 For global error treatment approaches based on context free grammars CFG or finite state techniques have been used For example CFG based systems have experimented with the relaxation of some constraints in the grammar 13 or have specially developed error grammars 14 Statistical parsers have also been used that get a measure of grammaticality 15 The relationship between ambiguity and error detection has been mentioned in very few o ccasions 16 17 Similarly to most NLP areas the development of tools for grammatical 
error detection finds ambiguity as a main obstacle for the design of efficient and accurate systems Birn 16 states that the errors accumulated through morphological and syntactic analysis make it difficult to detect grammatical errors 3 Agreement Errors in Basque Basque is an agglutinative language with free order among the elements of the sentence When classifying the errors related to agreement we can distinguish three types of contexts Table 1 Agreement error sbj subject obj object erg ergative abs absolutive Zentral nuklear r ak zakar erradiaktiboa eratzen dute Power station nuclear 0 the abs pl det rubish radioactive abs sg create aux sbj erg 3pl obj abs 3sg The nuclear power station create radioactive rubbish 284 M Oronoz A 4 4 1 General Linguistic Resources The Corpus The task of creating large sets of ungrammatical sentences is a necessary but time consuming activity A corpus of this type can be composed of sentences pro duced by language learners learner corpus or it can be taken from a general 
error corpus not necessarily produced in a language learning context 14 Although some approaches propose the automatic creation of ungrammatical sentences 18 we decided to use a set of genuine errors For evaluation we use two corpora 4 2 Syntactic Analysis The creation of NLP tools is a very expensive task so instead of preparing specially tailored resources for error processing we decided to use the existing systems in our group and perform the necessary adaptations to deal with illformed sentences For the analysis of the input texts we use the syntactic analysis chain for Basque 19 It is composed of three main components see figure 1 Design and Evaluation of an Agreement Error Detection System 285 Raw text Tokenizer Morphological analyzer Multi words MORFEUS Linguistic CG stochastic HMM disambiguation EUSTAGGER Named entities Shallow syntactic function disambiguation Noun and verb chains Morphosyntactic processing Chunking Level M1 M2 M3 M4 S1 S2 Linguistic features POS POS SubPOS POS SubPOS Case M3 rest 
of features SF nom verb chunks S1 main SF Method CG HMM CG HMM CG HMM CG CG CG EDGK Analyzed text MaltIxa Analyzed text Fig 1 The syntactic analysis chain for Basque and the disambiguation levels in it For the detection of agreement errors we applied Saroi a system developed to apply a set of query rules to dependency trees Saroi takes as input a group of analysis trees and a group of rules and obtains as output the dependency trees that fulfill the conditions described in the rules Its main general ob jective is the analysis of any linguistic phenomena in corpora 1 Labeled Attachment Score 286 M Oronoz A eratzen ncsubj zentral power station create agreement subj case n nk Detect ncsubj ncmod auxmod type nor nork ncsubj ncmod case ncobj zakar rubbish auxmod dute nor object ncmod nuklearrak nuclear case num per absolutive plural 3 ncmod erradiaktiboa radioactive case num per case num per absolutive singular 3 ergative singular 3 nork subject ERROR Fig 2 A rule left side detecting the agreement error in the 
dependency tree right side of the sentence in Basque Nuclear power station create radioactive rubbish Figure 2 shows an example of a rule that detects the error in the dependency tree of the same figure In the sentence the sub ject zentral nuklearrak nuclear power station in absolutive case and the auxiliary verb dute linked to the main verb eratzen create and which needs a sub ject in ergative do not agree Saroi uses as input the result of the syntactic analyzer see section 4 2 in which the relations between the elements of the sentence are ambiguous as a result of the remaining morphosyntactic ambiguity see figure 3 in which for example nuklearrak has 3 interpretations Then Saroi constructs all the set of non ambiguous trees starting from an initially ambiguous tree figure 3 The detection rules are applied to the expanded set of dependency trees eratzen 10 ncsubj ncobj auxmod 10 ncsubj ncobj auxmod 1 ncmod 2 6 ncmod 7 11 1 ncmod 2 10 ncsubj ncobj auxmod 6 ncmod 9 11 zentral 1 ncmod ncmod zakar 6 ncmod dute 
11 nuklearrak erradiaktiboa 2 4 5 7 9 Fig 3 Ambiguous tree and some of its corresponding non ambiguous trees 5 Experiments In this section we will first comment on the experimental settings of the evaluation 5 1 and 5 2 and then we will present the results obtained 5 4 5 1 Preprocessing When using the predefined linguistic analysis chain for the detection of agreement errors we had to take several aspects into account Design and Evaluation of an Agreement Error Detection System 287 Considering the problems mentioned in section 5 1 and being concerned about the impact of ambiguity in the quality of our analyzers we followed these steps 1 We chose the best option for morphological and syntactic disambiguation 2 Once we decided the appropiate disambiguation level we evaluated the system using two corpora correct and error corpora An important remark regarding evaluation is that we will not apply the standard development refinement test cycle but instead we will follow a development test metho dology a design of 
error detection rules in Saroi and b evaluation This means that there will not be a second step for the refinement of the rules after examining their results on a development set Our aim was to test the effectiveness of a set of clean error detecting rules over different settings corpus parser and ambiguity In that respect the rules examine clear and possibly naive linguistic statements e g the sub ject and verb must agree in case and number This also means that there will be room for improvement of the results after adapting the error detection rules to the details of real and or noisy data 2 http www ei ehu es 288 M Oronoz A 5 3 Election of the Disambiguation Level Due to morphosyntactic and syntactic ambiguity a number of trees ranging from 1 to more than 100 are generated for each sentence Taking into account the combinations of morphosyntactic and shallow syntactic function disambiguation levels the best disambiguation criteria should be those that a detect the highest number of errors in ungrammatical 
sentences b give the lowest number of false alarms in grammatical sentences and c generate the lowest number of analysis trees for each sentence efficiency With this ob jective we followed two steps 1 First we chose the best morphosyntactic disambiguation level 2 Second after the morphosyntactic disambiguation level was fixed we selected the best option for shallow syntactic function disambiguation For that reason we selected a set of 10 ungrammatical sentences and their respective corrections one for each sentence that is a total of 20 sentences The sentences were analyzed with the eight disambiguation combinations3 giving the results shown in table 2 The two combinations that generate the lowest number of trees with acceptable detection and false alarm rates were those performing the deepest morphosyntactic disambiguation that is M34 S1 and S2 Table 2 Looking for the best morphosyntactic disambiguation combination Disambiguation combinations M1 S1 M2 S1 M3 S1 M4 S1 M1 S2 M2 S2 M3 S2 M4 S2 Number of trees 
67 7 67 7 27 8 46 7 22 11 22 11 11 6 11 62 Errors in ungrammatical 5 5 6 6 5 5 6 6 False alarms in grammatical 0 0 1 1 0 0 1 0 Next we performed a deeper analysis to choose the best syntactic function disambiguation level S1 or S2 We soon realized that the grammar that assigns the dependency relations to correct texts need of relaxation when applied to ill formed ones For example in the sentence nik ez nago konforme I do not agree the word nik I was not tagged as subject as it carries the ergative case and the auxiliary verb asks for a sub ject in absolutive this is a constraint in the dependency grammar when assigning the subject tag We experimented relaxing all the conditions referred to the type of auxiliary in the rules assigning subject object and indirect object relations This relaxation is not performed for error detection this is done by means of error detection rules but it is necessary for the assigning of dependency relations to ungrammatical sentences Then in a second experiment we used a set of 
75 sentences containing agreement errors together with their corrections The sentences were analyzed with the M3 S1 Relaxed M3 S1 NotRelaxed M3 S2 Relaxed and M3 S2 NotRelaxed combinations The best results were obtained with the M3 S2 Relaxed option 3 4 8 combinations 4 morphosyntactic 2 syntactic Although the M4 S2 combination in table 2 seems to be good it sometimes creates too many trees and in other cases it does not obtain any analysis tree Design and Evaluation of an Agreement Error Detection System 289 that is the option that disambiguates the most and with the relaxed dependency relation assignment A deeper study about the impact of ambiguity in error detection is described in 2 5 4 Evaluation of the System After these tests we noticed that the results are directly proportional to the parser s accuracy When the relations are wrongly assigned the detection of agreement errors is difficult Sometimes a false detection occurs that is an erroneous sentence is flagged as incorrect but with a rule that is 
not the expected one The rules mark the sentence as incorrect but they fail in the diagnosis Correct corpora We evaluated our system against the Basque Dependency Treebank Its relations are presumably perfect there is no need of a parser neither the problem of ambiguity nor partial parsing so the system should perform well This experiment served to evaluate the system on false alarms A subset containing 1906 trees was used After applying the detection rules 161 errors were flagged 8 45 of the corpus As this implies a high false alarm rate we made a detailed analysis table 3 finding out that Table 3 Evaluation results on the Basque Treebank Flagged by the system Numb From FA From treebank Not considered FA 90 55 9 4 72 FA 63 39 13 3 30 Real errors 8 4 97 Total 161 8 45 290 M Oronoz A Error corpora We also performed an evaluation of the system on error corpora using both EDGK and MaltIxa We applied the agreement detection rules to all the possible analysis trees of the sentences We calculated four results 1 
Using a data driven parser MaltIxa M 2 The knowledge based parser EDGK E 3 MaltIxa and EDGK M E An error will be marked if it is flagged in the dependency trees obtained by MaltIxa and EDGK 4 MaltIxa or EDGK M E If an error is flagged on the output of either of the syntactic analyzers the sentence will be deemed erroneous Examining the results in table 4 we see that when applying the full set of error detection rules precision varies between 24 26 and 26 19 As could be expected the best precision results were reached with the option M E when the error is flagged in the trees analyzed by both analyzers the system is certain about the error However recall falls down 24 44 In general looking to both precision and recall the two best options seem to be M and M E In general the data driven parser gets better results with correct texts and it also behaves better with incorrect sentences showing a robust behaviour There are two error detection rules named two subj and two obj that account for most of the false 
alarms both with EDGK and MaltIxa These rules mark the attachment of two sub jects ob jects to a verb This phenomenon can occur as a consequence of a genuine agreement error but also because of an incorrect dependency analysis and is the reason for many false alarms We think that as the frequency of incorrect analysis trees is relatively high these rules cause more harm than good For that reason we perform three experiments to confirm this assumption In the second row of table 4 we show the results without considering the rule that detects two sub jects two subj In the third one the rule that checks the appearance of two ob jects is removed two obj Table 4 Agreement error detection with MaltIxa and edgk All the rules M E M M Without the rule M two subj E M M Without the rule M two obj E M M Without the rules M two subj E and M two obj M Number of errors Number of words Correctly detected 28 17 11 33 26 14 10 29 22 12 8 26 21 10 8 23 FA Detected P R F 81 109 25 68 62 22 36 35 53 70 24 28 37 77 29 56 31 42 26 
19 24 44 25 28 103 136 24 26 73 33 36 45 59 85 30 58 57 77 39 99 35 49 28 57 31 11 29 78 21 31 32 25 22 22 26 31 73 102 28 43 64 44 39 45 55 77 28 57 48 88 36 06 39 51 23 52 26 66 24 99 19 27 29 62 17 77 22 21 75 101 25 74 57 77 35 61 33 54 38 88 46 66 42 41 19 29 34 48 22 22 27 02 10 18 44 44 17 77 25 38 42 65 35 38 51 11 41 81 45 4995 E E E E E E E E Design and Evaluation of an Agreement Error Detection System 291 and finally the last row shows the result of removing both rules The best results are obtained in the last case 44 44 precision in the M E option against the worse recall 17 77 and f score of 25 38 Considering precision and recall MaltIxa gives the best results 38 88 precision and 46 66 recall f score 42 41 6 Conclusions and Future Work In this work we have presented a set of experiments on agreement error detection applied to an agglutinative and free constituent order language For this we have used Saroi a tool built for the inspection of dependency trees The tool allows us to design 
restrictions by means of query rules to be applied on the output of dependency parsers In the evaluation we have experimented tuning the ambiguity of the analysis chain we have used two general purpose dependency parsers and two types of corpora When analyzing the Basque Dependency Treebank we have detected illformed dependency trees that is manual annotation mistakes Additionally we have evaluated our system regarding false alarms obtaining a false alarm rate of 3 30 Most of the alarms could be easily avoided improving the verb subcategorization schemas we use and in this way leaving a minimal false alarm rate In consequence we think that the precision of our grammar rules is high One of the main problems is the lack of coverage of the dependency analyzers When the trees are not syntactically well formed the system is more prone to signal a false alarm Any improvement in syntactic analysis will have a positive effect on the error detection system In the future we want to analyze how complementary are 
MaltIxa and EDGK and how they could be combined to obtain suitable analysis trees Working with real texts also led us to consider the problem of ambiguity The best results are obtained when using the deepest disambiguation level both morphosyntactic and syntactic This can be explained by the explosion in the number of trees when all the ambiguity is considered References 1 Zubiri I 292 M Oronoz A 7 Aduriz I Aranzabe M Arriola J M Atutxa A TectoMT Modular NLP Framework Charles University in Prague Institute of Formal and Applied Linguistics popel zabokrtsky ufal mff cuni cz Abstract In the present paper we describe TectoMT a multi purpose open source NLP framework It allows for fast and efficient development of NLP applications by exploiting a wide range of software modules already integrated in TectoMT such as tools for sentence segmentation tokenization morphological analysis POS tagging shallow and deep syntax parsing named entity recognition anaphora resolution tree to tree translation natural language 
generation word level alignment of parallel corpora and other tasks One of the most complex applications of TectoMT is the English Czech machine translation system with transfer on deep syntactic tectogrammatical layer Several modules are available also for other languages German Russian Arabic Where possible modules are implemented in a language independent way so they can be reused in many applications Keywords NLP framework linguistic processing pipeline TectoMT 1 Introduction Most non trivial NLP natural language pro cessing applications exploit several tools e g tokenizers taggers parsers that process data in a pipeline For developers of NLP applications it is beneficial to reuse available existing tools and integrate them in the pro cessing pipeline However it is often the case that the developer has to spend more time with the integration and other auxiliary work than with the development of new tools and innovative approaches The auxiliary work involves studying do cumentation of the reused tools 
compiling and adjusting the tools in order to run them on the developer s computer training models if these are needed and not included with the tools writing scripts for data conversions the tools may require different input format or enco ding resolving incompatibilities between the tools e g different tagsets assumed etc Such a work is inefficient and frustrating Moreover if it is done in an ad ho c style it must be done again for other applications The described drawbacks can be reduced or eliminated by using an NLP framework that integrates the needed tools so the tools can be combined into various pipelines serving for different purposes Most of the auxiliary work is already implemented in the framework and developers can fo cus on the more creative part of their tasks Some frameworks enable easy addition of thirdparty tools usually using so called wrappers and development of new mo dules within the framework H Loftsson E 294 M Popel and Z In this paper we report on a novel NLP framework called TectoMT 
1 In Sect 2 we describe its architecture and main concepts Sect 3 concerns implementation issues Finally in Sect 4 we briefly describe and compare other NLP frameworks 2 2 1 TectoMT Architecture Blocks and Scenarios TectoMT framework emphasizes mo dularity and reusability at various levels Following the fundamental assumption that every non trivial NLP task can be decomposed into a sequence of subsequent steps these steps are implemented as reusable components called blocks Each block has a well defined and documented input and output specification and also a linguistically interpretable functionality in most cases This facilitates rapid development of new applications by simply listing the names of existing blocks to be applied to the data Moreover blocks in this sequence which is called scenario can be easily substituted with an alternative solution other blocks which attempts at solving the same subtask using a different approach or metho d 2 For example the task of morphological and shallow syntax 
analysis and disambiguation for English text consists of five steps sentence segmentation tokenization part of speech tagging lemmatization and parsing In TectoMT we can arrange various scenarios to solve this task for example Scenario A Sentence_segmentation_simple Penn_style_tokenization TagMxPost Lemmatize_mtree McD_parser Scenario B Each_line_as_sentence Tokenize_and_tag Lemmatize_mtree Malt_parser In the scenario A tokenization and tagging is done separately in two blocks Penn_style_tokenization and TagMxPost respectively whereas in the scenario B the same two steps are done in one block at once Tokenize_and_tag Also different parsers are used 3 1 2 3 http ufal mff cuni cz tectomt Scenarios can be adjusted also by specifying parameters for individual blocks Using parameters we can define for instance which model should be used for parsing Penn_style_tokenization is a rule based block for tokenization according to Penn Treebank guidelines http www cis upenn edu treebank tokenization html TagMxPost uses 
Adwait Ratnaparkhi s tagger 1 Tokenize_and_tag uses Aaron Coburn s Lingua EN Tagger CPAN module Lemmatize_mtree is a block for English lemmatization handling verbs noun plurals comparatives superlatives and negative prefixes It uses a set of rules about one hundred regular expressions inspired by morpha 2 and a list of words with irregular lemmatization McD_parser uses MST parser 0 4 3b 3 Malt_parser uses Malt parser 1 3 1 4 TectoMT Modular NLP Framework 295 TectoMT currently includes over 400 Applications in TectoMT correspond to end to end NLP tasks be they real end user applications such as machine translation or only NLP related experiments Applications usually consist of three phases 1 conversion of the input data to the TectoMT internal format possibly split into more files 2 applying a scenario i e a sequence of blocks to the files 3 conversion of the resulting files to the desired output format Technically applications are often implemented as Makefiles which only glue the three phases Besides 
developing the English Czech translation system 5 TectoMT was also used in applications such as 296 M Popel and Z 2 3 Layers of Language Description TectoMT profits from the stratificational approach to the language namely it defines four layers of language description listed in the order of increasing level of abstraction raw text word layer w layer morphological layer m layer shallow syntax layer analytical layer a layer and deep syntax layer layer of linguistic meaning tectogrammatical layer t layer The strategy is adopted from the Functional Generative Description theory 17 which has been further elaborated and implemented in the Prague Dependency Treebank PDT 18 We give here only a very brief summary of the key points Every do cument is saved in one file and consists of a sequence of sentences Each sentence is represented by a structure called bundle which stands for a bundle of trees Each tree can be classified according to TectoMT Modular NLP Framework 297 Fig 1 English Czech parallel text annotated 
on three layers of language description is saved in a TectoMT document each sentence in one bundle We show only a simplified representation of the trees in the first bundle 4 In the near future TectoMT will migrate to using ISO 639 language codes e g ar cs en de instead of full names 298 M Popel and Z Fig 2 Vauquois diagram for translation with transfer on tectogrammatical layer A layer and t layer structures are dependency trees so it is natural to handle them as tree data structures M layer structure is a sequence of tokens with asso ciated attributes which is handled in TectoMT as a special case of a tree all nodes except the technical root are leaves of the tree W layer raw text is represented as string attributes stored within bundles 3 3 1 TectoMT Implementation Design Decisions TectoMT is implemented in Perl programming language under Linux This do es not exclude the possibility of releasing platform independent applications made of selected components platform independent solutions are always 
preferred in TectoMT TectoMT modules are programmed in ob ject oriented programming style using inside out classes following 22 Some of the mo dules are just Perl wrappers for tools written in other languages especially Java and C TectoMT is a mo dern multilingual framework and it uses open standards such as Unico de and XML TectoMT is neutral with respect to the metho dology employed in the individual blocks fully stochastic hybrid or fully rule based approaches can be used 3 2 TectoMT Components and Directory Structure Each block is a Perl class inherited from TectoMT Block and each block is saved in one file The blocks are distributed into directories according to the languages and layers on which they operate For example all blocks for deep syntactic analysis of English i e for generating t trees from a trees are stored in a directory SEnglishA_to_SEnglishT In this paper we use for simplicity only short names TectoMT Modular NLP Framework 299 0 1 Fig 3 Components of the TectoMT framework of blocks so e g 
instead of the full name SEnglishA_to_SEnglishT Assign_grammatemes we write only Assign_grammatemes TectoMT is composed of two parts see Fig 3 The first part the versioned part which contains TectoMT core classes and utilities format converters blocks applications and in house tools is stored in an SVN repository so that it can be developed in parallel by more developers The second part the shared part which contains linguistic data resources downloaded third party tools and the software for visualization of TectoMT files Tree editor TrEd 23 is shared without versioning because a it is supposed to be changed rather additively b it is huge as it contains large data resources and c it should be automatically reconstructible simply by downloading and installing the needed components 3 3 Data Formats The internal TectoMT format tmt files is an XML with a schema defined in Prague Markup Language 24 It is similar to the format used for the Prague Dependency Treebank 2 0 18 but all representations of a textual do 
cument at the individual layers of language description are stored in a single file TectoMT includes converters for various formats and corpora e g Penn Treebank 25 CoNLL 12 EMILLE 26 PADT 27 Scenarios are saved in plain text files with a simple format that enables including of other scenarios 300 M Popel and Z 3 4 Parallel Processing of Large Data TectoMT can be used for pro cessing huge data resources using a cluster of computers 5 There are utilities that take care of input data distribution filtering parallel pro cessing logging error checks and output data collection In order to allow efficient and flawless processing input data i e corpora should be distributed into many tmt files with about 50 to 200 sentences per file Each file can be pro cessed independently so this metho d scales well to any number of computers in a cluster For example the best version of translation from English to Czech takes about 1 2 seconds per sentence plus 90 seconds for initial loading of blocks in memory per computer more 
precisely per cluster job 6 Using 20 computers in a cluster we can translate 2000 sentences in less than 4 minutes TectoMT was used to automatically annotate the parallel treebank CzEng 0 9 8 with 8 million sentences 93 million English and 82 million Czech words 4 Other NLP Frameworks In Tab 1 we summarize some properties of TectoMT and four other NLP frameworks 5 6 7 8 We use Sun Grid Engine http gridengine sunsource net Most of the initialization time is spent with loading translation and language models about 8 GiB Other applications presented in this paper are not so resourcedemanding so they are loaded in a few seconds http opennlp sourceforge net http weblicht sfs uni tuebingen de englisch index shtml TectoMT Modular NLP Framework 301 Table 1 Comparison of NLP frameworks Notes a ETAP 3 is a closed source project only a small demo is available developed since license for public use main prog language linguistic theory strictly rule based main applicatione uses deep syntax 5 Summary TectoMT is a 
multilingual NLP framework with a wide range of applications and integrated tools Its main properties are 302 M Popel and Z Acknowledgments This work was supported by the grants GAUK 116310 LC536 and FP7 ICT 2007 3 231720 EuroMatrix MSM0021620838 MSMT CR Plus We thank three anonymous reviewers for helpful comments References 1 Ratnaparkhi A A maximum entropy part of speech tagger In Proceedings of the conference on Empirical Methods in Natural Language Processing pp TectoMT Modular NLP Framework 303 13 Romportl J Zvy 304 M Popel and Z 29 Cunningham H Maynard D Bontcheva K Tablan V GATE an architecture for development of robust HLT applications In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics July 07 12 2002 30 Mel cuk I A Towards a functioning model of language Mouton 1970 31 Tyers F M Using Information from the Target Language to Improve Crosslingual Text Classification Gabriela Laboratory of Language Technologies National Institute for Astrophysics Optics and 
Electronics gabrielarr mmontesg villasen inaoep mx Faculty of Computer Science Autonomous University of Puebla dpinto cs buap mx 3 Department of Computer and Information Sciences University of Alabama at Birmingham solorio uab edu 1 2 Abstract Crosslingual text classification consists of exploiting labeled documents in a source language to classify documents in a different target language In addition to the evident translation problem this task also faces some difficulties caused by the cultural discrepancies manifested in both languages by means of different topic distributions Such discrepancies make the classifier unreliable for the categorization task In order to tackle this problem we propose to improve the classification performance by using information embedded in the own target dataset The central idea of the proposed approach is that similar documents must belong to the same category Therefore it classifies the documents by considering not only their own content but also information about the 
assigned category to other similar documents from the same target dataset Experimental results using three different languages evidence the appropriateness of the proposed approach Keywords Crosslingual text classification prototype based method unlabeled documents text classification 1 Introduction Text classification is the task of assigning do cuments into a set of predefined classes or topics 1 The leading approach for this task considers the application of machine learning techniques such as Support Vector Machines and H Loftsson E 306 G consists in exploiting labeled documents in a source language to classify do cuments in a different target language Because of the inherent language barrier problem of this approach most current CLTC metho ds have mainly addressed different translation issues In particular they have explored the translation from one language to another by means of machine translation approaches as well as by multilingual lexical resources such as dictionaries and ontologies 2 3 Although 
the language barrier is an important problem in CLTC it is not the only one It is clear that in spite of a perfect translation there are also some cultural discrepancies manifested in both languages that will affect the classification performance That is given that a language is the way of expression of a cultural and socially homogeneous community documents from the same category but different languages i e different cultures may concern very different topics As an example consider the case of news about sports from France in French and from USA in English while the first will include more do cuments about so ccer rugby and cricket the latter will mainly consider notes about baseball basketball and American football In order to tackle this problem recent CLTC metho ds have proposed to enhance the classification mo del by iteratively incorporating information from the target language into the training phase 4 5 6 their purpose is to obtain a classification mo del that is as close as possible to the target 
topic distribution The metho d proposed in this paper is a simple and inexpensive alternative for facing the problems caused the cultural discrepancies between both languages Different to previous iterative approaches it does not consider the mo dification or enrichment of the original classifier instead it attempts to improve the document classification by using more information to support the decision pro cess Mainly it is based on the idea that similar do cuments must belong to the same category and therefore it classifies the do cuments by considering their own information as usual as well as the information about the assigned category to other similar do cuments from the same target dataset In the following section we describe the proposed metho d for CLTC This metho d is based on the prototype based classification approach 7 but mo difies the traditional class assignment strategy in order to incorporate information from the set of similar do cuments Then in Section 3 we define the experimental 
configuration and show results in six different pairs of languages that demonstrate the usefulness of the proposed approach for CLTC Finally in Section 4 we present our conclusions and some ideas for future work 2 Prototype Based CLTC Method Given that prototype based classification is very simple and has demonstrated to consistently outperform other algorithms such as Using Information from the Target Language 307 training set is more similar to it and to its nearest neighbors from the same target language dataset Figure 1 shows the general schema of the proposed metho d It consists of four main pro cesses The first one carries out the translation of the training documents from the source language S to the target language T The second pro cess fo cuses on the construction of the class prototypes using the well known normalized sum technique 8 The third process involves the identification of the nearest neighbors for each do cument from the target language dataset DT Finally the fourth pro cess computes the 
classification for each do cument d DT considering information from their own and their neighbors Bellow we present a brief description of each one of these pro cesses Fig 1 General scheme of the proposed text classification method Document Translation Two basic architectures have been explored for CLTC one based on the translation of the target dataset to the source language and another one based on the translation of the training set to the target language We decided to adopt the latter option because training sets are commonly smaller than test sets and therefore their translation tend to be less expensive In particular the translation was achieved using the Worldlingo online translation machine1 Prototype Construction This pro cess carries out the construction of the class prototypes based on information from 1 http www worldlingo com es products services worldlingo translator html 308 G set organized in a predefined set of classes C and represented in their own term space it computes the prototype 
vector for each class ci C using Formula 1 Pi 1 d ci d d d ci 1 Nearest Neighbors Identification This process fo cuses on the identification of the k nearest neighbors for each do cument di from the target dataset DT refer to Formula 2 In order to do that we compute the similarity between two do cuments di and all other d in DT using the cosine formula refer to Formula 4 di Nk argmaxSj Sk d Sj sim d di 2 where Sk and sim are defined as follows Sk S S DT S k sim di dj Class Assignment In prototype based classification the class of a do cument d from the target dataset is traditionally determined by Formula 5 Our proposal extends this class assignment strategy by considering not only information from the do cument itself but also information about the assigned category to other similar do cuments from the same target dataset In particular given a do cument from the target dataset d DT in conjunction with its k nearest neighbors d we assign a class to d using Formula 6 Nk class d argmaxi sim d Pi 1 class d 
argmaxi sim d Pi 1 k where d nj Nk 5 inf d nj Using Information from the Target Language 309 3 3 1 Evaluation Datasets For the experiments we considered a subset of the Reuters RCV 1 Corpus 9 This subset considers three languages English French and Spanish and the news reports corresponding to four classes Crime Disasters Politics and Sports For each language we used 320 documents 80 per each class2 3 2 Evaluation Measure The evaluation of the performance of the proposed metho d was carried out by means of the F measure This measure is a linear combination of the precision and recall values from all class ci C It is defined as follows F M easure 1 C C i 1 7 Recall ci number of correct predictions of ci number of examples of ci number of correct predictions of ci number of predictions as ci 8 9 P recision ci 3 3 Baseline Experiments The goal of these experiments was to evaluate the performance of a traditional CLTC approach where do cuments from a source language are used to classify do cuments from a 
different target language For these experiments we applied the following standard pro cedure first we translated the training do cuments from the source language to the target language using Worldlingo then we constructed a classifier in the target language using the translated training set finally we used the built classifier to determine the class of each document from the target language dataset For the construction of the classifier we considered three of the most used metho ds for text classification namely 2 3 This corpus can be downloaded from http ccc inaoep mx mmontesg resources CLTC RCV Subset txt For NB and SVM we used the implementation and default configuration of WEKA 10 310 G Table 1 F measure results for six crosslingual experiments using a traditional CLTC approach Source language English English French French Spanish Spanish Target language French Spanish English Spanish English French Experiment PBC EF F ES S FE E FS S SE E SF F 0 616 0 814 0 956 0 879 0 851 0 790 NB 0 753 0 791 0 931 0 
882 0 891 0 802 SVM 0 764 0 625 0 616 0 658 0 486 0 723 using the class assignment function described in Formula 5 Table 1 shows the Fmeasure results obtained by these metho ds in six crosslingual experiments which correspond to all possible pair combinations of the three selected languages From these results those by PBC are of special interest since our metho d is an extension of this approach 3 4 Results from the Proposed Method As described in Section 2 the main idea of the proposed method is to classify the do cuments by considering not only their own content but also information from other similar do cuments from the same target dataset Particularly we adapted the traditional prototype based approach PBC to capture this information refer to Formula 6 being a constant that determines the relative importance of both components Considering the proposed metho d we designed some experiments in such a way that we could evaluate the impact on the classification results caused by the selection of different 
values of as well as the impact caused by the usage of different number of neighbor do cuments into the class assignment process In particular we used 0 0 1 0 2 1 and k 1 30 Experiments showed that the best results were achieved when using small values of indicating that information from the neighbor documents is of great relevance On the other hand they could not indicate a clear conclusion about the appropriate number of neighbors since several different values allowed to obtain similar classification improvements Figure 2 shows some results of the proposed method in the six crosslingual experiments These results correspond to three different values of 0 0 1 and 0 2 This figure also shows the results from the traditional prototype based approach which correspond to our metho d results using 1 The achieved results indicate that the proposed metho d clearly outperforms the traditional prototype based approach In order to summarize the results from the experimental evaluation Table 2 presents the best results 
achieved by the proposed metho d Comparing these results against those from Table 1 it is possible to notice that our metho d outperformed all used classification algorithms in all except one of the crosslingual Using Information from the Target Language 311 a Experiment EF F b Experiment SF F c Experiment ES S d Experiment FS S e Experiment FE E f Experiment SE E Fig 2 F measure results of the proposed method in the six crosslingual experiments using different values of and numbers of neighbors k The straight line corresponds to the PBC baseline result 1 experiments demonstrating the usefulness of considering information from the target dataset in crosslingual text classification At this point it is important to clarify that several different configurations of our metho d as shown in Figure 2 allowed obtaining competitive classification results One example is the configuration defined by 0 1 and k 11 which 312 G also outperformed most baseline results as shown in the last column of Table 2 We evaluated the 
statistical significance of the best achieved results using the z test with a confidence of 95 a indicates that the improvement over the PCB is statistically significant whereas a indicates the same regarding the best baseline result 4 Conclusions and Future Work In addition to the evident translation problem crosslingual text classification CLTC also faces some difficulties caused by the cultural discrepancies manifested in both languages by means of different topic distributions In this paper we proposed a simple and inexpensive approach for facing this problem This approach is based on the idea that similar do cuments must belong to the same category and therefore it classifies the documents by considering their own information as usual as well as the information about the assigned category to other similar do cuments In particular we implemented the proposed approach using the prototypebased classification algorithm In our implementation the decision about the category of each do cument from the target 
language is determined by the class whose prototype calculated from the training set is more similar to it and to its nearest neighbors from the same target language dataset This way the proposed metho d determines the category of do cuments taking advantage of information from the two languages As future work we plan to carry out an extensive analysis of several crosslingual experiments using different languages and a larger number of do cuments to establish a simple criterion for determining the appropriate values for parameters and k Once defined this criterion we also plan to use the proposed approach in conjunction with a semi supervised metho d as the one described by Rigutini et al 4 Our goal is to enhance the selection of the do cuments that will be iteratively included in the training set and consequently to obtain a classification mo del that is as close as possible to the target language distribution Acknowledgments This work was done under partial support of CONACyTMexico project grants 83459 
82050 106013 and 106625 and scholarship 239516 Using Information from the Target Language 313 References 1 Sebastiani F Machine learning in automated text categorization ACM Computing Surveys 34 Event Detection Using Lexical Chain Sangeetha S 1 R S Thakur2 and Michael Aro ck3 Department of Computer Applications National Institute of Technology 1 2 Abstract This paper describes a new architecture for event detection from text documents The proposed system correctly identifies the sentences that describe an event of interest to extract its participants It follows an unsupervised method for identifying the lexical chains from the raw sentences taken as a training data The lexical chain constructed using Wordnet lexicon is then used for identifying event mention The significance of the proposed system is it is the first system that applies lexical chain for event identification The entire architecture is divided into three tasks namely natural language pre processing lexical chain construction and event 
detection Keywords Event Extraction Lexical chain 1 Introduction Information extraction IE 1 is the process of extracting the structured information from the unstructured text IE systems were evaluated by Message Understanding Conferences MUC till 1998 Automatic content extraction ACE programme is the successor of MUC with the ob jective of developing extraction technology to support automatic pro cessing of source language data Event identification and characterization of ACE 3 programme identifies events by making use of event triggers Event trigger is the word that clearly expresses the o ccurrence of an event Event mention is the sentence in which the event is described An event comprises event participants which are the entities that participate in the event with different roles As the supervised machine learning requires a large set of event annotated data our approach uses unsupervised method for extracting the events The proposed metho d constructs the lexical chain from the un annotated or raw 
sentences of training do cuments Section 4 explains how the training do cuments are constructed Lexical chain holds the set of semantically related words of a given sentence or a document from which it was obtained Word net lexicon 15 is used for constructing lexical chain in the proposed work The significance of H Loftsson E Event Detection Using Lexical Chain 315 the proposed system is it is the first system that uses lexical chain for event extraction to the best of our knowledge The entire architecture is divided into three tasks namely natural language pre processing lexical chain construction and event detection The remaining sections of the paper are organized as follows The next section describes the related work in the field of information extraction generally and event extraction specifically Section 3 provides a description of proposed approach Section 4 explains how the training do cuments are constructed and partial experimental results and Section 5 concludes the paper 2 Related Work McCracken 
et al 2 had combined statistical and knowledge based technique for extracting events Its main fo cus was on summary report genre It had extracted the factual accounting of incidents from a person s life This resource had covered every instance of every verb in the corpus Xu et al 5 had developed a metho d for identifying event extent event trigger and event argument automatically using bootstrapping metho d The work had extracted the events from the Nobel Prize winning domain by obtaining extraction rules from the text fragments using binary relations as seeds Abuleil 4 proposed a metho d which extracted events by breaking each event into elements analysed and understood the syntax of each element identified the role played by each element in the event and how they formed relationship between related events David Ahn 7 broke down the task of extracting the events into subtasks such as anchor identification argument identification attribute assignment and event co reference Each task was performed with the 
help of machine learned classifier Some of the learners used are memory based learners and maximum entropy learners Aone et al 6 has identified events by tagging the text with name and noun phrase using pattern matching techniques and has resolved co references using rule based approach It did not consider the semantic relatedness All the above said existing systems extract the events without considering the semantic features of the text However consideration of meaning of the text improves the efficiency of event extraction and the information extraction on the whole The actual meaning of a sentence is identified from the meaning of individual words Our approach uses lexical chain for identifying the events because lexical chain holds the semantically related words The concept lexical chain 8 is based on the cohesion 9 which is a metho d for sticking different part of the text which is semantically related Lexical chaining has been used in various tasks such as text summarization 12 word sense 
disambiguation and web information retrieval 11 Naughton et al 10 stated that event identification using manual extraction of trigger terms from Lexicon such as Wordnet performs well Our approach improves this trigger by adding some more related words of the same concept by constructing lexical chain Naughton et al 10 manually extracted the terms related to an event type In our approach 316 Sangeetha S R S Thakur and M Arock we learn all the words used in the sentences which represent an event and also the related words from Wordnet semantic relations This includes more related words and will improve the result From the literature we have identified that a lexical chain holds set of semantically related words b to identify an event we need the words related to a particular event and also c to the best of our knowledge there is no event extraction system that uses lexical chain so far Hence we propose an architecture which identifies an event by constructing LC based on training sentences 3 Proposed Work The 
proposed architecture is broadly classified into three phases Fig 1 namely prepro cessing lexical chain construction and event detection Fig 1 Architecture Overview Pre processing Prepro cessing includes part of speech tagging stop word removal and named entity identification Collect the sentences describing a particular event of interest to form a single do cument and pass them as input to the prepro cessing stage We have adopted standard tree bank POS tagger for part of speech tagging and ACE named entity chunking available in 14 At the end of prepro cessing stage we obtain the tokenized tagged words from the training sentences along with the types of named entities Lexical Chain Construction The prepro cessing phase produces output as a list of words collected from all the training sentences These words are the candidate words for constructing lexical chain Several algorithms have been proposed for lexical chain We have used the algorithm developed by 8 with some enhancement using the Word net Event 
Detection Using Lexical Chain 317 lexicon Moriss and Hirst constructed lexical chain which includes only the words available in the text which are semantically related In our approach along with the words in the text we have included the words in the Word net lexicon that are used to establish the semantic relation between any two words in the text Moreover the proposed approach includes noun verb and adjective words as all these POS can act as an event trigger according to ACE 3 Since we are using the words in the lexical chain as a trigger word for identifying the event our pro cedure learns not only the semantically related words from the training data but also the related words from the Word net lexicon The pro cedure for constructing the lexical chain is given below All the senses of each candidate words are extracted Each sense of the word is represented by the set of words in its synonym at 0th level and hypernym hyponym meronym holonym at level 1 For each pair of candidate words each sense of the 
word Wi is compared with all other senses of word Wj If the match occurs the words Wi Wj and the list of matched words in the sense representation are included in the chain Along with the matched words path length with respect to the current level of matching and frequency of the word are also stored The frequent words with minimum path length are included in the lexical chain The list of all words forms overall LC From the set of LC s formed we manually select the LC which is related to the event As the sentences in the training do cuments are related to a particular event the required LC is always longer than other LC s constructed Event Detection and Extraction In a given do cument the sentences are searched for words in lexical chain Sentence score is calculated based on how many words of LC it contains The sentences with highest score are the sentences representing the type of event This metho d can be adopted to any type of event by selecting the training sentences denoting the particular event type We 
mark the event mention as relevant if its score is above threshold More number of words in the lexical chain reflects the large set of words related to the event which in turn identify large set of event mentions As a result the final precision and recall will improve Keeping this in mind we are pro ceeding our research and obtained initial set of results in Fig 2 which shows how redundancy in training do cument affects the number of words in the lexical chain To extract the event participants the event mention should be tokenized POS tagged stemmed and named entities should be recognized From the lexical chains patterns using preposition and type of named entities are identified to extract the required fields Then sub ject and ob ject of the sentence are identified which denote the agent and theme of the event which can be mapped to event participants 4 Experiment and Discussions We have collected 50 documents representing the education and administrative positions held by various international leaders from 
Wikipedia 13 The do cuments 318 Sangeetha S R S Thakur and M Arock are chosen in such a way that it uses different words to represent education and administrative positions details in each document Among the 50 do cuments we have used 25 do cuments for training and remaining 25 do cuments for testing The sentences in the training do cuments are manually separated into sentences describing education details and sentences describing details of administrative positions held and they are grouped under two different do cuments These do cuments are used as input to the preprocessing stage In our experiment first we have considered the do cument with the sentences related to education This system is under development Till now we have completed the construction of the lexical chain In this paper we illustrate the preliminary result of our proposed architecture If the number of words in lexical chain is greater we can identify the wide range event mentions which will improve the final precision and recall value Thus 
importance should be given to the number of words in the lexical chain Redundancy of context words in the training do cument is the main factor which is affecting the number of words in LC Our experimental result confirms it From the empirical analysis we have identified that if candidate words are less redundant in the same context the number of words in the lexical chain increases Whereas if redundancy is introduced in the context words of the training document number of words in the lexical chain decreases Fig 2 shows it as the preliminary result Fig 2 Lexical chaining vs redundancy in context words Event Detection Using Lexical Chain 319 5 Conclusion The proposed work provides an architecture that extracts the events by using lexical chains Most of the existing systems extract the events without considering the semantics features of the text However consideration of semantics of the text improves the efficiency of event extraction and the information extraction on the whole Our approach identifies the 
semantic relationship between words and constructs lexical chains based on the relationship Lexical chain is used to identify the event mention within the documents as relevant or irrelevant based on the sentence score From our empirical results we conclude that less redundant training sentences will acquire more number of unique words in the lexical chain and large set of words in lexical chain correctly identify large set of event mentions References 1 Cunningham H Information Extraction Automatic Encyclopaedia of Language and Linguistics Using Comparable Corpora to Improve the Effectiveness of Cross Language Information Retrieval Fatiha Sadat University of Quebec in Montreal Computer Science department 201 President Kennedy avenue Montreal QC Canada sadat fatiha uqam ca Abstract Large scale comparable corpora became more abundant and accessible than parallel corpora with the explosive growth of the World Wide Web From the Cross Language Information Retrieval point of view limitation of translation 
resources as well as ambiguity arising due to failure to translate query terms is largely responsible for large drops in the effectiveness below monolingual performance Therefore strategies on bilingual terminology extraction from comparable texts must be given more attention in order to enrich existing bilingual lexicons and thesauri and to enhance Cross Language Information Retrieval In the present paper we focus on the enhancement of CrossLanguage Information Retrieval using a two stage corpus based translation model that includes bi directional extraction of bilingual terminology from comparable corpora and selection of best translation alternatives on the basis of their morphological knowledge The impact of comparable corpora on the performance of the Cross Language Information Retrieval process is evaluated in this study and the results indicate that the effect is clearly positive especially when using the linear combination with bilingual dictionaries and JapaneseEnglish pair of languages Keywords 
Cross language information retrieval comparable corpora similarity translation disambiguation 1 Introduction Cross Language Information Retrieval CLIR deals with the problem of presenting an information retrieval task in one language and retrieving documents in one or several other languages The main methods for CLIR are presented in overviews by Oard Diekema 1998 and Pirkola et al 2001 Methods based on the translation of queries are categorized into either i dictionary based translation ii machine translation iii methods using parallel corpora and other approaches which are based on other existing linguistic resources such as the thesaurus However according to previous research Sadat et al 2002 the main problems which are largely responsible for large drops in the effectiveness of CLIR below monolingual performance are listed as follows First is the problem of inflection A commonly used method to deal with inflected words is to remove affixes from word forms The method is called stemming Morphological 
analysis also allows the normalization of words forms into H Loftsson E Using Comparable Corpora to Improve the Effectiveness 321 their base forms The second problem associated to CLIR is related to the translation ambiguity arising from polysemous words Third is the problem of compounds phrases multi words e g specialized vocabulary and their handling and failure of translation Compound words form an important part of natural language since compounding is a major way of forming new words From the information retrieval IR point of view compounds may be content bearing words in natural language sentences and therefore important for the retrieval result Hedlund 2002 The problem with compound handling for CLIR is acknowledged for many languages The forth problem encountered in CLIR is related to proper names named entities and other untranslatable words using existing translation tools bilingual dictionary and or machine translation Lexical coverage of existing bilingual dictionary limitation of general purpose 
dictionaries especially for specialized vocabulary and inexistence of translation tools for pairs of languages are among the merging problems that could be solved using large scale corpora In recent years two types of multilingual corpora have been an object of studies and research related to natural language processing and information retrieval parallel corpora and comparable corpora The parallel corpora are made up of original texts and their translations This allows texts to be aligned and used in applications such as computer aided translator training and machine translation systems This method could be expensive for any pair of languages or even not applicable for some languages which are characterized by few amounts of Web pages on the Web On the other hand non aligned comparable corpora more abundant and accessible resources than parallel corpora have been given a special interest in bilingual terminology acquisition and lexical resources enrichment Dagan Itai 1994 Dejean et al 2002 Diab Finch 2000 
Fung 2000 Gaussier et al 2004 Kaji 2003 Koehn Graehl 2002 Nakagawa 2000 Peters Picchi 1995 Rapp 1999 Sadat et al 2003a Sadat et al 2003b Sadat et al 2003c Sadat 2004 Shahzad et al 1999 Tanaka Iwasaki 1996 Utsuro et al 2002 Utsuro et al 2003 Comparable corpora are defined as collections of texts from pairs or multiples of languages which can be contrasted because of their common features in the topic the domain the authors the time period etc Comparable corpora could be collected from downloading electronic copies of newspapers and articles on the WWW for any specified domain This paper intends to bring solutions to the problem of lexical coverage of existing bilingual dictionaries but also to the improvement of the performance of CLIR The main contributions concern the enhancement of CLIR by an automatic acquisition of bilingual terminology from comparable corpora that will help cope with the limitation of CLIR especially in the query disambiguation process as well as during the query expansion with related 
terms Furthermore this study could be valuable for the extraction of unknown words and their translation and thus the enrichment and enhancement of bilingual dictionaries Therefore we present in this paper an approach of learning bilingual terminology from textual resources other than bilingual dictionaries such as comparable corpora and evaluations on CLIR First we propose a twostage corpus based translation model for the acquisition of bilingual terminology from comparable corpora The first stage concerns the extraction of bilingual translations from the source language to the target language also from the target language to the source language The two results are combined for the purpose of disambiguation In the second stage the extracted translation alternatives are filtered on the basis of their 322 F Sadat morphological knowledge A linguistics based pruning technique is applied in order to compare source words and their target language translation equivalents on the basis of their part of speech tags 
Furthermore we present a combined translation model involving the comparable corpora and readily available bilingual dictionaries In our evaluations we used a large scale test collection on Japanese English and different weighting schemes of SMART retrieval system and confirmed the effectiveness of the proposed translation model in CLIR The remainder of the present paper is organized as follows Section 2 presents an overview of the proposed model Section 3 presents the two stage corpus based translation model Section 4 introduces a combination of different translation models Experiments and evaluations in CLIR are related in Section 5 Section 6 concludes the present paper 2 An Overview of the Proposed Model Throughout this paper we will seek to exploit and explore benefits from collections of news articles for the acquisition of bilingual terminology in order to enrich existing multilingual lexical resources and help cross the language barrier for information retrieval We rely on such comparable corpora for 
the extraction of bilingual terminology in the form of translations and or expansion terms i e words that will help the query expansion in CLIR First a linguistic preprocessing is performed on the comparable corpora in order to replace each term with its inflectional root to remove most plural word forms to replace each verb with its infinitive form to remove stop words and stop phrases and finally to extract content words such as nouns verbs adjectives adverbs and foreign words which will constitute the main target of our study Second the task of bilingual terminology extraction is accomplished by a twostage corpus based translation model which is described in detail in Section 3 Third a linear combination involving the comparable corpora and bilingual dictionaries is completed in order to select best translation candidates of the source terms of a given query Finally documents are retrieved in the target language 3 Two Stage Corpus Based Translation Model A two stage corpus based translation model Sadat et 
al 2003a Sadat et al 2003b Sadat et al 2003c which is based on the symmetrical criterion in addition to the assumption of similar collocation aims to find translations of the source word in the target language corpus but also translations of the target words in the source language corpus Linguistic resources were used in the two stage corpus based translation model as follows i a collection of news articles from Mainichi Newspapers 19981999 for Japanese and Mainichi Daily News 1998 1999 for English were considered as comparable corpora because of their common feature on the time period Documents of NTCIR 2 test collection were also considered as comparable corpora in order to cope with special features of the test collection during evaluations ii morphological analyzers ChaSen version 2 2 9 Matsumoto et al 1997 for texts in Japanese and OAK Sekine 2001 for English texts were used in linguistic processing iii Using Comparable Corpora to Improve the Effectiveness 1 323 EDR 1996 and EDICT bilingual Japanese 
English and English Japanese dictionaries were considered in the translation of context vectors of source and target languages Japanese words written in Katakana representing foreign words and proper names that were not found in the bilingual dictionaries were manually translated A transliteration process could be used in order to convert those words to their English equivalence 3 1 First Stage in the Proposed Translation Model The two stage corpus based translation model for the acquisition of bilingual terminology is described as follows 1 2 3 A simple bilingual terminology acquisition from source language to target language to yield a first simple translation model represented by similarity vectors SIMST A simple bilingual terminology acquisition from target language to source language to yield a second simple translation model represented by similarity vectors SIM T S Merge the first and second models to yield a two stage translation model based on bi directional comparable corpora and represented by 
similarity vectors SIMST The simple approach for bilingual terminology acquisition from comparable corpora is based on the assumption of similar collocation i e If two words are mutual translations then their most frequent collocates are likely to be mutual translations as well We follow strategies of previous researches Dejean et al 2002 Fung 2000 Rapp 1999 Sadat et al 2003a Sadat et al 2003b Sadat et al 2003c The approach is described as follows First word frequencies context word frequencies in surrounding positions here three words window are estimated following statistics based metrics Context vectors for each term in the source language are constructed As well context vectors for terms in the target language are constructed We use the loglikelihood ratio Dunning 1993 for the estimation of context frequencies for each pair of words in either the source language or target language as expressed in equation 1 LLR w i wj K11log K11N K12N K21N K22N K12log K21 log K22log C1R1 C1R2 C2R1 C2R2 1 where C1 K11 K12 
C2 K21 K22 R1 K11 K21 R2 K12 K22 N K11 K12 K21 K22 K11 frequency of common occurrences of word wi and word wj in a specified window size of the monolingual corpus K12 corpus frequency of word wi in the corpus K11 K21 corpus frequency of word wj in the corpus K11 K22 N K11 K12 K21 Second context vectors of words in the source language are translated into the target language using a bilingual seed lexicon We consider all translation candidates keeping the same context frequency value as the source word This step requires a 1 http www csse monash edu au jwb wwwjdic html 324 F Sadat seed lexicon that will be enriched using the proposed bootstrapping approach of this paper The third step is the construction of similarity vectors for each pair of source word and target word Context vectors original and translated of words in both languages are compared using the cosine metrics Salton McGill 1983 as expressed in equation 2 Finally similarity vectors are normalized to yield a simple corpus based translation model 
Similarity wi w j v k k ik v jk 2 jk v v 2 ik k 2 where vik represents co occurrence frequencies of the source word wi with word wk The word wk is found in the translated context vectors of the source word wi vjk represents co occurrence frequencies of the target word wj with the word wk The word wk is found in the context vectors of the target word wj The merging strategy in the first stage of the two stage corpus based translation model is represented by the following equation 3 SIMST s t simST t s s t simST t s SIMST t s simT S s t SIMT S simST t s simST t s 3 Similarity vectors SIMST and SIMTS for the first and second models are constructed and merged to yield a bi directional acquisition of bilingual terminology from source language to target language The merging process will keep common pairs of source term and target translation s t which appear in SIMST as pairs s t but also in SIMTS as pairs t s to result in combined similarity vectors SIMST for each pair s t The product of similarity values simST t 
s and simTS s t of vectors SIMST and SIMTS consecutively will result in similarity values simST t s of vectors SIMST which will represent the first stage of the two stage corpus based translation model In further sections we name the simple approach for bilingual terminology acquisition from comparable corpora as simple corpus based translation and the translation model representing the first stage of the two stage corpus based translation as bi directional corpus based translation 3 2 Second Stage in the Proposed Translation Model Combining linguistic and statistical methods is becoming increasingly common in computational linguistics especially as more corpora become available Klavens Tzoukermann 1996 Sadat et al 2003c We propose to integrate linguistic concepts into the corpus based translation model Morphological knowledge such as Part ofSpeech POS tags context of terms etc could be valuable to filter and prune the extracted translation candidates The objective of the linguistics based pruning technique 
is the detection of terms and their translations that are morphologically close enough i e close or similar POS tags This proposed approach will select a fixed Using Comparable Corpora to Improve the Effectiveness 325 number of equivalents from the set of extracted target translation alternatives that match the Part of Speech of the source term POS tags are assigned to each source term Japanese via morphological analysis As well a target language morphological analysis will assign POS tags to the translation candidates We restricted the pruning technique to nouns verbs adjectives and adverbs although other POS tags could be treated in similar way For Japanese2 English pair of languages Japanese nouns MEISHI are compared to English nouns NN and Japanese verbs DOUSHI to English verbs VB Japanese adverbs FUKUSHI are compared to English adverbs RB and adjectives JJ while Japanese adjectives KEIYOUSHI are compared to English adverbs RB and adjectives JJ This is because most adverbs in Japanese are formed from 
adjectives Thus we select pairs of source term and target translation s t such as POS s NN and POS t MEISHI POS s VB and POS t DOUSHI POS s RB and POS t FUKUSHI or POS t KEIYOUSHI POS s JJ and POS t KEIYOUSHI or POS t FUKUSHI Note that Japanese vocabulary is frequently imported from other languages primarily but not exclusively from English The special phonetic alphabet here Japanese katakana is used to write down foreign words technical terms proper nouns and loanwords e g names of persons and entities Japanese foreign words were not pruned with the proposed linguistics based technique but could be treated via transliteration i e conversion of Japanese katakana to their English equivalence or to the alphabetical description of their pronunciation Knight Graehl 1998 Finally the generated translation alternatives are sorted in decreasing order by similarity values Rank counts are assigned in increasing order starting at 1 for the first sorted list item A fixed number of top ranked translation alternatives are 
selected and misleading candidates are discarded 4 Combining Different Translation Models Combining different translation models has showed success in previous research Dejean et al 2002 We propose a combined translation model involving comparable corpora and readily available bilingual dictionaries The proposed dictionary based translation model is derived directly from readily available bilingual dictionaries by considering all translation candidates of each source entry as equiprobable to yield a probabilistic translation model P2 t s The linear combination will involve the two probabilistic translation models P1 t s and P2 t s derived from the comparable corpora either the simple or the two stage model and readily available bilingual dictionaries respectively as follows 2 English POS tags NN refers to noun VB to verb RB to adverb JJ to adjective while Japanese POS tags MEISHI refers to noun DOUSHI to verb FUKUSHI to adverb and KEIYOUSHI to adjective with respect to their extensions 326 F Sadat P t s i pi 
t s Parameters 1 and 2 represent mixture weights of each translation source with i 1 Although the mixture weights could be adjusted using the EM algoi i rithm Dejean et al 2002 individual translation sources were assigned equiprobable weights in these preliminary evaluations Japanese vocabulary is frequently imported from other languages primarily but not exclusively from English Katakana the special phonetic alphabet is used to write down foreign words and loanwords example names of persons and other terms A probabilistic translation model representing the transliteration could be integrated in the combined model as well 5 Evaluation and Experiments Experiments have been carried out in order to measure the improvement of our proposal on bilingual terminology acquisition from comparable corpora on JapaneseEnglish tasks in CLIR i e Japanese queries to retrieve English documents 5 1 Evaluations on the Corpus Based Translation Model We considered the set of news articles as well as the abstracts of NTCIR 2 test 
collection as comparable corpora for Japanese English language pairs NTCIR2 contains abstracts from academic conference papers with much technical terms that may not be found in the standard dictionaries or in general domain news articles The abstracts of NTCIR 2 test collection are partially aligned more than half are Japanese English paired documents but the alignment was not considered in the present research in order to treat the set of documents as comparable Content words nouns verbs adjectives adverbs and Foreign words were extracted from English and Japanese corpora Context vectors were constructed for 13 552 481 Japanese terms and 1 517 281 English terms Similarity vectors were constructed for 96 895 255 Japanese English pairs of terms and 92 765 129 English Japanese pairs of terms Bi directional similarity vectors after merging and disambiguation resulted in 58 254 841 Japanese English pairs of terms 5 2 Evaluations on the Retrieval System Conducted experiments and evaluations were completed using 
a large scale test collection NTCIR 2 Kando 2001 SMART information retrieval system Salton 1971 which is based on vector model was used to retrieve English documents We used the monolingual English runs i e English queries to retrieve English documents and the bilingual Japanese English runs i e Japanese queries to retrieve English documents Topics of NTCIR 2 collection numbered 0101 to 0149 were considered and key terms contained in the fields title TITLE description DESCRIPTION and concept CONCEPT were used to generate 49 queries in Japanese and in English There is a variety of techniques implemented in SMART to calculate weights for individual terms in both documents and queries These weighting techniques are formulated by combining three parameters Term Frequency component Inverted Using Comparable Corpora to Improve the Effectiveness 327 Document Frequency component and Vector Normalization component The standard SMART notation to describe the combined schemes is XXX YYY The three characters to the left 
XXX and right YYY of the period refer to the document and query vector components respectively For example ATC ATN applies augmented normalized term frequency Bilingual translations were extracted from the collection of news articles using the simple translation model and the two stage translation model A fixed number p set to five of top ranked translation alternatives was retained for evaluations in CLIR Results and performances on the monolingual run as well as on the bilingual runs using the two stage corpus based translation model and the linear combination to bilingual dictionaries are illustrated in Table 1 Evaluations are based on the average precision differences in term of average precision of the monolingual counterpart and the improvement over the monolingual counterpart Retrieval methods are represented by the monolingual retrieval Mono dictionarybased translation DT the simple corpus based translation model SCT the bidirectional corpus based translation model BCT the two stage corpus based 
translation model TCT Linear combinations were represented by SCT DT for the combined simple corpus based translation and bilingual dictionaries BCT DT for the combined bidirectional corpus based translation and bilingual dictionaries TCT DT for the combined two stage corpus based translation and bilingual dictionaries Our interest Table 1 Evaluations on the proposed translation models using ATN NTC weighting schemes of SMART retrieval system Average Precision Monolingual and Improvement Mono DT SCT BCT TCT P 5 SCT DT BCT DT TCT DT 0 3368 0 2279 0 1417 0 1801 0 2008 0 2366 0 2721 0 2987 100 67 66 42 07 53 47 59 62 70 25 80 79 88 69 328 F Sadat in the present research is related to the evaluation of the proposed two stage translation model TCT and the combination to bilingual dictionaries TCT DT over the other retrieval methods As illustrated in Table1 the bi directional corpus based translation model BCT showed a better improvement in terms of average precision compared to the simple corpus based translation 
model SCT with 27 1 The two stage corpus based translation model TCT showed better performance in terms of average precision with 41 7 and 11 5 compared to SCT and to BCT respectively Linear combination of simple or comparable corpora and bilingual dictionaries showed better performances in terms of average precision compared to the models stand alone SCT DT showed 70 25 of the monolingual counterpart Mono 3 82 compared to the dictionary based translation DT and 66 97 compared to the simple corpus based translation SCT in the case of ATN NTC weighting scheme BCT DT showed better improvement with 80 79 of the monolingual counterpart Mono 19 4 of the dictionary based translation DT 51 08 of BCT and 15 of the combined SCT DT The proposed hybrid combination TCT DT of the two stage corpus based translation model and bilingual dictionaries showed the best performance with 88 69 of the monolingual retrieval in the case of ATN NTC weighting scheme TCT DT showed an improvement of 9 7 of the combined BCT DT and 26 24 
of the combined SCT DT Furthermore the different improvements in term of average precision were noticed through all weighting schemes of SMART retrieval system with ATN NTC showing the best results for all the retrieval methods involved in the present study Thus key techniques used in the proposed two stage corpus based translation model for bilingual terminology acquisition from comparable corpora can be summarized as follows The acquisition of bilingual terminology from bi directional comparable corpora yields a significantly better result than using the simple model The bi directional corpus based translation model is considered as one kind of symmetric probabilistic model that provides a disambiguation of the extracted translation alternatives and helps improve the accuracy of translation extraction The bi directional approach is more effective than the simple approach in a way that it includes a disambiguation process for the extracted translation alternatives Evaluations in CLIR showed an improvement 
of 27 1 in terms of mean average precision for the bi directional corpus based translation model of the simple corpus based translation model the main average precision goes from 0 1417 to 0 1801 for the ATN NTC weighting scheme The approach based on bi directional comparable corpora largely affected the translation because related words could be added as translation alternatives or expansion terms Linguistics based pruning technique has allowed a great improvement in the effectiveness of CLIR Therefore morphological knowledge such as part of speech could provide a valuable resource in filtering and pruning the translation candidates Combining different translation models yields a significantly better result than using each model by itself Translation models based on comparable corpora and bilingual dictionaries have completed each other and their linear combination has provided a valuable resource for query translation expansion in CLIR and has allowed an improvement in the effectiveness of information 
retrieval Using Comparable Corpora to Improve the Effectiveness 329 6 Conclusion In the present paper we investigated the approach of extracting bilingual terminology from comparable corpora in order to enhance CLIR especially in the disambiguation and query expansion processes and possibly enrich existing bilingual lexicons We proposed a two stage corpus based translation model consisting of bi directional extraction of bilingual terminology and linguistic based pruning Among the drawbacks of the proposed translation process is the introduction of many noisy terms or wrongly translated terms however most of those terms could be considered as efficient for the query expansion in CLIR but not for the translation Combination of two stage corpus based translation model and bilingual dictionaries yields to better translations and an effectiveness of information retrieval could be achieved across Japanese and English languages Further extensions include an integration of a transliteration model in the hybrid 
combination especially for loanwords and foreign words of Japanese language Second possible extension is the decomposition of the large scale corpora into comparable pieces instead of taking the whole corpus as a single piece could be investigated in the future Third translation on a word by word basis is not applicable to all compounds and technical terms especially when dealing with Japanese language Thus the problem of multiword phrases and compounds should be solved Evaluations using other combinations and more efficient weighting schemes that are not included in SMART retrieval system such as OKAPI which showed great success in information retrieval are among the future subjects of our research on CLIR References 1 Buckley C Allan J Salton G Automatic Routing and Ad hoc Retrieval using SMART In Proceedings of the Second Text Retrieval Conference TREC 2 pp 330 F Sadat 8 Fox A E Shaw A J Combination of Multiple Searches In Proceedings of the Second Text Retrieval Conference TREC 2 pp Using Comparable 
Corpora to Improve the Effectiveness 331 27 Renders J M Dejean H Gaussier E Assessing Automatically Extracted Bilingual Lexicons for CLIR in Vertical Domains XRCE Participation in the GIRT Track of CLEF 2002 In Peters C Braschler M Gonzalo J eds CLEF 2002 LNCS vol 2785 Springer Heidelberg 2003 28 Sadat F Maeda A Yoshikawa M Uemura S Exploiting and Combining Multiple Resources for Query Expansion in Cross Language Information Retrieval IPSJ Transactions of Databases 43 SIG 9 TOD 15 Semi automatic Endogenous Enrichment of Collaboratively Constructed Lexical Resources Piggybacking onto Wiktionary Franck Sa jous1 Emmanuel Navarro2 Bruno Gaume1 Laurent 1 CLLE ERSS CNRS Abstract The lack of large scale freely available and durable lexical resources and the consequences for NLP is widely acknowledged but the attempts to cope with usual bottlenecks preventing their development often result in dead ends This article introduces a language independent semi automatic and endogenous method for enriching lexical resources 
based on collaborative editing and random walks through existing lexical relationships and shows how this approach enables us to overcome recurrent impediments It compares the impact of using different data sources and similarity measures on the task of improving synonymy networks Finally it defines an architecture for applying the presented method to Wiktionary and explains how it has been implemented Keywords Collaboratively Constructed Lexical Resources Endogenous Enrichment Crowdsourcing Wiktionary Random Walks 1 Introduction While emerging pro cesses of creation and diffusion keep increasing the pro duction of digital do cuments the tools to process them still suffer from a lack of acceptable linguistic resources for most languages We desperately need linguistic resources is claimed in 1 after arguing that it is not realistic to assume that large scale resources can all be developed by a single institute or a small group of people and concluding that a collaborative effort is needed and that sharing 
resources is crucial In this paper we propose a new metho d for developing lexical resources which could meet these needs and we apply it to Wiktionary 1 the free online dictionary The system we describe automatically computes semantic relations namely synonyms to be added or not to a lexical network after being validated or invalidated by contributors In Section 2 we take inventory of the usual approaches and point out the impediments that hinder the success of such pro cesses We then investigate new trends which could help overcome this shortcoming We outline in Section 3 the key points of our metho d based on a 1 http www wiktionary org H Loftsson E Semi automatic Enrichment of Collaborative Lexical Resources 333 semi automatic endogenous enrichment process We explain in Section 4 how we compute the candidate relations by random walks over various graphs and using several measures that we evaluate regarding our specific purpose We present the architecture built to carry out the whole enrichment validation 
system in Section 5 and we describe possible future extensions of out metho d in Section 6 2 2 1 Lexical Resource Building Context Princeton WordNet 2 is probably the only successful large scale project among lexical resource building attempts which is widely used The subsequent projects EuroWordNet 3 and BalkaNet 4 were less ambitious in terms of coverage Moreover these resources froze as soon as the projects ended while Princeton WordNet kept on evolving EuroWordNet s weaknesses have been underlined in 5 and automatic metho ds to add missing lexical relations have been proposed Existing resources have been used in 6 to build WOLF a free French WordNet Pattern based approaches were first proposed in 7 to harvest semantic relations from corpora and refined in 8 by reducing the need for human supervision All of the latter three automatic processes would require validation by experts to pro duce reliable results However the cost of this validation work makes it difficult to afford or results in resources that 
are not freely accessible The problems of time cost and availability are increasingly becoming a matter of concern in corpus linguistics an AGILE like metho d borrowed from Computer Science has been proposed in 9 to address the problem of simultaneously maximizing corpus size and annotations while minimizing the time and cost involved in the creation of corpora To tackle the availability issue and build free corpora a metho d relying on metadata to automatically detect copylefted web pages is described in 10 In the domain of lexical resource building metho ds relying on crowdsourcing may help overcome recurrent bottlenecks 2 2 Collaboratively Constructed Resources CCR It has been claimed in 11 that the accuracy of Wikipedia comes close to Britanica who criticized the criteria of the evaluation 12 A more mo derate study 13 has shown in a task measuring the semantic relatedness of words that resources based on the wisdom of crowds are not superior to resources based on the wisdom of linguists but that CCRs are 
strongly competitive It has also been demonstrated that crowds can outperform linguists in term of coverage Collaborative and so cial approaches to resource building do not rely only on colleagues or students but on random people who do not share the NLP researchers interest for linguistic resource building Therefore building sophisticated and costly infrastructures that are empty shells waiting to be filled presents the risk of being platforms that no one would visit Indeed in the current web landscape competition for visitors is difficult and empty shells as promising as 334 F Sajous et al they can be are not attracting many people Any infrastructure that underestimates and do es not answer this attractiveness issue is doomed to fail However there are at least two main tracks to follow in order to avoid this pitfall Gamers Some language resource builders have been successful in designing simple web games in which many people come to play just for fun For instance the French serious game Jeux de Mots 2 14 
has been useful for collecting a great number of relations between words mostly non typed asso ciative relations but also better defined lexico semantic relations such as hypernymy meronymy etc However setting up an interesting game for collecting any kind of linguistic information is not easily feasible For instance domain specific resources might be harder to collect this way Secondly designing game play that really works is a difficult task in itself and it is likely that many game elicited resource initiatives will fail because of the game not being fun for random people Piggybackers Only a few collaborative or so cial infrastructures are really successful These resources and networks concentrate the ma jority of internet users Merely being asso ciated with one of these success stories affords the possibility of crowds of visitors Wiktionary and Wikipedia are probably the best examples The NLP community can offer some services to the users of these resources in order to take advantage of their huge 
amounts of visitors and contributors Significant steps towards such an architecture have been made in 15 16 Generalizing this approach to so cial networks while adding a gaming dimension is also possible and constitutes an interesting avenue to be explored Moreover simply adding plugins to existing solid and popular infrastructures requires much less effort and technical skill than setting up the whole platform though lots of technical difficulties o ccur to comply with and plug into these infrastructures 3 Outline of Proposal for a New Approach Taking into account the observations made in Section 2 and considering the benefits of using CCRs we propose a metho d for enhancing lexical resources that is reasonable in terms of time and cost based on i piggybacking onto Wiktionary ii computing similarity measures grounded on random walks through the graphs extracted from its lexical networks Sections 4 2 and 4 3 and iii giving an easy way for users to validate the candidate relations that we suggest 3 1 
Wiktionary Wiktionary is a free multilingual collaborative dictionary including definitions semantic relations and translations a detailed presentation can be found in 15 16 Its intrinsic features fulfill some of our needs it is publicly available its growth is fast and continuous and as its content is based on crowdsourcing the 2 See http www lirmm fr jeuxdemots jdm accueil php Semi automatic Enrichment of Collaborative Lexical Resources 335 reasonable cost constraint turns euphemistic However what is the quality of resources constructed by naive speakers as compared to those built by skilled professional lexicographers A recent study 17 evaluated three German resources designed in different manners expert built GermaNet semi controlled OpenThesaurus and collaboratively edited German Wiktionary This comparison demonstrated that all resources have a similar topology3 and lexical coverage but different density of semantic relations for instance Wiktionary has fewer hypernyms hyponyms than GermanNet but 
clearly outperforms both other resources in term of antonymy relations Table 1 gives the number of common nouns verbs adjectives and undirected synonymy and translation links for the French and English Wiktionaries in 2008 and 2010 These figures relate to all lexemes found conversely in 16 only the lexemes connected by synonymy links have been counted Translation and synonymy links have been counted after the graphs have been symmetrized i e two way links are counted once Table 1 Growth of French and English Wiktionaries from year 2008 to 2010 2008 2010 Nouns Verbs Adj Nouns Verbs Adj Lexemes 38 973 6 968 11 787 106 068 As we can see the number of lexemes has seen a growth that makes Wiktionary for these languages comparable to commercial printed dictionaries in term of lexical coverage the French Petit Robert includes 60 000 entries and the Longman Dictionary of Contemporary English features 50 000 entries Moreover all the resources that capture some aspect of linguistic knowledge can prove to be useful and 
interesting So traditional resources and collaborative resources should both continue to be developed especially since as mentioned by 19 their content do es not overlap too much Regarding semantic relations we have shown the sparseness of the synonymy networks extracted from Wiktionary in 2008 16 Synonymy relations grew at slower rate than lexeme coverage which makes the 2010 graphs even more sparse To help fill this gap we present below an endogenous enrichment metho d 3 2 Endogenous Enrichment Our aim is to be able to propose for an existing semantic lexical network new relations that are potentially missing To propose new pairs of words which may be synonymous we compute a similarity measure between any two nodes lexemes of the network by applying random walks through already existing lexical relations Details of the different data sources graph mo deling and measures we use are given in Section 4 3 Extracted graphs are small worlds with a heavy tailed degree distribution see 18 336 F Sajous et al As the 
potential new synonyms we compute are to be validated by contributors and not automatically added to the initial resource our purpose is i to suggest candidates for the greatest number of lexemes and ii for a given lexeme to propose a finite list of candidates including at least some relevant ones In our case it is better to propose no candidate at all than irrelevant ones and the system is not meant to suggest all relevant candidates first because a contributor won t check an endless list and secondly our metho d is an iterative computation suggestion validation cycle Thus if a relevant candidate is not initially proposed it may be the next in the list of suggestions which may be shifted when a suggested candidate is chosen So as the relations added to the network will change its structure and as the computation of candidates will be repro cessed regularly after the release of a new dump in the case of Wiktionary this relevant candidate may be proposed after some iterations Thus recall will increase with 
successive iterations and we focus therefore more on precision 3 3 Validation The candidates that we compute are suggested to the contributors via an interface described in Section 5 If a contributor validates a suggestion the relation is added to Wiktionary No cross validation system in which a relation would be added only if several contributors validate it has been designed to keep close to the wiki principle we did not add any additional regulation 4 but as we ease the addition of synonyms we fairly give an easy way to remove them too 4 Similarity Elicitation This section presents the metho ds used to compute from existing lexical networks new synonymy relations to be added We rely on different kinds of data and similarity measures and compare the results obtained by evaluating them against expert built gold standards 4 1 Data Networks have been extracted from English and French Wiktionaries for nouns verbs and adjectives thus splitting the global structure of the dictionaries into mono part of speech 
subparts Given a language version of Wiktionary we consider only the article sections dedicated to entries in the language of interest e g the English lexemes of the English Wiktionary From these sections we extract the existing synonymy and translation links as well as the glosses 4 2 Bipartite Graphs Model In order to homogenize and simplify the description of the experiments each type of data we used will be mo delled as a undirected bipartite graph G V V E 4 For some insights into the autoregulation of the Wikiprojects ecosystem see 20 Semi automatic Enrichment of Collaborative Lexical Resources 337 where the set of vertices V will always denote the lexemes of the language and part of speech of interest whereas another set of vertices V will vary depending on the sources of data The set of edges E is such that E To propose new synonymy relations we compute the similarity between any possible pair of lexemes the vertices from the graphs described in the previous section The intent is to propose as 
candidates the pairs with the highest scores which are not already known as synonyms in Wiktionary We test various similarity measures all based on short fixed length random walks Such approaches for measuring the topological resemblance in graphs are intro duced in 18 21 This kind of methods is applied to lexical networks in 22 to compute semantic relatedness We consider a walker wandering at random in the undirected bipartite graph G V V E starting from a given vertex v At each step the probability for the walker to move from nodes i to j is given by the cell i j of the transition matrix P defined as follow P ij 1 d i if i j E 0 otherwise 1 where d i is the degree of incidence number of neighbours of vertex i Thus starting from v the walker s position after t steps is given by the distribution of 5 6 As we parse only the dump of the language of interest we find the oriented link v t t as a translation of v in v s article and symmetrize it into v t Having a more subtle model with oriented edges requires 
parsing all dumps of all languages http www ims uni stuttgart de projekte corplex TreeTagger 338 F Sajous et al probabilities Xt v v P t where v is a row vector of dimension V V with 0 anywhere except 1 for the column corresponding to vertex v We note Xt v u the value of the coordinate u of this vector which denotes as aforementioned the probability of reaching u afer t steps starting from v This is the first measure7 called simple we use other measures are based on this one simple v u Xt v u avg v u cos v u dot v u w V 2 3 4 5 Xt v u Xt u v 2 w V Xt v w Xt u w w V Xt v w 2 w V Xt u w 2 Xt v w Xt u w Xt v w log X if Xt u w 0 t u w otherwise ZKL v u w V Xt v w 6 cos and dot are respectively the classical cosine and scalar pro duct ZKL is a variant of the Kullback Leibler divergence intro duced in 22 Let C v G t sim be the ordered list of candidates computed on graph G with the similarity measure sim and a random walk of length t starting from v i sim v ui sim v ui 1 i sim v ui 0 7 C v G t sim u1 u2 un with i 
v ui EW s where EW s is the set of existing synonymy links in Wiktionary The experiments below consist in evaluating the relevancy of C v when G and sim vary t 2 will remain constant 8 4 4 Evaluation Method In view of our application cf Section 5 2 and given the criteria defined in Section 3 2 for each lexeme we consider that a suggested list of candidates is acceptable if it includes at least one relevant candidate Indeed a user can contribute provided that at least one good candidate occurs in the suggested list Thus the evaluation will broadly consist in counting for how many lexemes the system computes a suggested list with at least one relevant candidates Let GGS VGS EF S be a gold standard synonymy network where VGS is a set of lexemes and EGS 7 8 All these measures are not strictly speaking similarity indeed simple and zkl10 are not symmetric t has to be even and preliminary experiments have shown that the best results are obtained with 2 or 4 t 2 gives similar results and is less complex Semi 
automatic Enrichment of Collaborative Lexical Resources 339 against the gold standard s relations We only evaluate the suggested lists for the lexemes that are known by the gold standard i e v VGS Indeed if a lexeme v V do es not belong to the gold standard i e v V VGS we consider that it is a lexical coverage issue so one cannot deem whether a relation v c is correct or not 9 For the same reason for each lexeme v we remove from C v the candidates absent from the gold standard Finally we limit the maximum number of candidates to k 5 For each lexeme v V VGS we note k v the evaluable suggested list of candidates k k i ci C v VGS 8 k v c1 c2 ck with i sim v ci sim v ci 1 Please note that k v contains a maximum of k candidates but it may be smaller or even empty Note also that k v depends on the gold standard We note k v the set of correct candidates within k v k v c k v v c EGS 9 We define the set Nk of lexemes having at least one candidate being proposed of lexemes for which at least one correct candidate is 
proposed and the set Nk Nk v V VGS k v Nk v V VGS k v 10 To compare the efficiency of different data sources used to compute the candidates we measure Pk the ratio between the acceptable suggested lists and the lexemes for which suggestions are done and Rk the ratio between the number of suggested lists and the number of evaluable target lexemes Pk Nk Nk Rk Nk VGS V 11 Although Pk and Rk are not precision and recall measures they intuitively refer to the same notions and we adopt below abusively this terminology 4 5 Results Gold Standards We used Princeton WordNet to evaluate the candidates for English and DicoSyn10 for French The extraction of the synonymy networks from these resources repro duces what has been done in 16 Similarity measures Applying the different similarity measures presented in Section 4 3 shows that all give pretty similar results As an example the results obtained for the intersection of the gold standards and the English and French Wiktionaries nouns and verbs are reported in Table 2 
The simple measure being as efficient as the others and having far less complexity further experiments have therefore been done using this measure 9 10 v may be a neologism or a domain specific word Less often it may be misspelling Any relation v c should therefore not be counted as false or true Dicosyn is a compilation of synonym relations extracted from seven dictionaries produced at ATILF and corrected at CRISCO units 340 F Sajous et al Table 2 P5 precision comparison for different data sources and measures Synonyms EN FR V N V N simple 41 4 32 4 58 6 47 3 avg 42 5 33 5 58 2 46 8 cos 43 4 34 6 60 2 47 9 dot 42 0 34 0 59 7 46 7 ZKL10 43 2 34 0 60 1 48 2 Translations EN FR V N V N 51 4 37 8 78 7 58 3 50 5 38 0 78 7 58 3 51 8 38 5 78 3 58 6 52 3 38 7 78 2 58 7 51 8 38 6 78 7 58 8 Syn EN V N 51 9 39 0 51 1 39 3 51 3 39 4 52 4 39 7 51 9 39 8 Trans FR V N 74 6 55 3 74 0 55 1 73 1 54 2 73 6 54 8 74 0 54 5 Data sources As we can see in Table 3 better results are obtained for French than for English This can be 
partly explained by the slightly lower density of the English networks cf Table 1 but is mainly due to the difference between the gold standards used networks extracted from WordNet are more sparse than the ones extracted from Dicosyn see 16 Morever Table 4 shows that some candidates rejected by the gold standards do not look unreasonable which makes it hard to draw definitive conclusions Nevertheless despite a potentially severe evaluation results look acceptable enough in view of our application The translations graph provides better precision than synonymy graphs This result was expected as in Wiktionary lexemes have more translation links than synonyms Moreover translations are often distributed over several languages which is more reliable than having a lot of translations into a given language The glosses graph s worse precision and higher recall was expected too almost all lexemes have glosses but information is less specific and we did not try any tricky edge weighting Combining synonyms and 
translations enables a better recall than with separated graphs and a similar precision for English For French it leads to a loss of precision compared to the translations only graph Table 3 Impact of different data sources on the simple similarity measure V 48930 196790 67649 41725 106068 17782 Synonyms Translations Syn Trans VGS V VGS P5 R5 P5 R5 P5 R5 21479 13742 46 3 24 9 53 5 23 4 53 7 34 6 117798 43236 32 4 17 1 37 8 24 9 39 0 32 4 11529 8890 41 4 33 2 51 4 43 5 51 9 53 8 9452 3958 61 2 24 9 76 1 19 8 69 6 34 2 29372 16084 47 3 23 2 58 3 22 2 55 3 35 4 9147 4037 58 6 22 3 78 7 36 8 74 6 45 8 Glosses P5 R5 26 1 98 3 14 9 98 9 27 0 99 9 32 2 96 1 20 7 99 4 41 1 99 4 Adj EN Nouns Verbs Adj FR Nouns Verbs Table 4 Example of propositions for nouns evaluated against gold standards GS in GS Yes No FR Yes No Propositions imprisonment captivity harmony peace filth dirt antipasto starter load burden possessive genitive rebirth renewal fool idiot dummy cheating fraud bypass circumvention dissimilarity variance 
pro benefit ouvrage travail renom gloire emploi fonction drapeau pavillon rythme cadence roulotte caravane chinois tamis drogue psychotrope fantassin bidasse force poigne salade bobard W C chiotte us tradition bisque soupe EN Semi automatic Enrichment of Collaborative Lexical Resources 341 5 Implementation The WISIGOTH Architecture In order to carry out our enrichment metho d we designed an architecture called WISIGOTH11 composed of a set of mo dules depicted in Fig 1 Fig 1 The WISIGOTH architecture 5 1 Computation of Candidates The first part of the architecture is made of a pro cessing pipeline which from a Wiktionary dump 12 builds the graphs intro duced in Section 4 2 and computes the candidate relations by applying the metho d described in Section 4 3 This pro cessing pipeline can be triggered each time a new dump is released or when a given threshold of edits has been registered 5 2 Suggestion and Validation of Candidates The interface we developed to suggest and validate or invalidate new relations 
materializes as a Firefox extension Once installed when a user browses the English or French Wiktionary the interface sends a request to the candidates service which returns for each known lexeme a list of potential synonyms Suggestion and Editing Next to each proposition appears a sign which triggers the automatic addition of the candidate as a synonym to the Wiktionary server As a contributor may want to add a synonym that has not yet been suggested we provide a free text area too Regardless of our enrichment metho d it enlarges the potential population of contributors not restricting it to wikico demasters acquainted with the underlying syntax As explained is Section 3 3 a sign is added to every synonym o ccuring in the page which handles the deletion of this synonym 11 12 WIktionarieS Improvement by Graph Oriented meTHods Wiktionaries dumps are available at http download wikipedia org 342 F Sajous et al Notification of editing Thus far wiktionaries dumps are released frequently Nevertheless to protect 
against irregular dumps which could result in a desynchronization between Wiktionary s current state and the lexical networks we extracted from it and therefore cause irrelevant suggestions the interface notifies our server the editing of synonyms Thus a remo delling of synonymy networks and a reprocessing of candidates may be done between two releases Storing these notifications will also later give us the opportunity to analyse which synonymy links look problematic e g a series of additions and deletions and how contributors behave 6 Conclusion and Future Work This paper has pointed out the problems usually encountered in the development of lexical resources It has shown how CCRs help overcome these difficulties and among them how we can take advantage of Wiktionary s infrastructure and content Nevertheless crowds are more prone to add new words than to provide semantic relations To encourage them we have designed a tool to assist collaborative editing by suggesting new synonyms to be validated We took the 
opportunity to compare the impacts of using different data sources and similarity measures The choice of the measure does not much affect the results whereas combining data sources permits us to gain precision or recall depending on the language Adding glosses to the Syn Trad graph presented and working on the weighting of the graphs edges should bring even better results Grounded on the topology of the graphs extracted from the lexical networks this system is language independent and moreover may be applied to other resources than Wiktionary contrary to methods like 23 which exploit the structure of hyperlinks between pages and are therefore bound to this resource It may help for example building WordNets that are still under construction as the Chinese one 24 Moreover not relying on other external resources makes this metho d endogenous and may be applicable to enhance lexical resources for under resourced languages When external resources are available for example stemming from distributional analysis 
over large corpora an exogenous enrichment mo dule can be coupled to our system and feed our edition interface A short term extension of this work will be the proposition of new translations by leveraging the same kind of graph mo del and similarity measures Linguistic observations should be done to characterize what other kinds of semantic relation than synonymy is captured by automatically computed relatedness Although we did not rely on a cross validation system for adding synonyms we think it could be useful to add a blacklist system to stop proposing a candidate judged as irrelevant by several contributors for a given target lexeme An interesting study would be the evaluation of the results of the endogenous enrichment pro cess at different stages of Wiktionary s growth This can be done by rebuilding the various past states of the lexical networks using the historical dump containing all articles revisions Such a study may show when it is appropriate to apply our metho d when we have enough material to 
start suggesting Semi automatic Enrichment of Collaborative Lexical Resources 343 new relations and when no more relevant relation is to be proposed and should be stopped Resources the Firefox extension presented in this paper and the structured data extracted from Wiktionary s dumps are publicly available at http redac univ tlse2 fr wisigoth References 1 Sekine S We desperately need linguistic resources 344 F Sajous et al 17 Meyer C M Gurevych I Worth its Weight in Gold or Yet Another Portable Extraction of Partially Structured Facts from the Web Andrew Salway1 Liadh Kelly1 Inguna 1 Centre for Digital Video Processing School of Computing Dublin City University Dublin 9 Ireland asalway lkelly gjones computing dcu ie 2 Tilde 75 Abstract A novel fact extraction task is defined to fill a gap between current information retrieval and information extraction technologies It is shown that it is possible to extract useful partially structured facts about different kinds of entities in a broad domain i e all kinds of 
places depicted in tourist images Importantly the approach does not rely on existing linguistic resources gazetteers taggers parsers etc and it ported easily and cheaply between two rather different languages English and Latvian Previous fact extraction from the web has focused on the extraction of structured data e g Building LocatedIn Town In contrast we extract richer and more interesting facts such as a fact explaining why a building was built Enough structure is maintained to facilitate subsequent processing of the information For example the partial structure enables straightforward template based text generation We report positive results for the correctness and interest of English and Latvian facts and for their utility in enhancing image captions Keywords Fact extraction multilingual information retrieval information extraction web image captioning 1 Introduction This paper proposes a novel fact extraction task which fills an important gap between current information retrieval IR and information 
extraction IE technologies in order to further exploit the vast quantities of multilingual information available on the web Search engines retrieve relevant web pages across diverse domains and across languages but the onus is on the user to read through and interpret the results By contrast IE systems provide structured facts and data from natural language texts which are amenable to further automated analysis and multi document summarization systems and question answering systems fuse information about an entity or topic of interest to reduce reading time However such systems are typically costly to port to new languages and the domains in which they work tend to be narrow and comprise only a small set of entity types and relations We believe that there are emerging applications H Loftsson E 346 A Salway et al such as automated image captioning and augmented reality which would benefit from exploiting information on the web across broad domains and multiple languages but which do not require fully 
structured information or the ma jority of all available information about an entity For example to automatically enhance an image caption we only require one interesting fact about the place in the image with enough structure for the fact to be inserted appropriately into a text generation template In sacrificing the requirements for full structure and comprehensive information about an entity we expect to gain considerably in broad coverage of domains and ease of porting between languages We elaborate these points in Section 2 as we define the Tell Me About task which is roughly to provide one or more of the most interesting facts about a given entity in a partially structured form that enables some further pro cessing and re use of the information Section 3 discusses related work in the fields of IR and IE with a focus on information extraction from the web multido cument summarization and question answering Section 4 presents a highly portable solution for extracting partially structured facts that 
exploits information redundancy on the web i e the fact that the same information about an entity is available in many forms on the web The crucial assumption is that at least one key fact about an entity will be expressed somewhere on the web in a simple form This means that we work with a few simple linguistic structures and shallow language processing and so the solution ports easily between languages We report positive results for the correctness and interest of facts in two rather different languages English and Latvian 128 facts each judged by an investigator and five sub jects Latvian is a highly inflected language nouns adjectives participles and verbs are all inflective and because of this rich morphology Latvian has quite free word ordering The utility of the Tell Me About task is demonstrated by enhancing the captions of tourist photographs using extracted facts for template based text generation with an evaluation of caption readability 90 image captions each judged by six sub jects In closing 
Section 5 considers generalising our solution to other domains and applications 2 The Tell Me About Task Let us elaborate on the details of this task and the motivation for it by considering one potential application automatic image captioning The number of digital images being archived in personal collections and shared in so cial image collections such as www flickr com and www panoramio com is increasing very rapidly When users view images from these collections it is desirable to have information describing each image available in a caption However people taking pictures will often either not know sufficient details about the place depicted in the image to do this effectively or will not take the time to do this so automated solutions are required There is also a burgeoning interest in augmented reality whereby a camera screen on a mobile device is updated automatically with caption like information about the place that the camera is pointed at Digital image capture devices are increasingly incorporating 
location sensing via Portable Extraction of Partially Structured Facts from the Web 347 GPS monitoring This can be combined with other image metadata such as the date and time of capture and cross referenced with geographic databases to generate simple descriptive captions for an image e g of the form North Bridge photographed in the afternoon 1 We see an opportunity to exploit the vast information content of the web in order to enhance such a caption with a key fact e g to output something like North Bridge which was built to link the New Town with the Old Town photographed in the afternoon Whilst we can be confident that information about many places is available in many languages on the web the challenge is to identify the most interesting facts for a given entity There is also the challenge of extracting information into partially structured facts that enable further pro cessing and re use of the information In the image captioning scenario simply adding whole sentences from the web to an existing 
caption would have unpredictable results for caption readability It could be that a long sentence contains information about more than one place so we need to identify just the relevant part of the sentence Also if we want to insert information into an existing caption i e into the middle of a sentence then we need to know something about how it phrased For the Tell Me About task we specify that facts should have the form of a triple Entity Cue Text Fragment where Cue is one of a fixed set of information cues loosely akin to relations and Text Fragment is a text fragment taken directly from a webpage such that Cue Entity Text Fragment reads naturally as a sentence e g North Bridge was built to link the New Town with the Old Town For template based image captioning this means we can for example insert information in a subclause starting with which for cues such as was built but removing which and the cue itself for cues like is The partial structure of the fact gives us control over text generation that we 
would not have if the fact was only a text fragment However because the right hand side of the fact is a text fragment and not another entity of fixed type as it would be in a standard IE template then the same cue can get quite different kinds of information allowing for much richer facts when available e g Hadrian s Wall was built in AD 122 130 on the orders of the Emperor Hadrian Hadrian s Wall was built to keep out the marauding Scottish To summarise the Tell Me About task proposed here is as follows Given the name of an entity and a specified language a list of facts about the entity should be returned in the form Entity Cue Text Fragment sorted with interesting facts ranked higher With regards to image captioning it is important to note that the place depicted in a photo may be one of very many different kinds of entity bridge monument beach church mountain plaza glacier etc Furthermore the most interesting aspect of one entity may not be the same as the most interesting aspect of another entity of the 
same type one church has spectacular stained glass windows another is known for an historical event that happened there a third offers amazing views from its tower Finally a caption for an image on a website may be required in many languages For these reasons as we discuss next current IE approaches are not appropriate 348 A Salway et al 3 Related Work Although we consider Tell Me About to be distinct from other natural language pro cessing tasks it do es clearly have similarity with established and well understood tasks within IR and IE The idea of ranking facts could be seen as similar to the ranking of documents for IR 2 and more specifically the retrieval and ranking of passages 3 Indeed snippets returned by web search engines are the starting point in our approach to fact extraction although by the end of the process the sorted facts are in a different order than the snippets ranked by the search engine The extraction of partially structured information makes our fact extraction look quite a lot like IE 
4 but whilst we do specify a set of cues similar to relations we do not require the structuring of the right hand side text fragment into a template which would for example make relations between entities explicit We have found that this makes it possible to pursue quite a generic approach to fact extraction across broad domains and multiple languages whereas IE systems require non trivial amounts of work to be adapted to different kinds of entities and languages Question answering systems return facts typically in response to factoid questions with answers that are dates locations organizations people etc 5 However for a given entity it is not possible to anticipate what if any factoid question will give the most interesting information That said our approach to fact extraction shares assumptions about the redundancy of information on the web with some question answering techniques e g 6 Multi document summarization systems do something rather like the Tell Me About task when they select a set of 
informative sentences about an entity e g 7 but with a focus on more than just a few key facts and the need to pro duce coherent text as output such systems typically depend on quite extensive linguistic resources at a minimum training corpora that mitigate against porting easily between many languages Previous work on information extraction from the web rather than from domain specific collections of a single text type has achieved impressive quantities of facts at high levels of precision e g 1 million ranked facts with a prespecified relation at 75 98 Precision 8 Under the rubric of open information extraction which discovers relations as well as facts a precision of 88 has been reported 9 In related work the TextRunner system extracted over 500 million tuples from 120 million web pages 10 However much of this previous work has focused on the extraction of wholly structured data to specify relations between two entities e g facts of the form City CapitalOf Country PersonBornIn Year or Company Acquired 
Company Whilst this effectively enables the storage analysis and retrieval of millions of facts in relational databases these relatively simple facts are unlikely to be interesting for applications such as image captioning An online demonstration do es suggest that the TextRunner system 11 can provide facts with unstructured right hand sides but our impression is that low quality of information is the price for exceptionally broad coverage Furthermore with regards to portability between languages the approaches described by 9 and 10 rely on a linguistic analysis of how relations are expressed in English and on syntactic parsers Although the approach in 8 Portable Extraction of Partially Structured Facts from the Web 349 avoids syntactic analysis and parsing it nevertheless works with text that has been part of speech tagged and draws on existing word distribution data Taggers parsers and other linguistic resources are not available for many languages and so we have developed an approach that do es not need 
them 4 Our Approach to Fact Extraction Here we present a first solution for the Tell Me About task We show how given an entity in this case any kind of place we return a list of facts in the form Entity Cue Text Fragment ranked according to a score which is intended to promote interesting and true facts The approach is generic across a broad range of entities and requires minimal effort to port between languages It is based on two assumptions i the same information is expressed in many ways across the web so it is only necessary to look for it in a small number of relatively simple forms and ii overlaps between what is written on different web pages can be used to compute an interest correctness score to rank facts 4 1 Algorithm Given an entity steps I IV generate a list of facts about it I Get Snippets from Search Engine A series of queries is made to a web search engine we used Yahoo s BOSS API 12 Each query takes the form Entity Cue the use of double quotes indicates that only exact matches are wanted i e 
text in which the given entity and cue are adjacent A set of cues is manually specified to capture some common and simple ways in which information about the general kind of entity is expressed For places we used cues like is a is famous for is popular with was built Although we worked with around 40 cues including single plural and present past forms it seems that a much smaller number are responsible for returning the ma jority of high ranking facts in particular and perhaps unsurprisingly the generic is seems most pro ductive The query may also include a disambiguating term For example streets and buildings with the same name may occur in different towns so we can include a town name in the query outside the double quotes e g West Street is popular with Bridport For each query all the unique snippets returned by the search engine up to a specified maximum are pro cessed in the next step typically a snippet is a few lines of text from a webpage around the words that match the query often broken in mid 
sentence II Shallow Chunk Snippets to Make Candidate Facts Because all the information that we retrieve about the entity is expressed as Entity Cue then we can use a simple extraction pattern to obtain candidate facts from the retrieved snippets For both English and Latvian the gist of the pattern is BOUNDARY ENTITY CUE TEXT FRAGMENT BOUNDARY such that TEXT FRAGMENT captures the Text Fragment part of a fact The details of the pattern are captured in a regular expression on a language specific basis e g to specify boundary words and punctuation to allow optional words to 350 A Salway et al appear in between ENTITY and CUE and to reorder the elements for non SVO languages A successful match of the pattern on a snippet leads to the generation of a candidate fact using the extraction pattern in the Appendix the snippet text in London Big Ben was named after Sir Benjamin Hall matches giving the candidate fact Big Ben was named after Sir Benjamin Hall but The square next to Big Ben was named in 1848 do es not 
match III Filter Candidate Facts Four filters are used as a quality control the first two of which require language specific word lists built manually over a number of runs of the algorithm General filter words a candidate fact containing any of the given filter words is removed this can be used to remove potentially sub jective statements containing me my our amazing fantastic etc Invalid end words to catch some erroneous shallow chunking most likely due to noisy web data or to a badly cut search engine snippet this filter removes candidate facts ending in words such to from by etc Length of Text Fragment a threshold can be set to filter out candidate facts with text fragments shorter than the specified number of words it seems that shorter text fragments are more likely to lead to incomplete or incorrect facts Words all in capitals when this filter is turned on any candidate fact containing a word all in capitals is removed this is good for removing spam and content in an informal style but of course it 
also removes candidate facts containing acronyms IV Score and Sort Facts Our idea here is to rank facts at least coarsely so that we are more likely to get correct and interesting facts at the top The notions of correctness and interest are each problematic and difficult to unpick for the purposes of algorithm design and evaluation Here we exploit the overlap between candidate facts for the same Entity Cue pair to capture these notions to some extent For each Entity Cue pair a keyword frequency list is generated by counting the occurrence of all words in the Text Fragments for that pair words in a stop word file are ignored The score for each fact is then calculated by summing the Entity Cue frequencies of each word in the Text Fragment so that facts containing words that were common in other facts with the same Entity Cue will score highly If shorter facts are wanted then the sum is divided by the word length of the Text Fragment We see two main ways in which the sum score for a fact can get high i there 
are many overlapping Text Fragments for an Entity Cue pair so there are some high word frequencies and ii a fact contains more of those high frequency words than other facts Thus we hope to get high ranked facts with the most appropriate Cue for the Entity and the best Text Fragment for the Entity Cue pair To give an impression of how ranking works Figure 1 shows the top and bottom 10 facts returned for Eiffel Tower using the sum only scoring The top ranked facts are generally rich in correct information about the given entity In contrast incomplete and trivial facts end up low down the list We see that 4 of the top 10 facts have the Cue was built which seems like a goo d cue for interesting information about an historical monument The high ranking facts with this Cue include words like Paris 1889 international exhibition Portable Extraction of Partially Structured Facts from the Web 351 Eiffel Tower was built in 1889 for an international exhibition in Paris Eiffel Tower was named after an ingenious engineer 
whose design of the tower turned it into a reality and pride of the French nation Eiffel Tower is an iron tower built during 1887 1889 on the Champ de Mars beside the Seine River in Paris Eiffel Tower was one of the first tall structures in the world to contain passenger elevators Eiffel Tower was one of the landmarks visited by Luigi when he came to save Paris from invading Koopa Troopas Eiffel Tower was built by Gustave Eiffel for the International Exhibition of Paris of 1889 commemorating the centenary of the French Revolution Eiffel Tower was one of the first structures in the world to have passenger elevators Eiffel Tower was built in 1889 for the Universal Exposition celebrating the centenary of the French Revolution Eiffel Tower was built as a temporary structure for an exhibition in 1889 Eiffel Tower is named after its designer and engineer Alexandre Gustave Eiffel Eiffel Tower is built for the Paris exposition Eiffel Tower was famous enough for everyone to know Eiffel Tower is made up of a base 
Eiffel Tower was made for the Exposition Universelle Eiffel Tower is made of over 10 Eiffel Tower is made from 18 Eiffel Tower is made of 3 platforms Eiffel Tower is made with 2 Eiffel Tower is famous throughout the world Eiffel Tower is famous for a reason Fig 1 The top 10 and bottom 10 facts for the entity Eiffel Tower which are likely to appear after Eiffel Tower was built on many web pages the fact with all four of these words is ranked highest For Latvian the top ranked fact was Francijas 352 A Salway et al North Bridge was originally built in 1772 to connect the burgh with the Port of Leith to the north North Bridge was built to link the New Town with the Old Town Bahnhofstrasse is where well heeled bankers and perfectly coiffed ladies shop for designer clothing and gold watches Bahnhofstrasse is Zurich s main shopping avenue Durdle Door is a natural limestone arch on the Jurassic Coast near Lulworth in Dorset Durdle Door is a limestone arch The Matterhorn was one of the last Alpine mountains to be 
ascended due to its imposing shape and unpredictable weather The Matterhorn was first climbed in 1865 Fig 2 Pairs of facts about places The first is the top ranked fact with simple sum score the second is the top ranked with sum divided by number of words 4 2 Evaluation This evaluation used 68 place names in English and 60 place names in Latvian from around Europe We chose an even mixture of urban rural and famous not famous places from European cities London Riga Zurich and Dublin and countryside UK Latvia Switzerland and Ireland and various types of place churches statues mountains rivers etc For each place the top ranked fact was used for evaluation see Appendix for the settings used to generate facts Evaluating Correctness of Facts Each of the facts in English was rated as correct or incorrect by an investigator by searching for the fact on the web in the following manner If the fact was found on Wikipedia or an official tourist website for the region and on one other website or if the fact was found on 
three independent websites it was marked as correct If part of the fact was found on the web using this technique then the fact was marked as partially correct Otherwise the fact was marked as incorrect Due to the lack of coverage in Latvian on the web Latvian facts were rated as correct if they were located on Wikipedia or an official tourist website for the region or if they were known to be correct by the investigator For the English experiment 35 of the 68 facts were marked as correct 51 13 were partially correct 19 and 20 30 were incorrect Analysing the partially correct facts revealed that 11 of the 13 were incomplete facts e g Dridzis Lake is the deepest lake not only in Latgale here it looks like our chunking pattern cut too soon i e on the word but although a similar problem o ccurs occasionally with the way the search engine creates snippets The other two partially correct facts had spurious material at the end of the fact e g Mount Titlis is the largest winter sports paradise in Central 
Switzerland even the most demanding skiers the unusual punctuation is missed by our chunking pattern Analysing the 20 incorrect facts we found that only six of them were actually false for example a fact which was supposed to be about the National Museum in Zurich actually referred to a museum in Prague this is despite our use of Zurich as a disambiguating term Eight of the Portable Extraction of Partially Structured Facts from the Web Table 1 Responses from 5 subjects for 68 English facts and 60 Latvian facts English Latvian 5 5 subjects 3 5 said 5 5 subjects 3 5 said said Yes Yes said Yes Yes Is this the type of fact 26 68 53 68 14 60 38 60 you would expect to 38 78 23 63 read in a travel guide 353 incorrect facts were unreadable for example Daugava river is soon to be a prelude of things to come that would prove 2000 wasn t Cappellini s year which we put down to web noise The correctness of the remaining 6 incorrect facts was actually indeterminable e g Bastejkalns Park was renovated during last winter we 
have since added words with temporal reference like last to the filter words list as well as deictic words like this Similar to the English results for the Latvian evaluation 32 of the 60 facts were marked as correct 53 19 32 were partially correct and 9 15 were incorrect Evaluating the Interest of Facts Ten native English speakers were each presented with 34 English facts to rate Ten native Latvian speakers were each presented with 30 Latvian facts to rate In this way each fact was rated by 5 sub jects The lists of facts presented to sub jects were randomly chosen using a Latin square For each fact sub jects answered yes or no to the question Is this the type of fact you would expect to read in a travel guide The question is intended to get at the notion of interest in a way specific to our application scenario i e we assume users would be happy with travel guide like facts added to their image captions Results are summarised in Table 1 which indicates that more often than not our algorithm is pro ducing as 
its top ranked fact something that most people find acceptable as a fact for something like a travel guide Our evaluation criteria for fact correctness were rather strict note that a majority of sub jects rated more facts as interesting 78 English and 63 Latvian than we ourselves rated as correct around 50 for both As noted it seems that some relatively simple changes to our extraction patterns and word lists will improve our correctness score quite considerably so overall we are confident that the fundamentals of the approach are sound Importantly the approach was very cheap to port between languages Porting from English to Latvian required only a small mo dification to the extraction pattern and the translation of the cue set see Appendix other word lists were also translated but for Latvian these had little impact on results 4 3 Enhancement of Image Captions Our motivation for doing fact extraction was to add information about places into image captions which provides a scenario for evaluating the utility 
of the facts that we extract Sets of 30 image captions were created in English and Latvian for images depicting urban and rural places o ccurring in Ireland UK Latvia 354 A Salway et al and Switzerland Half the captions were in the form PLACE photographed in LOCATION The other half were in the form Photo taken near PLACE in LOCATION Half of the captions also had the time of day inserted into the sentence Photo taken in the TIME OF DAY near PLACE in LOCATION Each of the 30 English captions had a fact added in two different ways 1 insert fact as a subclause in the original sentence and 2 append the fact to the original caption as a new sentence This led to 60 enhanced English captions For 1 the string which CUE TEXT FRAGMENT was inserted after the place name in the caption Recall we are only able to insert information as a subclause which keeps the captions more compact because we have partially structured facts cf Section 2 For 2 a second sentence was formed by adding PLACE CUE TEXT FRAGMENT after the 
original caption Insertion as subclause was deemed inappropriate for Latvian so we had just 30 enhanced Latvian captions with facts added as sentences We ensured that correct facts were added because we wanted to concentrate on evaluating the readability of the enhanced captions The 60 enhanced English captions were presented to 6 native English speakers in random orders for judgment The 30 enhanced Latvian captions were presented to 6 native Latvian speakers For each enhanced caption sub jects answered yes or no to the question Does this sentence read naturally to you When facts were added as new sentences then a ma jority of sub jects deemed 29 30 97 of the enhanced image captions to be readable both for English and for Latvian The results English only for inserting facts as subclauses seemed to depend on the form of the original caption For 15 15 100 of captions with the form Place photographed in location a ma jority of sub jects judged the enhanced caption with fact inserted as subclause to be readable 
For the other caption form only 7 15 47 enhanced captions were judged readable by a ma jority of sub jects Upon inspection it seemed that these captions tended to be quite long already including additional temporal information so a further subclause even though grammatically correct became awkward to read 5 Conclusions To summarise a new kind of fact extraction task was defined and a solution to the task was evaluated for two rather different languages It was shown that it is possible to extract useful partially structured facts about different kinds of entity in a broad domain using a common approach that ports easily between languages in the absence of existing linguistic resources In contrast with traditional IR techniques we pro duce output that is more amenable to further automated pro cessing In contrast with traditional IE techniques our approach has the potential to cover much broader domains and many more languages Of course we need to try other kinds of language before making strong claims about 
portability Although Latvian is a free word order language the SVO order do es dominate so we were able to get good results with just one extraction pattern However even in languages with more variation in word ordering we expect that we could use just a few extraction patterns based around cue sets Portable Extraction of Partially Structured Facts from the Web 355 What is less clear to us is the ease with which we can port to other domains Whilst we found interesting facts about many different kinds of places were expressed using a relatively small number of common cues this may not be the case for all kinds of entities That said in preliminary work we got some encouraging facts about people and organizations using just a few cues Beyond the image captioning application and template based text generation we see potential for the Tell Me About task in other areas For some kinds of queries to search engines users may benefit from being presented with a few facts about their topic of interest we feel that our 
chunking of information and ranking of facts can add value to the snippets returned by a search engine Recently some search websites have started to offer something more like knowledge retrieval on top of information retrieval 13 14 and our impression is that our kind of fact extraction could contribute to such endeavours Acknowledgement The research reported in this paper is part of the project TRIPOD supported by the European commission under contract No 045335 References 1 Purves R S Edwardes A J Sanderson M Describing the Where improving image annotation and search through geography In First Intl Workshop on Metadata Mining for Image Understanding 2008 2 Baeza Yates R Ribeiro Neto B Modern Information Retrieval ACM Press New York 1999 3 Salton G Allan J Buckley C Approaches to passage retrieval in full text information systems In 16th ACM SIGIR pp 356 A Salway et al Appendix Settings Used for Evaluation Runs English Cues used in queries to search engine is was is the was the is a was a is an was an is in 
is on is by is next to is near to is known is famous is located is one of was built is made of is named was named is home to was home to is used was used was completed was destroyed was damaged is the site of was the site of was the scene of was made famous is the most is the biggest is the largest is the smallest is the oldest can be seen from is popular is popular with features offers is located by is located on is located in is famous for is known for was built by was built in was built for was built to is open Regular Expression for Shallow Chunking of Snippets the The s ENTITY s CUE s b and b b but b ENTITY and CUE are interpolated at run time captures the Text Fragment Filter words I my me mine you your yours we us ours another recently this also other further must should could sensational fun deserves excellent amazing wonderful miles kilometres m km minutes min mins hours hour probably actually possibly Scoring stop words the of is for a an and Invalid final words a the those these with by and but 
which that for like as Latvian For both languages the maximum snippets returned from search engine for a single query was 20 scoring metric was simple sum and score threshold 3 Passage Retrieval in Log Files An Approach Based on Query Enrichment Hassan Saneifar1 2 LIRMM Univ Montpellier 2 CNRS France 2 Satin Technologies France saneifar laurent poncelet mroche lirmm fr stephane bonniol satin tech com http www lirmm fr saneifar laurent poncelet mroche Abstract The question answering systems are considered the next generation of search engines This paper focuses on the first step of this process which is to search for relevant passages containing answers Passage Retrieval can be difficult because of the complexity of data log files in our case Our contribution is based on the enrichment of queries by using a learning method and a novel term weighting function This original term weighting function used within the enrichment process aims to assign a weight to terms according to their relatedness to the context 
of answers Experiments conducted on real data show that our protocol of primitive query enrichment make it possible to retrieve relevant passages Keywords Information Retrieval Question Answering Passage Retrieval Query Enrichment Context Learning System 1 1 Introduction Information Retrieval IR aims to find do cuments related to a topic specified by a user The topic is normally expressed as a list of specific terms However the needs of some application domains make Information Retrieval IR metho ds inefficient Indeed when the goal is to find specific and concise answers Information Retrieval systems are not relevant according to the considerable amount of documents that they retrieve as possibilities Moreover the information found in retrieved documents is not always correlated with the given query That is why Question Answering QA systems are an important research topic nowadays Question Answering systems aim to find a relevant fragment of a do cument which could be regarded as the best possible concise 
answer to a question given by user There are two main categories of QA systems 1 Open domain and 2 Restricted domain In the first case questions arise about general domains and sources of information are large corpora consisting of do cuments of several fields e g corpus of web pages The evaluation of open domain QA systems has been built since 1999 in TREC1 Text REtrieval Conference American evaluation campaigns 1 http trec nist gov H Loftsson E 358 H Saneifar et al In the second category QA systems are designed to answer questions in a specific area In this kind of QA systems information resources are technical and specialized documents The restricted domains called also closed domains have certain characteristics which make the metho ds of open domain QA become less useful 3 We detail these characteristics and some features of specialized textual data which make answer retrieval more difficult in Sect 2 The Passage Retrieval represents an important phase of QA pro cess To give an efficient and reliable 
definition a passage is defined as a fixed length sequence of words which can begins and ends anywhere in a do cument 9 The Passage Retrieval is the task of searching for passages which may contain the answer to a given question In this paper we present our work on passage retrieval in a specialized domain We deal with a particular type of complex textual data which are log files generated by Integrated Circuit IC design tools These log files are digital reports on configurations conditions and states of systems In this area checking the quality of pro ducts requires to answer some technical and specialized questions At this stage our goal is to find segments of the logs that contain the answers to the questions of quality check The particularity of such textual data i e log files and characteristics of restricted closed domains impact significantly the accuracy and performance of passage retrieval in this context We propose in this paper a passage retrieval system based on a new approach of query enrichment 
Our query enrichment process is based on a learning approach and a new weighting function which gives a score to terms of corpus according to their relatedness to the context of answers The enrichment pro cess takes place in two phases First we propose a metho d for learning the context of questions based on the notion of lexical world In the learning phase we identify the terms representing the context of questions Then the initial queries are enriched by these terms Secondly we propose an original term weighting function which aims at giving a high score to terms of corpus which have a significant probability to exist in the relevant passages The terms having the highest scores are included in the query in order to improve its enrichment Our approach is an interactive system based on relevant feedback We show that our approach gives satisfactory results on real data In Section 2 we present the specific characteristics of log files and also the limits of QA systems in restricted domains Existing work 
concerning the QA systems are presented in Sect 3 Section 4 presents some notions used in enrichment pro cesses and also the first phase of query enrichment In Section 5 we develop our approach of passage retrieval and query enrichment by presenting our novel term weighting function Experiments on real data are presented in Sect 6 2 Problem Study Log files generated by Integrated Circuits design tools are not systematically exploited in an efficient way despite the fact that they contain the essential Passage Retrieval in Log Files An Approach Based on Query Enrichment 359 information to evaluate the design quality The particular characteristics of logs described below make classical techniques of Natural Language Pro cessing NLP and Information Retrieval irrelevant 2 1 Information Retrieval and Log Files We consider log files as a kind of complex textual data i e containing multisource heterogeneous and multi format data Indeed in the design of Integrated Circuits different design tools can be used in the 
same time while each tool generates its own log files Therefore despite the fact that the logs of the same design level contain the same information their structures and vocabulary can vary significantly depending on the used design tool More precisely in order to report the same information each design tool uses its own vocabulary In this domain the questions queries are expressed using a vocabulary that do es not necessarily correspond to the vocabulary of all tools However a system should be able to answer the questions regardless of the type of tools that generated the log files We explain this issue with an example Consider the sentence Capture the total fixed STD cell as a given question query We pro duce two log files eg log a and log b by two different tools The answer to the question in log a is expressed as follows Std cell area 77346 sites non fixed 74974 fixed 2372 While the answer in log b is expressed in this line preplaced standard cell is 24678 As shown above the same information in two log 
files produced by two different tools is represented by different structures and vocabulary The keywords of the question i e Fixed STD cell exist in the answer extracted from log a while the answer from log b contains only the word cell Insofar as there is a dictionary asso ciating the word STD with standard we can also consider the word standard However by giving these two words as a query to an information retrieval system irrelevant passages of log b are retrieved standard cell seeds is 4567 Total standard cell length 0 4536 This can be explained by the fact that the question is expressed using the vocabulary of log a which is different from the vocabulary of log b In other words for a given question the relevant answers found in the logs of some tools do not necessarily contain the keywords of the question Therefore the initial query created by taking the keywords of question may be relevant to logs generated by a tool but irrelevant to logs generated by another tool2 whereas we aim to answer questions 
regardless type of tools generating log files The existence of question keywords or their syntactic variants in a passage is an important factor to assess the relevance of the passage The approaches 2 While all of these logs report the same information using different vocabularies 360 H Saneifar et al which are based on the notion of common terms between questions and passages are detailed in Sect 3 Moreover the performance of a QA system depends largely on redundant o ccurrences of answers in the corpus in which answers are seek 1 7 The metho ds developed for QA systems are generally based on the assumption that there are several instances of answers in corpus But information is rarely repeated in the log files of IC design tools This means that for a question there is only one o ccurrence of the answer in the corpus and thus one relevant passage containing the answer In addition design tools change over time often unexpectedly Therefore the format of the data in the log files changes which makes automatic 
data management difficult Moreover the language used in these logs is a difficulty that impacts information extraction methods Although the language used in these logs is English their contents do not usually comply with classic grammar In the pro cessing of log files we also deal with multi format data textual data numerical data alphanumerical and structured data e g table and data block There are also many technical words that contain special characters which are only understandable considering the domain documentation Due to these specific characteristics of log files NLP and IR metho ds developed for texts written in natural language are not necessarily well adapted to log files We therefore suggest the enrichment of initial queries in order to make them relevant to all types of logs generated by any kind of design tools We explain the query enrichment process in Sect 4 2 2 Passage Retrieval in Log Files The passages retrieval phase influences significantly the performance of QA systems because final 
answers are sought in the retrieved passages Most QA systems for a given question extract a large number of passages which likely contain the answer But an important point in QA systems is to limit as much as possible the number of passages in which the final answer extraction is performed Since we are situated in a very specialized domain high precision in the final answers i e the percentage of correct answers is a very important issue This implies that the passage retrieval system has to classify relevant passages based on a relevance score in the top positions among all retrieved candidate passages 3 Related Work Most passage retrieval algorithms depend on occurrences of query keywords in corpus 9 To enhance the query the use of morphological and semantic variants of query keywords is studied in 2 The reformulation of questions also referred to as surface patterns and paraphrases is a standard metho d used to improve the performance of QA The technique is based on identifying various ways of expressing 
an answer given a natural language question 5 For example for a question like Who founded the Passage Retrieval in Log Files An Approach Based on Query Enrichment 361 American Red Cross QA systems based on surface patterns seek reformulations like the founder of the American Red Cross is X or X the founder of the American Red Cross The question reformulation using surface patterns is also exploited in TREC9 and TREC10 8 and 5 present different approaches to take semantic variations semantic reformulations into account in order to complement the syntactic variants To find relevant passages 6 evaluates each passage using a scoring function based on the coverage of question keywords which exist also in the passage QA systems also use query expansion metho ds to improve performance of retrieval These metho ds can use the thesaurus 4 or be based on the incorporation of the most frequent terms in the m relevant do cuments Despite the satisfactory results achieved by the use of surface patterns and syntactic 
variants in mentioned work these metho ds are irrelevant in the context of log files according to the problems described in Section 2 Indeed the main reasons for irrelevancy of such metho ds are related to the fact that an answer is not reformulated in different ways in a corpus of log files Also there is a lack of redundancy of answers in corpus of logs In addition there are several technical and alphanumeric keywords in the domain of log files for which the use of syntactic or semantic variants appears to be complex or unmeaning 4 Passage Retrieval Based on Query Enrichment We propose in this paper a passage retrieval approach based on a new interactive pro cess of query enrichment The enrichment of query is based on a context learning pro cess and is asso ciated with a novel and original term weighting function Our protocol of context learning is designed to determine the context of a given question by analyzing the terms 3 co o ccurring around the question keywords in the corpus The new term scoring 
function proposed in this paper identifies the terms which are related to the answers The architecture of our approach consists of three main modules 1 Enrichment of the initial query by context learning 2 Enrichment by terms which are likely related to answer 3 Passage retrieval using the enriched query The first mo dule enriches the initial query extracted from a question in natural language This mo dule aims at making the initial query relevant to all types of logs which are generated by different tools At this step by learning the context of question we enrich the initial query by the most significant terms of the context The second mo dule is activated for a second enrichment of the query in order to obtain a higher accuracy At this phase we aim at identifying the terms which are likely related to answer in order to integrate them in the query For 3 In this paper the word term refers to both words and multi word terms of the domain 362 H Saneifar et al this purpose we propose a process of scoring of 
terms based on a new weighting function The weighting function is designed to give a score to terms according to their relatedness to answers We devote Section 5 to this topic In the third mo dule we seek the relevant passages in the logs generated by a tool different from the one which has been used in the learning phase That is we have two different corpora of logs The first one called training corpus is used in learning phase and the second corpus called test corpus is the corpus in which we retrieve the passages relevant to given questions The logs of the test corpus have structures and a vocabulary significantly different from the logs of the training corpus In our approach we look for specialized context called hereafter lexical world of question keywords In order to characterize and present in a relevant way the specialized context of keywords we use the terminological knowledge extracted from logs Before explaining the query enrichment pro cesses we develop the concept of lexical world and the use of 
terminological knowledge in the following subsections 4 1 Lexical World The lexical world of a term is a small fragment of do cument in which the term is seen We consider this fragment specialized context of the term because terms located around a term within a small fragment generally have a strong semantic and or contextual relations Therefore by determining the lexical world of a term in a do cument we identify terms that tend to appear around it in that do cument We do not put any limit on the size of lexical worlds eg a few lines a few words etc as it have to be determined pragmatically based on the type of do cuments In order to present the lexical world of a term several solutions are possible As a first solution we characterize the lexical world of a term by a set of words called also bag of words which are located around the term and present Noun Verb or Adjective parts of speech As a second solution the lexical world of a term is presented by co o ccurring words like bigrams of words i e any two 
adjacent words or multi word terms few adjacent words forming a meaningful term which are seen around the term We detail this point in the next section 4 2 Terminological Knowledge As mentioned above the lexical worlds can be characterized in different ways By words multi word terms or bigrams of words According to our experiments the multi word terms and words are more representative than bigrams of words Hence we create two types of lexical world 1 Consisting of words and 2 Consisting of multi word terms and words In order to determine the multi word terms we extract the terminology of logs using the metho d presented in 13 This metho d adapted to the specific characteristics of logs extracts the multi word terms according to syntactic patterns in the log files To choose the relevant and domain specific terms we use the terminology validation and filtering proto col presented in 12 We have finally the valid and relevant multi word terms to characterize lexical worlds Passage Retrieval in Log Files An 
Approach Based on Query Enrichment 363 4 3 Query Enrichment by Context Learning We explain here the first module of our query enrichment approach For a given question we look initially to learn the context of the question and characterize it by its most significant terms These terms represent at best the context of the question Thus it is expected that the passages corresponding to the question share some of these terms regardless of different kind of log files Firstly we seek lexical worlds of question keywords The found lexical worlds and the initial query are vectorized We use an IR system based on Vector Space VS mo del in order to select the lexical world most correlated to the initial query Then we choose the most representative terms of selected lexical world For this purpose we select the n terms having the highest tf idf scores 11 We get the first enriched query by inserting the selected terms in the initial one Since the next phase of query enrichment based on the novel weighting function is the 
main contribution of this paper we develop it in Section 5 Thus we explain in the following subsection how we look for relevant passages once initial queries are enriched 4 4 Passage Retrieval in Log Files We detail here the pro cess of finding relevant passages on the test corpus of log files First we segment the logs of the test corpus Segmentation is performed according to the structure of the text like data blocks tables separating lines etc Each segment is seen as a passage of log files containing potentially the answer Second we enrich the initial query in order to make it relevant to all types of log files We remind that we aim at adapting the initial query to vocabulary of all types of log files by our query enrichment approach Third we build a system of IR in order to find the relevant passages The IR system uses an indexing function and a similarity measure In this phase we experiment our IR system by using tf idf and Binary representation as indexing functions The Cosine and Jaccard measures are 
used as a similarity measure The IR system gives a relevancy score to every passage segment according to the enriched queries Then we order the passages based on their relevancy score and propose the top ranked passages to the user Several approaches of passage retrieval return a considerable number of candidate passages Our experiments conducted on real data assert that in more than 80 of the cases the relevant passage is located among the three topranked passages That is our approach often ranks the relevant passage among the three top candidate passages returned by a system as possible results 5 How to Find Terms Correlated to Answers To improve the relevance of the query to different types of logs we propose a second module of enrichment of the query In this mo dule we have as input the query enriched in the phase of context learning and the test corpus of logs in 364 H Saneifar et al which we seek the relevant passages different from the training corpus which is used in the context learning phase Our 
motivation is to find the terms in the logs of the test corpus which are likely to exist in the relevant passage and are therefore related to the answer For this purpose we offer here a term selection pro cess based on a new term weighting scoring function Terms will be selected based on their score obtained by this scoring function This function gives a score to each term based on three assumptions 4 The learning process is performed on the logs of the training corpus which are generated by a tool using a significantly different vocabulary and structures from the tool generating the logs of the test corpus Passage Retrieval in Log Files An Approach Based on Query Enrichment 365 The final score of a term that we call T RQ Term Relatedness to Query is calculated using the following formula TRQ lwf 1 idf According to the experiments the most relevant value of is 0 255 This means that we give more weight to the frequency of terms in the corpus and a smaller weight but which influences the final results to lwf 
We explain with an example the pro cess of selection of terms which are likely related to the answer Supposing Q Wa Wb Wd as a query enriched by the first mo dule learning phase and logb as a log file containing seven segments S1 Wa Wk Wm Wb S2 Wd Wk S3 Wz S4 Wa Wc We Wq S5 Wb We S6 Wz S7 Wb Wc Wk We consider that the border of lexical worlds border of the selected fragment of text around a given term corresponds to the border of segments i e a lexical world is not bigger than the corresponding segment Among sept segments there are fives which are asso ciated with terms of Q Thus we obtain S1 S2 S4 S5 S7 as the set of lexical worlds of question keywords The following lists shows the lexical worlds associated to each keywords of the question6 Wa S1 S4 Wb S1 S5 S7 Wd S2 Here for instance the word Wa in the query Q is associated with two lexical worlds S1 and S4 The idf score of the word Wk for example is equal to log 5 3 0 22 because the word Wk exists in three lexical worlds S1 S2 and S7 among fives The value 
of lwf for the word Wk in the segment S1 is calculated as following lwf2 1 1 log 3 2 5 8 Indeed there are two words in the query Q i e Wa and Wb which are associated with the lexical world S1 the lexical world in which the word Wk is located We note that for a given term the value of lwf depends on the lexical world in which the given term is located For example the value of lwf for the word Wk located in segment S2 is equal to lwf2 2 1 log 3 1 2 1 as there is just one keyword of the query Q asso ciated to S2 This means that Wk located in the segment S2 is less significant less related to the query than when it is located in the segment S1 Once the T RQ score of all terms of lexical worlds are calculated we identify the k highest scores and select the terms having these scores However among the terms selected based on their T RQ scores there are terms having the same score To distinguish these terms we assess their tendency to appear close to the keywords of the initial query In other words we seek to 
calculate the dependence of selected terms to the keywords of question in the context For this purpose we choose the Dice measure This statistical measure has a goo d performance 5 6 We justify the selected value of by presenting some results in http www lirmm fr saneifar experiments TRQ pdf Since a word can be used in different contexts i e different fragments of document it can be associated with several lexical worlds 366 H Saneifar et al for tasks of text mining 10 For a term T and a query keyword W we calculate the Dice measure as following Dice T W 2 T W T W For us T W number of times T and W o ccur together corresponds to the number of times where T and W are located in the same line in the corpus of logs x shows the total number of o ccurrences of x in the corpus Finally the value of Dice measure allows to distinguish the terms obtained in the previous step which have equal T RQ score The final score of these terms is obtained by the sum of Dice value and the T RQ score Note that we select at first 
the terms according to their T RQ score i e terms having highest T RQ score are selected If we have the terms having the same T RQ score we distinguish them by calculating their Dice values Finally as described above the system ranks the terms based on their T RQ scores considering Dice values for terms having the same score The system recommends to the user the k top ranked terms in order to enrich the query Our system is also able to integrate automatically the k top ranked terms into the query in an autonomous mode The enriched query EQmod2 will be used in passage retrieval 6 Experiments We test the performance of our approach on a corpus of log files from the real industrial world data from the Satin IP company There are 26 questions which are expressed in natural language and are extracted from standard check list Log files are segmented according to their structures blank lines tables data blocks etc Each segment is potentially a relevant passage Note that for a given question there is only one 
relevant passage segment in the corpus i e there is just one occurrence of the answer in the corpus The relevance of passages is evaluated as whether the final answer is situated in the passage The test corpus that we use for the experiments contains 625 segments and is about 950 KB Each segment consists of approximately 150 words The training corpus used in the phase of learning the context of questions consists of logs reporting the same information as logs of the test corpus but generated by a totally different nb Question i 1 1 rank answer Passage Retrieval in Log Files An Approach Based on Query Enrichment 367 Table 1 Percentage of question P n for which the relevant passage is ranked as n a performance obtained by using the not enriched queries initial queries b performance obtained by using the enriched queries a tf idf Cos Jac P 1 9 8 P 2 5 4 P 3 4 3 M RR 0 51 0 50 Binary Cos Jac 11 8 3 3 2 2 0 58 0 48 b tf idf Cos Jac P 1 20 18 P 2 3 1 P 3 1 3 M RR 0 83 0 78 Binary Cos Jac 19 13 0 4 2 1 0 79 0 65 In 
the experiments we measure the performance of passage retrieval using the enriched queries We aim at comparing the performance of passage retrieval using the enriched queries with the performance of passage retrieval using the initial queries not enriched That shows how our query enrichment approach improves the relevance of initial queries Tables 1a 1b present respectively the results of the performance of passage retrieval using the not enriched query and the enriched ones P n is the percentage of questions for which the relevant passage is ranked as n among the retrieved candidate passages as possibilities Here we show the results for the three first ranks As shown in Tab 1a by using the not enriched queries we obtain a M RR value equal to 0 58 in best conditions While by enriching the initial queries as mentioned in this paper the M RR improves significantly and reaches 0 83 in best conditions According to the results in the best configuration of the IR mo dule and by using the enrichment of queries the 
relevant passage is ranked in 76 of cases as the first passage among the candidate passages returned by the system Also in 92 of cases the relevant passage is located ranked among the three top ranked passages returned by the system when there are about 650 passages in the corpus 7 Conclusions We have presented a pro cess of double enrichment of initial queries in order to improve the performance of passage retrieval in log files The heterogeneity of the vocabulary and structures of log files and the fact that the keywords used in the questions expressed in natural language do not exist necessarily in the logs make the passage retrieval difficult Despite these characteristics our approach makes it possible to adapt an initial query i e list of question keywords to all types of corresponding log files whatever is their vocabulary According to the results by our query enrichment protocol which is based on our novel weighting function called T RQ Term Relatedness to Query we obtained a value of M RR equal to 0 
83 while the value of M RR was equal to 0 58 by using the not enriched queries We plan to evaluate our system with other mo dels of Information Retrieval Improving the new term weighting function used in the second phase of query enrichment represents a ma jor point in the future work 368 H Saneifar et al References 1 Brill E Lin J Banko M Dumais S Ng A Data intensive question answering In Proceedings of the Tenth Text REtrieval Conference TREC pp Part of Speech Tagging Using Parallel Weighted Finite State Transducers Miikka Silfverberg and Krister Department of Modern Languages University of Helsinki Helsinki Finland miikka silfverberg krister linden helsinki fi Abstract We use parallel weighted finite state transducers to implement a part of speech tagger which obtains state of the art accuracy when used to tag the Europarl corpora for Finnish Swedish and English Our system consists of a weighted lexicon and a guesser combined with a bigram model factored into two weighted transducers We use both lemmas 
and tag sequences in the bigram model which guarantees reliable bigram estimates Keywords Weighted Finite State Transducer Part of Speech Tagging Markov Model Europarl 1 Introduction Part of Speech POS taggers play a crucial role in many language applications such as parsers speech synthesizers information retrieval systems and translation systems Systems which need to process a lot of data benefit from fast taggers Generally it is easier to find faster implementations for simple mo dels than for complex ones so simple models should be preferred when tagging speed is crucial We demonstrate that a straightforward first order Markov mo del is sufficient to obtain state of the art accuracy when tagging English Finnish and Swedish Europarl corpora Koehn 2005 The corpora were tagged using the Connexor fdg parsers H Loftsson E 370 M Silfverberg and K and the other one assigns weights for bigrams starting at o dd positions Both bigram mo dels are implemented as WFSTs The sentence WFST and bigram mo del WFSTs are 
combined using weighted intersecting composition Silfverberg and 2 Previous Research Statistical POS tagging is a common task in natural language applications POS taggers can be implemented using a variety of statistical mo dels including Hidden Markov Models HMM Church 1999 Brants 2000 and Conditional Random Fields Lafferty et al 2001 Markov mo dels are probably the most widely used technique for POS tagging Some older systems such as Cutting 1992 used first order mo dels but the accuracies reported were not very goo d E g Cutting 1992 report an accuracy of 96 for tagging English text Newer systems like Brants 2000 have used second order mo dels which generally lead to better tagging accuracy Brants 2000 reports accuracy of 96 46 for tagging the Penn Tree Bank More recent second order mo dels further improve on accuracy Collins 2002 reports 97 11 accuracy and Shen et al 2007 97 33 accuracy on the Penn Tree Bank We use lemmas in our bigram model as did Thede and Harper 1999 who used lexical probabilities in 
their second order HMM for tagging English and obtained improved accuracy 96 Part of Speech Tagging Using Parallel Weighted Finite State Transducers 371 3 Formulation of the POS Tagging Task In this section we formulate the task of Part of Speech POS tagging and describe probabilistic POS taggers formally By a sentence we mean a sequence of syntactic tokens s s1 sn and by a POS analysis of the sentence s we mean a sequence of POS analyzes t t1 tn We include lemmas in POS analyzes For each i the analysis ti corresponds to the token si in sentence s We denote the set of all sentences by S and the set of all analyzes by T A POS tagger is a machine which associates each sentence s with its most likely POS analysis ts To find the most likely POS analyzes for the sentence s the model estimates the probabilities for all possible analyzes of s using a distribution P For the sentence s and every possible POS analysis t the distribution P associates a probability P t s Keeping t fixed the mapping s P t s is a 
normalized probability distribution The most likely analysis ts of the sentence s is the analysis which maximizes the probability P t s i e ts arg max P t s t The distribution P can consist of a number of component distributions Pi each giving probability Pi s t for sentence s and analysis t The component probabilities are combined using some function F 0 1 n 0 1 to obtain P s t F Pi t s Pn t s The function F should be chosen in such a way that P is nonnegative and satisfies P t s 1 tT for each sentence s Often a convex linear function F is used to combine estimates given by the component models In such a case the model P is called a linear interpolation of the mo dels Pi 4 A Probabilistic First Order Model In this section we describe the idea behind our POS tagger We use a bigram mo del for POS tagging Thus the probability of a given tagging of a sentence is estimated using analyzes of word pairs Since we make use of extensive training material we may include lemmas in bigrams Although the training material 
is extensive the tagger will still encounter bigrams which did not o ccur in the training material or only occurred once or twice In such cases we want to use unigram probabilities for estimating the best POS analysis Hence we weight all analyzes using probabilities given by both the unigram and bigram models but weight bigram probabilities heavily while only giving unigram probabilities a small weight Hence unigram probabilities become significant only when bigram probabilities are very close to each other 372 M Silfverberg and K 4 1 The Unigram Model The unigram model emits plain unigram probabilities pu t sx for analyzes t given a word form sx we use the index x to signify that pu t sx is independent of the context of the word form sx Unigram probabilities are readily computed from training material The probability of the analysis t t1 tn given the sentence s s1 sn assigned by the unigram mo del is n Pu t s i 1 pu ti si In practice it is not possible to train the unigram model for all possible word forms 
in highly inflecting languages with pro ductive compounding mechanism such as Finnish or Turkish Instead the probabilities for analyzes given a word form need to be estimated using probabilities for words with similar suffixes For instance if the word form foresaw was not observed during training we can give it a similar distribution of analyzes as the word saw receives since saw shares a three letter suffix with foresaw In practice such estimation relying on analogy is accomplished by a so called POS guesser which seeks words with maximally long suffixes in common with an unknown word It then assigns probabilities for POS analyzes of the unknown word on basis of the analyzes of the known words Linden 2009a shows how a guesser can be integrated with a weighted lexicon in a consistent way 4 2 The Bigram Models We use two bigram mo dels Qo and Qe giving probabilities for bigrams starting at even and o dd positions in the sentence The estimates are built using plain bigram probabilities for tagging a word pair 
s1 and s2 with analyzes t1 and t2 respectively1 These probabilities pb t1 s1 t2 s2 are easily computed from a training corpus For an analysis t t1 t2k and a sentence s s1 s2k of even length 2k the models Qo and Qe give bigram scores k k 1 Qo t s i 1 pb t2i 1 s2i 1 t2i s2i Qe t s i 1 pb t2i s2i t2i 1 s2i 1 For an analysis t t1 t2k 1 and a sentence s s1 s2k 1 of o dd length 2k 1 the mo dels Qo and Qe give bigram scores k k Qo t s i 1 1 pb t2i 1 s2i 1 t2i s2i Qe t s i 1 pb t2i s2i t2i 1 s2i 1 In literature it is often suggested that one should instead compute probabilities of word form bigrams given POS analysis bigrams We cannot do this since we include lemmas in POS analyzes This makes the probability of a word form given a POS analysis either 0 or 1 since most analyzes only have one realization as a word form Part of Speech Tagging Using Parallel Weighted Finite State Transducers 373 4 3 Combining the Unigram and Bigram Models The standard way of forming a mo del from Pu Qo and Qe would be to use linear 
interpolation We do not want to do this since we aim to convert probabilities into penalty weights in the tropical semiring using the mapping p log p which is not compatible with sums Instead we take a weighted pro duct of powers of the component probabilities Hence we get a mo del P t s Pu t s wu Qo t s wo Qe t s we where wu we and wo are parameters which need to be estimated If each of the mo dels Pu Qe and Qo agree on the probability p of an analysis t given a sentence s we want P to give the same probability This is accomplished exactly when wu we wo 1 There does not seem to be any reason to prefer either of the mo dels Qe or Qo which makes it plausible to assume that we wo Hence an implementation of the model only requires estimating two non negative parameters the unigram parameter wu and the bigram parameter wb They should satisfy wu 2wb 1 It is possible that P t s will not be a normalized distribution when s is kept fixed but it can easily be normalized by scaling linearly with factor t P t s For the 
present implementation it is not crucial that P is normalized 5 Implementing the Statistical Model Using Weighted Finite State Transducers We describe the implementation of the POS tagger mo del using weighted finitestate transducers WFSTs We implement each of the components of the statistical mo del as a WFST which are trained using corpus data In order to speed up computations and prevent roundoff errors we convert probabilities p given by the mo dels into penalty weights in the tropical semiring using the transformation p log p In the tropical semiring the pro duct of probabilities pq translates to the sum of corresponding penalty weights log p log q The k th power of the probability p namely pk translates to a scaling of its weight k log p These observations follow from familiar algebraic rules for logarithms In our system tagging of sentences is performed in three stages using four different WFSTs The first two WFSTs a weighted lexicon and a guesser for unknown words implement a unigram mo del They pro 
duce weighted suggestions for analyzes of individual word forms The latter two WFSTs re score the suggestions using bigram probabilities The weights log p given by the unigram mo del and the bigram model are scaled by multiplying with a constant in order to prefer analyzes which are strong bigrams The scaled weights k log p are then added to give the total scoring of the input sentence This corresponds to multiplying the powers pk of the corresponding probabilities In the first stage we use a weighted lexicon which gives the five best analyzes for each known word form In initial tests the correct tagging for a known word 374 M Silfverberg and K could be found among the five best analyzes in over 99 of tagged word forms so we get sufficient coverage while reducing computational complexity For an unknown word x we use a guesser which estimates the probability of analyzes using the probabilities for analyzes of known words We find the set of known word forms W whose words share the longest possible suffix with 
the word form x We then determine the five best analyzes for the unknown word form x by finding the five best analyzes for words in the set W For each word si in a sentence s s1 sn we form a WFST Wi which is a disjunction of its five best analyzes t1 t5 according to the weights w si ti given by the unigram mo del In case there are less than five analyzes for a word we take as many as there are We then compute a weighted concatenation Ws of the individual WFSTs Wi The transducer Ws is the disjunction of all POS analyzes of the sentence s where each word receives one of its best five analyzes given by the unigram model To re score the analysis suggestions given by the lexicon and the guesser we use two WFSTs whose combined effect gives the bigram weighting for the sentence One of the mo del scores bigrams starting at even positions in the sentence and the other one scores bigrams starting at o dd positions Thus we give a score for all bigrams in the sentence without having to compute a WFST equivalent to the 
intersection of the models which might be quite large Using weighted intersecting composition Silfverberg and Using a tagged corpus we form a weighted lexicon L which re writes word forms to their lemmas and analyzes POS analyzes for a word form si are weighted according to their frequencies which are transformed into tropical weights In order to estimate the weights for words which were not seen in the training corpus we construct a guesser For an unknown word the guesser will try to construct a series of analyzes relying on information about the analyzes of known similar words Figure 1 shows an example guesser which can be constructed from a reversed weighted lexicon Guessing begins at the end of the word We allow guessing at a particular analysis for a word only if the word has a suffix agreeing with the analysis See Linden 2009a for more information on guessers 5 2 The Bigram Models To re score analyzes given by the unigram model we use two WFSTs whose combination serves as a bigram model The first one 
Be scores each known word form analysis bigram s2k s2k 1 and t1 t2 in the sentence starting at an Part of Speech Tagging Using Parallel Weighted Finite State Transducers 375 50 0 0 o o 0 g 0 0 0234 1 0 gs mon n 0 fni v 3 77 2 0 tab 3 0 g 4 5 d d 0 0 6 0 0 7 50 Fig 1 Guesser constructed from a weighted lexicon Guessing starts at the end of a word Skipping letters gives a high penalty and analyzes where equally many letters are skipped are weighted according to the frequency of the analyzes even position 2k according to the maximum likelihoo d estimate of the tag bigram t1 t2 w r t the word form bigram s2k s2k 1 The WFST Bo is similar to Be except it weights bigrams starting at odd positions s2k 1 s2k Given a word form pair s1 s2 we compute the probability P t1 s1 t2 s2 for each POS analysis pair t1 t2 These sum to 1 when w1 and w2 remain fixed Then we form a transducer B whose paths transform word form pairs s1 s2 into analysis pairs t1 t2 with weight log P t1 s1 t2 s2 Lastly we disjunct B with a default 
bigram which transforms arbitrary word form sequences to arbitrary analyzes with a penalty weight which is greater than the penalty received by all other transformations In addition to the mo del B we also compute a general word mo del W which transforms an arbitrary sequence of symbols into an arbitrary lemma and an analysis The word mo del W is used to skip words at the beginning and end of sentences From the transducers above we form the mo dels Be and Bo using weighted finite state operations Be W B W 0 1 and Bo B W 0 1 Here W 0 1 signifies an optional instance of W and tab cc tab 1 100 2 will tab v auxmod 3 6 8 tab 9 4 7 dog tab n nom sg 5 50 dog tab v inf 0 Fig 2 A small example of an even bigram model Be signifies an arbitrary symbol and signifies an arbitrary POS analysis symbol 376 M Silfverberg and K 5 3 Parsing Using Weighted Intersecting Composition In our system parsing a sentence S is in principle equivalent to finding the best path of the transducer S L Be Bo Since the intersection of Bo and 
Be could become prohibitively large we instead use intersecting composition Silfverberg and 6 Data In this section we describe the data used for testing and training the POS tagger For testing and training we used the Europarl parallel corpus Koehn 2005 The Europarl parallel corpus is a collection of pro ceedings of the European Parliament in eleven European languages The corpus has markup to identify speaker and some html markup which we removed to pro duce a file in raw text format We used the Finnish English and Swedish corpora Since the training and testing materials are the same for all three languages the results we obtain for the different languages are comparable We parsed the Europarl corpora using Connexor functional dependency parsers fi fdg for Finnish sv fdg for Swedish and en fdg for English Table 1 Some figures describing the test and training material for the POS tagger Language English Finnish Swedish Syntactic tokens 43 million 25 million 38 million Sentences 1 million 1 million 1 million 
POS tag sequences 122 2194 243 Table 1 describes the data used in training and testing the POS tagger We see that the fi fdg parser for Finnish emitted more than ten times as many tag sequences as sv fdg for Swedish or en fdg for English The en fdg parse emitted clearly fewest tag sequences Part of Speech Tagging Using Parallel Weighted Finite State Transducers 377 7 Training the Model We now describe training the model which consists of two phases In the first phase we build the weighted lexicon and guesser and the bigram mo dels In the second phase we estimate experimentally coefficients wu and wb which maximize the accuracy of the interpolated model P t s Pu t s wu Qo t s wb Qe t s wb Using a small material covering 1000 syntactic tokens we estimated wu 0 1 and wb 0 45 This shows that it is beneficial to weight the bigram mo del heavily which seems natural since bigrams provide more information than unigrams 100 1 GRAM 2 GRAM 95 90 ACCURACY 85 80 75 70 2 2 5 3 3 5 4 4 5 SIZE OF TRAINING CORPUS 10 n 5 5 5 
6 Fig 3 The accuracy for the English POS tagger as a function of the size of training data We used between 102 and 106 sentences for training The lower curve displays the accuracy using only the unigram model whilst the upper curve displays the accuracy of the combined unigram and bigram model Figure 3 shows learning curves for the English language POS tagger using 102 to 106 sentences for training The lower curve displays accuracies for the unigram mo del and the upper curve shows the accuracy for the combined unigram and bigram mo del For the unigram model we can see that little improvement is obtained by increasing the training data from 104 sentences In contrast there is significant improvement 0 82 for the bigram mo del even when we move from 105 to 106 sentences 378 M Silfverberg and K 8 Evaluation We describe the metho ds we used to evaluate the POS tagger and the results we got We used ten fold cross validation to evaluate the POS tagger that is we split the training material in ten equally sized 
parts and used nine parts for training the model and the remaining part for testing Varying the tenth used for testing we trained ten POS taggers for each language For each of the languages we trained two sets of taggers One set used only unigram probabilities for assigning POS tags The other used both unigram and bigram probabilities We may consider the unigram taggers as a baseline For each tree languages we computed the average and standard deviation of the accuracy of the unigram and bigram taggers In addition we computed the Wilcoxon matched pairs signed ranks test for the bigram and unigram accuracies in all three languages The test does not assume that the data is normally distributed unlike the paired t test The results of our tests can be seen in table 2 Table 2 Average accuracies and standard deviations for POS taggers in Finnish English and Swedish The sixth column shows the improvement which results for adding the bigram model In the seventh column we show the results of the Wilcoxon matched 
pairs signed ranks test between unigram and bigram accuracies Language English Finnish Swedish Unigram Acc 93 10 94 38 94 12 0 09 0 07 0 20 Bigram Acc 98 29 96 63 97 31 0 01 0 03 0 11 Diff 5 19 2 25 3 19 Conf 99 8 99 8 99 6 9 Discussion and Future Work It is interesting to see that a bigram tagger can perform equally well or better than trigram taggers at least on certain text genres The mean accuracy 98 29 we obtained for tagging the English Europarl corpus is exceptionally high for example Shen et al 2007 report a 97 33 accuracy on tagging the Penn Tree Bank The improvement of 5 19 percentage points from the unigram model to the combined unigram and bigram model is also impressive There is also a clear improvement for Finnish and Swedish when the bigram mo del is used in tagging and accuracy for these languages is also high We had problems finding accuracies figures for statistical taggers of Finnish but for Swedish Megyesi 2001 reports accuracies between 94 and 96 which means that we get state of the art 
accuracy for Swedish Of course the Europarl corpus is probably more homogeneous than the Penn Tree Bank or the Brown Corpus both of which include texts from a variety of genres Furthermore tagging is easier because the en fdg parser only emits 122 different POS analyzes Still Europarl texts represent an important genre Part of Speech Tagging Using Parallel Weighted Finite State Transducers 379 because the EU is constantly producing written materials which need to be translated into all official languages of the union The accuracy for Finnish shows less improvement than English and Swedish We believe this is a result of the fact that Finnish words carry a lot of information but the bonds between words in sentences may be quite weak This conclusion is supported by the fact that unigram accuracy for Finnish is best of all three languages We do not believe that using trigram statistics would bring much improvement for Finnish Instead we would like to write a set of linguistic rules which would cover most 
typically occurring tagging errors Especially we would like to try out constraints which would mark certain analyzes as illegal in some contexts Such negative information is hard to learn using statistical metho ds Still it may be very useful so it could be provided by hand crafted rules Clearly our figures for accuracy need to be considered in relation to the tagging accuracy of the fdg parsers We did not succeed in finding a study on the POS tagging accuracy of the fdg parsers Instead we examined the POS tagging for one word per twenty thousand in the first tenth of the Europarl corpora for Finnish English and Swedish This amounted to 131 examined words for Finnish 219 examined words for English and 191 examined words for Swedish According to these tests the POS tagging accuracy of the fdg parsers for Finnish is 95 4 for English it is 97 3 and for Swedish it is 97 5 10 Conclusions We intro duced a model for a statistical POS tagger using bigram statistics with lemmas included We showed how the tagger can 
be implemented using WFSTs We also demonstrated a new way to factor a first order mo del into a mo del tagging bigrams at even positions in the sentence and another mo del tagging bigrams at o dd positions In order to test our mo del we implemented POS taggers for Finnish English and Swedish training them and evaluating them using Europarl corpora in the respective languages and Connexor fdg parsers We obtained a clear statistically significant improvement for all three languages when compared to the baseline unigram tagger At least for English and Swedish we obtain state of the art accuracy Acknowledgements We thank the anonymous referees We also want thank our colleagues in the Hfst team The first author is funded by Langnet Graduate School for Language Studies References Brants 2000 Brants T 380 M Silfverberg and K Collins 2002 Collins M Discriminative Training Methods for Hidden Markov Models Theory and Experiments with Perceptron Algorithms In EMNLP 2002 Cutting 1992 Cutting D Kupiec J Pedersen J Sibun 
P A Practical Part ofSpeech Tagger In Proceedings of the Third Conference on Applied Natural Language Processing 1992 Automated Email Answering by Text Pattern Matching Eriks Sneiders Department of Computer and Systems Sciences Stockholm University Forum 100 SE 16440 Kista Sweden eriks dsv su se Abstract Answering email by standard answers is a common practice at contact centers Our research assists this process by creating reply messages that contain one or several standard answers Our standard answers are linked to representative text patterns that match incoming messages The system works in three languages The performance was evaluated on two email sets the main advantage of our email answering technique is good correctness of the delivered replies Keywords Automated email answering automatic email response text message answering question answering text patterns 1 Introduction It is not unusual that an email flow to a contact center aka customer care center customer service contains frequently reoccurring 
inquiries therefore agents who communicate with customers use predefined response templates as draft answers Finding a predefined answer if it exists is a task that a computer can do Answering a generic question e g Can I pay with my Visa card would be easy More specific requests e g Please update my address in your customer database are less trivial Fortunately many companies have a web based self service system where a customer logs in and interacts with the system without mediation of a contact center agent Hence an automated answer can advise using the self service system where appropriate Katakis et al 1 present a good introduction to email management techniques and their application domains Most research has been done assuming personal use of email Automated message answering at contact centers has raised less interest Most email answering systems pursue the text classification approach A typical system perceives a message as a bag of words represented by a term vector with tf idf weights Normally the 
words are stemmed and stop words removed Further a typical system is trained on sample documents in predefined classes The most popular learning algorithms are Support Vector Machine and H Loftsson E 382 E Sneiders Weng and Liu 6 assign a set of representative concepts with weighted terms to each class of messages When a new message arrives its weight is calculated with respect to each concept set considering the terms in the message and their weights Very few email answering systems do text generation Marom and Zukerman 7 create new response texts by selecting the most representative sentences from previous responses to messages similar to the new incoming message Kosseim at al 8 follow the tradition of Information Extraction and operate a number of templates for capturing intention concepts named entities and relations When a new message arrives the system fills the templates performs semantic validation and generates the answer This paper introduces an email answering approach that has been used for 
sending replies without any human intervention as well as generating draft replies for contact center agents The system operates a database of standard answers and text patterns linked to these answers that record expected wordings in messages to be answered 2 Pattern Based Email Answering This research stems from an earlier work in automated FAQ answering in a restricted domain the system could answer about 70 of the queries 9 out of 10 answers correct 9 A natural step forward is to adapt the technique to answer larger pieces of text such as email messages 2 1 Question Templates The email answering system has a few standard answers that respond to the most frequent inquiries Each standard answer has a title that summarizes it in one sentence which helps to avoid confusion while reading the automatic reply if the answer does not quite correspond to the original message Furthermore each standard answer is linked to a number of question templates that record the expected text patterns of the future inquiries 
to be answered by this standard answer Fig 1 The syntax of our text patterns resembles that of regular expressions the text patterns are less rigorous though Each question template contains two patterns The required pattern matches a piece of message text if the message fits this standard answer The forbidden pattern must not match the message it detects details that disqualify the answer Please observe that the question templates are created before the actual email answering starts answ answ 2 answ 1 Required pattern Forbidden pattern Fig 1 Standard answers with their question templates Automated Email Answering by Text Pattern Matching 383 2 2 Steps of the Answering Process During the email answering process a new incoming message is split into paragraphs each paragraph into sentences each sentence into An empty synonym set matches everything therefore bright light matches bright and light with 0 3 any lexical items in between Phrases match compound words For example new paper matches new paper and 
newspapers Because compound words are popular in some languages such as Swedish we have special syntax for them e g news paper is equal to news paper 384 E Sneiders We define reoccurring pieces of text patterns as substitutes and reuse them For example we define ford volvo as car_name and use it in car_name car vehicle repair In order to minimize words stem ambiguity the stems are required to have at least five letters before the asterisk In the syntax for compound words a component must be at least four letters long or at least two if there is another component at least five letters long This has proved a sufficient trade off between the ambiguity of word stems and their ability to represent concepts 2 4 Test Beds Before we discuss real life examples of the text patterns let us introduce our test beds The email answering system was implemented at two contact centers The first one was an insurance company that employed fully automated email answering with 11 standard answers in Swedish The system scanned 
through all incoming messages If it could answer the message it sent a reply to the from and cc addresses of the original message The reply informed its reader that it was computer created and contained simple instructions how to reach a human agent if necessary Messages that could not be answered were passed to a human agent The second contact center worked for a telecom service provider It used 4 standard answers in Latvian Here email answering was not fully automated the system created draft reply messages for the agents of the contact center 2 5 Flexibility of a Text Pattern The following example illustrates what a text pattern may look like 2_4_hjul vad kosta A text pattern is more than a loose regular expression it embodies a question specific synonym dictionary and a quasi ontology Furthermore a combination of words and matching rules is delicate even small changes may influence the accuracy of detecting relevant texts Therefore at the current stage of our research management of the text patterns 
requires a manual effort In order to get an impression about the potential size of a text pattern let us see how many terms in a piece of query text i e a paragraph one pattern can match taking the insurance case see Section 2 4 as an example The 11 standard answers were linked to 161 required text patterns In average a text pattern matched 5 terms in a paragraph if the system delivered an answer Potentially the text patterns could match between 3 and 40 terms in a paragraph Fig 2 shows the largest top curve and smallest bottom curve number of terms in a query paragraph that a text pattern could possibly match for each of the 161 patterns lined up on the horizontal axis On the very left side there are text patterns that can match just over 40 query terms In the middle of the line up the text patterns can match up to 15 20 query terms The smallest text patterns on the right side can match just over 5 Automated Email Answering by Text Pattern Matching 45 40 35 30 25 20 15 10 5 0 161 required text patterns 385 
terms in a query paragraph The bottom curve oscillates like an electrocardiogram with average 4 8 terms in a query paragraph that a text pattern must match as a minimum Why can one text pattern match a variable number of query terms A synonym set may contain phrases of different length The largest smallest number of matching terms can be reached if the system always selects the longest shortest synonym phrase or a basic lexical item instead of the shortest phrase in each synonym set Fig 2 suggests that a text pattern may contain a large number of parallel wordings that capture a variety of paraphrases of a meaningful statement Furthermore text patterns are built to match future messages and we are uncertain what exactly these messages will look like We are likely to compensate this uncertainty with some redundancy in the text patterns caused by guessing the future wordings Hereby the text patterns are more complex but also more expressive than just a set of keywords and a synonym dictionary 3 Spelling 
Correction Email texts are often untidy Tang et al 10 inspected more than five thousand webbased newsgroup messages in English and discovered that 73 2 of them needed paragraph normalization 85 4 needed sentence normalization 47 1 needed upperlower case restoration and 7 4 of the messages contained misspellings Dalianis 11 inspected spelling in another context of Internet based communication and found out that about 10 of the search queries submitted to a search engine of the Swedish tax authority were misspelled We did not count misspelled messages processed by our system yet we know that without spelling alterations our text pattern matching would fail to identify many pertinent messages because the matching rules embodied in these patterns are strict and even one letter wrong would result in a no match Furthermore our system faces three challenges First of all spelling alternatives are sought among words word stems and phrases from the text patterns rather than in off the shelf tools Number of matching 
query terms Fig 2 Number of query terms that can match a required text pattern 386 E Sneiders The second challenge is multiple languages The system works with texts in English Swedish Germanic Indo European languages and Latvian Baltic IndoEuropean language The third challenge is substitution of language specific character sets with the ISO8859 1 or ASCII character sets present on virtually all computers Of the three languages that our system works with Latvian emails are most exposed to character set substitution The Latvian alphabet has 33 letters of which 11 are not included in the ISO 8859 1 character set and may get replaced with English letters For example becomes a or aa becomes k or kj or kk etc There are no rules everything goes as long as people grasp the text In this case we deal with a deliberately altered syntax a pidgin language rather than misspellings The cure however is the If a standalone word stem not a part of a phrase is considered as a spelling alternative the corrected word must be no 
more than three letters longer than the stem in order to hinder replacement of a compound word with the stem of its first component Automated Email Answering by Text Pattern Matching 387 Phrases as spelling alternatives Phrases from the text patterns simple ones without other embedded phrases are the main spelling alternatives for misspelled compound words The system tests a phrase as a spelling alternative of a query word according to the following principle Let us consider an example and test 4 Performance Measurements Before the email answering system can start operating it is trained to recognize email texts that fit a given standard answer We say trained in quotes because this is not training as understood in machine learning We use some text filtering clustering and aggregation tools in order to group messages Then the messages are analyzed and text patterns created Today the text patterns are crafted manually increased automation of this process is further research and lies outside the scope of this 
paper Training messages Section 2 4 introduced our test 388 E Sneiders In the insurance case we had access to 5148 messages before the performance test many of these messages had been analyzed in order to manually create and adjust the text patterns In the telecom case we had access to two sets of messages Initially we had 4782 messages of which 706 messages corresponded to the 4 standard answers that the system included in its automated replies During the operation of the system but before the performance test we acquired 3768 more messages that were analyzed in order to manually adjust the text patterns Because the systems were running in production settings the training phase was the entire period the systems had been in operation While doing the training we paid more attention to correctness of the replies rather than recall We rather process a message manually than increase the risk of an incorrect reply Test messages The data for the performance measurements came from the systems logs We had no prior 
access to these messages we could not have used them in order to train the systems In the insurance case 3526 consecutive messages were analyzed In the telecom case 1314 consecutive messages were analyzed The correspondence between query messages and their automated replies was judged by humans third party observers 4 1 Precision Recall and Correctness We applied two evaluation criteria The first criterion was precision and recall calculated for each query message separately The second criterion was correctness of the replies actually given We distinguished between Table 1 shows precision and recall calculated for each query message separately Let us explain the insurance case From the 3526 inspected messages 395 messages got recall 1 20 messages got recall 0 5 179 messages got recall 0 In total 594 messages 395 20 179 had relevant answers in the system s database and some recall value The average Automated Email Answering by Text Pattern Matching Table 1 Precision and recall of the automatic replies 
Precision value Insurance case 1 0 67 0 5 0 33 Average 0 987 for 415 queries Telecom case 1 0 5 Average 0 98 for 124 queries 119 5 1 0 5 0 Total 124 Average 0 758 for 161 query 120 4 37 Total 161 404 1 9 1 Total 415 Average 0 682 for 594 queries Total 594 1 0 5 0 395 20 179 Num queries with the precision value 389 Recall value Num queries with the recall value Table 2 Correctness of the automatic replies Fully correct Insurance case 371 76 65 Telecom case 111 76 03 6 4 11 9 6 16 4 2 74 16 10 96 146 100 28 5 79 24 4 96 20 4 13 41 8 47 484 100 Technically correct Partial recall 1 Partial recall 1 Incorrect Total A sharp eyed reader has probably noticed that numbers in Table 1 and Table 2 do not match Let us take the insurance case in order to explain these numbers 371 fully answered plus 24 20 partially answered queries make 415 total precision queries Incorrectly answered queries have either zero recall or do not have any recall and precision values at all Technically correct replies were judged as incorrect 
when precision and recall where calculated because these queries should not have been answered Therefore technically correct replies did not get any recall and precision values Both implementations of the system show surprisingly similar correctness and average precision percentages despite different languages and knowledge domains Correctness of the replies is good From all the answered queries 390 E Sneiders around 76 were answered fully around 85 were answered fully or partially around 90 got the issues identified fully or partially fully correct technically correct partial replies 4 2 Performance Figures in Context The performance figures are easier to grasp if observed in the context of related systems mentioned in the introduction whose performance measurement methods are similar to those of ours Message classification Busemann et al 2 managed to choose the right category in 56 23 cases using Support Vector Machine Weng and Liu 6 reached top performance i e the highest F value at 62 77 recall and 77 52 
precision by finding representative terms in query messages and weighing these terms with respect to message classes Mapping a message to a standard answer Malik et al 5 made a human equivalent selection of answer templates in 61 cases human equivalent or incomplete selection in 73 4 by first extracting a set of representative questions for each standard answer training and then mapping these questions to questions in query messages test calculating taxonomy distance between words Information extraction and answer text generation Kosseim et al 8 delivered 66 4 correct answers by applying Information Extraction templates and performing semantic validation of the extracted information These figures give us an intuitive insight into viable quality of email answering In order to make any formal claims which system performs better we have to test the systems using the same input data and applying the same conditions 5 Advantages and Limitations Following are the advantages of our text pattern based automated 
email answering The system demonstrates a good correctness of the replies and a superior ability to identify questions and problem statements relevant to standard answers if the message is answered The text patterns operate in isolated narrow single answer knowledge domains where they are self sufficient Therefore the system can start operation with rendering one standard answer and the number of standard answers can gradually rise to as many as needed Because each text pattern is autonomous the system can easily handle several questions in one query message and put several standard answers into one reply message Because the text patterns are self sufficient the system has a low technological threshold or barrier that impedes its deployment i e the system operates without components such as part of speech taggers stemmers generic or domain ontologies We do need some tool support to analyze training messages however as stated in further research Automated Email Answering by Text Pattern Matching 391 Because 
of the low technological threshold our approach is affordable for rare domains small businesses and small languages where open source linguistic and knowledge representation tools as well as technologically advanced manpower are not readily available We have tested the system for English Swedish and Latvian and consider it portable to most European languages Our approach is most advantageous in settings where correctness of the replies is crucial where we want to maximize the end users experience where a list of ten candidate answers is not an option For example in fully automated email answering without any human mediation Especially advantageous our approach is for email flows with a high ratio of reoccurring inquiries such as the email flow in 4 where 9 standard answers cover 72 of all messages Still our automated email answering approach has at least two limitations First it is designed for narrow and stable domains only It should not be considered for text classification tasks in arbitrary text 
collections Second at the current stage of our research development of the text patterns is manual which lessens the practical value of our technique until at least partial automation of this process is achieved 6 Conclusions and Further Research Our automated email answering system maps incoming messages to standard answers by matching text patterns linked to the standard answers The technique was designed for narrow and stable domains such as an email flow at a contact center The main advantage of our technique is good correctness of the delivered replies Performance evaluation on two email collections in two languages showed that about 85 of the messages could be answered fully or partially about 90 had their questions and problem statements correctly identified We consider the technique applicable to the majority of the European languages Currently we are working on performance comparison between our system and Support Vector Machine References 1 Katakis I Tsoumakas G Vlahavas I Email Mining Emerging 
Techniques for Email Management In Vakali A Pallis G eds Web Data Management Practices Emerging Techniques and Technologies pp 392 E Sneiders 2 Busemann S Schmeier S Arens R G Message classification in the call center In Proc Sixth Conference on Applied Natural Language Processing pp A System to Control Language for Oral Communication Laurent Spaggiari1 and Sylviane Cardey2 Human Factors Dept EDYDNX section 527 M0151 0 Airbus Operations SAS 316 route de Bayonne F 31060 Toulouse France 2 Centre 1 Abstract In this paper we discuss the use of controlled languages not for written texts but for oral communication which has never been done before and this in safety critical domains Interference between languages could effectively cause accidents due to misunderstanding of messages whatever they are We discuss how firstly we could automatically detect eventual possibilities of misunderstanding due to mispronunciation or bad interpretation and secondly how to prevent these problems by using controlled languages We 
show that our methodology which is intensional in nature is much more productive than working in extension Keywords Controlled language oral communication language interferences 1 Introduction In this paper we discuss the use of controlled languages not for written texts but for oral communication which has never been done before and this in safety critical domains Interference between languages could effectively cause accidents due to misunderstanding of messages whatever they are Though research concerning interferences between different languages has been carried out at Airbus Operations SAS and in Centre 2 Natural Language Whether it be oral written or signed a communication will be considered as successful and efficient when the received message complies with the mental process used for reconstructing and interpreting the information within the message However natural language not only allows everyone to create many variations for the same H Loftsson E 394 L Spaggiari and S Cardey 3 Controlled Languages 
Contrary to natural language controlled languages CLs are favoured by industry because they refer to systems that limit the number of core vocabulary words of applicable grammar and stylistic rules Industry does not need Shakespeare or Chaucer industry needs clear concise communicative A System to Control Language for Oral Communication 395 4 The Oral Aspect Creators of CLs usually base their grammar restrictions on well established writing principles e g write short sentences with only one topic avoid passive form Furthermore despite the fact that these languages do not have many rules in common 7 they do share one main characteristic they deal with the written aspects of language and not with the oral It is exactly this oral aspect that we address here The fact is that messages are not only read but can also be heard using synthetic recorded voices in nuclear plants and airports for example Because the receiver of the message may not have the same mother tongue as the one he she hears and because one 
cannot expect him her to master it a syntactically and lexically controlled message may not be sufficient Indeed when looking at the following pairs one can easily imagine the potential consequences in case of a misunderstanding increase the temperature versus decrease the temperature the gear is uplocked versus the gear is unlocked In French for an Anglophone dessus and dessous will sound the same As a further example ambiguities can result from English phonemes not present in Thai as is illustrated in Table 1 8 We do not enter into the complexity of the matter here but we can already see that half for example becomes harp as well as ball which is pronounced born Table 1 Ambiguities resulting from English phonemes not present in Thai the phonetic transcriptions are in SAMPA English phonemes T D f v s z l r Sound in Thai t d d p w d t n l Ambiguities birth bird they day half harp vine wine bus bud buzz but ball born free flee 5 A System to Help When Creating Sentences That Are to Be Pronounced Based on the 
observations in the previous section we have devised a system that can help when creating sentences that are to be pronounced This system has the ability to detect within a list not only all the homophones e g night knight and the minimal pairs e g brake brain but also quasi homophones e g increase decrease for a proposed word according to the source language North American English 396 L Spaggiari and S Cardey 5 1 The Database The database we used for checking the pronunciation is the Carnegie Mellon University Pronouncing Dictionary also known as cmudict This dictionary is a public domain machine readable pronunciation dictionary for North American English that contains over 130 000 words and their phonetic transcriptions The pronunciation of the words is encoded using a modified form of the Arpabet system each phoneme having a unique code e g ABRACADABRA AE2 B R AH0 K AH0 D AE1 B R AH0 This dictionary is used in different projects such as the Festival speech synthesis system and also the CMU Sphinx speech 
recognition system As a result of this database our system was able to retrieve all the words with the same pronunciation However we also wanted to obtain quasi homophones e g increase decrease from this list So we devised an algorithm that looks at the phonetic differences between words 5 2 The Algorithm The algorithm performs the following steps Calculation of the number of phonemes for the submitted word 11 for abracadabra Retrieval from the database of all the words that have o The same number of phonemes o The same number 1 of phonemes o The same number 1 of phonemes Calculation of the similarity number of different phonemes in the same order between the submitted word and the retrieved words Calculates the proximity between the two phonetic strings using the Levenshtein distance function Levenshtein distance is defined as the minimum number of necessary characters to be changed inserted or modified for transforming a string into another one It is commonly used for spelling checking speech recognition 
DNA analysis etc The system works by requesting a check for a specific word one by one So it is not possible to get statistics on the numbers of pairs of words retrieved for a specific language The algorithm is able to retrieve any string of any length monosyllabic to very long words The number of retrievals decreases with the length of the submitted word Also the words retrieved are sometimes irrelevant different enough to be not mistaken This is due to the fact that we weakened the constraints Indeed a difference of 2 phonemes is considered in our algorithm Also we did consider the phoneme itself as a whole and not as a sum of phonological features We think the retrievals will be more relevant when we will look at the divergences of phonological features and only consider one phoneme of difference instead of 2 as at present 5 3 Oral Communication Involving Different Mother Tongues The receiver of the message can have a different mother tongue from English Consequently some phonemes may not exist for him or 
her In this case what will the receiver understand Assuming the fact that he she will reconstruct words using existing phonemes in their own language the system should replace the phonemes by A System to Control Language for Oral Communication 397 those existing in another language and check in the database if homophones or quasi homophones exist To do this we devised a resource that gives for each language Arabic French and Chinese for the moment the list of non existing English phonemes and their counterpart s Table 2 illustrates an extract from this table Table 2 Non existing English phonemes and their counterpart s extract Phonemes Ax Ax Ax From English to Replaced by Aa Ae Ao Language Arabic Arabic Arabic As a result of this resource our system is now able depending on the language selected to reconstruct the pronunciation and then retrieve all the words with the same pronunciation in English An illustration of the system s interface showing the results for tomato is shown in Fig 1 Fig 1 Screen shot of 
the System to Help when Creating Sentences that are to be Pronounced for the word tomato 398 L Spaggiari and S Cardey 6 Results and Improvements The system behaves as intended because it retrieves all the homophones for a term feel F IY1 L fiel feil foell it retrieves all the minimal pairs for a term feel F IY1 L fail fall feat feed fees fell file fill foal foil fool foul fowl full peal peel it retrieves quasi homophones for a term depending on the language selected thought TH AO1 T for Chinese fought sawed sod sought and for French fought sought taught taut tot Using this information one can easily decide when creating a spoken message if one can use a word or if one should change it e g use reduce instead of decrease or even reformulate the whole sentence However when looking just at English some of the words that were retrieved could be avoided as they are different enough not to be mistaken We think that the reasons for this lie in the constraints insufficient we have applied for calculating the 
similarity Indeed we performed this calculation by counting the number of different phonemes between two words Also to be able to retrieve for example increase decrease we had to consider an acceptable number of 2 differences whatever they are As a consequence many words with 2 differences are retrieved A better way would be to take into account for each phoneme its phonological features and to count the different ones to get a much more precise result For example we would continue to take a difference of 2 phonemes as a maximum but reducing this time the maximum number of differences allowed between features we would reduce the numbers of results The 2 different phonemes in increase decrease share the same phonological features except one nasal vs oral Another improvement would consist in considering the whole sentence that is to say to consider the assimilations that occur between the words once these are put together Finally the phonetics of the reconstructed sentences could be saved in an ssml file 9 
This file can easily be enriched with plenty of information concerning prosody and style voice emphasis break pitch speaking rate and volume of the speech output text structure etc This would allow us to use this file as an entry to obtain these phonetic strings pronounced by a synthetic voice for example Microsoft US English Anna An illustration of the system s interface enriched for voice synthesis word by word only is shown in Fig 2 A System to Control Language for Oral Communication 399 Fig 2 Screen shot of the System to Help when Creating Sentences that are to be Pronounced enriched for voice synthesis 7 Conclusion We have seen in this paper why language interferences have to be avoided and we have proposed a methodology and a system to find and solve these problems Some work about interference had already been done in our laboratory with native speakers but our system automatically detecting possible interference revealed itself much more efficient Much work still has to be done at the level of the 
boundaries between words but the methodology which consists in working at the level of phonemes and distinctive features rather than trying to find individual words seems to be more productive and easier to generalise for solving the problems of interferences which are due to bad pronunciation or bad interpretation The methodology used allows tracing back to the cause of the problems which is essential in safety critical applications The results of this research can be applied to different domains This is because the specific data i e lexicon by domain is tested against the general pronunciation dictionary as we cannot know the level of English people have So the methodology is not domain dependant and can be applied to any specific domain as long as the domain has its own dictionary 400 L Spaggiari and S Cardey References 1 Goyvaerts P Controlled English Curse or Blessing A User s Perspective In Proceedings of CLAW 1996 Leuven Belgium March 26 27 pp Robust Semi supervised and Ensemble Based Methods in Word 
Sense Disambiguation Anders Centre for Language Technology University of Copenhagen Njalsgade Abstract Mihalcea 1 discusses self training and co training in the context of word sense disambiguation and shows that parameter optimization on individual words was important to obtain good results Using smoothed co training of a naive Bayes classifier she obtains a 9 8 error reduction on Senseval 2 data with a fixed parameter setting In this paper we test a semi supervised learning algorithm with no parameters namely tri training 2 We also test the random subspace method 3 for building committees out of stable learners Both techniques lead to significant error reductions with different learning algorithms but improvements do not accumulate Our best error reduction is 7 4 and our best absolute average over Senseval 2 data though not directly comparable is 12 higher than the results reported in Mihalcea 1 Keywords co training tri training word sense disambiguation 1 Introduction Word sense disambiguation WSD is the 
task of deciding which sense a word has in a particular context The Senseval 2 shared task provides labeled data that can be used for supervised learning of WSD of 29 English nouns This data set was used by Mihalcea 1 in a line of experiments using inference from unlabeled data in addition to the Senseval 2 data namely instances drawn from the British National Corpus The baseline in Mihalcea 1 is a naive Bayes classifier trained on the labeled data She then considers the potential of self training and co training algorithms for making use of unlabeled data 4 She first shows that performance is very sensitive to parameter setting but nevertheless co training leads to a significant error rate reduction of 9 8 with a global parameter setting which specifies the number of iterations growth size and pool size In particular co training is run twice with a pool of 5000 data points selecting the 50 points most confidently labeled In this work we consider a parameter free semi supervised learning algorithm introduced 
in Li and Zhou 2 The algorithm is described in Sect 2 Sect 3 H Loftsson E 402 A intro duces a method for constructing ensembles of classifiers that form robust and accurate end classifiers namely random subspaces 3 In Sect 4 we apply tri training and random subspaces to supervised classifiers trained on Senseval 2 data incl naive Bayes decision stumps PART and logistic boosting In Sect 5 we discuss our results and conclude The reported prepro cessing of the data in Mihalcea 1 is very complicated and the prepro cessed data is no longer available Mihalcea p c Consequently results reported here are not directly comparable Moreover some test examples in the Senseval 2 have multiple labels and to simplify things we only evaluate our algorithms on test examples with a unique label 2 Tri training This section presents the tri training algorithm originally proposed by Li and Zhou 2 Let L denote the labeled data and U the unlabeled data Assume that three classifiers c1 c2 c3 same learning algorithm have been trained 
on three bootstrap samples of L In tri training an unlabeled datapoint in U is now labeled for a classifier say c1 if the other two classifiers agree on its label i e c2 and c3 Two classifiers inform the third If the two classifiers agree on a labeling there is a good chance that they re right The algorithm stops when the classifiers no longer change The three classifiers are combined by ma jority voting Li and Zhou 2 show that under certain conditions the increase in classification noise rate is compensated by the amount of newly labeled data points The most important condition is that the three classifiers are diverse If the three classifiers are identical tri training degenerates to self training Diversity is obtained in Li and Zhou 2 by training classifiers on bootstrap samples In their 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 for i 1 3 do Si bootstrap sample L ci train classifier Si end for repeat for i 1 3 do for x U do Li if cj x ck x j k i then Li Li x cj x end if end for ci train classifier L Li end 
for until none of ci changes apply majority vote over ci Fig 1 Tri training Li and Zhou 2005 Robust Semi supervised and Ensemble Based Methods in WSD 403 experiments they consider classifiers based on the C4 5 algorithm BP neural networks and naive Bayes classifiers The algorithm is sketched in a simplified form in Figure 1 see Li and Zhou 2 for all the details Tri training has to the best of our knowledge not been applied to WSD before but it has been applied to other NLP classification tasks incl Chinese chunking 5 and question classification 6 3 Random Subspaces Random subspaces was introduced in Ho 3 The idea is simple Randomly select a subset of the components of the feature vector and train a classifier In other words from a d dimensional data set we project n new k dimensional data sets by random projection Each new data set is given as input to the learning algorithm and the base classifiers are combined to form a stronger end classifier by ma jority voting One weakness with this metho d is that some 
of the subspaces may lack the ability to separate different classes For this reason we expect random subspaces to perform well with boosting algorithms see 4 4 1 Experiments Data The data sets were prepared to be as close as possible to the data sets used in Mihalcea 1 Briefly put we use a small set of local features incl context word forms and POS tags and a larger set of global features i e a small bag of content words Unlike Mihalcea 1 we do not use collocations as global features and we only use a single pool of unlabeled data points rather than pruning them by collocations See Mihalcea 1 for more details The complete list of words used in Mihalcea 1 i e the nouns in the Senseval 2 data Table 1 English nouns in Senseval 2 art circuit hearth restraint authority day holiday sense bar bum detention dyke lady material space stress chair channel child church facility fatigue feeling grip mouth nation nature post yew The average number of labeled examples for each word is about 100 with an additional pool for 
testing of about half that figure The average number of unlabeled examples in our experiments is 14 355 compared to 7 085 in Mihalcea 1 Mihalcea 1 prunes the unlabeled data by collocations to avoid noise but using the raw unlabeled data makes it easier to repro duce our results 404 A 4 2 Learning Algorithms We consider three baseline learning algorithms in our experiments Sect 3 motivated our choice of using logistic boosting 8 Briefly put logistic boosting considers the more well known boosting algorithms as generalized additive mo dels and applies the cost functional of logistic regression Naive Bayes is the standard choice in WSD and is known to perform reasonably well on Senseval 2 data We also ran our algorithms on decision stumps because they are fundamental to a wide range of learning algorithms incl logistic boosting Finally we include a rule based learning algorithm PART 9 because of their intuitive nature and potential usefulness in more descriptive computational linguistics 4 3 Results Our results 
using the tri training algorithm on Senseval 2 data are listed below We use self training until convergence as our semi supervised baseline see e g Abney 4 We only present average accuracy rather than accuracies on the 29 individual words is the absolute difference between our baseline and tritraining resp random subspaces Table 2 Results for tri training learner LogitBoost naive Bayes PART DecisionStump baseline self training tri training 65 56 66 39 66 43 0 87 64 33 64 09 62 74 1 59 60 37 60 57 60 84 0 47 58 23 58 59 58 86 0 63 Tri training leads to reasonable error reductions when applied to logistic boosting 2 5 and PART 1 2 in light of our relatively strong baselines but not when applied to naive Bayes Our results using random subspaces on Senseval 2 data are listed here Table 3 Results for random subspaces baseline learner LogitBoost 65 56 64 33 naive Bayes 60 37 PART DecisionStump 58 23 bagging random subspaces 66 80 68 11 63 61 64 01 62 68 63 16 56 62 58 65 2 55 32 2 79 0 42 The result obtained by 
random subspaces over logistic boosting even takes us considerably beyond what can be obtained with linear support vector machines We use bagging as our ensemble based baseline Robust Semi supervised and Ensemble Based Methods in WSD 405 Combining Tri Training and Random Subspaces Somewhat surprisingly performance degrades if we try to combine tri training and random subspaces We tri trained a random subspace model with logistic boosting but accuracy dropped by more than 2 percentage points to 66 00 compared to our random subspaces baseline 5 Conclusion Tri training and random subspaces are by and large robust metho ds for boosting supervised classifiers in the context of word sense disambiguation Naive Bayes is probably too stable to be amenable to tri training although Li and Zhou 2 report positive results and it is clear why random subspaces hurts the performance of naive Bayes Since naive Bayes implements a strong independence assumption between features and consult each feature independently there is 
little to gain from randomly constructed subspaces Both tri training and random subspaces work particularly well with logistic boosting It seems logistic boosting helps overcome the potential weaknesses of random subspaces and it is unlike support vector machines for example sensitive enough to bootstrap samples to be amenable to tri training Generally we reported competitive results on the Senseval 2 68 11 and obtained a 7 4 error reduction wrt logistic boosting Since the data set is not directly comparable to the data set used in Mihalcea 1 we plan to reimplement smoothed co training for direct comparison We also plan to compare tri training to semi supervised support vector machines 10 References 1 Mihalcea R Co training and self training for word sense disambiguation In CONLL Boston MA 2004 2 Li M Zhou Z H Tri training exploiting unlabeled data using three classifiers IEEE Transactions on Knowledge and Data Engineering 17 11 The Effect of Semi supervised Learning on Parsing Long Distance Dependencies in 
German and Swedish Anders Center for Language Technology University of Copenhagen Njalsgade Abstract This paper shows how the best data driven dependency parsers available today 1 can be improved by learning from unlabeled data We focus on German and Swedish and show that labeled attachment scores improve by 1 5 2 5 Error analysis shows that improvements are primarily due to better recovery of long distance dependencies Keywords dependency parsing semi supervised learning long distance dependencies 1 Introduction Rimell et al 2 argue that long distance dependencies are particularly interesting in parser evaluation since they provide a strong test of the parser s knowledge of grammar and since recovering long distance dependencies is necessary to completely represent the underlying predicate argument structure of the sentence useful for applications such as question answering and information extraction Rimell and Clark show that state of the art constituent parsers have accuracies below 50 on a new dataset of 
English unbounded dependencies The purpose of this paper is two fold i It is shown that it is possible to improve the accuracy of the best available dependency parsers for German and Swedish by learning from unlabeled data ii It is shown that improvements are primarily due to better recovery of long distance dependencies In particular it is shown that a novel semi supervised learning algorithm called generalized tri training is able to improve labeled attachment scores LASs on standard datasets by 1 72 German and 2 36 Swedish in general If we limit attention to long distance dependencies 7 however increases in F score are even more dramatic i e 5 09 German and 8 58 Swedish Semi supervised learning of structured variables is a difficult problem that has received considerable attention recently but most results have been negative 3 This paper uses stacked learning 4 to reduce structured variables i e dependency graphs to multinomial variables i e attachment and labeling decisions which are easier to manage in 
semi supervised learning scenarios H Loftsson E The Effect of Semi supervised Learning 407 Ensemble based metho ds such as stacked learning are used to reduce the instability of classifiers to average out their errors and to combine the strengths of diverse learning algorithms Ensemble based metho ds have attracted a lot of attention in dependency parsing recently 5 6 7 1 8 9 Nivre and McDonald 7 were first to intro duce stacking in the context of dependency parsing This paper applies a generalization of tri training 10 a form of co training that trains an ensemble of three learners on labeled data and runs them on unlabeled data to two classification problems attachment and labeling that together approximate dependency parsing Semi supervised dependency parsing has attracted a lot of attention recently 11 12 13 but there has to the best of our knowledge been no previous attempts to apply tri training or related combinations of ensemble based and semi supervised metho ds to any of these tasks except for the 
work of Sagae and Tsujii 14 However tri training has been applied to Chinese chunking 15 and question classification 16 We compare generalized tri training to the original tri training algorithm and to semi supervised support vector machines 17 Sect 2 first intro duces the dependency parsing problem and defines stacked learning Stacked learning is then generalized to dependency parsing and we describe how stacked dependency parsers can be further stacked as input for two end classifiers that can be combined to pro duce dependency structures These two classifiers will learn multinomial variables attachment and labeling from a combination of labeled data and unlabeled data using a generalization of the tri training algorithm presented in Li and Zhou 10 Sect 2 also intro duces generalized tri training Sect 3 describes our experiments We describe the data sets and how the unlabeled data were prepared Sect 4 presents our results Sect 5 presents an error analysis and shows that improvements are primarily due to 
better recovery of long distance dependencies and Sect 6 concludes the paper 2 2 1 Background Dependency Parsing Dependency parsing mo dels a sentence as a tree where words are vertices and grammatical functions are directed edges dependencies Each word thus has a single incoming edge except one called the root of the tree Dependency parsing is thus a structured prediction problem with trees as structured variables Each sentence has exponentially many possible dependency trees Our observed variables are sentences with words labeled with part of speech tags The task for each sentence is to find the dependency tree that maximizes an ob jective function which in our case is learned from a combination of labeled and unlabeled data More formally a dependency tree for a sentence x w1 wn is a tree T 0 1 n A with A 408 A is projective if every vertex has a continuous projection i e if and only if for every arc i j A and node k V if i k j or j k i then there is a subset of arcs i i1 i1 i2 ik 1 ik A such that ik k The 
German and Swedish data sets used in our experiments below have 27 8 resp 9 8 non projective dependency trees 2 2 Stacked Dependency Parsing Stacked generalization or simply stacking was first proposed by Wolpert 4 Stacking is an ensemble based learning metho d where multiple weak classifiers are combined in a strong end classifier The idea is to train the end classifier directly on the predictions of the input classifiers Say each input classifier ci with 1 i n receives an input x and outputs a prediction ci x The end classifier then takes as input x c1 x cn x and outputs a final prediction c0 x c1 x cn x Training is done by crossvalidation In sum stacking is training a classifier on the output of classifiers Stacked learning can be generalized to structured prediction tasks such as dependency parsing Architectures for stacking dependency parsers typically only use one input parser but otherwise the intuition is the same the input parser is used to augment the dependency structures that the end parser is 
trained and evaluated on Nivre and McDonald 7 first showed how the MSTParser 18 and the MaltParser 19 could be improved by stacking each parser on the predictions of the other Martins et al 1 generalized their work considering more combinations of parsers and stacking the end parsers on non local features from the predictions of the input parser e g siblings and grand parents In this work we parse the German and Swedish data sets from the CONLL X Shared Task and use three stacked dependency parsers for each language parser1 p1 parser2 p2 parser3 p3 Ge mst2 malt mst2 D malt mst1 E Sw mst2 mst2 D malt mst2 D malt mst1 A The notation malt mst2 means that the second order MSTParser has been stacked on MaltParser The capital letters refer to feature configurations Configuration A only stacks the level 1 parser on the predicted edges of the level 0 parser along with the input features Configuration D stacks a level 1 parser on several non local features of the predictions of the level 0 parser the predicted edge 
siblings grand parents and predicted head of candidate mo difier if predicted edge is 0 Configuration E stacks a level 1 parser on the features in configuration D and all the predicted children of the candidate head The chosen parser configurations are those that performed best in Martins et al 1 There are two reasons that our input parsers perform slightly worse than those reported on in Martins et al 1 i We use about 5 000 tokens of the training data for development ii For both datasets we used projective rather than pseudoprojective parsing in MaltParser For MSTParser level 0 we also The Effect of Semi supervised Learning 409 reduced training time by iterating three times over the German data rather than 10 times as in Martins et al 1 2 3 Stacking Stacked Dependency Parsing The input features of the input classifiers in stacked learning x can of course be removed from the input of the end classifier It is also possible to stack stacked classifiers This leaves us with four strategies for recursive stacking 
namely to constantly augment the feature set with level n classifiers trained on the predictions of the classifiers at all n 1 lower levels with or without the input features x or simply to train a level n classifier on the predictions of the level n 1 classifiers with or without x In this work we stack stacked dependency parsers by training classifiers on the output of three stacked dependency parsers and POS tags Consequently we use one of the features from x since this led to better results on development data Note that we train classifiers and not parsers on this new level 2 The reduction is done the following way First we train a classifier on the relative distance from a word to its head to induce attachments For example we may obtain the following features from the predictions of our level 1 parsers label p1 p2 p3 POS 1 1 1 1 NNP 0 0 0 0 VBD In the second row all input parsers p1 3 in column label p1 p2 p3 POS SBJ SBJ SBJ SBJ NN ROOT ROOT ROOT COORD VBN 2 4 Generalized Tri training Tri training was 
originally intro duced in Li and Zhou 10 The metho d involves three learners that inform each other Let L denote the labeled data and U the unlabeled data Assume that three classifiers c1 c2 c3 have been trained on L In the original algorithm the three 410 A classifiers are obtained by applying the same learning algorithm to three bootstrap samples of the labeled data but in generalized algorithms three different learning algorithms are used An unlabeled datapoint in U is labeled for a classifier say c1 if the other two classifiers agree on its label i e c2 and c3 Two classifiers inform the third If the two classifiers agree on a labeling we assume there is a good chance that they are right In the original algorithm learning stops when the classifiers no longer change in generalized tri training a fixed stopping criterion estimated on development data is used The three classifiers are combined by ma jority voting Li and Zhou 10 show that under certain conditions the increase in classification noise rate is 
compensated by the amount of newly labeled data points The most important condition is that the three classifiers are diverse If the three classifiers are identical tri training degenerates to self training As already mentioned Li and Zhou 10 obtain this diversity by training classifiers on bootstrap samples In their experiments they consider classifiers based on decision ive Bayes inference trees BP neural networks and 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 for i 1 3 do ci train classifier li L end for repeat for i 1 3 do for x U do Li if cj x ck x j k i then Li Li x cj x end if end for ci train classifier L Li end for until stopping criterion is met apply ci Fig 1 Generalized tri training Our predictions are those of the random forests classifier after a fixed number of rounds optimized on development data On development data this led to slightly better results than ma jority votes The Effect of Semi supervised Learning 411 3 3 1 Experiments Data We use the German and Swedish datasets from the CONLL X Shared 
Task i e the TIGER treebank 21 and Talbanken05 22 The TIGER treebank contains 700 000 tokens or 39 200 sentences and Talbanken05 contains 191 000 tokens or 11 000 sentences We use the official train test splits with one important exception we use the first approx 5 000 lines in the training data as development data The unlabeled data were the Leipzig Corpora Collection corpora for German and Swedish except we only used the first 75 000 sentences for the German corpus 3 2 POS Tags The unlabeled data were POS tagged using the freely available SVMTool 23 model 4 left right left The German data set contains 52 POS tags and the Swedish data set contains 37 POS tags The accuracy of SVMTool on the two data sets is about 95 3 3 Algorithm Once our data has been prepared we train the stacked dependency parsers listed in Sect 2 3 and use them to parse our development data our test data and our unlabeled data This gives us three sets of predictions for each of the three data sets From each triad of predictions we 
construct two data sets one for our attachment classifier say data set A and one for our dependency labeler say data set B Using 5 fold cross validation we train three classifiers on the development test data in A and B The entire architecture can be depicted as follows tri training nb stacking mst2 mst2 malt mst2 malt mst1 stacking mst2 malt mst1 forests tree We first stack three dependency parsers as described in Martins et al 1 We then stack three classifiers on top of these dependency parsers and POS tags a 412 A on development data and unlabeled data in A and a stopping criterion for our dependency label variable on development data and unlabeled data in B The stopping criteria are used when we update the classifiers trained on our test data in 5 fold cross validation 3 4 Baselines The best of the stacked input parsers is of course our natural baseline Since we have generalized tri training we include the original tri training algorithm as a semi supervised baseline The original tri training algorithm 
is run with the same decomposition and the same features as our generalized tri training algorithm We use the two learning algorithms of the three originally used in Li and Zhou 10 that we had available namely naive Bayes and C4 5 Finally we include S3VMs as a semi supervised baseline Since S3VMs pro duce binary classifiers and one vs many combination would be very time consuming we train a binary classifier that produces a probability that any candidate arc is correct and do greedy head selection We optimized the feature set and included a total of seven features head POS dependent POS dependent left neighbor POS distance direction predictions of the three classifiers 4 Results Our results on the German and Swedish data sets are presented in Figure 2 We first list the individual parsers malt and mst2 and the stacked parsers Since Martins et al 1 found that for German the level 0 parser mst2 outperformed any configuration of mst2 mst2 there is no figure for mst2 mst2 for German The best input parser for 
German is mst2 i e the second order MSTParser whereas the best input parser for Swedish is malt mst2 i e the second order MSTParser stacked on MaltParser with feature configuration D see Sect 2 3 Generalized tri training leads to highly significant improvements on both data sets p 0 001 The row tri training lists the results of the initial greedy head selection whereas tri training MST lists the results of reparsing using CLE to pro duce well formed dependency trees Reparsing hurts labeled attachment score LAS a bit but differences are small Since using the Eisner algorithm for reparsing 24 led to a slightly better result for Swedish than using CLE we report both results We only applied our semi supervised baselines to unlabeled parsing but it is quite evident that these learning strategies seem less promising than generalized tri training S3VMs seem to average out the input parsers for German and lead to a relatively small 0 5 improvement for Swedish The original tri training algorithm leads to scores well 
below any of the input parsers for Swedish but to a considerable improvement for German The Effect of Semi supervised Learning German malt mst2 malt mst2 malt mst1 s3vms orig tri training nb orig tri training C4 5 tri training tri training MST tri training excl punct Martins et al 2008 excl punct Swedish malt mst2 mst2 mst2 malt mst2 malt mst1 s3vms org tri training nb org tri training C4 5 tri training tri training MST tri training Eisner tri training excl punct Martins et al 2008 excl punct LAS 80 08 84 25 81 40 81 35 85 97 85 88 85 84 87 44 LAS 83 29 81 95 82 32 83 50 83 45 85 86 85 71 85 79 85 94 85 16 UAS 82 70 87 20 84 18 84 16 84 51 89 06 89 22 90 24 90 13 90 69 UAS 87 52 87 46 87 98 87 96 87 87 88 45 87 22 87 36 91 32 91 16 91 09 91 95 LA LAS p value 88 02 91 38 89 94 90 06 92 55 1 72 0 0005 92 55 1 61 0 0009 91 53 LA 87 54 87 41 87 20 88 24 88 12 90 12 90 12 90 12 89 15 LAS p value 413 2 36 0 0001 2 21 0 0001 2 29 0 0001 Fig 2 Results on the German and Swedish data sets Scores are including 
punctuation unless otherwise noted and p value is difference with respect to best input parser The results in Martins et al 2008 are incomparable since they did not take out 5000 sentences for development and since we did not convert the treebanks into projective trees before training MaltParser 5 Error Analysis and Discussion Our error reductions in LAS over the best of our stacked input parsers are 11 16 for German and 12 01 for Swedish in unlabeled attachment score UAS it is 21 42 resp 19 55 The most striking difference between our errors and those committed by our best input parser is their distribution across dependency length as illustrated in Figure 3 Inference from large amounts of unlabeled data seems to make our parser much better at predicting and labeling long distance dependencies and dependencies of the root node In general generalized tri training improves LASs by 1 72 German and 2 36 Swedish but if we limit attention to long distance dependencies 7 increases in F score are even more dramatic 
i e 5 09 German and 8 58 Swedish 414 A F score 97 63 94 64 91 16 88 99 91 19 0 71 0 62 1 33 3 09 5 09 F score 95 12 95 47 92 92 87 10 89 85 3 62 1 40 1 16 4 54 8 58 Fig 3 Recall precision and F score binned on dependency length Number of tokens is number of dependencies of length n in the gold standard Scores are including punctuation is the difference between system and baseline F scores Our errors and those committed by our best input parser seem to be distributed over POS tags in much the same way Distributions over dependency labels do shed some more light on what kind of long distance dependencies our parsers learn to recover Consider the recall precision and F score of labeled attachment of the 10 most frequent dependency relations in German excl ROOT in Figure 4 and the same results for the 10 most frequent dependency relations in Swedish excl ROOT in Figure 5 It is evident that ma jor improvements are primarily due to improvements with coordinating conjunctions punctuations and complements that can 
move relatively freely in and across clauses e g sub jects clausal ob jects and other ob jects In Swedish adverbs and some postnominal mo difiers move rather freely and we see big improvements here as well Since we train on less material than Martins et al 1 taking out 5000 sentences for development our point of departure is significantly worse than theirs The best stacking configuration for Swedish i e stacking the second order MSTParser on MaltParser with features D has a LAS of 85 16 on the full training section with projectivization and deprojectivization but only a LAS of 83 93 on our smaller subset excl punct Consequently the fact that tri training leads to results considerably better than those reported in Martins et al 1 0 78 excl punct says something about the potential of inference from unlabeled data Since we greedily select the best head for each word our output is not guaranteed to be wellformed dependency trees This is similar to Zeman and Zabokrtsky 25 The percentage of cyclic structures 
produced by our ensemble is in both cases below 5 Surdeanu and Manning 9 observe similar figures in ensemblebased greedy head selection Reparsing only leads to a small decrease in LAS The Effect of Semi supervised Learning 415 tag AG CD CJ MNR MO NK OA OC PUNC SB tok 150 129 172 153 772 1721 206 220 808 425 meaning genitive attribute coord conjunction conjunct postnominal mod modifier noun kernel mod accusative object clausal object punctuation subject Baseline rec 73 33 73 64 67 44 58 17 72 28 96 22 74 27 90 45 84 28 86 35 prec 76 39 71 97 67 05 54 27 74 40 95 61 72 17 85 04 84 28 82 84 F score 74 83 72 80 67 24 56 15 73 32 95 91 73 20 87 66 84 28 84 56 System rec 82 00 75 19 69 19 58 82 76 55 95 64 73 30 92 27 86 76 87 53 prec 72 35 74 62 67 61 54 55 76 16 96 31 72 25 89 82 86 65 88 15 F score 76 87 74 90 68 39 56 60 76 35 95 97 72 77 91 03 86 70 87 84 2 04 2 10 1 15 0 45 3 03 0 06 0 43 3 37 2 42 3 28 Fig 4 Recall precision and F score of labeled attachment score of 10 most frequent dependency relations in 
German tag AA AT CC DT ET IP OO PA SS tok 184 266 234 220 556 342 312 284 677 508 meaning coord conjunction other adverbial nom pre modifier conjuncts determiner other nom postmod period other object prep compl other subject Baseline rec 91 30 62 41 96 15 79 09 94 96 72 22 91 35 85 92 95 57 90 55 prec 89 84 64 59 96 15 79 82 90 26 73 73 91 35 78 96 94 04 89 67 F score 90 56 63 48 96 15 79 45 92 55 72 97 91 35 82 29 94 80 90 11 System rec 95 65 67 67 96 58 82 27 95 32 82 16 100 00 89 08 96 01 93 70 prec 94 62 67 92 95 76 83 80 92 66 79 15 100 00 84 05 95 73 91 54 F score 95 13 67 79 96 17 83 03 93 97 80 63 100 00 86 49 95 87 92 61 4 57 4 31 0 02 3 58 1 42 7 66 8 65 4 20 1 07 2 50 Fig 5 Recall precision and F score of labeled attachment score of 10 most frequent dependency relations in Swedish Generalized tri training leads to much better results than the other two semisupervised learning algorithms The original tri training algorithm gives a big improvement for German but for Swedish it makes things much 
worse S3VMs do not lead to improvements but seem to average out the ensemble used in the stacking 6 Conclusion This paper showed how the stacked dependency parsers intro duced in Martins et al 1 can be improved by inference from unlabeled data Briefly put we stacked three diverse classifiers on triads of stacked dependency parsers and let them label unlabeled data for each other in a co training like architecture Our error 416 A reductions in LAS over the best of our stacked input parsers were 12 24 for German and 12 01 for Swedish in UAS it was 21 42 resp 19 55 Error analysis shows that improvements are primarily due to better recovery of long distance dependencies Generalized tri training improves LASs by 1 72 German and 2 36 Swedish but if we limit attention to long distance dependencies 7 increases in F score are even more dramatic i e 5 09 German and 8 58 Swedish References 1 Martins A Das D Smith N Xing E Stacking dependency parsers In EMNLP Honolulu Hawaii 2008 2 Rimell L Clark S Steedman M Unbounded 
dependency recovery for parser evaluation In EMNLP Singapore 2009 3 Abney S Semi supervised learning for computational linguistics Chapman and Hall Boca Raton 2008 4 Wolpert D Stacked generalization Neural Networks 5 The Effect of Semi supervised Learning 417 20 Breiman L Random forests Machine Learning 45 Shooting at Flies in the Dark Rule Based Lexical Selection for a Minority Language Pair Linda Wiechetek1 Francis M Tyers2 and Thomas Omma3 Giellatekno Romssa Universitehta Norway linda wiechetek uit no Dept Lleng i Sist Inform Universitat d Alacant Spain ftyers dlsi ua es 3 Divvun 1 2 Abstract This paper presents a set of rules which form the prototype lexical selection component of a rule based machine translation system between two closely related minority languages North 1 Introduction North H Loftsson E Shooting at Flies in the Dark 419 Both languages have phonemic orthographies with differences resulting from differing conventions in standardisation These differences can largely be handled by rules 
thus a bilingual dictionary between the two was created from scratch by simply converting the orthography This process provides an adequate lexicon but when inspecting the resulting translations with a native Lule b c The languages also diverge on the lexical level and although the automatically constructed bilingual lexicon often provides adequate translations in many cases word use is actually quite different In some cases historical word roots are different in other cases words in one of the languages have acquired a new sense which does not exist in the other language or appear in specific syntactic or semantic construction which is resolved differently in the other language The need for lexical selection2 came up when seemingly straightforward translations were not accepted by Lule 1 2 The latter part of the lexicalised construction is originally a genitive too Lexical selection is defined by 1 as the principled selection of a lexical items and b the syntactic structure for input constituents based on 
lexical semantic pragmatic and discourse clues available in the input 420 L Wiechetek F M Tyers and T Omma 2 Objectives The objectives behind the development of a machine translation MT system between the two languages are largely guided by the sociolinguistic situation Following 2 applications of machine translation can be divided in two main groups with different requirements assimilation that is to enable a user to understand what the text is about and dissemination that is to help in the task of translating a text to be published Assimilation may be possible even when the text is far from being grammatically correct however for dissemination the effort needed to correct post edit the text must be lower than the effort needed to translate it from scratch A majority to minority language system will mainly be used for dissemination purposes where post editing the output should be faster than translating from scratch and intelligibility is less important In a minority to majority language system on the other 
hand intelligibility is the main goal as MT is mainly used for assimilation for instance to answer vital questions such as what are they writing about me in the minority language newspaper The system described in this paper falls outside the usual 3 Technical Background This section gives a brief overview of the two main technologies used in the construction of the prototype system 4 Apertium 5 a rule based machine translation platform and Constraint Grammar 3 a rule based framework for the disambiguation and annotation of text 3 1 Apertium The Apertium platform was originally aimed at the Romance languages of the Iberian peninsula but has also been adapted for other language pairs such 3 4 5 http avvir no The prototype system may be tested online at http victorio uit no cgi bin francis index php http www apertium org Shooting at Flies in the Dark 421 as Welsh 4 and Basque 5 The whole platform both programs and data is available from the project website under the GPL licence 6 The engine largely follows a 
shallow transfer approach to machine translation 6 Finite state transducers 7 are used for lexical processing first order hidden Markov models HMM and optional Constraint Grammar are used for partof speech tagging and finally multi stage finite state based chunking is used for structural transfer SL text deformatter morph analyser constraint grammar lexical selection lexical transfer structural transfer chunker interchunk interchunk postchunk morph generator reformatter TL text Fig 1 Modular architecture of the Apertium MT platform Bold indicates adjustments made for the North As this paper focuses on the lexical selection aspect a more detailed description of the pipeline figure 1 will not be made 3 2 Constraint Grammar The formalism used for both disambiguation and annotation is Constraint Grammar which is a linguistically based approach used for the bottom up analysis of running text The 6 7 http www fsf org licensing licenses gpl html http visl sdu dk constraint_grammar html 422 L Wiechetek F M Tyers and 
T Omma The lexical selection module is also implemented in Constraint Grammar and annotates words which are ambiguous in translation in a disambiguated source language sentence with references to their translation in the target language This method is inspired by other MT systems including rule based lexical selection such as the Dan2Eng system 8 which successfully uses 17 000 handwritten lexical transfer rules 4 4 1 Lexical Selection Potential Candidates The bilingual North Within the Constraint Grammar semantic information is encoded in semantic sets within the lexical selection module The bilingual lexicon specifies one or more alternative translations the default labelled with S0 and the alternatives Shooting at Flies in the Dark 423 labelled with consecutive numbers from one The rules make use of morphological syntactic and semantic information Rules were inspired by comments by a native speaker of Lule The rule selecting the translation vuoras for boaris old makes use of the fact that personal pronouns 
in first and second person usually denote a human Syntactic information is specifically used with the polysemous verb orrut stay seem which translates into vuojnnet before a noun adjective in essive case8 or a predicative as in example 3 3 Orru leamen buorre North The last type of constraints are narrower lexical or even idiosyncratic constructions The adjective buorre good is translated into jasskat before particular nouns such as iesdovdu self confidence Example 4 shows an example of this kind of constraint in the example North Semantic information is used in a number of rules The noun 8 9 The essive case expresses a temporary state or quality Psych verbs are those verbs which designate a psychological state or process 424 L Wiechetek F M Tyers and T Omma of suhttat get angry at where it gets a metaphorical meaning which is not conveyed by the word SET ANIMAL ealga rievssat SET HUMAN Prop Mal Prop Fem Prop Sur Fig 2 Two constraint grammar lexical selection rules to select between two translations of boaris 
old and orrut stay seem The second rule selects sense 1 S1 of the intransitive verb IV orrut stay seem if there is a subject predicative SPRED one position to the right as in example 5 a Orru buorre North b 5 Evaluation For the evaluation the North 10 The corpus of test sentences may be downloaded from http www dlsi ua es ftyers sme smj testsentences tar gz Shooting at Flies in the Dark 425 non equivalent translations were considered but only equivalent translations were included when calculating the percentage of correct translations Equivalent constructions are those where the lexical item is translated by a possible equivalent of the same part of speech Derivations which do not change the lexical category of the word e g Noun Noun and compounds are permitted In some cases it was difficult to decide whether the translation is a possible equivalent or a different word As is the case with the North 7 Rather than aiming at a system that translates 6 into 7 the aligned sentence should be discarded in favour of 
a more literal translation Some potential Lule 426 L Wiechetek F M Tyers and T Omma Table 1 Evaluation of the lexical selection rules over the New Testament The first column gives the word in North Word 6 Discussion As can be seen in table 1 the rules perform best on words where the non default scope is quite narrow such as the rule for buorre good where the non default is only picked in some lexical contexts Bad performance of some of the other rules is due to the selection of the wrong default as e g in the case of boaris old the existence of several variants that have not been considered and might be even restricted to a Biblical context as in muitalit tell where giehttot and not subtsastit or mujttalit get most hits and difficulties in excluding synonymy It is also due to the inclusion of various potentially deviating contexts in the total number of equivalent sentences as in the case of muitalit tell Categorising the rules with regard to their linguistic level and complexity the simple lexical rules 
referring to nearly idiosyncratic contexts are written very quickly and make up the ones performing best with the exception of the rule for Shooting at Flies in the Dark 427 11 1 Corinthians 3 16 428 L Wiechetek F M Tyers and T Omma The New Testament examples showed that even in a seemingly straightforward word pair the realisation in text can diverge in both directions This may result in several alternative translations partly synonymous Writing lexical selection rules does not only help to pick the correct equivalent but also to acquire knowlege about the correct equivalent Even though North 7 Conclusion The paper has explored the use of lexical selection in machine translation to improve lexical choice in translation In the case of such little researched language pairs as North Shooting at Flies in the Dark 429 References 1 Pustejovsky J Nirenburg S Lexical selection in the process of language generation In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics Morristown 
NJ USA pp Author Index Infante Lopez Gabriel Isahara Hitoshi 162 39 127 Jean Louis Ludovic 150 Johannsen Anders 401 Jones Gareth J F 345 Kanzaki Kyoko 162 Karanasou Panagiota 167 Karao glan Bahar 238 Karlsson Stefan 179 Kelly Liadh 345 79 Fahrni Angela 215 Fay Nicolas 263 Feijs Loe 250 Fellbaum Christiane D Ferret Olivier 150 Forcada Mikel L 121 Gaume Bruno 332 Gauvain Jean Luc 269 Gojenola Koldo 281 Groves Declan 121 HaCohen Kerner Yaakov Haji c Jan 1 2 138 432 Author Index Penkale Sergio 121 305 Villase nor Pineda Luis 39 85 305 Villatoro Tello 418 293 
17	a	Speaker Independent Urdu Speech Recognition Using HMM Javed Ashraf Naveed Iqbal Naveed Sarfraz Khattak and Ather Mohsin Zaidi College of Signals National University of Sciences and Technology NUST Rawalpindi 46000 Pakistan javed ashraf khattakn mcs edu pk naveedrao gmail com athar mcs nust edu pk Abstract Automatic Speech Recognition ASR is one of the advanced fields of Natural Language Processing NLP Recent past has witnessed valuable research activities in ASR in English European and East Asian languages But unfortunately South Asian Languages in general and Urdu in particular have received very less attention In this paper we present an approach to develop an ASR system for Urdu language The proposed system is based on an open source speech recognition framework called Sphinx4 which uses statistical based approach HMM Hidden Markov Model for developing ASR system We present a Speaker Independent ASR system for small sized vocabulary i e fifty two isolated most spoken Urdu words and suggest that this 
research work will form the basis to develop medium and large size vocabulary Urdu speech recognition system Keywords Speech recognition Urdu language Hidden Markov Model CMU Sphinx4 1 Introduction There are between 60 and 80 million native speakers of standard Urdu Khari Boli According to the SIL Ethnologue 1999 data Hindi Urdu is the fifth most spoken language in the world 13 According to George Weber s article Top Languages The World s 10 Most Influential Languages in Language Today Hindi Urdu is the fourth most spoken language in the world with 4 7 percent of the world s population after Mandarin English and Spanish 1 Despite a large number of populations speaking Urdu very little research work has been done related to ASR for Urdu language Most of the previous work related to SR for Urdu language is restricted to digits only using ANN However modern generalpurpose speech recognition systems are generally based on Hidden Markov Models HMMs This research work focuses on recognition of Urdu words using 
HMMs 2 Related Work Centre for Research in Urdu Language Processing CRULP Lahore Pakistan 2 and National Language Authority NLA Islamabad Pakistan 3 are the most prominent C J Hopfe et al Eds NLDB 2010 LNCS 6177 pp Speaker Independent Urdu Speech Recognition Using HMM 141 names among those who are doing dedicated research in NLP for Urdu Language CRULP and NLA have done research in many areas of Urdu Language Processing However not much published research work is available related to ASR for Urdu language M U Akram and M Arif 4 try to analyse and implement Pattern Matching and Acoustic Modelling approaches to SR for Urdu language They propose an Urdu SR system using the later approach They have used ANN to classify a set of frames into phonetic based categories at each frame To search the best sequence path for the given word to be recognised they used Viterbi search algorithm S M Azam et al 5 use ANN to propose SR system for Urdu They have developed SR system for isolated Urdu digits only Moreover the 
proposed system works only for one speaker Abdul Ahad et al 6 also use ANN using Multilayer Perceptron MLP Again they propose a system for Urdu digits for mono speaker database only S K Hasnain et al 7 also use ANN to implement their proposed system They suggest feed forward ANN model implemented using Matlab and Simulink Again the model is developed only for isolated Urdu digits 3 HMM Based Speech Recognition Traditional SR software falls into one of three categories 8 These categories are Template based approaches Knowledge based approaches and Statistical based approaches Template based approaches compare speech against a set of pre recorded words Knowledge based approaches involve the hard coding of known variations of speech into a system Both of these methods become impractical for a larger number of words Finally in Statistical based approaches e g HMM variations in speech are modelled statistically using automatic learning procedures This approach represents the current state of SR and is the most 
widely used technique today HMM are statistical models which output a sequence of symbols or quantities Speech signal could be viewed as a piecewise stationary signal or a short time stationary signal That is one could assume in a short time in the range of 10 milliseconds speech could be approximated as a stationary process Speech could thus be thought of as a Markov model for many stochastic processes 9 HMM can be trained automatically and are simple and computationally feasible to use In speech recognition the HMM would output a sequence of n dimensional realvalued vectors with n being a small integer such as 10 outputting one of these every 10 milliseconds The vectors would consist of cepstral coefficients which are obtained by taking a Fourier transform of a short time window of speech and decorrelating the spectrum using a cosine transform then taking the first most significant coefficients The HMM will tend to have in each state a statistical distribution that is a mixture of diagonal covariance 
Gaussians which will give a likelihood for each observed vector Each word or for more general SR systems each phoneme will have a different output distribution a HMM for a sequence of words or phonemes is made by concatenating the individual trained HMMs for the separate words and phonemes 9 142 J Ashraf et al We propose an Urdu ASR system using open source SR framework called Carnegie Mellon University CMU Sphinx4 which is also based on HMM Sphinx has never been used to develop Urdu ASR system This research work will open new doors for researchers of Urdu ASR systems 3 1 CMU Sphinx4 Framework Sphinx 4 is a state of the art speech recognition system written entirely in the Java programming language It was created via a joint collaboration between the Sphinx group at CMU Sun Microsystems Laboratories Mitsubishi Electric Research Labs MERL and Hewlett Packard HP with contributions from the University of California at Santa Cruz UCSC and the Massachusetts Institute of Technology MIT 10 11 3 2 Sphinx4 
Architecture The Sphinx 4 architecture has been designed with a high degree of modularity There are three main blocks in the design the frontend the decoder and the knowledge base KB These are controllable by an external application The Frontend module takes in speech and parameterizes it into a sequence of output features The KB or Linguist provides the information the decoder needs to do its job It is made up of three modules Acoustic Model Dictionary and Language Model Acoustic Model contains a representation often statistical of a sound created by training using acoustic data Dictionary has the pronunciation of all the words to be recognised Language Model contains a representation often statistical of the probability of occurrence of words The decoder is the main block of Sphinx 4 and performs the actual recognition It comprises a graph construction module which translates any type of standard language model provided to the KB by the application into an internal format and together with information from 
the dictionary and structural information from one or more sets of acoustic models constructs a Language HMM The latter is then used by the search module to determine the most likely sequence of words that could be represented by a series of features 12 13 3 3 SphinxTrain SphinxTrain is the acoustic training toolkit for CMU Sphinx versions 1 2 3 and 4 It is a suite of programs scripts and documentation for building acoustic models from speech data for the Sphinx recognisers 17 4 Urdu Words Recognition Application 4 1 Brief Overview of Urdu Language Urdu is written right to left in an extension of the Persian alphabet which is itself an extension of the Arabic alphabet Urdu uses characters from the extended Arabic character set used for Persian It further extends this set to represent sounds which are present in Urdu but not it Arabic or Persian including aspirated stop and alveolar Speaker Independent Urdu Speech Recognition Using HMM 143 consonants and long vowels 14 Altogether there are 58 letters in Urdu 
given in Figure 1 15 other sources may give slightly different set Fig 1 Urdu Character Set 4 2 Roman Urdu Urdu is occasionally also written in the Roman script Roman Urdu is the name used for the Urdu language written with the Roman alphabet Roman Urdu has been used since the days of the British Raj partly as a result of the availability and low cost of Roman movable type for printing presses The use of Roman Urdu was common in contexts such as product labels Today it is regaining popularity among users of textmessaging and Internet services and is developing its own style and conventions We have used Roman Urdu script for this research work However bare transliterations of Urdu into Roman letters omit many phonemic elements that have no equivalent in English or other languages commonly written in the Latin alphabet such as or 16 4 3 System Components The system components are shown in figure 2 below Fig 2 System Components 144 J Ashraf et al 4 3 1 Dictionary Lexicon The Lexicon development process consists 
of defining a phonetic set and generating the word pronunciation for training acoustic and language models Our dictionary contains fifty two most commonly used spoken Urdu words including ten Urdu digits 0 to 9 The pronunciations of each word are shown in Table 1 4 3 2 Acoustic Model Acoustic Model contains a representation of a sound created by training using acoustic data Sphinx 4 uses models created with SphinxTrain Two main acoustic models that are used by Sphinx 4 TIDIGITS and Wall Street Journal WSJ are already included in the lib directory of Sphinx 4 binary distribution These acoustic models are also used for other Latin languages such as French or Italian due to similarities between these languages and English language from phoneme point of view However as mentioned before there are great differences between English and Urdu language We also tried to use English acoustic model provided with Sphinx4 libraries to recognise Urdu words but as expected the results were very poor WER obtained was about 60 
Developing speech corpus or acoustic model is not a trivial task and requires resources to be allocated Unfortunately such resources were not available for this research For this research we managed to get recording of ten samples of each utterance from ten different persons The speech recording and breaking each sample of fifty two words into one word file was very time consuming process Table 1 Urdu Words with their pronunciation Speaker Independent Urdu Speech Recognition Using HMM 145 Feature Extraction For every recording in the training corpus a set of feature files was computed from the speech training data Each utterance of a word was transformed into a sequence of feature vectors using the front end executables provided with Sphinx 4 training package The detailed process can be found from 10 and 12 The training was performed using 5200 utterances of speech data from 10 different speakers 52 words x10 samples 520 utterances by each speaker Transcription File Each recording is accurately transcribed 
any error in the transcription will mislead the training process later This process is done manually i e we listen to the recordings then we match exactly what we hear into text Even the silence and the noise should also be represented in the transcription file 18 4 3 3 Language Model Sphinx 4 supports three types of language models The Context Free Grammar CGF is usually used in speech applications based on command and control The n gram grammars are used for free speech form The simplest type of language model is called wordlist grammar in which instead of breaking a word into sub units or phonemes complete word is taken as one phoneme For example Chaar English four is taken as one phoneme The wordlist grammar is recommended for isolated word recognition Being an isolated Urdu words recognition application we have used wordlist grammar in our proposed system 5 Experimental Results 5 1 Experimental Setup We performed two types of experiments for evaluating the performance of our system First type of test 
involved evaluating the performance of the system using speakers involved in training Second type of test was carried out using speakers who were not involved in the training phase We observed the behaviour of the system for 1 2 4 8 and 16 Gaussian models 5 2 Evaluation Criteria The evaluation was made according to recognition accuracy and computed using Word Error Rate WER defined by the equation 1 below which aligns a recognised word string against the correct word string and compute the number of substitutions S deletions D and insertions I from the number of correct words N WER S D I N 100 1 WER of 10 means that there was an error substitution deletion or insertion of one word out of ten words spoken by a speaker 146 J Ashraf et al 5 3 Results We found that 8 Gaussian models produced the best results in both the tests Recognition results for first test are shown in table 2 Five different speakers who were also part of training phase participated in this test Each speaker was asked to utter at random 
twenty Urdu words thrice The results are shown for each speaker Table 2 Results for the first test of Urdu Words Recognition Application WER Test3 10 10 10 0 0 Mean Speaker Speaker1 Speaker2 Speaker3 Speaker4 Speaker5 Test1 10 10 0 0 10 Test2 0 10 0 0 10 Mean WER 6 66 10 00 3 33 0 00 6 66 5 33 Recognition results for the second test are shown in table 3 Five different speakers who did not participate in the training phase were involved in this test Again each speaker was asked to utter at random any twenty Urdu words thrice The results are shown for each speaker Note that the WER increased in this case because of new speakers Table 3 Results for the second test of Urdu Words Recognition Application WER Test2 Test3 10 10 10 20 0 10 10 10 10 10 Mean Speaker Speaker1 Speaker2 Speaker3 Speaker4 Speaker5 Test1 10 10 10 20 10 Mean WER 10 00 13 33 6 66 13 33 10 00 10 66 5 4 Findings of the Experiments There are very important findings that we would like to discuss here 1 Words with more distinct phones produced 
less accuracy in recognition 2 Words with similar type of phones were recognised interchangeably for example system got confused with words saat chaar and aath 3 We first experimented with recognition of ten digits only and then increased the vocabulary to fifty two words We found that Wordlist grammar is good for small size vocabulary however as we increased number of words the accuracy of recognition decreased Speaker Independent Urdu Speech Recognition Using HMM 147 4 As mentioned earlier transliterations of Urdu into Roman letters omit many phonemic elements that have no equivalent in English for example letter in word in word etc Transcription in Latin Urdu should increase the accuracy of recognition 6 Conclusions and Future Work In this research work we have developed first Urdu ASR system based on CMU Sphinx 4 The recognition results are much better as compared to Urdu ASR systems developed earlier We demonstrated that how Sphinx 4 framework can be used to build small to medium sized vocabulary Urdu 
ASR system The acoustic model for this ASR system is entirely developed by the authors of this work Since this research was first of its kind we kept it to isolated words recognition We identified the steps involved to develop such system This system can be used in applications where small vocabulary Urdu speech recognition is required Moreover this research work could form the basis for further research in Urdu ASR systems An immediate future work is to increase the vocabulary size to medium about 200 words Later on research can focus on developing Large Vocabulary Continuous Speech Recogniser LVCSR system for Urdu language We have already started our work on former We are also planning to continue our research to build LVCSR for Urdu language using Arabic also called Perso Arabic script phonemes instead of roman Urdu phonemes to cater for Urdu phonemes missing in the roman Urdu Acknowledgements This research work was possible due to easily available open source CMU Sphinx Speech Recogniser References 1 
Most Widely Spoken Languages Saint Ignatius http www2 ignatius edu faculty turner languages htm 2 Centre for Research in Urdu Language Processing CRULP http www crulp org 3 National Language Authority NLA Islamabad http www nlauit gov pk 4 Akram M Arif M Design of an Urdu Speech Recognizer based upon acoustic phonetic modelling approach In IEEE INMIC 2004 pp 148 J Ashraf et al 9 Speech Recognition http en wikipedia org wiki Speech_recognition 10 Walker W et al Sphinx 4 A Flexible Open Source Framework for Speech Recognition http cmusphinx sourceforge net sphinx4 doc Sphinx4Whitepaper pdf 11 Lamere1 P et al Design of the CMU Sphinx 4 Decoder 12 CMU Sphinx 4 http cmusphinx sourceforge net sphinx4 13 Satori H Harti M Chenfour N Arabic Speech Recognition System Based on CMU Sphinx In 3rd International Symposium on Computational Intelligence and Intelligent 
18	a	Software Framework for Topic Modelling with Large Corpora Natural Language Processing Laboratory Masaryk University Faculty of Informatics Large corpora are ubiquitous in today s world and memory quickly becomes the limiting factor in practical applications of the Vector Space Model VSM In this paper we identify a gap in existing implementations of many of the popular algorithms which is their scalability and ease of use We describe a Natural Language Processing software framework which is based on the idea of document streaming i e processing corpora document after document in a memory independent fashion Within this framework we implement several popular algorithms for topical inference including Latent Semantic Analysis and Latent Dirichlet Allocation in a way that makes them completely independent of the training corpus size Particular emphasis is placed on straightforward and intuitive framework design so that modifications and extensions of the methods and or their application by interested 
practitioners are effortless We demonstrate the usefulness of our approach on a real world scenario of computing document similarities within an existing digital library DML CZ 1 Introduction Controlling complexity is the essence of computer programming Brian Kernighan Kernighan and Plauger 1976 The Vector Space Model VSM is a proven and powerful paradigm in NLP in which documents are represented as vectors in a high dimensional space The idea of representing text documents as vectors dates back to early 1970 s to the SMART system Salton et al 1975 The original concept has since then been criticised revised and improved on by a multitude of authors Wong and Raghavan 1984 Deerwester et al 1990 Papadimitriou et al 2000 and became a research field of its own These efforts seek to exploit both explicit and implicit document structure to answer queries about document similarity and textual relatedness Connected to this goal is the field of topical modelling see e g Steyvers and Griffiths 2007 for a recent review 
of this field The idea behind topical modelling is that texts in natural languages can be expressed in terms of a limited number of underlying concepts or topics a process which both improves efficiency new representation takes up less space and eliminates noise transformation into topics can be viewed as noise reduction A topical search for related documents is orthogonal to the more well known fulltext search which would match particular words possibly combined through boolean operators Research on topical models has recently picked up pace especially in the field of generative topic models such as Latent Dirichlet Allocation Blei et al 2003 their hierarchical extensions Teh et al 2006 topic quality assessment and visualisation Chang et al 2009 Blei and Lafferty 2009 In fact it is our observation that the research has rather gotten ahead of applications the interested public is only just catching up with Latent Semantic Analysis a method which is now more than 20 years old Deerwester et al 1990 We 
attribute reasons for this gap between research and practice partly to inherent mathematical complexity of the inference algorithms partly to high computational demands of most methods and partly to the lack of a sandbox environment which would enable practitioners to apply the methods to their particular problem on real data in an easy and hasslefree manner The research community has recognised these challenges and a lot of work has been done in the area of accessible NLP toolkits in the past couple of years our contribution here is one such step in the direction of closing the gap between academia and ready to use software packages1 Existing Systems The goal of this paper is somewhat orthogonal to much of the previous work in this area As an example of another possible direction of applied research we cite Elsayed et al 2008 While their work focuses on how to compute pair wise document similarities from individual document representations in a scalable way using Apache Hadoop and clusters of computers our 
work here is concerned with how to scalably compute these document representations in the first place Although both steps are necessary for a complete document similarity pipeline the scope of this paper is limited to constructing topical representations not answering similarity queries There exist several mature toolkits which deal with Vector Space Modelling These include NLTK Bird and Loper 2004 Apache s UIMA and ClearTK Ogren et al 2008 Weka Frank et al 2005 OpenNLP Baldridge et al 2002 Mallet McCallum 2002 MDP Zito et al 2008 Nieme Maes 2009 Gate Cunningham 2002 Orange Dem sar et al 2004 and many others These packages generally do a very good job at their intended purpose however from our point of view they also suffer from one or more of the following shortcomings No topical modelling Packages commonly offer supervised learning functionality i e classification topic inference is an unsupervised task Interest in the field of document similarity can also be seen from the significant number of requests 
for a VSM software package which periodically crop up in various NLP mailing lists Another indicator of interest are tutorials aimed at business applications see web search results for SEO myths and LSI for an interesting treatment on Latent Semantic Indexing marketing 1 Models do not scale Package requires that the whole corpus be present in memory before the inference of topics takes place usually in the form of a sparse termdocument matrix Target domain not NLP IR The package was created with physics neuroscience image processing etc in mind This is reflected in the choice of terminology as well as emphasis on different parts of the processing pipeline The Grand Unified Framework The package covers a broad range of algorithms approaches and use case scenarios resulting in complex interfaces and dependencies From the user s perspective this is very desirable and convenient From the developer s perspective this is often a nightmare tracking code logic requires major effort and interface modifications 
quickly cascade into a large set of changes In fact we suspect that the last point is also the reason why there are so many packages in the first place For a developer as opposed to a user the entry level learning curve is so steep that it is often simpler to roll your own package rather than delve into intricacies of an existing proven one Core interfaces As mentioned earlier the core concept of our framework is document streaming A corpus is represented as a sequence of documents and at no point is there a need for the whole corpus to be stored in memory This feature is not an afterthought on lazy evaluation but rather a core requirement for our application and as such reflected in the package philosophy To ensure transparent ease of use we define corpus to be any iterable returning documents for document in corpus pass In turn a document is a sparse vector representation of its constituent fields such as terms or topics again realised as a simple iterable 2 for fieldId fieldValue in document pass This is 
a deceptively simple interface while a corpus is allowed to be something as simple as corpus 1 0 8 8 0 6 2 System Design Write programs that do one thing and do it well Write programs to work together Write programs to handle text streams because that is a universal interface Doug McIlroy McIlroy et al 1978 Our choices in designing the proposed framework are a reflection of these perceived shortcomings They can be explicitly summarised into Corpus size independence We want the package to be able to detect topics based on corpora which are larger than the available RAM in accordance with the current trends in NLP see e g Kilgarriff and Grefenstette 2003 Intuitive API We wish to minimise the number of method names and interfaces that need to be memorised in order to use the package The terminology is NLPcentric Easy deployment The package should work out of thebox on all major platforms even without root privileges and without any system wide installations Cover popular algorithms We seek to provide novel 
scalable implementations of algorithms such as TF IDF Latent Semantic Analysis Random Projections or Latent Dirichlet Allocation We chose Python as the programming language mainly because of its straightforward compact syntax multiplatform nature and ease of deployment Python is also suitable for handling strings and boasts a fast high quality library for numerical computing numpy which we use extensively this streaming interface also subsumes loading storing matrices from to disk e g in the Matrix Market Boisvert et al 1996 or SVMlight Joachims 1999 format and allows for constructing more complex real world IR scenarios as we will show later Note the lack of package specific keywords required method names base class inheritance etc This is in accordance with our main selling points ease of use and data scalability Needless to say both corpora and documents are not restricted to these interfaces in addition to supporting iteration they may and usually do contain additional methods and attributes such as 
internal document ids means of visualisation document class tags and whatever else is needed for a particular application The second core interface are transformations Where a corpus represents data transformation represents the process of translating documents from one vector space into another such as from a TF IDF space into an LSA space Realization in Python is through the dictionary mapping notation and is again quite intuitive from gensim models import LsiModel lsi LsiModel corpus numTopics 2 lsi new_document 0 0 197 1 0 056 from gensim models import LdaModel lda LdaModel corpus numTopics 2 lda new_document 0 1 0 2 In terms of the underlying VSM which is essentially a sparse field document matrix this interface effectively abstracts away from both the number of documents and the number of fields We note however that the abstraction focus is on the number of documents not fields The number of terms and or topics is usually carefully chosen with unwanted token types removed via document frequency 
thresholds and stoplists The hypothetical use case of introducing new fields in a streaming fashion doesn t come up as often in NLP 2 1 Novel Implementations While an intuitive interface is important for software adoption it is of course rather trivial and useless in itself We have therefore implemented some of the popular VSM methods two of which we will describe here in greater detail Latent Semantic Analysis LSA Developed in late 80 s in Bell Laboratories Deerwester et al 1990 this method gained popularity due to its solid theoretical background and efficient inference of topics The method exploits cooccurrence between terms to project documents into a lowdimensional space Inference is done using linear algebra routines for truncated Singular Value Decomposition SVD on the sparse term document matrix which is usually first weighted by some TF IDF scheme Once the SVD has been completed it can be used to project new documents into the latent space in a process called folding in Since linear algebra routines 
have always been the front runner of numerical computing see e g Press et al 1992 some highly optimised packages for sparse SVD exist For example PROPACK and SVDPACK are both based on the Lanczos algorithm with smart reorthogonalizations and both are written in FORTRAN the latter also has a Clanguage port called SVDLIBC Lightning fast as they are adapting the FORTRAN code is rather tricky once we hit the memory limit for representing sparse matrices directly in memory For this and other reasons research has gradually turned to incremental algorithms for computing SVD in which the matrix is presented sequentially an approach equivalent to our document streaming This problem reformulation is not trivial and only recently have there appeared practical algorithms for incremental SVD Within our framework we have implemented Gorrell s Generalised Hebbian Algorithm Gorrell 2006 a stochastic method for incremental SVD However this algorithm proved much too slow in practice and we also found its internal parameters 
hard to tune resulting in convergence issues We have therefore also implemented Brand s algorithm for fast incremental SVD updates Brand 2006 This algorithm is much faster and contains no internal parameters to tune3 To our knowledge our pure Python numpy implementation is the only publicly available implementation of LSA that does not require the term document matrix to be stored in memory and is therefore independent of the corpus size4 Together with our straightforward document streaming interface this in itself is a powerful addition to the set of publicly available NLP tools Latent Dirichlet Allocation LDA LDA is another topic modelling technique based on the bag of words paradigm and word document counts Blei et al 2003 Unlike Latent Semantic Analysis LDA is a fully generative model This algorithm actually comes from the field of image processing rather than NLP Singular Value Decomposition which is at the heart of LSA is a universal data compression noise reduction technique and has been successfully 
applied to many application domains 4 This includes completely ignoring the right singular vectors during SVD computations as the left vectors together with singular values are enough to determine the latent space projection for new documents 3 where documents are assumed to have been generated according to a per document topic distribution with a Dirichlet prior and per topic word distribution In practice the goal is of course not generating random documents through these distributions but rather inferring the distributions from observed documents This can be accomplished by variational Bayes approximations Blei et al 2003 or by Gibbs sampling Griffiths and Steyvers 2004 Both of these approaches are incremental in their spirit so that our implementation again in pure Python with numpy and again the only of its kind that we know of only had to abstract away from the original notations and implicit corpus size allocations to be made truly memory independent Once the distributions have been obtained it is 
possible to assign topics to new unseen documents through our transformation interface 2 2 Deployment The framework is heavily documented and can be accessed at http nlp fi muni cz projekty gensim This website contains sections which describe the framework and provide usage tutorials as well as sections on download and installation instructions The framework is open sourced and distributed under an OSI approved LGPL license 3 Application of the Framework An idea that is developed and put into action is more important than an idea that exists only as an idea Hindu Prince Gautama Siddharta the founder of Buddhism 3 1 Motivation Many digital libraries today start to offer browsing features based on pairwise document content similarity For collections having hundreds of thousands documents computation of similarity scores is a challenge Elsayed et al 2008 We have faced this task during the project of The Digital Mathematics Library DML CZ Sojka 2009 The emphasis was not on developing new IR methods for this task 
although some modifications were obviously necessary such as answering the question of what constitutes a token which differs between mathematics and the more common English ASCII texts With the collection s growth and a steady feed of new papers lack of scalability appeared to be the main issue This drove us to develop our new document similarity framework 3 2 Data As of today the corpus contains over 61 293 fulltext documents for a total of about 270 million tokens There are mathematical papers from the Czech Digital Mathematics Library DML CZ http dml cz 22 991 papers from the NUMDAM repository http numdam org 17 636 papers and from the math part of arXiv http arxiv org archive math 20 666 papers After filtering out word types that either appear less than five times in the corpus mostly OCR errors or in more than one half of the documents stop words we are left with 315 167 distinct word types Although this is by no means an exceptionally big corpus it already prohibits storing the sparse term document 
matrices in main memory ruling out most available VSM software systems 3 3 Results We have tried several VSM approaches to representing documents as vectors term weighting by TF IDF Latent Semantic Analysis Random Projections and Latent Dirichlet Allocation In all cases we used the cosine measure to assess document similarity When evaluating data scalability one of our two main design goals together with ease of use we note memory usage is now dominated by the transformation models themselves These in turn depend on the vocabulary size and the number of topics but not on the training corpus size With 315 167 word types and 200 latent topics both LSA and LDA models take up about 480 MB of RAM Although evaluation of the quality of the obtained similarities is not the subject of this paper it is of course of utmost practical importance Here we note that it is notoriously hard to evaluate the quality as even the preferences of different types of similarity are subjective match of main topic or subdomain or 
specific wording plagiarism and depends on the motivation of the reader For this reason we have decided to present all the computed similarities to our library users at once see e g http dml cz handle 10338 dmlcz 100785 SimilarArticles At the present time we are gathering feedback from mathematicians on these results and it is worth noting that the framework proposed in this paper makes such side by side comparison of methods straightforward and feasible ematical Library as well as further improving the range efficiency and scalability of popular topic modelling methods Acknowledgments We acknowledge the support of grant MUNI E 0084 2009 of the Rector of Masaryk University program for PhD students research We would also like to thank the anonymous reviewer for providing us with additional pointers and valuable comments 5 References 4 Conclusion We believe that our framework makes an important step in the direction of current trends in Natural Language Processing and fills a practical gap in existing software 
systems We have argued that the common practice where each novel topical algorithm gets implemented from scratch often inventing unfortunately yet another I O format for its data in the process is undesirable We have analysed the reasons for this practice and hypothesised that this partly due to the steep API learning curve of existing IR frameworks Our framework makes a conscious effort to make parsing processing and transforming corpora into vector spaces as intuitive as possible It is platform independent and requires no compilation or installations past Python numpy As an added bonus the package provides ready implementations of some of the popular IR algorithms such as Latent Semantic Analysis and Latent Dirichlet Allocation These are novel pure Python implementations that make use of modern stateof the art iterative algorithms This enables them to work over practically unlimited corpora which no longer need to fit in RAM We believe this package is useful to topic modelling experts in implementing new 
algorithms as well as to the general NLP community who is eager to try out these algorithms but who often finds the task of translating the original implementations not to say the original articles to its needs quite daunting Future work will include comparison of the usefulness of different topical models to the users of our Digital Math J Baldridge T Morton and G Bierner 2002 The OpenNLP maximum entropy package Technical report http maxent sourceforge net Steven Bird and Edward Loper 2004 NLTK The Natural Language Toolkit Proceedings of the ACL demonstration session pages the European Chapter of the Association for Computational Linguistics EACL Trento Italy pages processing framework Frontiers in Neuroinformatics 2 http mdp toolkit sourceforge net 
19	a	ICASSP 2010 Dallas TX LANGUAGE MODEL ADAPTATION USING WWW DOCUMENTS OBTAINED BY UTTERANCE BASED QUERIES Andreas Tsiartas Panayiotis Georgiou and Shrikanth Narayanan Speech Analysis and Interpretation Laboratory Department of Electrical Engineering University of Southern California Los Angeles CA 90089 tsiartas usc edu georgiou sipi usc edu shri sipi usc edu ABSTRACT In this paper we consider the estimation of topic for extracting keywords from the ASR hypothesis transcripts include that proposed by Lecorve et al 3 In spite of these approaches being unsupervised they heavily depend on the quality of the ASR hypothesis transcripts Also such approaches are not appealing for real time systems since downloading documents and re estimating the LMs can be computationally expensive and time consuming While the efforts described above assume that no prior topic information is available various other methods have been proposed to generate queries from a training text using language 978 1 4244 4296 6 10 25 5406 ICASSP 
2010 language model exists For instance Wan et al 10 proposed two methods for selecting topic 1 If T is odd one of the two sets will have one more element than the other set Similarly we estimate the distribution of the utterances in T2 and we denote the probability of an event u in T2 as P u T2 Likewise we estimate the distribution of the utterances in Q and we denote the probability of an event u in Q as P u Q In the second step we rank all utterances u T2 in descending order according to the value given by D u Q T1 which is 5407 MFCCs and energy along 0 4 Relative Frequency one utterance per line Also 0 35 0 3 0 25 0 2 0 15 0 1 0 05 Proposed utterance based Keyword based 2 2 1 Keyword based 6 2 0 0 2 4 6 8 10 Log10 scaled number of documents available per query Fig 2 The normalized histogram of the log10 scaled number of documents available per query This histogram does not include samples when zero documents were returned The end goal of the query retrieval process is to obtain the required data but also 
the most relevant data therefore the appropriate queries can 5408 0 7 Relative Frequency 0 6 0 5 0 4 0 3 0 2 0 1 0 0 Proposed utterance based Keyword based 2 2 1 Keyword based 6 2 ics as explained in section 2 2 In addition the performance boost comes from the fact that utterance based queries return documents that are closer to the domain of interest 6 CONCLUSION In this paper we have introduced a novel method for generating high quality in domain queries that do not require any language 7 REFERENCES Log10 scaled number of words per document 2 4 6 8 Fig 3 The normalized histogram of the number of words per document in log 10 scale of more topic 35 34 5 34 33 5 Proposed utterance based Keyword based 2 2 1 Keyword based 6 2 33 32 5 32 31 5 31 0 50 100 150 Number of words in millions used in LM training Fig 4 WER against the number of words used to train the LM 5 2 Results The ASR performance as a function of the number of the words used in training the Web LM is shown 1 Berger A and Miller R Just in time 
language modelling in Proceedings of the IEEE International Conference on Acoustics Speech and Signal Processing Seattle WA USA 1998 vol II pp WER 5409 
20	a	Data Min Knowl Disc 2010 21 Using interesting sequences to interactively build Hidden Markov Models Szymon Jaroszewicz Received 20 March 2009 Accepted 6 March 2010 Published online 7 April 2010 The Author s 2010 Abstract The paper presents a method of interactive construction of global Hidden Markov Models HMMs based on local sequence patterns discovered in data The method is based on finding interesting sequences whose frequency in the database differs from that predicted by the model The patterns are then presented to the user who updates the model using their intelligence and their understanding of the modelled domain It is demonstrated that such an approach leads to more understandable models than automated approaches Two variants of the problem are considered mining patterns occurring only at the beginning of sequences and mining patterns occurring at any position both practically meaningful For each variant algorithms have been developed allowing for efficient discovery of all sequences with given 
minimum interestingness Applications to modelling webpage visitors behavior and to modelling protein secondary structure are presented validating the proposed approach Keywords Interesting 1 Introduction Sequence databases are at the heart of several important applications such as Web Mining bioinformatics or speech analysis and recognition Mining frequent sequences is an important approach to analysis of such data Unfortunately as is the case with most frequent pattern based approaches sequence mining typically produces thousands of frequent sequences which are very difficult for the user to analyze due not only to Responsible editor Johannes 123 Using interesting sequences to interactively build Hidden Markov Models 187 their quantity but also to redundancies between them This paper addresses the problem by incorporating an explicit global model of background knowledge into the sequence mining problem The model helps identify truly interesting patterns taking into account what is already known about the 
data The user provides a possibly empty description of their current knowledge about the domain and a database from which new knowledge is to be discovered Knowledge description has the form of a global probabilistic model from which precise probabilistic inferences can be made In Jaroszewicz and Simovici 2004 Jaroszewicz and Scheffer 2005 and Jaroszewicz et al 2009 a Bayesian network was used for this purpose due to its flexibility understandability as well as the fact that it represents a full joint probability distribution making inference possible In the case of sequence data similar advantages are shared by Hidden Markov Models HMMs Rabiner 1989 Welch 2003 and for that reason they have been used to model background knowledge in the presented approach Based on the model and the data interesting patterns are discovered A pattern is defined to be interesting if its frequency in the data differs significantly from that predicted by the global model The patterns are then presented to the user whose task is 
to interpret them and update the model The new updated model is then used again together with the data to find a new set of interesting patterns The procedure is repeated several times until the desired quality of the global model has been reached The approach to mining interesting sequences presented in this paper follows the same cycle First sequences are discovered whose frequency in a database differs significantly from the predictions of the HMM Such sequences are considered interesting and are shown to the user who updates the HMM by adding more hidden states representing new underlying behavior by modifying lists of possible output symbols in existing states or by adding new transitions between states The HMM parameters are then retrained using the Baum Welch algorithm a variant of the Expectation Maximization approach and the process is repeated In Jaroszewicz et al 2009 it has been demonstrated that interactive model construction using human intelligence in the process gives models which represent 
the domain much better than models built using fully automated methods Similar conclusions have been reached in this paper The advantage of this approach is that the new states added to the HMMs have clear user defined meaning The lists of symbols emitted in each state are also much shorter and more coherent The resulting model is thus understandable and all hidden states have clear interpretations As our experiments have shown this is usually not possible with automatic methods An additional benefit is that the Baum Welch algorithm turned out to work much better with hand built models resulting in much faster convergence and better avoidance of local optima A drawback of the proposed method is that it requires intensive human involvement and sometimes a significant effort is needed to explain the discovered interesting patterns However it is argued that such an approach is usually necessary if meaningful internal structure is to be obtained The approach has been tested on the web server log of the National 
Institute of Telecommunications in Warsaw author s employer The application proved that the proposed approach is highly practical and produces accurate models which are 123 188 S Jaroszewicz understandable and easy to interpret Another application which is presented in this paper is about modelling protein secondary structure using publicly available data 2 Related research There has been a significant amount of work on mining frequent patterns in sequence data full discussion is beyond the scope of this work see for example thorough overviews in Laxman and Sastry 2006 and in Han et al 2007 An idea of incorporating background knowledge defined as a formal model giving well defined predictions into the pattern discovery process has already been suggested in Hand 2002 The paper presented a high level framework of which the current approach and that in Jaroszewicz and Simovici 2004 Jaroszewicz and Scheffer 2005 Jaroszewicz et al 2009 can be considered a specific case The paper Hand 2002 is very general and does 
not give specific details on how the background knowledge should be represented or what discovery methods should be used Section 5 of Laxman and Sastry 2006 describes approaches to testing significance of sequence patterns based on comparing with a background model The purpose however is to test statistical significance and the models are thus simple based on independence assumption and fixed throughout the discovery process user s background knowledge and intervention do not come into the picture as they do in our approach In Laxman and Sastry 2005 a separate small HMM with special structure is built for each temporal pattern episode Such small HMMs are later used for significance testing The use of a global model being a mixture of small episode models is alluded to but not developed further In Prum et al 1995 a similar approach has been used to derive expected probabilities of DNA sequences Zaiane and his collegues Zaiane et al 2007 Satsangi and Zaiane 2007 have worked on discovering so called contrast 
sequences This is similar to a single stage of the interactive model building process described here except that two datasets are compared while in our approach one of the datasets is replaced by an HMM As a result Zaiane et al s overall methodology and the discovery algorithms are very different from methods proposed in this paper There has been some related work in the fields of biological sequence modelling and speech recognition Low Kam et al 2009 where a set of unexpected patterns is found whose probability as predicted by an HMM is low The idea however is more in line with Laxman and Sastry 2006 than with the approach proposed here No explicit model building is considered and in fact the exact structure of the model used is not described presumably it is just a simple Markov model over the symbols The model does not change during the discovery process Furthermore contrary to our approach stationarity assumption is made which is not suitable for shorter sequences In Spiliopoulou 1999 and Li et al 2007 
background knowledge is also incorporated into the sequence mining process However it is stored as a set of rules and there is no global probabilistic model Unexpectedness is defined in a syntactic rather than probabilistic fashion See Jaroszewicz and Simovici 2004 and Jaroszewicz et al 2009 for a discussion of drawbacks of rule based representations and advantages of having a unified model 123 Using interesting sequences to interactively build Hidden Markov Models 189 A somewhat related technique is adaptive HMM learning Huo et al 1995 Lee and Gauvin 1996 Huo and Lee 1997 The approach is essentially based on placing a prior on the HMM parameters The prior is usually obtained using the Empirical Bayes procedure Huo et al 1995 Robbins 1956 This allows for example for tuning a speech recognition system to a new speaker using a prior based on parameters obtained for several other speakers While there are similarities to our approach e g the prior plays the role of background knowledge the method is 
fundamentally different There is no notion of an interesting pattern model parameters are updated based on new data only The update is meant to adapt the model to idiosyncrasies of new data while using older data as a prior not to build a single general model In our approach there is a single dataset which remains static it is the interesting patterns used to update the model which change from iteration to iteration Also contrary to our approach in adaptive HMM learning the update is done automatically and only to model s parameters not its structure Model s understandability is not an explicit goal of adaptive HMM learning Another related technique is semisupervised learning of HMMs Inoue and Ueda 2003 Ji et al 2009 where labeled examples may be viewed as user provided guides modifying the model built on unlabeled data The nature of the process is clearly different from the proposed approach This work is based on an earlier paper Jaroszewicz 2008 by the author but has been significantly expanded All parts 
related to mining interesting sequences starting at arbitrary point in time are new So is the application to modelling protein secondary structure Comparison with automatically built models has also been significantly expanded 3 Definitions and notation Let us begin by describing the mathematical notation used Vectors are denoted by lowercase boldface Greek letters and matrices with uppercase boldface roman letters All vectors are assumed to be row vectors explicit transposition is used for column vectors Superscripts are used to denote time or more precisely position counting from the beginning of a database sequence and subscripts to denote elements of vectors t denotes i th element of the vector at time t Since matrices will not change e g i with time in our applications superscripts on matrices will denote matrix power e g P t is the matrix P raised to the power t parentheses around the matrix are added to further avoid confusion Let us fix a set of symbols O o1 om All sequences will be constructed over 
this set It will also play the role of the set of output symbols of all HMMs Sequences of symbols will be denoted with bold lowercase letters s and d where s will denote temporary sequences being worked on and d sequences in the database s denotes the length of sequence s Following our convention st is the symbol of s at time t or more precisely the symbol at distance t from the beginning of the sequence with the first symbol being at index 0 Further if o O then so will denote a sequence 123 190 S Jaroszewicz obtained by appending o to s and os the sequence obtained by prepending o at the beginning of s An empty sequence will be denoted with 3 1 Hidden Markov Models Let us now describe HMMs which will be used to represent knowledge about systems with discrete time Only most important facts will be given full details can be found in literature Rabiner 1989 Welch 2003 A HMM is a modification of a Markov Chain such that the state the model is in is not directly visible Instead every state emits output symbols 
according to its own probability distribution For example while modelling website visitors behavior the internal states could correspond to visitor s intentions e g wants to find a specific content and output symbols to the pages he she actually visits Formally an HMM is a quintuple Q O 0 P E where Q q1 qn is 0 a set of states O o1 om the set of output symbols 0 0 1 n a vector of initial probabilities for each state and P an n by n transition matrix where Pi j is the probability of a transition occurring from state qi to state q j Finally E is an n by m emission probability matrix such that Ei j gives the probability of observing an output symbol o j provided that the HMM is in state qi Where convenient we will use shorthand notation Eio to denote the probability of emitting symbol o in state qi Notice that the vector t of state probabilities at time t can be computed as t 0 P t Similarly E is the vector of probabilities of observing each symbol provided that is the vector of state probabilities We will now 
discuss how to determine the probability that a given sequence of output symbols is produced by an HMM For that aim a key concept of forward probabilities will be introduced full details can be found in Rabiner 1989 and Welch 2003 Let s be a sequence of symbols t s Further let z t denote the state the HMM is in at time t The forward probabilities of a sequence s in the HMM are defined as s i Pr u 0 s0 u 1 s1 u t 1 st 1 z t qi where u 0 u 1 u t 1 is the sequence of symbols output by the HMM at times 0 1 t 1 respectively and s0 s1 st 1 are symbols at respective positions in the sequence s Simply speaking this is the probability that the HMM has emitted the sequence s and ended up in state qi at time t that is after emitting the last symbol of s Grouping the forward probabilities for all ending states we obtain a vector s s 1 s 2 s n The probability that the HMM emits a sequence s starting at t 0 can be computed by summing the elements of s An important property of the probabilities is that they can be 
efficiently computed using dynamic programming by extending the sequence symbol by symbol using the following formula 123 Using interesting sequences to interactively build Hidden Markov Models 0 i i n 191 so i j 0 s j E jo P ji 1 Another important problem related to HMMs is estimating their parameters starting transition and emission probabilities based on a given sequence database This is usually achieved using the Baum Welch algorithm which is a variant of the Expectation Maximization method The details can be found in Rabiner 1989 and Welch 2003 and will be omitted here The Baum Welch algorithm only guarantees convergence to a local minimum and has been reported to be slow In practice we have noticed the algorithm is very dependent on the emission probabilities matrix E If output symbols convey a reasonable amount of information about internal states the algorithm converges quickly and reliably if on the other hand this information is highly ambiguous the process is slow and often ends up in a local 
optimum This issue is discussed in detail and illustrated by experiments in later sections where it will be argued that estimating parameters of interactively built HMMs is significantly easier than for fully connected ones 4 Discovery of interesting sequences starting at the beginning of database sequences We begin with a more detailed description of how background knowledge is being represented As mentioned above user s knowledge about the problem is encoded using a HMM However the model will usually be much sparser than fully connected models used in most HMM applications The user needs to provide the hidden states of the model typically new states are added incrementally during the discovery process For every hidden state the user needs to specify possible transitions symbols which can be emitted from that state and whether the state can be an initial state i e can have a nonzero starting probability The user thus provides the structure of the model but not the starting transition and emission 
probabilities which will be estimated from data using the Baum Welch algorithm The key component of the interactive global HMM construction using the proposed framework is finding interesting sequences of output symbols There are several ways to formulate the concept of an interesting sequence This paper will provide two such formulations In this section we will present an approach which assumes that mined sequences are prefixes of database sequences i e they start at t 0 and in the following section we will extend the approach to sequences starting at arbitrary locations within database sequences Both approaches have important practical applications examples of which will be shown in later sections Let the provided database D contain N sequences of symbols each starting at t 0 D d1 d N 123 192 S Jaroszewicz Let s be a sequence of symbols also starting at time t 0 Denote by Pr HMM s the probability that the sequence s is generated by HMM starting at time t 0 Analogously the probability of observing that 
sequence in data is defined as Pr D s d D s is a prefix of d D that is as the percentage of sequences in the database of which s is a prefix A sequence whose probability of occurrence is greater than or equal to is called frequent The interestingness of s is defined analogously to Jaroszewicz and Simovici 2004 as I s Pr D s Pr HMM s 2 that is as the absolute difference between the probabilities predicted by the HMM and observed in data A sequence is called interesting if its interestingness is not lower than It is easy to see that the following observation holds Observation 1 If I s then either Pr D s or Pr HMM s That is for a sequence to be interesting it is necessary for it to be frequent either in data or in the HMM This observation motivates an algorithm for finding all interesting symbol sequences starting at t 0 for a user specified minimum interestingness threshold The algorithm is shown in Fig 1 its key steps are described in detail in the following paragraphs 4 1 Finding frequent sequences in data 
There are several algorithms for finding frequent sequences in data Han et al 2007 The situation analyzed in this section is much simpler however since all sequences Fig 1 The algorithm for finding all interesting sequences starting at t 0 123 Using interesting sequences to interactively build Hidden Markov Models 193 Fig 2 An algorithm for finding all frequent sequences in a HMM are assumed to start at t 0 A significantly less complicated approach is therefore used First all sequences in D are sorted in lexicographical order and scanned sequentially When scanning the i th sequence di the longest common prefix p of di and di 1 is computed Counts of all prefixes of p are incremented by 1 All prefixes of di 1 which are longer than p will never appear again because of the lexicographical scanning order so those whose support is lower than are removed and those whose support is greater than or equal to are added to the result set The scanning process then moves on to the next record 4 2 Finding frequently 
occurring sequences in a HMM A more interesting problem is to find all sequences which are emitted by an HMM with probability greater or equal to a given threshold This part has been implemented in the style of a depth first version of the well known Apriori algorithm Agrawal et al 1993 The sequences are built symbol by symbol beginning with the empty sequence We use the fact that appending additional symbols to a sequence cannot increase its probability of occurrence so if a sequence is found to be infrequent all sequences derived from it can be skipped To efficiently compute the probability of each sequence being emitted by the HMM we use the forward probabilities which can be efficiently updated using Eq 1 The updating is performed simultaneously with appending symbols to sequences and the forward probabilities are simply passed down the search tree The algorithm is shown in Fig 2 The approach is very efficient since computing probabilities in HMMs can be done much more efficiently than computing supports 
in large datasets 5 Discovery of interesting sequences starting at arbitrary time In this section we present a modification of the above approach to mining patterns which can start at any point in time not necessarily at t 0 For this to be possible some of the definitions introduced above need to be modified In order to avoid 123 194 S Jaroszewicz unnecessarily complicated notation this section will intentionally redefine some of the symbols used before The definitions of support in data and in the HMM become somewhat more involved since they need to account for the fact that each sequence may appear at several moments in time First we need to define the support of s in a single database sequence d suppd s 0 t d s s0 dt s1 dt 1 N supp D s i 1 suppdi s that is as the number of occurrences of s in the whole database Let Pr HMM t s denote the probability that the HMM emits the sequence s with s0 emitted at time t Define the support of a sequence s with time horizon T as suppHMM T s T s t 0 Pr HMM t s that is as 
the expected number of occurrences of the sequence s in the first T symbols emitted by the HMM Notice that this quantity can indeed be interpreted as statistical expectation since expectation of a sum is a sum of expectations even for dependent random variables Since the support of a sequence in an HMM will be compared with its support in data we introduce the following definition Definition 1 The support of sequence s in a HMM with respect to a database D d1 d N is defined as supp HMM D N s i 1 suppHMM di s Intuitively this is the expected support of s in data if the data were generated from the HMM The sum in the formula comes from the fact that in order to generate D the model HMM would have been used N times to generate N sequences each of length equal to the corresponding sequence in D The expected support of s in i th sequence in D would then be equal to suppHMM di s To define the interestingness of a sequence s we need to compare its support in the database D with analogous support computed based on 
the HMM s expectations I s supp D s suppHMM D s N i 1 di 3 123 Using interesting sequences to interactively build Hidden Markov Models 195 When the database contains only a single sequence D d the definition simplifies to I s suppd s suppHMM d s d Let us now make an observation which will allow us to find all sequences with given minimum interestingness threshold Observation 2 If I s then either N supp D s i 1 di or suppHMM D s N di i 1 This motivates an algorithm for finding all interesting sequences beginning at any point in time which is practically identical to the one in Fig 1 with the only exception that the minimum support thresholds used are iN 1 di instead of just The details have thus been omitted The algorithm does however require finding sequences starting at arbitrary point in time which are frequent in data and frequent according to the HMM Those tasks differ significantly from the case of mining interesting sequences beginning at t 0 Finding frequent sequences in data is a well researched 
problem many algorithms are available The description of those methods is beyond the scope of this paper see e g Laxman and Sastry 2006 or Han et al 2007 for an overview The modification to accommodate mining in more than one database sequence at a time is easily incorporated into those approaches We used a depth first version of the sequence oriented variant of the Apriori algorithm with an extra optimization that a list of locations at which a sequence was found is reused when counting support of its supersequences We will now focus on the more interesting problem of mining frequent sequences starting at any time point in HMMs As the development will be fairly long it has been placed in a separate section 6 Finding frequent sequences in HMMs Before we begin describing the mining algorithm we need to introduce another important concept related to HMMs namely that of backward probabilities Rabiner 1989 Welch 2003 Let s be a sequence of output symbols the backward probabilities of s are defined as s t i s i 
Pr u t s0 u t 1 s1 u t s 1 s s 1 z t qi 123 196 S Jaroszewicz where u t u t 1 u t s 1 is the sequence of symbols output by the HMM at times t t 1 t s 1 respectively s0 s1 st 1 are symbols at respective positions in the sequence s and z t is the state the HMM is in at time t Intuitively this is the probability that starting at time t the model will emit the sequence s given that at time t it was in state qi As will soon become apparent those probabilities do not depend on t so the index can be dropped Denote s s 1 s 2 s n Similarly to forward probabilities backward probabilities can be efficiently updated using dynamic programming i 1 n os i Eio j 1 Pi j s j 4 The name of those quantities comes from the fact the probabilities are updated backwards beginning with the last emitted symbol Note also that as mentioned before the above formulas do not depend on the starting time t This property is crucial for computing supports of sequences since the same backward probabilities of a sequence can be reused to 
compute its support at any point in time More details on backward probabilities can be found in Rabiner 1989 or Welch 2003 We are now ready to discuss issues related to the main topic of this section To effectively mine frequent sequences in an HMM we need to satisfy two conditions monotonicity property of support must hold and an efficient way of computing supports must be devised These two points are addressed below Theorem 1 Let s be a sequence o an output symbol D d1 d N a database of sequences and HMM a Hidden Markov Model Then suppHMM D s suppHMM D os Proof Notice first that Pr HMM t s Pr HMM t 1 os the probability of observing a sequence s at time t cannot be less than the probability of observing s at time t and observing the symbol o at time t 1 Using this fact and the definitions we have supp HMM D N s i 1 supp HMM di N di s s i 1 t 0 Pr HMM t s Pr HMM t 1 os N di s i 1 t 1 Pr HMM t s N di s i 1 t 1 N di s 1 i 1 t 0 Pr HMM t os suppHMM D os 123 Using interesting sequences to interactively build 
Hidden Markov Models 197 Notice that we are extending the sequence backwards since that is how the probabilities used in the algorithm are computed We will now address the issue of efficient computation of supports of sequences in an HMM Notice that the probability of observing a sequence s beginning at time t can be computed as Rabiner 1989 Welch 2003 Pr HMM t s t s where denotes vector transpose and t is the state probability distribution at time t Since the probabilities do not depend on t they need to be computed only once for each sequence and can then be reused at each time step Let us now rewrite the definition of support of a sequence in an HMM with respect to a database D using the above formula Denote N di s s i 1 t 0 t We now obtain the following equations suppHMM T s and suppHMM D s N i 1 T s t 0 Pr HMM t s T s t 0 t s T s t 0 t s suppHMM di s N di s i 1 t 0 t s s s 5 Notice that s depends only on the length of the sequence s and can thus be cached and computed only if a given sequence length has 
not been seen before Moreover a very efficient way to compute s will be presented below Equation 5 together with Theorem 1 motivate the frequent sequence mining algorithm presented in Fig 3 Computing s An obvious brute force way to compute s is to perform successive multiplications by the transition matrix to get values of t for all necessary values of t Time needed for computing s using this method can become significant for long sequences Indeed the time complexity is O n 2 T n iN 1 di where T maxi di is the time horizon and n the number of states in the model The first term comes from the computation of t vectors by matrix multiplications and the second is the time spent computing the actual sums A much faster approach is described below We will make use of the concept of diagonalizability van Loan and Golub 1996 Meyer 2001 A matrix P is diagonalizable if it can be written in the form P VLV 1 123 198 S Jaroszewicz Fig 3 An algorithm for mining sequences frequent in an HMM with respect to a database D 
where L is a diagonal matrix containing eigenvalues of P and V is a matrix whose columns are eigenvectors of P It is a well known and easy to prove fact that if the matrix P is diagonalizable then its powers can be computed using the following formula P t V L t V 1 Suppose that the transition matrix P is diagonalizable The equation N di s N di s N di s s i 1 t t 0 i 1 N di s i 1 t 0 0 P t 0 t 0 i 1 t 0 V L t V 1 0V L t V 1 shows that the problem can be reduced to computing another matrix L di s N L t Since the matrix L is diagonal so is the matrix L Moreover i 1 t 0 computing it can be performed separately for each element on the diagonal Let li denote the i th element on the diagonal of L and li the i th element on the diagonal of L We have N di s li i 1 t 0 li t N 1 li max 0 di s 1 i 1 1 li N i 1 max 0 di s 1 di s t N li i 1 t 0 if li 1 if li 1 if li 1 li 1 6 The equation is obtained by using formulas for partial sums of the geometric series The last sum is used for complex eigenvalues with module 1 The 
maxima are included to ensure that database sequences shorter than s are omitted from the sums Notice that since the transition matrix is the so called stochastic matrix the modules of all 123 Using interesting sequences to interactively build Hidden Markov Models 199 its eigenvalues are less than or equal to one Meyer 2001 so all possible cases are covered While it is easy to construct a non diagonalizable transition matrix transition matrices learnt from data were in our experiments always diagonalizable The justification is as follows The set of non diagonalizable matrices has measure zero so it is very unlikely for a random matrix not to be diagonalizable Uhlig 2001 It turns out that the transition matrices learned from real data are close enough to random to be almost always diagonalizable If however the transition matrix is not diagonalizable the implementation detects this and performs the calculations using brute force approach Sufficient conditions for diagonalizability of a matrix are that all its 
eigenvalues be distinct or that its eigenvectors be linearly independent van Loan and Golub 1996 Meyer 2001 Uhlig 2001 The computation of eigenvalues and the inversion of the V matrix can be performed once and reused for all s When the matrix is diagonalizable the time of computing s is O n 3 n N where the n 3 part n is the number of states in the HMM comes from matrix multiplication inversion and eigenvalue decomposition and the n N part comes from the computations performed in Eq 6 for each eigenvalue An exception is the case when P has complex eigenvalues with module 1 when the time complexity may become O n 3 n iN 1 di This case however seems to only occur for specially crafted transition matrices and we never encountered it in our experiments In any case the time complexity is still better than for the brute force approach as long as n T quite a reasonable assumption Notice that except the unlikely case of complex eigenvalues with module 1 the computation time needed to count the support of a given 
sequence does not depend on the length of sequences in the database As a consequence the algorithm for mining frequent sequences in an HMM will not depend on the length of sequences only on the number of states in the HMM and weakly on the number of sequences Sequences of arbitrary length can thus be mined with ease 7 Experimental evaluation mining web server logs We will first consider the method for mining interesting sequences starting at time t 0 The approach will be evaluated experimentally on web server log data Web log of the server of the National Institute of Telecommunications in Warsaw has been used Full data covered the period of about 1 year the first 100 000 events have been used for experiments The presented method has been applied to create a model of the behavior of visitors to the Institutes s webpage After starting with a simple initial model new internal states were added with the assumption that the new states represent underlying patterns of user behavior such as access to e mail 
account through the web interface or access a paper in the Institutes s journal The identification of the underlying behavior and model updating was done entirely by the user Recall that the user specifies the structure of the model that is the states possible transitions between those states and symbols which can be emitted in each state Actual values of all probabilities are found automatically using the Baum Welch algorithm 123 200 S Jaroszewicz 7 1 Data preprocessing Each record in a weblog contains a description of a single event such as a file being retrieved Several items such as date and time of the event the referenced file an error code etc are recorded The required data format is however a database of sequences each sequence corresponding to a single user session of pages visited by users Unfortunately the log does not contain explicit information about sessions and the data had to be sessionized in order to form training sequences There are several sessionizing methods Dunham 2003 Here the 
simplest approach has been used events originating from a single source which were 30 min apart were considered to belong to the same session Despite its simplicity the method worked very well Other methods e g involving a limit on total session time were problematic for example could not handle automated web crawlers sessions which were very long At the end of each training sequence an artificial _END_ symbol has been added such that an end of a session could be explicitly modelled Another choice that had to be made was the set of output symbols Since the server contains a very large number of available files assigning a separate symbol to every one of them would have introduced thousands of output symbols This would lead to enormous models adversely affecting understandability Also the analyst is typically more interested in the behavior of visitors at a higher level and cannot investigate each of the thousands of files separately On top of that probability estimation in case of rarely accessed files would 
be highly unreliable To solve this problem only the toplevel directory of each accessed file was used as an output symbol As the Institute s website s content is logically divided into subdirectories such as journal people etc such an approach gives a better overview of users behavior than specific files If finer level analysis is required it is probably better to build a separate model which covers only a subset of available pages in greater detail 7 2 An initial model As the author had no idea on how the initial model should look like an empty model given in Fig 4 was used The _all_ state can initially emit any of the symbols present in the log The quit state can emit only the artificial _END_ symbol The model corresponds to a user randomly navigating from page to page before ending the session As more states are added emitted symbols will be removed from the _all_ state This will allow for better identification of the internal state of the model based on the output symbol leading to better 
understandability as well as better and more efficient parameter estimation using the Baum Welch algorithm Fig 4 The initial HMM describing the behavior of webpage visitors 123 Using interesting sequences to interactively build Hidden Markov Models 201 7 3 Typical patterns of model updating We will now discuss two general patterns of model updates which will become useful while building the model Of course the list is by no means exhaustive there are very diverse possible underlying behaviors which can cause a pattern to become interesting identifying all of them is impossible However the following two patterns occur quite frequently as will be seen in the weblog model construction The first pattern occurs when a distribution of the number of successive visits to a given directory needs to be modelled This can be achieved by adding a chain of internal states each emitting only the symbol corresponding to that directory Nodes in the chain typically have edges to nodes outside the chain for example to the node 
denoting the end of a session Such a chain can exactly model the distribution of the number of visits not greater than its length The final state of a chain often has a self loop to model larger numbers of visits using the geometric distribution The actual transition probabilities will easily be found by the Baum Welch algorithm The chain pattern will be very frequently employed in the Web log data modelling below Sometimes several symbols are output repeatedly mixed with each other The actual order of the symbols is not important or interesting but their probability differs from model predictions resulting in patterns being output by the algorithm In such a case we can add a collection of fully connected hidden states each emitting one of the symbols After learning the weights the underlying behavior will be modelled well and patterns related to that group of output symbols will disappear allowing new possibly more interesting patterns to become visible This pattern will be employed below to model access to 
various elements of the main webpage such as images CSS stylesheets and JavaScript 7 4 Model construction We will now describe a few most interesting stages of model construction The first interesting sequences discovered were related to the Sophos antivirus program whose updates are available on the intranet The most interesting sequence was sophos sophos its probability in data was 11 48 while the initial model predicted it to be only 1 17 The second most interesting sequence was one in which the sophos directory has been accessed four times It is curious that every session contained either two or four or more accesses to this directory for over a year there has not been a single session where the directory would have been accessed only once or only three times The reason most probably lies in the internal behavior of the antivirus update software In order to better predict the probabilities of those sequences the model has been updated by adding new states shown in Fig 5 Each of the new states except _all_
sink emits only the sophos symbol In addition that symbol has been removed from the list of symbols emitted by the _all_ state As the new states emit only the symbol sophos which has been removed from remaining initial states any session which begins by accessing the antivirus directory has to pass through the leftmost node in Fig 5 It is thus clear that this part of the HMM 123 202 S Jaroszewicz Fig 5 The fragment of the HMM describing accesses to Sophos antivirus updates models only sessions accessing this section of the website Of course the probability of starting in the _all_ state will now have decreased by about 0 117 that is by the probability of starting in the sophos2a state Looking further into the segment newly added to the HMM we see that there are in total five states emitting only the sophos symbol connected in a chain Notice that the first state can only transition to the second sophos state with probability 1 Therefore any session which begins in the sophos directory has to visit it at least 
twice From the second state in the chain we can either end the session moving to the quit state or visit the sophos directory twice more After that we can either get more data from the antivirus directory sophos_more state or start visiting other pages For that purpose the _all_sink state is used to model sessions in which after some initial sequence arbitrary pages can be visited Here it is used to model sessions where after downloading antivirus updates the visitor moves to another part of the website This state will be reused throughout the HMM to model final parts of other types of sessions too The above modification is a typical application of the chain of states pattern described in Sect 7 3 Notice that we don t have to set specific probabilities e g that of quitting after visiting the sophos directory exactly twice They will be established automatically by the Baum Welch algorithm Of course it is possible that a session occurs which does not match the chain of states exactly its probability will then 
be predicted to be zero This is not a problem if such sessions occur infrequently otherwise they will themselves become interesting patterns and the model will have to be updated to better accommodate them After the new states have been added the probabilities of related sequences were predicted accurately and new sequences became interesting The most interesting sequences with respect to the updated model were about the directory journal containing articles in PDF format published in the journal edited at the Institute Almost 2 of all sessions contained the sequence journal journal favicon ico which according to the model should appear extremely infrequently In addition the favicon ico file was absent and generated an error It turned out that the file favicon ico is the default location for a small icon appearing in the web browser next to webpage address On the Institute s website this file was however located at img favicon ico which was marked in the headers of all HTML files PDF files however did not 
contain this information which caused 123 Using interesting sequences to interactively build Hidden Markov Models 203 Fig 6 The fragment of the HMM describing visitors accessing journal articles the browser to try the default location and since the file was not there triggered an HTTP error It is interesting that very often the command to access PDF files has been issued twice in a row After inspecting the log it turned out that the transmission of the same file has often been restarted The author was not able to explain this effect To model sessions accessing journal pages several states have been added as shown in Fig 6 The idea here is quite similar to the model fragment shown in Fig 5 and won t be discussed in detail Again we can see the pattern of a chain of states each emitting only the journal symbol with branches leaving the states to model behavior such as finishing the session A large proportion about 10 of all sessions was initiated by automated web crawlers Such sessions can be easily recognized 
and modelled as they first access the robots txt file describing the site s policy towards such visitors A further 5 of all sessions were initiated by RSS news readers primarily Google reader The final model is shown in Fig 7 Several other states have been added to the model in the above fashion This includes states relating to web interface to e mail or pages related to conferences organized at the Institute An interesting part of the model corresponds to the main webpage It beings in a state denoted main which emits the symbol index html It is interesting to see that only about 6 7 of all traffic enters through the main page After the main state the pattern of a cluster of connected nodes has been used to model accesses to various elements of the main page such as images CSS stylesheets and JavaScript code This pattern was used as the actual order of visits to this types of elements was deemed uninteresting but had to be modelled in order to make other interesting sequences visible Despite a fairly large 
number of states the model is quite easy to understand Essentially the model consists of several parts which follow the chain pattern described in the case of the Sophos antivirus Each begins with a state emitting only one symbol corresponding to a single directory on the Web server then proceeds with some or none intermediate steps which model behavior dependent on the number of accesses to the directory All such chains share a common _all_sink state which models any remaining activity and a common quit state ending the session An exception is the part of the model related to the top level webpage which contains a more complicated 123 204 S Jaroszewicz Fig 7 The final HMM describing website visitors behavior loopback behavior involving images cascading stylesheets and JavaScript This exception shows that automatically adding chains of states based on interesting symbols is not a good overall strategy 123 Using interesting sequences to interactively build Hidden Markov Models 205 It should also be stressed 
that each state in the model has been explicitly added by the user and thus its meaning and purpose are clear and well defined Also the list of emitted symbols in each state is usually short significantly improving understandability This is typically not the case for automatically built models as will be shown in Sect 10 where interactive and automatic approaches are compared The final model in Fig 7 predicts the probability of all possible input sequences with accuracy better than 0 01 We can thus say that either a sequence is modelled well or it appears very infrequently and is thus of little importance to the overall picture of visitors behavior Let us briefly comment on the efficiency of the proposed approach Since the special case of sequences starting at t 0 is considered frequent sequence mining in data and in HMM is very fast The computation time is almost completely dominated by the Baum Welch algorithm The algorithm can be quite slow but we discovered that when the emission probability matrix E 
provides enough information about the internal state of the model given the output symbol the algorithm requires few iterations This is the case in the presented application where many states may emit only a single symbol The model shown in Fig 7 converged in just five iterations The computation took a few minutes on a dataset of 100 000 log events using a Python implementation A more thorough performance analysis is given for the more computationally demanding task of mining sequences starting at an arbitrary point in time 8 Experimental evaluation protein secondary structure In this section we present an application of the algorithm for finding interesting sequences starting at an arbitrary time point to analysis of protein secondary structure Let us first give a brief description of the problem Proteins are long sequences of amino acids When a protein molecule is synthesized in a living cell amino acids are added to it one by one The overall structure of the protein molecule does not physically remain 
linear but folds in a very complicated way The shape of the molecule determines its chemical properties so predicting the structure of the protein molecule based on the sequence of amino acids from which it is built is a practically important problem An easier subproblem is predicting the so called secondary structure that is not the shape of the whole molecule just the shape of small local fragments This is the problem we will look at in this section More biological background can be found in Hunter 1993 A dataset from the UCI repository containing several amino acid sequences annotated with secondary structure of protein they encode at each given location was used in the experiments Overall there are 20 amino acids denoted with uppercase letters Additionally there are three high level types of secondary structure helix strand and coil denoted by letters h e and c respectively Each sequence element contains both the amino acid and the type of secondary structure present at this location The problem now is 
to discover relationships between sequences of aminoacids and secondary structure of the protein they encode In this section the proposed method of mining interesting sequences beginning at arbitrary time points is used to address this task The problem has received significant 123 206 S Jaroszewicz attention in literature Qian and Sejnowski 1988 Bouchaffra and Tan 2006 Asai et al 1993 and we do not aim at building a competitive secondary structure prediction system Rather we want to demonstrate the presented approach on real data The set of output symbols has been chosen to have 60 elements Each output symbol consists of two letters the first denoting one of the 20 amino acids and the second the type of structure at this location The information on the amino acid and the structure is thus encoded jointly in the symbol Another solution could be to use HMMs with multiple output symbols per state The initial HMM contained only a single state which could emit all possible symbols and had an edge to itself with 
transition probability 1 The minimum interestingness threshold of 0 0001 was used The first five most interesting sequences are given in the Table 1 Due to large number of symbols interestingness values are not high but nevertheless it is possible to see clear patterns In all cases the probability predicted by the HMM was lower than data count thus the values of interestingness are marked as positive An interpretation is suggested for each pattern Looking at all the patterns the hypothesis is that sequences of amino acids A K L tend to produce the helix structure h and sequences of amino acids A G and S the coil structure c Note that A is present in the rules for both helix and coil structures This is not a contradiction and means that it is simply less likely to produce the third type of structure strand To validate the hypothesis we also looked at most interesting sequences of length three shown in Table 2 They confirm the above findings and suggest further that D and K might also be related to the coil 
structure In order to update the model to reflect those findings three new states have been added state h 1 which emits only symbols Ah Kh Lh and whose meaning is that a helix is being generated by a sequence of As Ks and Ls state c1 which emits only symbols Ac and Kc This state denotes the process of generating a coil while adding a sequence of amino acids A and K Table 1 The first five most interesting sequences discovered from the protein structure data Sequence Ah Ah Gc Sc Ac Ac Ah Lh Kh Ah Interestingness 0 0029 0 0027 0 0027 0 0025 0 0025 Proposed interpretation A sequence of As is likely to produce a helix A sequence of Gs and Ss is likely to produce a coil A sequence of As is likely to produce a coil A sequence of As and Ls is likely to produce a helix A sequence of Ks and As is likely to produce a helix Table 2 The most interesting sequences of length three discovered from the protein structure data Sequence Ac Ac Ac Kc Ac Kc Dc Gc Sc Ah Ah Kh Kh Ah Ah Interestingness 0 0010 0 0008 0 0008 0 0008 0 
0006 123 Using interesting sequences to interactively build Hidden Markov Models Table 3 The most interesting sequences found in the protein structure data after the model has been updated Sequence Lc Pc Ve Te Vh Ah Gc Tc Eh Lh 207 Interestingness 0 0024 0 0023 0 0022 0 0021 0 0021 state c2 which emits only symbols Gc Sc and Dc This state represents the process of generating a coil by a sequence of amino acids other than those covered by the previous state Transitions between all states are possible transition and emission probabilities are computed using the Baum Welch algorithm The reason for splitting the coil generating state is to treat amino acids A and K specially as they can generate two types of secondary structure The mining procedure was repeated with the new model The new most interesting sequences are given in Table 3 The following modifications have been made to accommodate them state c3 has been added which emits only symbols Lc and Pc state e1 is created which emits only symbols Ve and Te and 
whose meaning is that a strand is being generated In addition Vh and Eh were added to the list of symbols emitted in state h 1 In the next iteration we omit the details due to its similarity to the previous steps symbols Vc and Tc were added to the emission list of state c2 the symbol Sh to the list of emitted symbols of h 1 and Se to the list for state e1 After the above iterations we looked at interesting sequences of length three and discovered for example that the sequence Ac Ac Ac occurs more frequently then expected This suggests that assigning a single state c1 to both Ac and Kc was too rough an approximation It should be split into two states since long sequences of As tend to produce a coil with a slightly higher probability As the purpose of this section is to demonstrate the methodology we finished the modelling process at this stage There is of course a problem of verifying the validity of the constructed model Luckily the UCI dataset comes with a description of an imperfect domain knowledge so 
the discoveries made in this section could be compared against it It turned out that the background knowledge identifies sequences of all amino acids except for S whose symbols are emitted by h 1 with formation of the helix structure Similarly V T and L emitted by e1 are associated with forming a strand This corroborates our findings Unfortunately the rules for coils are not explicit in the background knowledge We conjecture however that the discovered sequences are not accidental and correspond to true patterns responsible for coil formation Notice also that no special meaning has been given by the provided background knowledge to long sequences of As generating a coil so the sequence Ac Ac Ac can be considered a novel discovery in that context 123 208 S Jaroszewicz Fig 8 Efficiency of mining interesting sequences Computation times versus minimum interestingness threshold 9 Experimental evaluation performance We will now evaluate the performance of the proposed algorithms We will concentrate on mining 
sequences starting at an arbitrary point in time as this problem is much more challenging computationally To test performance a large database had to be used To this purpose we downloaded raw DNA data from the DNA Data Bank of Japan We used the TSA dataset for our experiments 1 which contains 3 226 raw DNA sequences totalling 2 730 222 symbols The algorithm was implemented in Python and tested on a 1 7 GhZ Pentium machine The HMM had 10 hidden states transition and emission probabilities were calculated using the Baum Welch algorithm We will separately report computation times for the whole interesting sequence mining procedure including the Baum Welch algorithm and mining frequent sequences in data and for mining frequent patterns in the HMM as this is one of the main contributions of the paper and takes just a small fraction of the total computation time The HMM mining times include both the time of finding frequent sequences in the HMM and of counting support of sequences which were frequent only in data 
Step 5 Fig 1 We begin by reporting times for the first 100 sequences 58 503 symbols with varying minimum interestingness threshold The relatively small size of data was chosen such that we can experiment with a broad range of thresholds The results are shown in Fig 8 As can be expected both times grow fast when the minimum interestingness becomes small but the computations remain possible even for very low thresholds It is also clear that mining in the HMM takes only a small fraction of the total time so improvements in efficiency are possible by e g using a better algorithm for mining frequent sequences in data As already mentioned this is a well researched area which is beyond the scope of this paper 1 Available by anonymous FTP from ftp ftp ddbj nig ac jp ddbj_database ddbj ddbjtsa seq gz Retrieved on March 19 2009 123 Using interesting sequences to interactively build Hidden Markov Models 209 Fig 9 Efficiency of mining interesting sequences Computation times versus the size of the database We now analyze 
the performance of the algorithm with respect to the size of the data By size we mean the total length of all sequences in D The results are shown in Fig 9 In order to vary the size of the database we simply included a certain number of sequences occurring first thus the irregular numbers for the sizes in the axis labels It can clearly be seen that the method works in reasonable time even for very long sequences It is also clear that mining patterns in the HMM is very quick compared to the remaining steps The right part of Fig 9 clearly shows that the time of mining sequences in the HMM indeed does not depend on the length of sequences in D even though the definition of support in HMM does involve the database This is the result of the method of computation of s by diagonalizing the transition matrix We now investigate the performance of mining frequent patterns in the HMM with respect to the number of hidden states in the model These experiments were performed with very low minimum support threshold of 0 
0001 Their results are shown in Fig 10 It can be seen that thousands of hidden states can be handled probably beyond the scope of interactive model construction The figure also compares with the brute force approach based on explicit multiplication of probability vectors by the transition matrix An extra optimization has been Fig 10 Computation time for mining frequent sequences in the HMM versus the number of hidden states in the HMM Timing is shown for the proposed diagonalization based approach and the brute force approach 123 210 S Jaroszewicz used of caching probability vectors for each t It can be seen that the time needed to mine frequent sequences in the model using the brute force method is much longer and for larger but still practical numbers of states would have a noticeable contribution to the total pattern mining time Fig 9 10 Comparison with automatic HMM construction In this section a thorough experimental comparison with automatic HMM construction will be presented Several aspects of both 
types of models such as complexity prediction accuracy and understandability will be analyzed By automatic HMM construction we mean creating a model with a specified number of states and random parameters and applying the Baum Welch algorithm to the model In order to recover the structure transitions which are assigned small probabilities are assumed to be absent The same rule is applied to emission probabilities The Baum Welch algorithm is assumed to have converged when the maximum difference between any of the probabilities in the model between successive iterations was 0 001 The algorithm was allowed to run until convergence occurred there was no limit on the number of iterations 10 1 A small artificial example We will begin by showing a small artificial example which will illustrate several issues and differences between automatic and interactive construction of HMMs We will start with a small HMM shown in Fig 11 generate a sequence database using that model and try to reconstruct the original using 
automatic and interactive approaches Despite its simplicity the model will be sufficient to demonstrate several interesting properties of both methods The HMM has three states and two output symbols a and b Emission probabilities of the symbols in each state are shown inside the ellipse depicting the state Two of the Fig 11 A small artificial HMM used in learning examples 0 5 a 0 8 b 0 2 1 0 5 a 0 8 b 0 2 0 5 0 5 a 0 2 b 0 8 0 5 0 5 123 Using interesting sequences to interactively build Hidden Markov Models 211 Fig 12 Artificial example automatically built HMM states are more likely to output the symbol a and the remaining one the symbol b The overall structure is such that the a symbols are more likely to be emitted in sequences whose length is even The relationship is of course not perfect but the behavior is easily detected by looking at output frequencies For example the probability of the sequence aa being emitted by the model at any time point is about 0 38 while for ab and ba it is about 0 22 and for 
bb about 0 18 In the first experiment an attempt was made to relearn the HMM from Fig 11 based on data simulated from it The training data consisting of 100 sequences each 1 000 symbols long have been generated from the original model The data thus consists of one hundred thousand symbols which should be sufficient to learn all the probabilities well The learned model is given in Fig 12 The model has been shown as the matrices of its parameters not as a graph since there is little structure in the learned model and the graph would be too cluttered Probabilities in transition and emission matrices may not add up to one due to rounding One immediately notices that the original structure has not been uncovered Contrary to the original model all possible edges are present in the HMM Also the transition and initial probabilities are quite different The emission matrix shows some similarity to the original HMM with two states much more likely to emit an a but the probabilities themselves are significantly 
different On the other hand it turns out that the model fits the data very well One way to check this is to mine interesting sequences in the learned model with respect to the data it was trained on The most interesting sequence turned out to be baaab with interestingness of 0 0062 So the probabilities of all sequences in data and in the learned model differ by at most this relatively small value Another way is to compare the likelihoods of the original and relearned models given the training data The likelihood of the original model was 0 123 212 S Jaroszewicz true underlying process external knowledge in the approach proposed in this paper human intelligence and understanding of the analyzed domain must in general be included in the learning process Another problem is that of convergence the Baum Welch algorithm is only guaranteed to converge to a local minimum and the convergence can be quite slow It took 153 iterations to learn the three state model described above which is quite long compared to other 
models analyzed in this section Problems arising from the local optimality of solutions will be discussed below Since in real situations the underlying model is not known it is difficult to decide how well the model fits the data just based on its likelihood Interesting patterns present a viable alternative here as the value of interestingness is easy to interpret Let us now try to discover the structure using the interactive procedure proposed in this paper We begin with a model shown in Fig 13a The model has two states each emitting one of the symbols all transitions between states are possible After setting the parameters to random initial values and applying the Baum Welch algorithm model weights are obtained We now find interesting sequences in the training data with respect to the model of which five most interesting ones are shown in Table 4 Signed values of interestingness are shown to facilitate interpretation The meaning of discovered patterns is not immediately clear but after a careful 
examination comparing the first and fourth pattern shows that the predicted probability of the sequence aa is too low and that of the sequence a too high This suggests that modelling sequences with even number of as needs special attention Other patterns confirm this as patterns with an even number of as have their probability predicted to be too low and patterns with an odd number of as too high All two element sequences mined from the model in Fig 13a had very low interestingness Fig 13 Models used in the interactive HMM construction procedure a a b b Table 4 Five most interesting sequences found in data generated from the model in Fig 11 with respect to the model in Fig 13a Sequence baab aab aaa bab baa I Pr D PrHMM 0 013 0 010 0 010 0 010 0 010 Pr D 0 064 0 149 0 230 0 071 0 149 PrHMM 0 051 0 139 0 240 0 081 0 139 123 Using interesting sequences to interactively build Hidden Markov Models 213 An obvious way to fix the initial model is shown in Fig 13b Additional two states have been added in order to 
explicitly model the cases when a occurs an even number times The interestingness of the most interesting pattern was 0 0014 comparable to the change in the values of modelled parameters used as the stopping criterion for Baum Welch algorithm It is thus reasonable to assume that the model explains the data sufficiently well While this HMM has more states than the original one it has a much more understandable structure Moreover each state emits only one symbol which is good for convergence of the Baum Welch algorithm only 35 iterations were necessary The likelihood of the model given training data was 0 123 214 Fig 14 Automatically built model with state output symbols fixed S Jaroszewicz a b a a to uncover the true structure especially since there may be many possible structures but in any case the nature of the incorrectly modelled aspect of the data something causes pairs of as to occur more frequently one after another in the example above is much easier to see from the patterns Also the patterns may 
give the user a hint on how to set the initial emission probabilities thus helping an automatic algorithm to discover better model parameters The method also provides an indicator of the quality of the model often more practical than the value of model s likelihood 10 2 Web log and protein secondary structure examples continued Let us now compare the final interactively built model for the Web log data given in Fig 7 with models built automatically An arbitrary number of 20 hidden states has been picked for the HMM and the Baum Welch algorithm has been used to compute the transition and emission probabilities Figure 15 shows the resulting HMM retaining only edges corresponding to transition probabilities 0 01 Figure 16 displays the same automaton with all edges corresponding to transition probabilities 0 001 Each node contains the symbols emitted by the corresponding state for clarity only symbols with emission probabilities 0 01 are shown Note that this is in contrast with user built models where state 
labels were assigned by the user based on each state s intended meaning Looking at Fig 15 it can be seen that the model s structure is not a good description of users behavior In the upper part of the picture there is a connected group of nodes related to the web based e mail interface but they are not connected to the rest of the graph Moreover nodes related to e mail are also present in the large cluster of nodes below The Sophos antivirus nodes are present and the discovered patterns could be inferred from their transition probabilities albeit with some effort Other discovered patterns are not clearly visible and it would be very hard to infer them from transition probabilities Adding more edges to the picture as seen in Fig 16 does not help On the contrary it makes the automaton practically incomprehensible It should also be noted that the Baum Welch algorithm on the hand built model converged much faster than in the case of an automatically built model The automatic case usually required about a 100 
iterations This is another this time real life example of the above described phenomenon of negative influence of ambiguity in emitted symbols on the convergence and efficiency of the Baum Welch algorithm 123 Using interesting sequences to interactively build Hidden Markov Models 215 Fig 15 Automatically build HMM for the weblog example Only edges corresponding to probabilities 0 01 are shown We will now give a more principled comparison of the complexity and quality of automatically and interactively built models To this end we need to define precise measures of model quality and complexity It is tempting to measure the quality of a model using the value of its likelihood given the data This approach however turns out not to be suitable for the problem at hand The reason is that in the interactively built models several nodes are left unconnected corresponding to zero transition probabilities If any such transition occurs in the data model s likelihood will be simply zero We have thus assessed model s 
quality based on its prediction accuracy i e how well it predicts the next symbol in a sequence based on symbols preceding it More formally model s prediction accuracy on a given database is defined as d D Pr HMM so Pr HMM s d D Acc D HMM so is a prefix of d d 1 that is the average of the probabilities of each symbol in data conditional on the sequence preceding it If the HMM predicts a zero probability for s an arbitrary choice of 0 is made for the predicted conditional probability that is the next symbol is assumed to be predicted incorrectly 123 216 S Jaroszewicz Fig 16 Automatically build HMM for the weblog example Edges corresponding to probabilities 0 001 are shown Model s complexity is defined simply as the number of nonzero parameters in the model that is the total number of nonzero initial transition and emission probabilities 0 0 i j Pi j 0 i j Ei j 0 Compl HMM i i Of course the original automatically built model is likely to have very high complexity In order to make the comparison meaningful we 
must be able to control the complexity of automatically built models To that end we use a postprocessing step in which a given percentage of lowest probabilities in the model are set to zero The final interactively built model in Fig 7 has 36 states so for the current experiment an automatically built model with 36 states has been used for comparison As in the previous cases the automatically built model has been learned using the BaumWelch algorithm starting with randomly chosen parameter values In order to help assess the models future accuracy the data has been split into training and test sets The training set was used to build the model and the test set to assess its accuracy Figure 17 shows the complexity vs accuracy tradeoff for automatically built model for the weblog data The final interactively built model from Fig 7 is marked on the 123 Using interesting sequences to interactively build Hidden Markov Models 217 Fig 17 The tradeoff between complexity and accuracy of the automatically built HMM for 
the Web log mining example The black dot shows the complexity and accuracy of the interactively built model Fig 18 The tradeoff between complexity and accuracy of the automatically built HMM for the protein secondary structure example The black dot shows the complexity and accuracy of the interactively built model figure with a black dot An analogous experiment has been conducted for the protein secondary Structure example the results are shown in Fig 18 In the automatically built models many probabilities turned out to be close to zero so even a tiny probability threshold significantly reduces model complexity with little effect on accuracy For this reason the figures show complexity only up to 500 and 200 respectively giving a more detailed picture in the interesting region Low overall accuracy for the protein example is the result of a large number of symbols 60 which makes prediction difficult Interactively built models have overall worse accuracy than complete automatically built ones In principle 
nothing prevents the user from continuing with interactive updates to the model until the accuracy increases further However reaching the accuracy of full automatically built models would be quite laborious and would probably defy the purpose of interactive modelling since very small divergences between the model and data would be hard to explain and the main appeal of the proposed method lies in the fact that all elements are included in the model for a well understood reason Automatically built HMMs do not possess this advantage It can also be seen that for a given complexity interactively built models were more accurate This is especially true in the case of protein data For the Web log 123 218 S Jaroszewicz data the difference is less visible but interactive models still performed better It can also be seen that the difference between interactively and automatically built HMMs is slightly larger on the test set which suggests that human explanations generalize better Notice also that low complexity does 
not mean understandability Recall Fig 15 The HMM shown in this figure has complexity of only 139 yet as discussed above it conveys very little information about the internal structure of the process 11 Conclusions An approach to interactive construction of global HMMs based on local patterns has been presented Two cases most important from the practical point of view have been discussed mining interesting sequences starting at the beginning of database sequences and mining interesting sequences starting at arbitrary location have been addressed Experiments have shown that the proposed methodology is capable of producing highly understandable yet accurate models In the experiments interactively built HMMs achieved accuracy equal to or better than automatic models of comparable complexity Moreover interactively built models were much more understandable because they were built by a human and every new state was added with a clear intention of its function Additionally human added model states had a limited 
number of emitted symbols and different states emitted disjoint sets of symbols which further improved model s clarity In contrast automatically built models were hard to interpret typically states emitted large numbers of symbols and their sets of emitted symbols overlapped Experimental evidence has been given that automatic HMM weight learning often fails to discover the internal structure of even very simple models This turns out to be inevitable as it is possible for several HMMs to generate the exact same stochastic process In view of this requiring an analyst to do the actual updating while being guided by the interesting patterns seems to be the correct approach if models with meaningful internal structure are required A disadvantage of the approach is that the process requires manual labor and that considerable effort may be required to understand the causes of interesting patterns However since automatic procedures cannot in general be guaranteed to discover meaningful internal structure such an 
approach seems indeed unavoidable if information is to be gleaned on the underlying behavior of the process which is generating the data Efficiency of the proposed approach is very good in the presented experiments even very large datasets have been successfully analyzed This is especially true for the step of the proposed algorithms in which frequent sequences in HMMs are mined The proposed approach based on diagonalization of the transition matrix makes the computation time of this step negligible The simple brute force approach based on repeated matrix multiplications has been shown to perform much worse and to have running time which may have a tangible contribution to the running time of the whole interesting pattern discovery process 123 Using interesting sequences to interactively build Hidden Markov Models 219 It should also be noted that parameter learning using the Baum Welch algorithm is much easier for interactively constructed models The convergence is much faster tens instead of hundreds of 
iterations and there is much less of a chance of ending up in a local minimum This has been conjectured and confirmed experimentally to result from the fact that output symbols in interactively built models depend on the internal state in a less ambiguous fashion Moreover it is sometimes possible to use the interesting patterns to set some of the initial model parameters e g assign initial emission probabilities to states such that a local minimum can be avoided References Agrawal R Imielinski T Swami A 1993 Mining association rules between sets of items in large databases In ACM SIGMOD conference on management of data pp 123 220 S Jaroszewicz Low Kam C Mas A Teisseire M 2009 Mining for unexpected sequential patterns given a Markov model http www math univ montp2 fr mas lmt_siam09 pdf Meyer C 2001 Matrix analysis and applied linear algebra book and solutions manual SIAM Philadelphia Prum B Rodolphe F de Turckheim E 1995 Finding words with unexpected frequencies in DNA sequences J R Stat Soc Ser B 57 123 
Reproduced with permission of the copyright owner Further reproduction prohibited without permission 
21	a	Janet M Baker Li Deng James Glass Sanjeev Khudanpur Chin Hui Lee Nelson Morgan and Douglas O Shaughnessy dsp EDUCATION Research Developments and Directions in Speech Recognition and Understanding Part 1 T o advance research it is important to identify promising future research directions especially those that have not been adequately pursued or funded in the past The working group producing this article was charged to elicit from the human language technology HLT community a set of well considered directions or rich areas for future research that could lead to major paradigm shifts in the field of automatic speech recognition ASR and understanding ASR has been an area of great interest and activity to the signal processing and HLT communities over the past several decades As a first step this group reviewed major developments in the field and the circumstances that led to their success and then focused on areas it deemed especially fertile for future research Part 1 of this article will focus on historically 
significant developments in the ASR area including several major research efforts that were guided by different funding agencies and suggest general areas in which to focus research Part 2 to appear in the next issue will explore in more detail several new avenues holding promise for substantial improvements in ASR performance These entail cross disciplinary research and specific approaches to address three to five year grand challenges aimed at stimulating advanced research by dealing with realistic tasks of broad interest SIGNIFICANT DEVELOPMENTS IN SPEECH RECOGNITION AND UNDERSTANDING The period since the mid 1970s has witnessed the multidisciplinary field of ASR proceed from its infancy to its coming of age and into a quickly growing number of practical applications and commercial markets Despite its many achievements however ASR still remains far from being a solved problem As in the past we expect that further research and development will enable us to create increasingly powerful systems deployable on 
a worldwide basis This section briefly reviews highlights of major developments in ASR in five areas infrastructure knowledge representation models and algorithms search and metadata Broader and deeper discussions of these areas can be found in 12 16 19 23 24 27 32 33 41 42 and 47 Readers can also consult the following Web sites the IEEE History Center s Automatic Speech Synthesis and Recognition section and the Saras Institute s History of Speech and Language Technology Project at http www sarasinstitute org INFRASTRUCTURE Moore s Law observes long term progress in computer development and predicts doubling the amount of computation achievable for a given cost every 12 to 18 months as well as a comparably shrinking cost of memory These developments have been instrumental in enabling ASR researchers to run increasingly complex algorithms in sufficiently short time frames e g meaningful experiments that can be done in less than a day to make great progress since 1975 The availability of common speech corpora 
for speech training development and evaluation has been critical allowing the creation of complex systems of ever increasing capabilities Speech is a highly variable signal characterized by many parameters and thus large corpora are critical in modeling it well enough for automated systems to achieve proficiency Over the years these corpora have been created annotated and distributed to the worldwide community by the National Institute of Science and Technology NIST the Linguistic Data Consortium LDC and other organizations The character of the recorded speech has progressed from limited constrained speech materials to huge amounts of progressively more realistic spontaneous speech The development and adoption of rigorous benchmark evaluations and standards nurtured by NIST and others have been critical in developing increasingly powerful and capable systems Many labs and researchers have benefited from the availability of common research tools such as CarnegieMellon University Language Model CMU LM toolkit 
Hidden Markov Model Toolkit HTK Sphinx and Stanford Research Institute Language Modeling SRILM Extensive research support combined with workshops task definitions and system evaluations sponsored by t h e U S Department of Defense Advanced Research Projects Agency DARPA and others have been essential to today s system developments KNOWLEDGE REPRESENTATION Major advances in speech signal representations have included perceptually motivated mel frequency cepstral coefficients MFCC 10 29 and perceptual linear prediction PLP coefficients 21 as well as normalizations via cepstral mean subtraction CMS 16 44 relative spectral RASTA filtering 20 and vocal tract length normalization VTLN 13 Architecturally the most important Digital Object Identifier 10 1109 MSP 2009 932166 1053 5888 09 25 IEEE SIGNAL PROCESSING MAGAZINE 75 MAY 2009 Authorized licensed use limited to MICROSOFT Downloaded on April 29 2009 at 21 13 from IEEE Xplore Restrictions apply dsp EDUCATION continued development has been searchable unified graph 
representations that allow multiple sources of knowledge to be incorporated into a common probabilistic framework Noncompositional methods include multiple speech streams multiple probability estimators multiple recognition systems combined at the hypothesis level e g Recognition Output Voting Error Reduction ROVER 15 and multipass systems with increasing constraints bigram versus four gram within word dependencies versus cross word and so on More recently the use of multiple algorithms applied both in parallel and sequentially has proven fruitful as have feature based transformations such as heteroscedastic linear discriminant analysis HLDA 31 feature space minimum phone error fMPE 40 and neural net based features 22 MODELS AND ALGORITHMS The most significant paradigm shift for speech recognition progress has been the introduction of statistical methods especially stochastic processing with hidden Markov models HMMs 3 25 in the early 1970s 38 More than 30 years later this methodology still predominates A 
number of models and algorithms have been efficiently incorporated within this framework The expectationmaximization EM algorithm 11 and the forward backward or Baum Welch algorithm 4 have been the principal means by which the HMMs are trained from data Despite their simplicity N gram language models have proved remarkably powerful and resilient Decision trees 8 have been widely used to categorize sets of features such as pronunciations from training data Statistical discriminative training techniques are typically based on utilizing maximum mutual information MMI and the minimum error model parameters Deterministic approaches include corrective training 1 and some neural network techniques 5 35 Adaptation is vital to accommodating a wide range of variable conditions for the channel environment speaker vocabulary topic domain and so on Popular techniques include maximum a posteriori probability MAP estimation 17 38 51 maximum likelihood linear regression MLLR 34 and eigenvoices 30 Training can take place on 
the basis of small amounts of data from new tasks or domains that provide additional training material as well as oneshot learning or unsupervised training at test time SEARCH Key decoding or search strategies originally developed in nonspeech applications have focused on stack decoding A search 26 and Viterbi or N best search 50 Derived from communications and information theory stack decoding was subsequently applied to speech recognition systems 25 37 Viterbi search broadly applied to search alternative hypotheses derives from dynamic programming in the 1950s 6 and was subsequently used in speech applications from the 1960s to the 1980s and beyond from Russia and Japan to the United States and Europe 3 7 9 36 45 46 48 49 METADATA Automatic determination for sentence and speaker segmentation as well as punctuation has become a key feature in some processing systems Starting in the early 1990s audio indexing and mining have enabled high performance automatic topic detection and tracking as well as 
applications for language and speaker identification 18 GRAND CHALLENGES MAJOR POTENTIAL PROGRAMS OF RESEARCH Grand challenges are what our group calls ambitious but achievable three to fiveyear research program initiatives that will significantly advance the state of the art in speech recognition and understanding Previous grand challenges sponsored by national and international initiatives agencies and other groups have largely been responsible for today s substantial achievements in ASR and its application capabilities Six such potential programs are described below Each proposed program has defined measurable goals and comprises a complex of important capabilities that should substantially advance the field and enable significant applications These are rich task domains that could enable progress in several promising research areas at a variety of levels As noted below each of these program initiatives could also benefit from or provide benefit to multidisciplinary or cross area research approaches 
EVERYDAY AUDIO This is a term that represents a wide range of speech speaker channel and environmental conditions that people typically encounter and routinely adapt to in responding and recognizing speech signals Currently ASR systems deliver significantly degraded performance when they encounter audio signals that differ from the limited conditions under which they were originally developed and trained This is true in many cases even if the differences are slight This focused research area would concentrate on creating and developing systems that would be much more robust against variability and shifts in acoustic environments reverberation external noise sources communication channels e g far field microphones cellular phones speaker characteristics e g speaking style nonnative accents emotional state and language characteristics e g formal informal styles dialects vocabulary topic domain New techniques and architectures are proposed to enable exploring these critical issues in environments as diverse as 
meeting room presentations and unstructured conversations A primary focus would be exploring alternatives for automatically adapting to changing conditions in multiple dimensions even simultaneously The goal is to deliver accurate and useful speech transcripts automatically under many more environments and diverse circumstances than is now possible thereby enabling many more applications This challenging problem can productively draw on expertise and knowledge from related disciplines including natural language processing information retrieval and cognitive science IEEE SIGNAL PROCESSING MAGAZINE 76 MAY 2009 Authorized licensed use limited to MICROSOFT Downloaded on April 29 2009 at 21 13 from IEEE Xplore Restrictions apply RAPID PORTABILITY TO EMERGING LANGUAGES Today s state of the art ASR systems deliver top performance by building complex acoustic and language models using a large collection of domainspecific speech and text examples For many languages this set of language resources is often not readily 
available The goal of this research program is to create spoken language technologies that are rapidly portable To prepare for rapid development of such spoken language systems a new paradigm is needed to study speech and acoustic units that are more language universal than language specific phones Three specific research issues need to be addressed 1 cross language acoustic modeling of speech and acoustic units for a new target language 2 cross lingual lexical modeling of word pronunciations for new language and 3 cross lingual language modeling By exploring correlation between these emerging languages and well studied languages cross language features such as language clustering and universal acoustic modeling could be utilized to facilitate rapid adaptation of acoustic and language models Bootstrapping techniques are also keys to building preliminary systems from a small amount of labeled utterances first using these systems to label more utterance examples in an unsupervised manner incorporating new 
labeled data into the label set and iterating to improve the systems until they reach a performance level comparable with today s high accuracy systems Many of the research results here could be extended to designing machine translation natural language processing and information retrieval systems for emerging languages To anticipate this growing need some language resources and infrastructures need to be established to enable rapid portability exercises Research is also needed to study the minimum amount of supervised label information required to create a reasonable system for bootstrapping purposes SELF ADAPTIVE LANGUAGE CAPABILITIES State of the art systems for speech transcription speaker verification and language identification are all based on statistical models estimated from labeled training data such as transcribed speech and from human supplied knowledge such as pronunciation dictionaries Such built in knowledge often becomes obsolete fairly quickly after a system is deployed in a real world 
application and significant and recurring human intervention in the form of retraining is needed to sustain the utility of the system This is in sharp contrast with the speech facility in humans which is constantly updated over a lifetime routinely acquiring new vocabulary items and idiomatic expressions as well as deftly handling previously unseen nonnative accents and regional dialects of a language In particular humans exhibit a remarkable aptitude for learning the sublanguage of a new domain or application without explicit supervision The goal of this research program is to create self adaptive or self learning speech technology There is a need for learning at all levels of speech and language processing to cope with changing environments nonspeech sounds speakers pronunciations dialects accents words meanings and topics to name but a few sources of variation over the lifetime of a deployed system Like its human counterpart the system would engage in automatic pattern discovery active learning and 
adaptation Research in this area must address both the learning of new models and the integration of such models into preexisting knowledge sources Thus an important aspect of learning is being able to discern when something has been learned and how to apply the result Learning from multiple concurrent modalities e g new text and video may also be necessary For instance an ASR system may encounter a new proper noun in its input speech and may need to examine contemporaneous text with matching context to determine the spelling of the name Exploitation of unlabeled or partially labeled data would be necessary for such learning A motivation for investing in such research is the growing activity in the allied field of machine learning Success in this endeavor would extend the lifetime of deployed systems and directly advance our ability to develop speech systems in new languages and domains without the onerous demands of labeled speech essentially by creating systems that automatically learn and improve over 
time This research would benefit from cross fertilization with the fields of natural language processing information retrieval and cognitive science DETECTION OF RARE KEY EVENTS Current ASR systems have difficulty in handling unexpected and thus often the most information rich lexical items This is especially problematic in speech that contains interjections or foreign or out of vocabulary words and in languages for which there is relatively little data with which to build the system s vocabulary and pronunciation lexicon A common outcome in this situation is that high value terms are overconfidently misrecognized as some other common and similar sounding word Yet such spoken events are crucial to tasks such as spoken term detection and information extraction from speech Their accurate registration is therefore of vital importance The goal of this program is to create systems that reliably detect when they do not know a valid word A clue to the occurrence of such error events is the mismatch between an 
analysis of a purely sensory signal unencumbered by prior knowledge such as unconstrained phone recognition and a word or phrase level hypothesis based on higher level knowledge often encoded in a language model A key component of this research would therefore be the development of novel confidence measures and accurate models of uncertainty based on the discrepancy between sensory evidence and a priori beliefs A natural sequel to detection of such events would be to transcribe them phonetically when the system is confident that its word hypothesis is unreliable and to devise errorcorrection schemes IEEE SIGNAL PROCESSING MAGAZINE 77 MAY 2009 Authorized licensed use limited to MICROSOFT Downloaded on April 29 2009 at 21 13 from IEEE Xplore Restrictions apply dsp EDUCATION continued One immediate application that such detection would enable is subword e g phonetic indexing and search of speech regions where the system suspects the presence of errors Phonetic transcription of the error prone regions would also 
enable the development of the next generation of self learning speech systems the system may then be able to examine new texts to determine the identity of the unknown word This research has natural synergy with natural language processing and information retrieval research COGNITION DERIVED SPEECH AND LANGUAGE SYSTEMS A key human cognitive characteristic is the ability to learn and adapt to new patterns and stimuli The focus of this project would be to understand and emulate relevant human capabilities and to incorporate these strategies into automatic speech systems Since it is not possible to predict and collect separate data for any and all types of speech topic domains and so on it is important to enable automatic systems to learn and generalize even from single instances episodic learning or limited samples of data so that new or changed signals e g accented speech noise adaptation could be correctly understood It has been well demonstrated that adaptation in automatic speech systems is very beneficial 
An additional impetus for looking now at how the brain processes speech and language is provided by the dramatic improvements made over the last several years in the field of brain and cognitive science especially with regard to the cortical imaging of speech and language processing It is now possible to follow instantaneously the different paths and courses of cortical excitation as a function of differing speech and language stimuli A major goal here is to understand how significant cortical information processing capabilities beyond signal processing are achieved and to leverage that knowledge in our automated speech and language systems The ramifications of such an understanding could be very far reaching This research area would draw on the related disciplines of brain and cognitive science natural language processing and information retrieval SPOKEN LANGUAGE COMPREHENSION MIMICKING AVERAGE LANGUAGE SKILLS AT A FIRST TO THIRD GRADE LEVEL Today s state of the art systems are designed to transcribe spoken 
utterances To achieve a broad level of speech understanding capabilities it is essential that the speech research community explore building languagecomprehension systems that could be improved by the gradual accumulation of knowledge and language skills An interesting approach would be to compare an ASR system with the speech performance of children less than ten years of age in listening comprehension skill Just like a child learning a new subject a system could be exposed to a wide range of study materials in a learning phase In a testing stage the system and the children would be given written questions first to get some idea what kind of information to look for in the test passages Comprehension tests could be in oral and written forms The goal of this research program is to help develop technologies that enable language comprehension It is clear such evaluations would emphasize the accurate detection of information bearing elements in speech rather than basic word error rate Natural language 
understanding of some limited domain knowledge would be needed Four key research topics need to be explored 1 partial understanding of spoken and written materials with a focused attention on information bearing components 2 sentence segmentation and name entry extraction from given test passages 3 information retrieval from the knowledge sources acquired in the learning phase and 4 representation and database organization of knowledge sources Collaboration between speech and language processing communities is a key element to the potential success of such a program The outcomes of this research could provide a paradigm shift for building domain specific language understanding systems and significantly affect the education and learning communities IMPROVING INFRASTRUCTURE FOR FUTURE ASR RESEARCH CREATION OF HIGH QUALITY ANNOTATED CORPORA The single simplest best way for current state of the art recognition systems to improve performance on a given task is to increase the amount of task relevant training data 
from which its models are constructed System capabilities have progressed directly along with the amount of speech corpora available to capture the tremendous variability inherent in speech Despite all the speech databases that have been exploited so far system performance consistently improves when more relevant data are available This situation clearly indicates that more data are needed to capture crucial information in the speech signal This is especially important in increasing the facility with which we can learn understand and subsequently automatically recognize a wide variety of languages This capability will be a critical component in improving performance not only for transcription within any given language but also for spokenlanguage machine translation cross language information retrieval and so on If we want our systems to be more powerful and to understand the nature of speech itself we must collect and label more of it Well labeled speech corpora have been the cornerstone on which today s 
systems have been developed and evolved The availability of common speech corpora has been and continues to be the sine qua non for rigorous comparative system evaluations and competitive analyses conducted by the U S NIST and others Labeling for most speech databases is typically at the word level However some annotation at a finer level e g syllables phones features and so on is important to understand and interpret speech successfully Indeed the single most popular speech database available from the Linguistic Data Consortium LDC is TIMIT a very compact acoustic phonetic database created by IEEE SIGNAL PROCESSING MAGAZINE 78 MAY 2009 Authorized licensed use limited to MICROSOFT Downloaded on April 29 2009 at 21 13 from IEEE Xplore Restrictions apply MIT and Texas Instruments where the speech is associated with a subword phonetic transcription Over the years many significant speech corpora such as Call Home Switchboard Wall Street Journal and more recently Buckeye have been made widely available with 
varying degrees and types of annotation These corpora and others have fundamentally driven much of our current understanding and growing capabilities in speech recognition transcription topic spotting and tracking and so on There is a serious need today to understand the basic elements of speech with much larger representative sets of speech corpora both in English and other languages In order to explore important phenomena above the word level databases need to be labeled to indicate aspects of emotion dialog acts and semantics e g Framenet 14 and Propbank 28 Human speech understanding is predicated on these factors For systems to be able to recognize these important characteristics there must be suitably labeled speech data with which to train them It is also likely that some new research may be required to explore and determine consistent conventions and practices for labeling itself and for future development and evaluation methodologies to accommodate at least minor differences in labeling techniques 
and practices We must design ASR systems that are tolerant of labeling errors NOVEL HIGH VOLUME DATA SOURCES Thanks in large part to the Internet there are now large quantities of everyday speech that are readily accessible reflecting a variety of materials and environments only recently available Some of it is of quite variable and often poor quality such as user posted material from YouTube Better quality audio materials are reflected in the diverse oral histories recorded by organizations such as StoryCorps available at www storycorps net University course lectures seminars and similar material make up another rich source one that is being placed online in a steady stream These materials all reflect a less formal more spontaneous and natural form of speech than present day systems have typically been developed to recognize Weak transcripts such as closedcaptioning and subtitles are available for some of these audio materials The benefit of working with materials such as this is that systems will become 
more capable as a consequence an important development in increasing robustness and expanding the range of materials that can be accurately transcribed under a wide range of conditions Much of what is learned here is also likely to be of benefit in transcribing casual everyday speech in languages other than English TOOLS FOR COLLECTING AND PROCESSING LARGE QUANTITIES OF SPEECH DATA Over the years the availability of both open source e g Carnegie Mellon University s CMU Sphinx and commercial speech tools e g Entropic Systems and Cambridge University s HTK has been very effective in quickly bringing good quality speech processing capabilities to many labs and researchers New Web based tools could be made available to collect annotate and then process substantial quantities of speech very costeffectively in many languages Mustering the assistance of interested individuals on the World Wide Web in the manner of open source software and Wikipedia could generate substantial quantities of language resources very 
efficiently and at little cost This could be especially valuable in creating significant new capabilities for resource impoverished languages New initiatives though seriously underfunded at present include digital library technology aiming to scan huge amounts of text e g the Million Book Project 44 and the creation of largescale speech corpora e g the Million Hour Speech Corpus 2 aiming to collect many hours of speech in many world languages If successful these projects will significantly advance the state of the art in the automation of world language speech understanding and proficiency They will also provide rich resources for strong research into the fundamental nature of speech and language itself ACKNOWLEDGMENTS This article is an updated version of the MINDS IEEE SIGNAL PROCESSING MAGAZINE 79 MAY 2009 Authorized licensed use limited to MICROSOFT Downloaded on April 29 2009 at 21 13 from IEEE Xplore Restrictions apply dsp EDUCATION continued Johns Hopkins University in Baltimore Maryland He works on 
the application of information theoretic and statistical methods to human language technologies including automatic speech recognition machine translation and information retrieval Chin Hui Lee chl ece gatech edu has been a professor at the School of Electrical and Computer Engineering Georgia Institute of Technology since 2002 Before joining academia he spent 20 years in industry including 15 years at Bell Labs Murray Hill New Jersey where he was the director of dialog system research Nelson Morgan morga n ic si berkeley edu is the director and speech group leader at ICSI a University of California Berkeley affiliated independent nonprofit research laboratory He is also professor in residence in the University of California Berkeley EECS Department the coauthor of a textbook on speech and audio signal processing and a Fellow of the IEEE Douglas O Shaughnessy dougo emt inrs ca is a professor at INRS EMT University of Quebec a Fellow of the IEEE and of the Acoustical Society of America ASA and the editor in 
chief of EURASIP Journal on Audio Speech and Music Processing REFERENCES 8 L Breiman J Friedman R Olshen and C Stone Classification and Regression Trees Pacific Grove CA Wadsworth Brooks 1984 9 J Bridle M Brown and R Chamberlain A one pass algorithm for connected word recognition in Proc IEEE ICASSP 1982 pp 30 R Kuhn P Nguyen J C Junqua L Goldwasser N Niedzielski S Fincke K Field and M Contolini Eigenvoices for speaker adaptation in Proc Int Conf Spoken Language Sydney Australia 1998 pp 1 L R Bahl P F Brown P V de Souza and R L Mercer Estimating hidden Markov model parameters so as to maximize speech recognition accuracy IEEE Trans Speech Audio Processing vol 1 no 1 pp IEEE SIGNAL PROCESSING MAGAZINE 80 MAY 2009 Authorized licensed use limited to MICROSOFT Downloaded on April 29 2009 at 21 13 from IEEE Xplore Restrictions apply 
22	a	Updated MINDS Report on Speech Recognition and Understanding Part 2 The MIT Faculty has made this article openly available Please share how this access benefits you Your story matters Citation Baker J et al Updated MINDS report on speech recognition and understanding Part 2 DSP Education Signal Processing Magazine IEEE 26 4 2009 78 85 As Published Publisher Version Accessed Citable Link Terms of Use Detailed Terms dsp EDUCATION Updated MINDS Report on Speech Recognition and Understanding Part 2 Janet M Baker Li Deng Sanjeev Khudanpur Chin Hui Lee James R Glass Nelson Morgan and Douglas O Shaughnessy T his article is the second part of an updated version of the MINDS Digital Object Identifier 10 1109 MSP 2009 932707 psychological aspects of human speech perception include the essential psychoacoustic properties that underlie auditory masking and attention Such key properties equip human listeners with the remarkable capability to cope with cocktail party effects that no current ASR techniques can successfully 
handle Intensive studies are needed in order for ASR applications to reach a new level delivering performance comparable to that of humans Specific issues to be resolved in the study of how the human brain processes spoken as well as written language are the way human listeners adapt to nonnative accents and the time course over which human listeners reacquaint themselves with a language known to them Humans have amazing capabilities to adapt to nonnative accents Current ASR systems are extremely poor in this regard and improvement is expected only after we have sufficient understanding of human speech processing mechanisms One specific issue related to human speech perception and linked to human speech production is the temporal span over which speech signals are represented and modeled One prominent weakness in current hidden Markov models HMMs is inadequacy in representing long span temporal dependency in the acoustic feature sequence of speech which is an essential property of speech dynamics in both 
perception and production The main cause of this handicap is the conditional independence assumptions inherent in the HMM formalism The HMM framework also assumes that speech can be described as a sequence of discrete units usually phones or phonemes In this symbolic invariant approach the focus is on the linguistic phonetic information and the incoming speech signal is normalized during preprocessing in an attempt to remove most of the paralinguistic information However human speech perception experiments have shown that such paralinguistic information plays a crucial role in human speech perception Numerous approaches have been taken over the past dozen years to address the weaknesses of HMMs described above 2 4 17 19 26 46 51 64 65 These approaches can be broadly grouped into two categories The first a parametric structure based approach establishes mathematical models for stochastic trajectories segments of speech utterances using various forms of parametric characterization 17 19 22 51 The essence of 
such an approach is that it exploits knowledge and mechanisms of human speech perception and production so as to provide the structure of the multitiered stochastic process models These parametric models account for the observed speech trajectory data based on the underlying mechanisms of speech coarticulation and reduction directly relevant to human speech perception and on the relationship between speaking rate variations and the corresponding changes in the acoustic features The second nonparametric and template based approach to overcoming the weaknesses of HMMs involves direct exploitation of speech feature trajectories i e templates in the training data without any modeling assumptions 2 4 64 65 This newer approach is based on episodic learning as seen in many recent human speech perception and recognition experiments 28 43 Due to the recent dramatic increase of speech databases and computer storage capacity available for training as well as exponentially expanded computational power nonparametric 
methods and episodic learning IEEE SIGNAL PROCESSING MAGAZINE 78 JULY 2009 Authorized licensed use limited to MIT Libraries Downloaded on November 23 2009 at 16 59 from IEEE Xplore Restrictions apply 1053 5888 09 25 provide rich areas for future research 43 61 64 65 The essence of the template based approach is that it captures strong dynamic segmental information about speech feature sequences in a way complementary to the parametric structure based approach The recent Soundto Sense project in Europe has been devoted to this area of research FROM TRANSCRIPTION TO MEANING EXTRACTION Another rich area for future research is to develop machine representations of meaning that capture the communicative intent of a spoken utterance This would be a complement to word error rate the most prevalent criterion for ASR performance Machine representations are unlikely to achieve universal representations of meaning but for specific domains of speech understanding they should be defined in a way that is consistent with 
human judgment of meaning in spoken utterances This new performance measure could provide feedback to the low level components of future ASR systems For example if ASR systems are designed with a component that represents articulation effort then the degree to which the correct meaning is recognized should correlate with the tolerance of a range of the articulation effort Greater accuracy of meaning understanding or more success in communicating the intent from the speaker to the listener should allow the recognizer to tolerate a wider range of speaking efforts on the part of speaker and hence a greater degree of acoustic variability This meaning representation may also become the output of the speech system for downstream processing in some applications such as speech translation in which a verbatim transcript preserving every acoustic detail is neither necessary nor desirable UNDERSTANDING HOW CORTICAL SPEECH LANGUAGE PROCESSING WORKS Major advances in high resolution imaging technologies are now enabling 
brain scientists to track the spatial and temporal characteristics of how the brain processes speech and language 10 15 25 44 45 A combination of direct and EEG recordings with neuroimaging studies using functional MRI fMRI positron emission tomography PET and magnetoencephalography MEG has revealed substantial information about cortical processing of speech and language In the near term we can hope to gain significant insights into how the human brain processes this information and try to use that knowledge to improve ASR models processing and technology Many phenomena can now be directly and quantifiably observed such as the time course and details of adaptation and facilitation semantic dissonance and so on A scientific understanding of cortical processing and adaptation could help us understand how our automated systems should adapt to new acoustic environments or to accented speech as well as the role that episodic learning plays in human speech perception and word recognition Insights from recent 
linguistic phonetic and psychological research should be used to understand the interaction of the prior structure of speech as the knowledge source with the acoustic measurement of speech data and to inform and construct ASR models beyond the current flat structured HMMs in ASR Newly constructed models may need to exhibit similar behavior to that of humans when listening and responding to their native languages accented and unaccented and foreign languages Here accented speech or foreign languages represent situations where the knowledge source is weak on the part of the listener The counterpart situation where the information about the data or signal becomes weak is when the listeners perform ASR under adverse acoustic environments Understanding of the interplay between these contrasting situations in human speech perception would provide a wealth of information enabling the construction of better models better than HMMs that reflect particular attributes of human auditory processing and the linguistic 
units used in human speech recognition For example to what extent may human listeners use mixed word or phrase templates and the constituent phonetic phonological units in their memory to achieve relatively high performance in speech recognition for accented speech or foreign languages weak knowledge and for acoustically distorted speech weak observation How do human listeners use episodic learning e g direct memory access and parametric learning related to smaller phonetic units analogous to what we are currently using for HMMs in machines in speech recognition and understanding Answers to these questions would benefit the design of next generation machine speech recognition models and algorithms HETEROGENEOUS KNOWLEDGE SOURCES FOR AUTOMATIC SPEECH RECOGNITION Heterogeneous parallelism in both ASR algorithms and computational structure will be important for research in the next decade While the incorporation of new types of multiple knowledge sources has been on the research agenda for decades particularly 
for ASR we are entering a period in which the resources will be available to support this strategy in a much more significant way For instance it is now possible to incorporate both larger sound units than the typical phone or subphone elements even for large vocabulary recognition while still preserving the advantage of the smaller units 67 additionally more fundamental units such as articulatory features can be considered 23 62 At the level of the signal processing front end of ASR we no longer need to settle on the single best representation as multiple representations differentiated by differing tie scales or decompositions of the timefrequency plane have been shown to be helpful 7 46 At the other end of the process the incorporation of syntactic and semantic cues into the recognition process is still in its infancy It is possible that deeper semantic representations like Propbank 38 and Framenet 21 could become important in disambiguating similar sounding recognition hypotheses The incorporation of 
multiple knowledge sources is a key part of what could IEEE SIGNAL PROCESSING MAGAZINE 79 JULY 2009 Authorized licensed use limited to MIT Libraries Downloaded on November 23 2009 at 16 59 from IEEE Xplore Restrictions apply dsp EDUCATION continued also be called multistream analysis In the cases referred to above streams correspond to information in quite heterogeneous forms However the streams can consist of more homogeneous elements such as the signals from multiple sensors e g microphone arrays 20 On the other hand the streams can be even more heterogeneous for instance coming from different modalities boneconducted vibration cameras or lowpower radar 48 In all of these cases architectures are required that can aggregate all of the modules responses Various approaches for this have been tried for some time but we are only now beginning to tackle the task of integrating so many different kinds of sources due to the emerging availability of the kinds of resources required to learn how best to do the 
integration FOCUSING ON INFORMATIONBEARING ELEMENTS OF THE SPEECH SIGNAL While speech recognition is often viewed as a classification task any real system must contend with input that does not correspond to any of the desired classes These unexpected inputs can take the form of complete words that are not in the recognition vocabulary including words in a foreign language word fragments environmental noises or nonverbal vocal output such as laughter Thus in addition to the closed set classification task speech recognition systems must also reject sounds that do not correspond to members of the desired set Equivalently we need to know when ASR may be strongly confident that a word is known and we must also know when there is low confidence in an ASR result 34 In many applications knowing when we don t know could be as or even more important than merely having a low word error rate Additionally ASR tends to have poor performance for words within the system vocabulary for which there are few training examples 
However such low frequency words often contain critical information for instance if it is a named entity Learning how to deal more effectively with both interfering sounds and information bearing sounds that are poorly represented in our training is a critical area for future research 37 NOVEL COMPUTATIONAL ARCHITECTURES FOR KNOWLEDGE RICH SPEECH RECOGNITION For decades Moore s law has been a dependable indicator of the increasing capability for calculation and storage in our computational systems The resulting effects on systems for speech recognition and understanding have been enormous permitting the use of ever larger training databases and recognition systems and the incorporation of increasingly detailed models of spoken language Many of the projections for future research implicitly depend on a continued advance in computational capabilities an assumption that certainly seems justified given recent history However the fundamentals of this progression have recently changed 3 49 As Intel and others have 
noted recently the power density on microprocessors has increased to the point that higher clock rates would begin to melt the silicon die Consequently at this point industry development is focused on implementing microprocessors on multiple cores Dualcore CPUs are now very common and four and eight processor systems are coming out The new road maps for the semiconductor industry reflect this trend and future speed increases will come more from parallelism than from having faster individual computing elements For the most part algorithm designers for speech systems have ignored the investigation of such parallelism since the advance of scalar capabilities has been so reliable Future progress in many of the directions we discuss here will require significantly more computation consequently researchers concerned with implementation will need to consider parallelism explicitly in their designs This will be a significant change from the status quo In particular tasks such as decoding for which extremely clever 
schemes to speed up single processor performance have been developed will require a complete rethinking of the algorithms 31 MODELS ALGORITHMS AND SEARCH ADAPTATION AND SELF LEARNING IN SPEECH RECOGNITION SYSTEMS Learning Speech recognition has traditionally been cast as a task in which spoken input is classified into a sequence of predefined categories such as words 33 55 ASR development typically proceeds via a heavily supervised training phase that makes use of annotated corpora followed by a deployment testing phase during which model parameters may be adapted to the environment speaker topic and so on while the overall structure remains static In other words ASR systems typically do not learn they undergo supervised training and are relatively static thereafter Such an approach stands in stark contrast to human processing of speech and language where learning is an intrinsic capability 6 11 42 Humans can integrate large amounts of unlabeled or at best lightly annotated speech 14 27 35 From these data we 
can learn among other things the phonetic inventories of a language and word boundaries and we can use these abilities to acquire new words and meanings 36 54 59 In humans learning and the application of learned knowledge are not separated they are intertwined However for the most part speech recognizers are not inherently designed to learn from the data they are meant to classify There are many degrees of learning ranging from one shot methods to learning from small amounts of data to learning from partially or poorly labeled or even unannotated data 53 Research in this latter area would enable systems to benefit from the enormous quantities of data becoming available online and could reduce the expense and delay associated with our current dependency on high quality annotations for training This is especially true for languages for which there are few or no existing large annotated corpora Finally research IEEE SIGNAL PROCESSING MAGAZINE 80 JULY 2009 Authorized licensed use limited to MIT Libraries 
Downloaded on November 23 2009 at 16 59 from IEEE Xplore Restrictions apply directed towards self learning such as unsupervised pattern discovery methods could ultimately prove useful for the general problem of language acquisition a long standing grand challenge problem in the research community GENERALIZATION Over the past three decades the speech research community has developed and refined an experimental methodology that has helped to foster steady improvements in speech technology The approach that has worked well and has been adopted in other research communities is to develop shared corpora software tools and guidelines that can be used to reduce differences between experimental setups down to the basic algorithms so that it becomes easier to quantify fundamental improvements Typically these corpora are focused on a particular task As speech technology has become more sophisticated the scope and difficulty of these tasks have continually increased from isolated words to continuous speech from speaker 
dependent to speakerindependent from read to spontaneous from clean to noisy from utterance to content based and so on Although the complexity of such corpora has continually increased one common property of such tasks is that they typically have a training portion that is quite similar in nature to the test data Indeed obtaining large quantities of training data that is closely matched to the test is perhaps the single most reliable method for improving ASR performance This strategy is quite different from the human experience however Over our entire lifetimes we are exposed to all kinds of speech data from uncontrolled environments speakers and topics i e everyday speech Despite this great variation in our own personal training data we are all able to create internal models of speech and language that are remarkably adept at dealing with variations in the speech chain This ability to generalize is a key aspect of human speech processing that has not yet found its way into modern speech recognizers Research 
on this topic should produce technology that will operate more effectively in novel circumstances and that can generalize better from smaller amounts of data Examples include moving from one acoustic environment to another and among different tasks and languages One way to support research in this area would be to create a large corpus of everyday speech and a variety of test sets drawn from different conditions Another research area could explore how well information gleaned from large resource languages and or domains generalizes to smaller resource languages and domains MACHINE LEARNING This is an exciting time in the machine learning community Many new algorithms are being explored and are achieving impressive results on a wide variety of tasks Recent examples include graphical models conditional random fields partially observable Markov decision processes reinforcement based learning and discriminative methods such as largemargin or log linear maximum entropy models Recent developments in effective 
training of these models make them worthy of further exploration The speech community would do well to explore common ground with the machine learning community in these areas LANGUAGE ACQUISITION The acquisition of spoken language capability by machine through unsupervised or lightly supervised human intervention remains one of the grand challenges of artificial intelligence While the amount of innate language ability possessed by humans is open to debate 6 10 42 the degree of variation in languages across different cultures indicates that linguistic knowledge itself is acquired through interaction with and exposure to spoken language 36 54 59 Although there has been some research in unsupervised acquisition of phones words and grammars 8 9 12 16 40 52 60 63 there remains much opportunity for research in pattern discovery generalization and active learning A research program in language acquisition could have many quantifiable components based on either speech or text based inputs Particular opportunities 
exist where natural parallel e g multilingual or multimodal e g audiovisual corpora exist since alternative communication channels provide additional sources of constraint 58 ROBUSTNESS AND CONTEXT AWARENESS IN ACOUSTIC MODELS FOR SPEECH RECOGNITION Probabilistic models with parameters estimated from sample speech data pervade state of the art speech technology including ASR language identification LID and speaker verification 32 50 68 The models seek to recover linguistic information such as the words uttered the language spoken or the identity of the speaker from the received signal Many factors unrelated to the information being sought by the models also significantly influence the signal presented to the system SPEAKER S ACOUSTIC ENVIRONMENT AND THE SPEECH ACQUISITION CHANNEL The acoustic environment in which speech is captured e g background noise reverberation overlapping speech and the communication channel through which speech is transmitted prior to its processing e g cellular land line telephone or 
VoIP connection along with call to call variability represent significant causes of harmful variability responsible for drastic degradation of system performance Existing techniques such as Wiener filtering and cepstral mean subtraction 57 remove variability caused by additive noise or linear distortions while methods such as RASTA 29 compensate for slowly varying linear channels However more complex channel distortions such as reverberation or variable noise along with the Lombard effect present a significant challenge SPEAKER CHARACTERISTICS AND STYLE It is well known that speech characteristics e g age nonnative accent vary widely among speakers due to many IEEE SIGNAL PROCESSING MAGAZINE 81 JULY 2009 Authorized licensed use limited to MIT Libraries Downloaded on November 23 2009 at 16 59 from IEEE Xplore Restrictions apply dsp EDUCATION continued factors including speaker physiology speaker style e g speech rate spontaneity of speech emotional state of the speaker and accents both regional and nonnative 
The primary method currently used for making ASR systems more robust to variations in speaker characteristics is to include a wide range of speakers in the training Speaker adaptation mildly alleviates problems with new speakers within the span of known speaker and speech types but usually fails for new types Current ASR systems assume a pronunciation lexicon that models native speakers of a language Furthermore they train on large amounts of speech data from various native speakers of the language A number of modeling approaches have been explored in modeling accented speech including explicit modeling of accented speech adaptation of native acoustic models via accented speech data 24 41 and hybrid systems that combine these two approaches 66 Pronunciation variants have also been tried in the lexicon to accommodate accented speech 30 Except for small gains the problem is largely unsolved Similarly some progress has been made for automatically detecting speaking rate from the speech signal 47 but such 
knowledge is not exploited in ASR systems mainly due to the lack of any explicit mechanism to model speaking rate in the recognition process LANGUAGE CHARACTERISTICS DIALECT VOCABULARY GENRE Many important aspects of speaker variability derive from nonstandard dialects Dialectal differences in a language can occur in all linguistic aspects lexicon grammar syntax and morphology and phonology This is particularly damaging in languages where spoken dialects differ dramatically from the standard form e g Arabic 39 The vocabulary and language use in an ASR task change significantly from task to task necessitating estimation of new language models for each case A primary reason language models in current ASR systems are not portable across tasks even within the same language or dialect is that they lack linguistic sophistication they cannot consistently distinguish meaningful sentences from meaningless ones nor grammatical from ungrammatical ones Discourse structure is also rarely considered merely the local 
collocation of words Another reason why language model adaptation to new domains and genres is very data intensive is the nonparametric nature of the current models When the genre changes each vocabulary sized conditional probability distribution in the model must be reestimated essentially independently of all the others Several contexts may share a backing off or lower order distribution but even those in turn need to be reestimated independently and so on With a few exceptions such as vocal tract length normalization VTLN 13 and cepstral mean subtraction CMS 57 models used in today s speech systems have few explicit mechanisms for accommodating most of the uninformative causes of variability listed above The stochastic components of the model usually Gaussian mixtures are instead burdened with implicitly modeling the variability in a frame by frame manner Consequently when the speech presented to a system deviates along one of these axes from the speech used for parameter estimation predictions by the 
models become highly suspect The performance of the technology degrades catastrophically even when the deviations are such that the intended human listener exhibits little or no difficulty in extracting the same information TOWARDS ROBUST SPEECH RECOGNITION IN EVERYDAY ENVIRONMENTS Developing robust ASR requires going away from the matched training and test paradigm along one or more of the axes mentioned above To do so a thorough understanding of the underlying causes of variability in speech and subsequently accurate and parsimonious parameterization of such understanding in the models will be needed The following issues however transcend specific methodologies and will play a key role in any solution in the future A large corpus of diverse speech will have to be compiled containing speech that carries information of the kind targeted for extraction by the technology and exhibits large but calibrated extraneous deviations of the kind against which robustness is sought such as a diverse speaker population 
with varying degrees of nonnative accents or different local dialects widely varying channels and acoustic environments diverse genres and so on Such a corpus will be needed to construct several training and test partitions such that unseen conditions of various kinds are represented Multistream and multiple module strategies will have to be developed Any robust method will have to identify reliable elements of the speech spectrum in a data driven manner by employing an ensemble of analyses and using the analysis that is most reliable in that instance A multiplemodule approach will also entail a new search strategy that treats the reliability of a module or stream in any instance as another hidden variable over which to optimize and seeks the most likely hypothesis over all configurations of these hidden variables New robust training methods for estimating models from diverse labeled data will be required To adequately train a model from diverse data either the data will have to be normalized to reduce 
extraneous variability or training condition adaptive transformations will have to be estimated jointly with a conditionindependent model e g speakeradaptive training SAT 1 of acoustic models in ASR Detailed unsupervised adaptation will become even more important in unseen test conditions than it is today In case of adaptive model transformations a hierarchical parameterization of the transforms will have to be developed e g from parsimonious ones like VTLN or CMS through multiclass maximum likelihood linear regression MLLR to a detailed transformation of every Gaussian density IEEE SIGNAL PROCESSING MAGAZINE 82 JULY 2009 Authorized licensed use limited to MIT Libraries Downloaded on November 23 2009 at 16 59 from IEEE Xplore Restrictions apply in order to permit both robust transform estimation during training and unsupervised transform estimation from test data Exploitation of unlabeled or partially labeled data will be necessary to train the models and to automatically select parts of the unlabeled data 
for manual labeling in a way that maximizes its utility This need is partly related to the above mentioned compilation of diverse training data The range of possible combinations of channel speaker environment speaking style and domain is so large that it is unrealistic to expect transcribed or labeled speech in every configuration of conditions for training the models However it is feasible to simply collect raw speech in all conditions of interest Another important reason for unsupervised training will be that the systems like their human baseline will have to undergo lifelong learning adjusting to evolving vocabulary channels language use and so on Substantial linguistic knowledge will need to be injected into structural design and parameterization of the systems particularly the statistical language models There are numerous studies indicating that short segments of speech are locally ambiguous even to human listeners permitting multiple plausible interpretations Linguistically guided resolution of 
ambiguity using cues from a very wide context will be needed to arrive at the correct interpretation Some form of semantics or representation of meaning in addition to syntactic structure will have to be used in the system All available metadata and context dependent priors will have to be exploited by the systems In a telephony application for instance geospatial information about the origin and destination of the call known priors about the calling and called parties and knowledge of world events that influence the language vocabulary or topic of conversation will have to be used by the system Discriminative criteria 5 for parameter estimation throughout the system and multipass recognition strategies both being pursued today will also be vital The former yield more robust models by focusing on categorization rather than description of the training data while the latter lead to more robust search by quickly eliminating implausible regions of the search space and applying detailed models to a small set of 
hypotheses likely to contain the correct answer 56 Language universal speech technology is a significant research challenge in its own right with obvious rewards for resource impoverished languages and exploiting language universals could yield additional robustness even in resource rich languages Human performance on actual test data will have to be measured and used 1 for evaluation of robustness giving systems greater latitude where there is genuine ambiguity and insisting on meeting the gold standard where there is no ambiguity and 2 for gaining insights from specific instances in which humans are robust and those in which they are not leading eventually to new technological solutions A research program that emphasizes the accurate transcription of everyday speech by which we mean speech acquired in realistic everyday situations with commonly used microphones from native and nonnative speakers in various speaking styles on a diversity of topics and tasks will advance the robustness of speech recognition 
systems along one or more of the axes of variability mentioned above NOVEL SEARCH PROCEDURES FOR KNOWLEDGE RICH SPEECH RECOGNITION As noted above search methods that explicitly exploit parallelism may be an important research direction for speech understanding systems Additionally as innovative recognition algorithms are added there will be an impact on the search component For instance rather than the left to right and sometimes right to left recognition passes that are used today there could be advantages to either identifying islands of reliability or islands of uncertainty and relying on alternate knowledge sources only locally in the search process The incorporation of multiple tiers of units such as articulatory feature subphone state phone syllable word and multiword phrase could have consequences for the search process Finally so called episodic approaches to ASR are being investigated 64 These rely on examples of phrases words or other units directly as opposed to statistical models of speech While 
this seems to be a throwback to the days before the prominence of HMMs the idea is gaining new prominence due to the availability of larger and larger speech databases and thus more and more examples for each modeled speech unit It could well be that an important future direction would be to learn how best to incorporate these approaches into a search that also uses statistical models which have already proven their worth CONCLUSIONS We have surveyed historically significant events in speech recognition and understanding that have enabled this technology to become progressively more capable and cost effective in a growing number of everyday applications With additional research and development significantly more valuable applications are within reach A set of six ambitious achievable and testable grand challenge tasks has been proposed Successful achievement of these would lay the groundwork for bringing a number of high utility applications to reality Each of these challenge tasks should benefit and be 
benefited by collaboration and cross fertilization with related humanlanguage technologies especially machine translation information retrieval and natural language processing as well as brain and cognitive science Research achievements in speech recognition and understanding have IEEE SIGNAL PROCESSING MAGAZINE 83 JULY 2009 Authorized licensed use limited to MIT Libraries Downloaded on November 23 2009 at 16 59 from IEEE Xplore Restrictions apply dsp EDUCATION continued demonstrably led to major advances in related human language technologies as well as more general areas such as pattern recognition To enable and implement these grand challenges a number of especially promising research directions were outlined and supported Though these have been largely unfunded so far the pursuit of these initiatives would contribute to a substantial increase in the core technology on which robust future applications depend ACKNOWLEDGMENTS The authors acknowledge significant informative discussions with several 
colleagues whose opinions and advice is reflected in the text above We wish to thank Andreas Andreou James Baker Mary Harper Hynek Hermansky Frederick Jelinek Damianos Karakos Alex Park Raj Reddy Richard Schwartz and James West AUTHORS Janet M Baker janet_baker email com is a cofounder of Dragon Systems and founder of Saras Institute in West Newton Massachusetts She lectures in academic and business venues on speech technology strategic planning and entrepreneurship Li Deng deng microsoft com is principal researcher at Microsoft Research in Redmond Washington and affiliate professor at the University of Washington Seattle He is a Fellow of the IEEE and of the Acoustical Society of America ASA and a member of the Board of Governors of the IEEE Signal Processing Society Sanjeev Khudanpur khudanpur jhu edu is an associate professor of electrical and computer engineering in the GWC Whiting School of Engineering of the Johns Hopkins University in Baltimore Maryland He works on the application of information 
theoretic and statistical methods to human language technologies including ASR machine translation and information retrieval Chin Hui Lee chl ece gatech edu has been a professor since 2002 at the School of ECE Georgia Institute of Technology in Atlanta Before joining academia he spent 20 years in industry including 15 years at Bell Labs Murray Hill New Jersey where he was the director of dialog system research James R Glass glass mit edu is a principal research scientist at the MIT Computer Science and Artificial Intelligence Laboratory where he heads the Spoken Language Systems Group and is a lecturer in the Harvard MIT Division of Health Sciences and Technology Nelson Morgan morgan icsi berkeley edu is the director and Speech Group leader at ICSI a University of California 10 A Chan S Cash E Eskandar J M Baker C Carlson O Devinsky W Doyle R Kuzniecky T Thesen C Wang K Marinkovic and E Halgren Decoding semantic category from MEG and intracranial EEG in humans in Proc Neuroscience 2008 Conf Washington DC 11 
N A Chomsky Knowledge of Language Is Nature Origin and Use New York Praeger 1986 12 A Clark Unsupervised language acquisition Theory and practice Ph D dissertation Univ Sussex Brighton U K 2001 13 J Cohen T Kamm and A G Andreou Vocal tract normalization in speech recognition Compensating for systematic speaker variability J Acoust Soc Amer vol 97 no 5 pp 1 T Anastasakos J McDonough and J Makhoul Speaker adaptive training A maximum likelihood approach to speaker normalization in Proc IEEE Int Conf Acoustics Speech and Signal Processing Apr 1997 pp IEEE SIGNAL PROCESSING MAGAZINE 84 JULY 2009 Authorized licensed use limited to MIT Libraries Downloaded on November 23 2009 at 16 59 from IEEE Xplore Restrictions apply 31 A Janin Speech recognition on vector architectures Ph D dissertation Univ California Berkeley 2004 32 F Jelinek Statistical Methods for Speech Recognition Cambridge MA MIT Press 1997 33 F Jelinek Continuous speech recognition by statistical methods Proc IEEE vol 64 no 4 pp stochastic modeling for 
speech recognition IEEE Trans Speech Audio Process vol 4 no 5 pp Processing Systems L Saul Ed Cambridge MA MIT Press vol 16 2004 61 H Strik How to handle pronunciation variation in ASR By storing episodes in memory in Proc ITRW on Speech Recognition and Intrinsic Variation SRIV2006 Toulouse France May 2006 pp IEEE SIGNAL PROCESSING MAGAZINE 85 JULY 2009 Authorized licensed use limited to MIT Libraries Downloaded on November 23 2009 at 16 59 from IEEE Xplore Restrictions apply 
23	a	USEFUL TRANSCRIPTIONS OF WEBCAST LECTURES Cosmin Munteanu A thesis submitted in conformity with the requirements for the degree of Doctor of Philosophy Department of Computer Science University of Toronto Copyright c 2009 by Cosmin Munteanu Abstract Webcasts are an emerging technology enabled by the expanding availability and capacity of the World Wide Web This has led to an increase in the number of lectures and academic presentations being broadcast over the Internet Ideally repositories of such webcasts would be used in the same manner as libraries users could search for retrieve or browse through textual information However one major obstacle prevents webcast archives from becoming the digital equivalent of traditional libraries information is mainly transmitted and stored in spoken form Despite voice being currently present in all webcasts users do not benefit from it beyond simple playback My goal has been to exploit this information rich resource and improve webcast users experience in browsing and 
searching for specific information I achieve this by combining research in Human Computer Interaction and Automatic Speech Recognition that would ultimately see text transcripts of lectures being integrated into webcast archives In this dissertation I show that the usefulness of automatically generated transcripts of webcast lectures can be improved by speech recognition techniques specifically addressed at increasing the accuracy of webcast transcriptions and the development of an interactive collaborative interface iii iv that facilitates users contributions to machine generated transcripts I first investigate the user needs for transcription accuracy in webcast archives and show that users performance and transcript quality perception is affected by the Word Error Rate WER A WER equal to or less than 25 is acceptable for use in webcast archives As current Automatic Speech Recognition ASR systems can only deliver in realistic lecture conditions WERs of around 45 50 I propose and evaluate a webcast system 
extension that engages users to collaborate in a wiki manner on editing imperfect ASR transcripts My research on ASR focuses on reducing the WER for lectures by making use of available external knowledge sources such as documents on the World Wide Web and lecture slides to better model the conversational and the topic specific styles of lectures I show that this approach results in relative WER reductions of 11 Further ASR improvements are proposed that combine the research on language modelling with aspects of collaborative transcript editing Extracting information about the most frequent ASR errors from user edited partial transcripts and attempting to correct such errors when they occur in the remaining transcripts can lead to an additional 10 to 18 relative reduction in lecture WER Contents List of Figures List of Tables 1 Introduction 1 1 1 2 1 3 1 4 1 5 Overview Motivation Statement of Thesis Contributions Structure of the Dissertation 1 5 1 1 5 2 1 5 3 1 5 4 Research Design Outline of the Proposed 
Research xiii xix 1 2 3 6 7 8 8 9 Outline of the Surveyed Related Work 10 Peer Reviewed Publications 11 13 2 Webcasting and Automatic Speech Recognition 2 1 Lecture Webcasting 14 2 1 1 2 1 2 An Overview of Webcasting 15 Research Challenges 17 v vi 2 1 3 CONTENTS Integrating Text with Webcast Media 20 2 2 Automatic Speech Recognition for Webcast Lectures 21 2 2 1 An Overview of the Automatic Speech Recognition Process 21 2 2 2 2 2 3 Improving Automatic Speech Recognition 24 Research Challenges 26 3 The Acceptable Word Error Rate of Machine Generated Webcast Transcripts 29 3 1 Related Work 33 3 2 Research Questions and Hypotheses 36 3 3 Methods 37 3 3 1 3 3 2 3 3 3 3 3 4 3 3 5 3 3 6 3 3 7 3 3 8 3 3 9 Overview 37 Independent Variables 37 Task 38 Measures and Instruments 38 System 42 Acoustic Models 45 Language Models 45 Lexicon 46 Recordings 47 3 3 10 Recognition 47 3 3 11 Experimental Design 48 3 3 12 Participants 49 3 3 13 Procedures 49 3 3 14 Data Analysis 50 CONTENTS 3 4 vii Results Task Performance 51 3 4 
1 3 4 2 Performance Quality Hypothesis 51 Performance Quality Hypothesis Breakdown by Quiz Question Type 54 3 4 3 Performance Quality Hypothesis Breakdown by Demographic Information 56 3 5 Results User Perception 60 3 5 1 3 6 3 7 Experience Quality Hypothesis 61 Limitations and Generalizations 67 Summary and Discussion 68 4 Improving Automatic Speech Recognition for Webcast Lectures 4 1 Existing Research on Adapting and Building Language Models for Dedicated Domains 73 4 2 4 3 Previous Work on Automatic Speech Recognition for Lectures Web Based Language Modelling for Automatic Lecture Transcription 82 4 3 1 4 3 2 4 3 3 4 4 General Algorithm 83 Corpora Adjustment 84 LM and ASR Scope Alternatives 85 78 71 Empirical Evaluation 87 4 4 1 4 4 2 4 4 3 4 4 4 Test Data 88 Web Based Lecture Models 89 Baseline Models 89 Results WER Reduction 91 viii 4 4 5 CONTENTS Results Precision Recall of Keywords 93 4 5 Limitations and Generalizations 95 4 6 Summary and Discussion 97 5 Wiki editing of Webcast Transcripts 99 5 1 
Related Research 101 5 2 Enhancing Webcasts with Transcripts 103 5 3 Managing Imperfect Transcripts 103 5 3 1 Features of the Transcript Edit Tool 105 5 4 A Field Study 106 5 4 1 Research Objectives 106 5 5 Methods 107 5 5 1 5 5 2 5 5 3 5 5 4 System 108 Task and Procedures 109 Participants 110 Instruments and Measures 110 5 6 Results 115 5 6 1 5 6 2 5 6 3 5 6 4 Task completion 115 User Experience 117 Users Involvement and Motivation 122 General User Feedback 123 5 7 Interface Re design and Re evaluation 124 5 7 1 5 7 2 5 7 3 Assessment of Current Design 124 Extended Editing Mode 125 Evaluation of the Re designed System 127 5 8 Limitations and Generalizations 131 CONTENTS 5 9 ix Summary and Discussion 131 Speech Recognition for Webcast Lectures 133 6 Automatic Learning from Wiki enabled Transcript Corrections 6 1 6 2 6 3 Related Work 136 Transformation Based Learning 140 Minimally Trained Transformation Based Learning for Webcast Transcription 142 6 3 1 6 3 2 6 3 3 6 3 4 6 4 TBL Algorithm for ASR Output 
Correction 143 Rule Discovery for Lecture Transcripts 143 Scoring Function for the TBL rules 145 Rule Application 150 Empirical Evaluation 151 6 4 1 6 4 2 6 4 3 6 4 4 6 4 5 Evaluation Data 151 Language Models 153 Training and Test Data Partitioning 154 Scoring Functions 155 Results 156 6 5 6 6 Limitations and Generalizations 159 Summary and Discussion 164 167 7 Contributions Conclusions and Future Work 7 1 Contributions How Good is Good Enough and What to Do When It Isn t 168 7 1 1 ASR how good is good enough 168 x 7 1 2 CONTENTS What to do when ASR is not good Bibliography A Abbreviations B Glossary of Technical Terms 181 203 205 CONTENTS C Instruments Used in Experiments xi 209 xii CONTENTS List of Figures 2 1 2 2 2 3 2 4 The ePresence system displaying an archived lecture webcast 16 The display pane of the MIT lecture browser 19 The speech recognition process 21 An overview of the automatic speech recognition process with the different parts of the process grouped under three levels the recognition level 
the evaluation side and the resource level 24 3 1 3 2 3 3 The transcript enhanced ePresence system 30 The histogram of the relative quiz scores for WER 25 57 The histogram of the relative quiz scores for WER 25 only for subjects that haven t used the system or a similar one before 58 4 1 The algorithm for using the lecture slides and the World Wide Web to build corpora on which lecture specific LMs are trained 84 4 2 Examples of slide bullets used as web queries apparently un related examples are from the These same lecture Conceptual Design of the third year Computer Science undergraduate course The Design of Interactive Computational Media 85 xiii xiv LIST OF FIGURES 4 3 Examples of documents retrieved from the World Wide Web relevant to two of the queries in Figure 4 2 86 4 4 Average WER scores across all lectures for the baseline model built on general purpose conversational texts SWB the interpolation based optimization SWB KEYW and the best web based LMs LECT 93 4 5 Average Precision and Recall scores 
for keyword detection across all lectures when using the baseline model built on general purpose conversational optimization texts SWB the and interpolation based SWB KEYW the best web based LMs LECT 94 5 1 Our transcript enhanced ePresence system displaying a screen capture of the system with transcripts of 45 WER 102 5 2 Wiki like editing of imperfect transcripts 104 5 3 The percentage of edited transcript lines and relative WER reduction for each of the 21 lectures after all transcripts were corrected 114 5 4 The extended editing mode allowing for full control of the audio playback and for editing of consecutive transcript lines 126 LIST OF FIGURES 6 1 The TBL algorithm for correcting ASR output xv Transformation rules are learned from the alignment of manually transcribed text T with automatically generated transcripts TASR of training data ranked according to a scoring function S and applied to the ASR output TASR of test data 144 6 2 The discovery of transformation rules as part of the TBL algorithm 
described in Figure 6 1 146 6 3 6 4 6 5 7 1 An example of the rule discovery mechanism from Figure 6 2 147 An example of rule scoring and selection 149 An example of rule application 150 The ASR based search interface of the MIT lecture browser 176 C 1 Consent Form page 1 for the experiment in Chapter 3 210 C 2 Consent Form page 2 for the experiment in Chapter 3 211 C 3 Consent Form page 1 for the main study in Chapter 5 212 C 4 Consent Form page 2 for the main study in Chapter 5 213 C 5 Consent Form page 1 for the study following the interface re design in Chapter 5 214 C 6 Consent Form page 2 for the study following the interface re design in Chapter 5 215 C 7 The introductory quiz page 1 administered before the start of the experiment in Chapter 3 216 C 8 The introductory quiz page 2 administered before the start of the experiment in Chapter 3 217 xvi LIST OF FIGURES C 9 The introductory quiz page 3 administered before the start of the experiment in Chapter 3 218 C 10 An example of the quiz administered 
during the experiment in Chapter 3 Four quizzes with questions selected from the introductory quiz were used in a latin square setup as described in Section 3 3 219 C 11 The questionnaire administered after a session with a level of WER of either 0 25 or 45 from the experiment in Chapter 3 220 C 12 The questionnaire administered after a session with a level of WER of NT no transcripts from the experiment in Chapter 3 221 C 13 The questionnaire page 1 administered at the end of the experiment in Chapter 3 222 C 14 The questionnaire page 2 administered at the end of the experiment in Chapter 3 223 C 15 The questionnaire page 3 administered at the end of the experiment in Chapter 3 224 C 16 The questionnaire page 4 administered at the end of the experiment in Chapter 3 225 C 17 The web based questionnaire page 1 administered at the end of the field study in Chapter 5 226 C 18 The web based questionnaire page 2 administered at the end of the field study in Chapter 5 227 C 19 The web based questionnaire page 3 
administered at the end of the field study in Chapter 5 228 LIST OF FIGURES C 20 The web based questionnaire page 4 administered at the xvii end of the field study in Chapter 5 229 xviii LIST OF FIGURES List of Tables 3 1 The variable used to control the training overfitting of the lecture language models 43 3 2 The training overfitting variables values for the target WERs of 25 and 45 44 3 3 Mean relative quiz scores for each level of WER and tests of significance over all levels of WER 52 3 4 Trend analyses over the ordinal values of WER and over all values of WER 53 3 5 Multiple comparisons between WER NT and each of the ordinal levels of WER 53 3 6 Mean relative NotOnSlide scores for each level of WER and tests of significance over all levels of WER 55 3 7 Trend analyses over the ordinal values of WER and over all values of WER for NotOnSlide scores 55 3 8 Multiple comparisons between WER NT and each of the ordinal levels of WER for NotOnSlide scores 55 3 9 Mean relative scores for each level of WER 
across novice users and tests of significance and trends over all levels of WER 56 xix xx LIST OF TABLES 3 10 Mean relative perception of difficulty and confidence in performance levels for each level of WER and tests of significance over all levels of WER Lower values mean increased confidence choice 1 on questionnaire indicated being very confident and perception of an easier task choice 1 indicated a very easy task 61 3 11 Trend analyses over the ordinal values of WER and over all values of WER for perception and confidence levels 62 3 12 Multiple comparisons between WER NT and each of the ordinal levels of WER for perception and confidence levels 63 3 13 Mean relative perception of helpfulness levels for each level of WER and tests of significance and trends over all levels of WER Lower values mean increase helpfulness choice 1 on questionnaire indicated transcripts helped solved the quiz faster better 65 4 1 Web based lecture LM the WERs corresponding to the training options described in Section 4 3 3 
90 4 2 The WERs corresponding to the best web based lecture models LECT compared to the baseline model built on general purpose conversational texts SWB to the baseline model built using manually extracted keywords defining the lecture topic KEYW and to the baseline obtained through interpolation SWB KEYW 91 LIST OF TABLES 4 3 Precision and Recall scores for keyword detection when using the best training options as indicated in Table 4 1 for the web based models compared to the baseline model built on general purpose conversational texts SWB to the baseline model built using manually extracted keywords defining the lecture topic KEYW and to the baseline obtained through interpolation SWB KEYW 5 1 5 2 5 3 5 4 xxi 92 User acceptance of the transcript enhanced webcast system 116 Users attitudes toward imperfect transcripts 117 Users attitudes toward wiki editing 119 Users perception of the indirect benefits of wiki editing of transcripts 120 5 5 Users confidence in using the system as a relation of transcript 
quality 121 5 6 Users attitudes toward wiki editing with the re designed system 129 6 1 6 2 The evaluation data 152 The WER values for instructor R for which the ASR output using the WSJ 5K language model is corrected by TBL rules that are scored by the approximation functions XER and XER NoS as baselines and by the proposed globally scoped non heuristic scoring function SW ER 160 xxii LIST OF TABLES 6 3 The WER values for instructor R for which the ASR output using the WEB LECT language model is corrected by TBL rules that are scored by the approximation functions XER and XER NoS as baselines and by the proposed globally scoped non heuristic scoring function SW ER The average WER when using the SW ER function and the training parameter RT 2 is 40 161 6 4 The WER values for instructor G for which the ASR output using the ICSISWB language model is corrected by TBL rules that are scored by the approximation functions XER and XER NoS as baselines and by the proposed globally scoped non heuristic scoring 
function SW ER 162 6 5 The WER values for instructor K for which the ASR output using the WSJ 5K language model is corrected by TBL rules that are scored by the approximation functions XER and XER NoS as baselines and by the proposed globally scoped non heuristic scoring function SW ER 163 Chapter 1 Introduction Humankind has long relied on written text to share knowledge from handwritten letters to books and to printed mass media The advent of affordable broadband Internet and personal and portable computing devices is contributing to dramatic changes in the way people exchange information and store knowledge Universities colleges and other public institutions are not excluded from these changes Nowadays more lectures presentations and talks are being made available online In order to provide the same access to information as written materials online media must be accompanied by textual transcripts Two major Computer Science research areas are directly concerned with improving access to information in this 
context Automatic Speech Recognition ASR which focuses on automatically producing better quality text transcripts and Human Computer Interaction HCI which is dedicated to better and more naturally facilitating the information transfer between users and machines There is little evidence however of combined efforts in these two major 1 2 CHAPTER 1 INTRODUCTION research areas One finds instead rather disparate attempts to address the challenge of improving the usability of webcast archives from either one The research proposed in this dissertation brings together the ASR and HCI areas in order to achieve the goal of providing improved access to webcast lectures and presentations This chapter presents an overview of the ASR and HCI areas outlines the challenges common to these two areas introduces the motivation for this work and establishes the contributions and the statement of a thesis 1 1 Overview Automatic Speech Recognition ASR is one of the oldest areas of research that could now be called Natural 
Language Processing NLP Despite the progress recorded over the past half century that led to today s current commercial ASR systems the research opportunities in this area have not yet been exhausted Current state of the art research systems still operate under serious restrictions Furui 2005a Glass et al 2007 such as the need for individual training on each user restricted applicability to a dedicated domain such as travel reservations or even both for critical applications such as devices controlled through voice commands ASR systems that can transcribe large vocabulary speaker independent noisy environment continuous speech as is required for lectures and presentations are still a research goal for the future Deng and Huang 2004 Another well established research area is that of Human Computer Interaction HCI and similar to ASR numerous research opportunities Useful Transcriptions of Webcast Lectures 1 2 MOTIVATION 3 exist here Often such opportunities of merging research from two different domains appear 
due to advances in both areas One particular example is the domain of natural language based dialog systems Bernsen et al 1998 a research area that emerged more than 20 years ago and one in which HCI and ASR NLP research must work together Webcasts are one of the emerging technologies associated with the expansion of the World Wide Web and this is one area that could certainly benefit from research in both HCI and ASR NLP Currently there is little evidence of HCI and ASR research working together on improving webcast systems Despite voice being currently present in nearly all webcasts it can rarely be used beyond playback My goal is to exploit this information rich resource and thereby improve the usefulness of webcast archives 1 2 Motivation The recent increase in the availability and affordability of broadband Internet connections has led to an increase in the use of Internet broadcasting Ritter 2004 For example major media corporations offer newscasts and universities deliver lectures through the Internet 
Most such webcast media are stored after being delivered live and can be accessed by users through interactive systems such as ePresence http epresence tv A detailed review of webcast systems can be found in Baecker 2003 In contrast to archives of text documents video and audio archives pose some challenges to their users Useful Transcriptions of Webcast Lectures 4 archives given a text query and CHAPTER 1 INTRODUCTION example a user must listen to or watch a long recording in order to locate a specific passage instead of quickly skimming through the content of a text document looking for visual landmarks and textual cues This represents an important hurdle in making webcast archives the digital equivalent from a user s perspective of libraries Various methods propose improved access to speech recordings by manipulating the audio playback Arons 1997 Sawhney and Schmandt 2000 or by delivering better keyword based indexing and searching through the audio stream Chelba et al 2007 Hori et al 2007 as well as to 
webcast archives through a table of contents Baecker et al 2003 Toms et al 2005 although such methods have certain limitations User studies Dufour et al 2005 suggest however that transcripts are a much needed tool for carrying out complex tasks that require browsing comprehension and information seeking from webcast archives even in the presence of other tools such as table of contents or search capabilities Unfortunately there are several challenges to obtaining transcripts of spoken documents Manual transcription is an expensive process Replacing the transcript with a manually produced set of keywords is also not a Useful Transcriptions of Webcast Lectures 1 2 MOTIVATION 5 because annotating natural language based resources is in general susceptible to inter annotator disagreements diverse speakers with particular speech styles and various accents including non native ones and large vocabularies determined by the large pool of topics In perfect conditions anechoic room slow speaking rate limited vocabulary 
ASR system previously trained on the same speaker state of the art systems can achieve a Word Error Rate WER1 of less than 3 For less restricted domains with good acoustic conditions such as broadcast news the state of the art WER is about 20 25 Gauvain et al 2002 When acoustic conditions degrade such as in lectures or conference talks WER can increase to 40 45 Leeuwis et al 2003 Park et al 2005 Hsu and Glass 2006 We must therefore first establish what a satisfactory quality for archive transcripts is in order to improve the overall webcast archives user experience Equally importantly since ASR techniques that achieve close to 0 WER will likely not be available in the near future Whittaker and Hirschberg 2003 more studies are needed to understand users expectations from transcripts and to explore how imperfect transcripts should 1 WER is defined as the edit distance in words between the correct sentence and the output sentence from the ASR system Useful Transcriptions of Webcast Lectures 6 CHAPTER 1 
INTRODUCTION be integrated into a highly interactive webcast system Therefore three directions of research present themselves for supporting the delivery of useful webcast lecture transcripts 1 studying how humans deal with and what they expect from error laden transcripts particularly in the context of webcast archives 2 improving ASR for lectures mainly by adapting language models or the corpora used in building language models to dedicated domains and 3 finding HCI based solutions for improving the usefulness of transcripts 1 3 Statement of Thesis In this dissertation I show that users access to and interaction with information rich media such as archives of webcast lectures can be improved through an integrated inter disciplinary approach that combines research in Automatic Speech Recognition and Human Computer Interaction In particular I claim and demonstrate that the usefulness of archived webcast lectures as well as users experience when interacting with these is improved by enhancing webcast systems 
with automatically generated transcripts of Word Error Rate of 25 or less I also show that significant transcript quality improvements toward the acceptable Word Error Rate are achieved by integrating speech recognition techniques specifically addressed at increasing the accuracy of webcast transcriptions with the development of an interactive collaborative interface that facilitates users editing of machine generated transcripts Useful Transcriptions of Webcast Lectures 1 4 CONTRIBUTIONS 7 1 4 Contributions As part of a larger research effort in the webcasting community to make archives of webcast lectures and presentations the digital equivalent of traditional libraries my research goals focus on providing useful transcriptions of lectures and presentations One particular research direction in aid of this goal is enhancing webcasts with automatically generated textual transcripts The quality of automatically generated transcripts should also be a focus of any effort directed towards improving access to 
information rich webcasts This dissertation s contributions are a reflection of these two inter dependent directions A study of how humans deal with error laden transcripts and what they expect from them I have determined the acceptable speech recognition quality for which transcripts of webcast lectures become useful and improve the usability2 of archived webcasts as well as discovered how users performance and experience is influenced by the quality of transcripts Improvement of Automatic Speech Recognition for lectures Several solutions are proposed to reduce the WER for both lectures and academic presentations by ways of domain oriented language modelling corpora HCI based solutions for 2 and automatically building webcast specific improving the usefulness of A definition of these terms is given in Section 3 3 4 Useful Transcriptions of Webcast Lectures 8 transcript enhanced webcasts CHAPTER 1 INTRODUCTION In order to further improve the quality of webcast transcripts a collaborative and interactive tool 
that supports user editing of transcripts is proposed and evaluated In addition to directly benefitting from the corrections made by users solutions are proposed that exploit the user edits to learn ASR correction rules that further improve transcript quality 1 5 Structure of the Dissertation Each research contribution is introduced in a separate chapter Chapter 2 introduces and outlines the topics addressed in the dissertation by surveying related work In addition existing research relevant to each topic is presented in the first section of the corresponding chapter 1 5 1 Research Design The first step toward proving the thesis stated in Section 1 3 was to identify the acceptable threshold of quality for automatically generated transcripts of webcast lectures when used in an information mining and question answering task as well as assessing the influence of transcripts quality on webcast users experience After determining this threshold I have investigated ASR methods that improve the quality of lecture 
transcripts through topic specific language modelling using lecture slides and relevant documents from the World Wide Web I have also proposed HCI based solutions for improving the quality of such transcripts by developing and evaluating an interactive tool that allows users of webcast to collaboratively correct lecture transcripts Useful Transcriptions of Webcast Lectures 1 5 STRUCTURE OF THE DISSERTATION 9 I then show that the transcript corrections enabled by such collaborative tool can be used to learn patterns of errors that serve as a mechanism of further improving the accuracy of lecture specific ASR systems 1 5 2 Outline of the Proposed Research The dissertation is structured as follows Chapter 2 Webcasting and Automatic Speech Recognition This introductory chapter presents an overview of webcasting the problem of enhancing the usability and usefulness of webcasts through transcripts and defines the problem of automatically obtaining transcripts for lectures and presentations outlining the challenges 
thereof Chapter 3 The Acceptable Webcast Word Error Rate of Machine Generated Transcripts The research introduced in this chapter focuses on determining the acceptable speech recognition quality that enables the usefulness of webcasts and enhances users experience by adding transcripts Chapter 4 Improving Automatic Speech Recognition for Webcast Lectures As part of my research on improving the ASR for lecture transcription several novel techniques are proposed here that address the problems of automatically building domain specific language models and improving the transcription of lecture specific speech Useful Transcriptions of Webcast Lectures 10 CHAPTER 1 INTRODUCTION Chapter 5 Wiki editing of Webcast Transcripts In order to further improve the usefulness of transcripts for webcasts an interactive and collaborative tool is introduced that allows users to correct errors in transcripts while watching an archive of a webcast This tool is evaluated through several in situ studies and refinements to the 
design are carried out Chapter 6 Automatic Speech Recognition for Webcast Lectures Learning from Wiki enabled Transcript Corrections In this chapter the research on improving ASR is integrated with the HCI aspects of my research by exploiting edits carried out by users through the collaborative editing tool to improve ASR even further rules extracted from the user edited transcripts of the first few minutes of each lecture are used to learn the most frequent ASR errors and adapt the system to correct them automatically Chapter 7 Contributions Conclusions and Future Work The final chapter contains further discussion of these contributions as well as a presentation of future research directions stemming from this work 1 5 3 Outline of the Surveyed Related Work The existing research relevant to each topic is presented as follows Chapter 2 Webcasting and Automatic Speech Recognition webcasting webcasting in education human factors in using webcast interfaces speech recognition Useful Transcriptions of Webcast 
Lectures 1 5 STRUCTURE OF THE DISSERTATION Chapter 3 The Acceptable Word Error Rate users 11 of dealing Machine Generated Webcast Transcripts with imperfect transcripts Chapter 4 Improving Automatic Speech Recognition for Webcast Lectures ASR for lectures and academic presentations language model adaptation to dedicated domains Chapter 5 Wiki editing of Webcast Transcripts computer suported collaborative work used to compensate for limitations in Artificial Inteligence systems Chapter 6 Automatic Speech Recognition for Webcast Lectures Learning from Wiki enabled from Transcript Corrections bootstrapping ASR manual transcripts corrections transformation based learning in NLP and ASR 1 5 4 Peer Reviewed Publications The contributions of this dissertation has been presented at several peer reviewed scientific conferences Munteanu et al 2006a 2006b 2006c Chapter 3 Webcasting and Automatic Speech Recognition Munteanu et al 2007 Chapter 4 Improving Automatic Speech Recognition for Webcast Lectures Munteanu et al 
2006d 2008a Chapter 5 Wiki editing of Webcast Transcripts Useful Transcriptions of Webcast Lectures 12 CHAPTER 1 INTRODUCTION of the research in Munteanu et al 2008b a general overview Chapters 3 4 and 5 and summarized in Section 7 1 Useful Transcriptions of Webcast Lectures Chapter 2 Webcasting and Automatic Speech Recognition Internet broadcasting webcasting is becoming an increasingly popular method of delivering lectures and academic presentations over the Internet Baecker et al 2003 mainly sustained by the increased availability and affordability of broadband Internet connections For example major media corporations offer newscasts and universities deliver lectures over the Internet At the same time more of these media are being archived and accessed by users through interactive systems While interactive media such as webcasts of lectures and presentations has become increasingly ubiquitous current interfaces to online media archives do not provide the same experience in accessing information as humans 
are accustomed to when using traditional information repositories like libraries This is in part due to the absence of transcripts of the digital media s audio channel Spoken Language Processing and in 13 14 CHAPTER 2 WEBCASTING AND AUTOMATIC SPEECH RECOGNITION particular Automatic Speech Recognition is one of the research areas focused on developing algorithms and technologies dedicated to better and more naturally facilitating access to information stored in spoken form In this chapter I will present an overview of webcasting and the problem of enhancing the usefulness of webcasts through transcripts Section 2 1 I will then define the problem and outline the challenges of automatically obtaining transcripts of lectures and presentations Section 2 2 2 1 Lecture Webcasting Currently webcasting is a common medium found in numerous Internet based applications ranging from entertainment to news Beside the technological advancements facilitating this its widespread adoption can also be attributed to the fact 
that in most instances users were already familiar with similar unidirectional broadcasting media such as television In contrast educational environments for which lecture webcasting applications are developed are significantly more interactive and richer in information exchanges Although lecture webcasting is still a research area in itself there are several webcasting system exclusively dedicated to lectures Among those University of California s BIBS Rowe et al 2001 and University of Toronto s ePresence Baecker et al 2003 are some of the early webcasting systems that were extensively tested in academic environments the BIBS system is supposedly the first lecture webcast system having streamed the first on demand replay of a seminar recording over the Internet in 1995 More Useful Transcriptions of Webcast Lectures 2 1 LECTURE WEBCASTING 15 recently other systems became available such as the MIT Lecture Browser Glass et al 2007 or the Microsoft Research LecCasting System Zhang et al 2008 A comprehensive 
overview of available lecture dedicated webcasting technology can be found in Deal 2007 2 1 1 An Overview of Webcasting Traditionally webcasts differ from other broadcasts in that the transmission medium is the Internet a review of webcasting can be found in Baecker 2003 More recently however with the emergence of research on making webcasting more interactive the delivery of richer media is increasingly seen as a mean to enhance the participation of webcast users and to make knowledge sharing more accessible A typical webcast archive1 system allows users to access audio video recordings which are synchronized with other information media such as slides and often with higher level content structure such as tables of contents and time progress indicators As a framework for my research the ePresence webcasting system http epresence tv is used which is part of an ongoing research project that has the goal of making webcasting highly interactive more engaging and more accessible and webcast archives more useful 
and usable Baecker et al 2003 As illustrated in Figure 2 1 the ePresence system gives users full control of the archive through the display of slides used in lectures and video recording through interaction with a table of contents 1 Throughout this dissertation the terms webcast archive or archives are used to denote a collection repository of interactive recordings of webcasts Useful Transcriptions of Webcast Lectures 16 CHAPTER 2 WEBCASTING AND AUTOMATIC SPEECH RECOGNITION Figure 2 1 The ePresence system displaying an archived lecture webcast Useful Transcriptions of Webcast Lectures 2 1 LECTURE WEBCASTING 17 headings and the titles of the slides and through the timeline an interactive clickable fine grained time progress indicator 2 1 2 Research Challenges While development work for lecture webcasts is currently focused mainly on facilitating remote viewing either live or offline research efforts are beginning to be dedicated to improving and enhancing users experience One important aspect being studied 
is that of webcast systems enabling users the same experience as attending in a lecture in person Birnholtz et al 2008 However since classrooms are complex environments with rich interactions webcasts do not provide a real substitute for most aspects of the classroom experience Recent research showed that students do not perceive lecture webcasts as replacing the classroom experience Brotherton and Abowd 2004 Chandra 2007 but instead complement it This finding can in part be explained by some of the shortcomings of current webcast systems such as the lack of adequate support for real time interaction Brotherton and Abowd 2004 or a diminished sense of awareness among remote lecture participants Birnholtz 2006 Despite the current challenges faced by lecture webcast systems the use of webcasts for lectures is not only positively accepted by students some studies show improvement in students activities such as lecture reviewing Brotherton and Abowd 2004 Furthermore it was reported that students academic 
performance also increased when using webcast archives of pre recorded lectures Day and Foley 2006 As illustrated in the review by Deal 2007 several other studies show contradictory findings Useful Transcriptions of Webcast Lectures 18 CHAPTER 2 WEBCASTING AND AUTOMATIC SPEECH RECOGNITION For example students indicated they consider watching a webcast a better academic experience than going to classes although the same number of student mentioned that they would attend the classes even if webcasts are available The findings of the various studies on webcast lectures mentioned here suggest that such systems will not replace traditional educational environments but rather complement and enhance them As it is pointed out in Day and Foley 2006 webcasts can be used to deliver the main static content of a lecture allowing students to benefit from more interaction with their instructor during regular classrooms However for such changes to be effective the webcast system must allow and facilitate efficient access to 
the information delivered during lectures One research area that is recently receiving increased attention is that of improving access to lectures Although the main focus of this is on facilitating access for students with disabilities it is to be expected that everyone will benefit from improved access to and interaction with lectures content a phenomenon known as curb cuts effects Hesse 1995 As webcasts can be used to complement educational environments instead of aiming at replacing physical classrooms an interdisciplinary approach is needed for improving students experience when interacting with webcast lectures from delivering better information access tools to further analyzing the social aspects of web based interactions in educational environments Several other research areas where information access is of critical importance have turned to Natural Language Processing and Automatic Speech Recognition in particular to provide solutions Useful Transcriptions of Webcast Lectures However such 2 1 LECTURE 
WEBCASTING 19 research has been rather limited in its application to lecture webcasts Beside the work presented in this thesis only a small number of other lecture systems such as MIT s Lecture Browser Glass et al 2005 or Tokyo Institute of Technology s Julius system Kawahara 2004 Furui 2005a look at ASR as a solution for facilitating access to information Unfortunately these system do not integrate the ASR based advancements with extensive HCI research although the MIT Lecture Browser http web sls csail mit edu lectures offers a web based interface for displaying transcripts synchronized with video recordings of lectures as illustrated in Figure 2 2 Enabling speech recognition for lecture webcasts offers the opportunity to improve access through a range of text based tools ranging from full transcripts of a lecture to summaries and to search and indexing However advances in ASR research for lectures must be complemented by similar progress in all other critical aspects related to lecture webcasting such as 
Human Computer Interaction HCI or Computer Supported Collaborative Work CSCW Therefore this dissertation will focus on an interdisciplinary approach to enhancing access to webcast lectures by combining research in HCI CSCW and ASR Useful Transcriptions of Webcast Lectures Figure 2 2 The display pane of the MIT lecture browser 20 CHAPTER 2 WEBCASTING AND AUTOMATIC SPEECH RECOGNITION 2 1 3 Integrating Text with Webcast Media As more media become available on line and more information is shared through webcasting one of the striking differences from traditional forms of knowledge sharing is that newer media are often not accompanied by text Although there are several proposals for audio based techniques that improve access to recordings Arons 1997 Sawhney and Schmandt 2000 user studies Dufour et al 2005 suggest that transcripts are a much needed tool for carrying out complex tasks that require information seeking from archives of webcast media Moreover providing access to users with hearing impairments Wald and 
Basson 2003 only makes for an even stronger case in favour of offering text transcripts along with audio video media in archives of webcasts Obtaining transcriptions for online media would also improve the way human users search for organize and retrieve specific information from large collections Hauptmann et al 2003 As mentioned in Section 1 2 given the increasing size of information rich media archives automatic speech recognition is generally seen as a feasible solution for generating text transcripts of such archives The following section presents an introduction to the problem of automatic speech recognition and the challenges in using ASR to automatically transcribe archives of webcast media such as lectures and presentations Useful Transcriptions of Webcast Lectures 2 2 AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES 21 2 2 Automatic Speech Recognition for Webcast Lectures 2 2 1 An Overview of the Automatic Speech Recognition Process Speech recognition is a pattern matching process Schroeder 1999 
Statistical pattern matching techniques based on dynamic programming and Hidden Markov Models together with the mathematical theory of digital signals and spectral analysis are the foundation of speech processing A more practical definition is introduced in Jelinek 1997 along with a schematic description Figure 2 3 A speech recognizer is a device that automatically transcribes speech into text Figure 2 3 The speech recognition process As seen in Figure 2 3 the speech recognition process can be divided into two stages Acoustic Processing and Linguistic Decoding The Acoustic Processing stage transforms the electrical signal coming from a microphone capturing an utterance into acoustic symbols such as phonemes or other Useful Transcriptions of Webcast Lectures 22 CHAPTER 2 WEBCASTING AND AUTOMATIC SPEECH RECOGNITION The Linguistic Decoding finer grained acoustic units sub phonemes stage then processes these symbols and outputs a set of word hypotheses representing with a certain degree of probability the text 
of the utterance While acoustic processing is an important part of the speech recognition process this is not the main focus of the research presented here Instead this section is focused on Linguistic Decoding and follows the introduction from Jelinek 1997 Complete descriptions of the speech recognition process and of all methods involved can be found in several books such as Rabiner and Juang 1993 Becchetti and Ricotti 1999 De Mori 1998 Manning and W This represents a string of words W w1 w2 wn that are the most probable to have been spoken given the observed sequence of acoustic symbols A Using Bayes formula the probability of a sequence of words given an observed sequence of acoustic symbols can be written as P W P A W P A P W A Useful Transcriptions of Webcast Lectures 2 2 AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES 23 Since A is fixed there is no other acoustic data the output of the speech recognizer can be expressed as argmax P W P A W W W Since P A W cannot be computed beforehand due to the 
extremely large number of possible pairings between all words in the vocabulary and all acoustic symbols this probability is computed during the speech recognition process For this an acoustic model pronunciation model is employed This would model the way a specific word is pronounced Acoustic models are trained on transcribed speech i e aligned sequences of acoustic symbols and linguistic units words in this particular case Usually acoustic models are built using Hidden Markov Models HMMs Rabiner 1989 Rabiner and Juang 1986 Acoustic modeling alone cannot solve the speech recognition problem A language model is also needed P W represents the probability of uttering a specific word sequence W More specifically the language model gives the probability of uttering a specific word wn after a sequences of words w1 wn 1 was uttered n gram models Language models are trained on various types of corpora from transcribed speech to collection of texts from the World Wide Web Since many of the domains where ASR systems 
are deployed are characterized by a certain topic vocabulary or speaking style significant research efforts are spent on finding appropriate ways to build corpora and to train language models that will better reflect that particular domain Useful Transcriptions of Webcast Lectures 24 CHAPTER 2 WEBCASTING AND AUTOMATIC SPEECH RECOGNITION 2 2 2 Improving Automatic Speech Recognition From a user point of view as well as from a research perspective the speech recognition process can be viewed as structured on three levels what can be done on the recognition level on the evaluation side and on the resource level It is not the purpose of this document to give a complete introduction to speech recognition Therefore I will only present a top level view of the recognition process structured on these three levels in Figure 2 4 by extending Figure 2 3 from Section 2 2 1 Figure 2 4 An overview of the automatic speech recognition process with the different parts of the process grouped under three levels the recognition 
level the evaluation side and the resource level The Recognition Level This level refers to the direct manipulation of the multi modal input channel For presentations three modalities can be identified speech signal needs to be processed in order to extract the feature vectors used in recognition Several particularities of the presentation speech Useful Transcriptions of Webcast Lectures 2 2 AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES 25 may need special attention such as the acoustic characteristics noise level reverberation level etc of the presentation room A task particularly important for lecture presentation speech recognition is the segmentation of the speech signal based on topic speaker events etc interaction if possible the movement and action of the pointing device used by the speaker should be captured supporting text includes any text if available that has been used in the presentation such as the content of the slides preferably synchronized with the speech signal This supplemental data 
can complement the speech signal during the recognition process In my research I will mainly focus on exploiting the supporting text and improve the linguistic decoding phase of the recognition process The Resource Level The resources available for speech recognition are the most critical aspect of the recognition process Several kinds of corpora are needed natural language texts speech signal collections and presentation texts While unannotated corpora are not a scarce resource especially with the increased amount of information available through the internet it is annotated corpora much more expensive to be produced that are needed for developing better speech recognition tools Through training these resources lead to the development of the language and acoustic models used in recognition A significant part of the research on improving speech recognition in particular Useful Transcriptions of Webcast Lectures 26 CHAPTER 2 WEBCASTING AND AUTOMATIC SPEECH RECOGNITION for automatic lecture transcription 
including some of the focus of this thesis is dedicated to building corpora and models specific to a particular domain The Evaluation Level The evaluation level consists of the word hypotheses proposed by the speech recognizer At this level accuracy evaluation can enhance the output by adding confidence labels to the word hypotheses The confidence labels can be used as a general measure for evaluating the speech recognition or can be used for further improvements to the recognition process As part of the specific research on lecture and presentation transcription my research is focused on providing measures for assessing the quality of an ASR system that would better reflect the fact that automatically generated lecture and presentation transcripts are used by humans 2 2 3 Research Challenges The most commonly used measure for the quality of a speech recognition system is word error rate WER the percent of incorrectly recognized words in a spoken utterance 2 In ideal conditions an ASR system can produce a 
WER close to 0 typically 1 3 In order to achieve this level of accuracy the recording must take place in an anechoic room the speaker should speak at a very slow rate e g text reading with pauses between sentences and the ASR system must be trained on previous recordings of the same 2 WER is defined as the edit distance percentage of substituted deleted and inserted words between the correct sentence and the output sentence from the ASR system Huang and Hon 2001 Useful Transcriptions of Webcast Lectures 2 2 AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES 27 speaker collected in similar conditions Several practical applications can deal well with noisy conditions but they usually carry with them significant restrictions such as a vocabulary limited to numbers or address book names for voice dialing State of the art speech recognizers dedicated to large vocabulary continuous speech LVCS such as broadcast news transcription systems can achieve WERs around 20 The conditions for webcast recordings are in stark 
contrast with these ideal conditions The archives consist of diverse speakers with particular speech styles and various accents including non native various acoustical conditions regular lecture or meeting rooms and the vocabulary is extremely large determined by the large pool of topics the archives unsuitable for training an ASR system This makes The recognition task for ePresence archives can be classified as LVCS speaker independent noisy background ASR As mentioned previously the state of the art WER for LVCS ASR broadcast news transcription is about 20 and for conference talk ASR described throughout Section 4 2 is about 40 As shown in Weintraub et al 1996 recognition accuracy could be degraded by a factor as large as 1 5 for each external condition that becomes non optimal for speech recognition such as moving from reading to conversation as well as from conversation to spontaneous speech Therefore it should be expected that automatically generated webcast transcriptions will have a lower WER than 
those of conference talks given the much more hostile both acoustically linguistically and pragmatically environments of ePresence recordings lecture open meetings etc Due to the adverse conditions characterizing lecture speech typical WER Useful Transcriptions of Webcast Lectures 28 CHAPTER 2 WEBCASTING AND AUTOMATIC SPEECH RECOGNITION for lecture speech can reach rates as high as 50 when general purpose ASR systems are used Park et al 2005 Munteanu et al 2007 While transcription accuracies in the range of 3 Glass et al 2007 report lecture WERs as low as 17 when transcripts for 29 hours of previous lectures by the same instructor are available Useful Transcriptions of Webcast Lectures Chapter 3 The Acceptable Word Error Rate of Machine Generated Webcast Transcripts Despite efforts to improve the quality of ASR systems current ASR systems do not perform satisfactorily in domains such as transcribing lectures or conference presentations This is caused by poor acoustic conditions diverse speakers with 
particular speech styles and various accents including non native and large vocabularies determined by the large pool of topics In perfect conditions anechoic room slow speaking rate limited vocabulary ASR system previously trained on the same speaker state of the art systems can achieve a Word Error Rate WER 1 of less than 3 1 For less While WER might not always be an adequate measure of transcript quality Wang and Chelba 2003 it is widely used due to practical considerations Thus it was also our choice as a measure of ASR accuracy 29 30 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS restricted domains with good acoustic conditions such as broadcast news the state of the art WER is about 20 25 Gauvain et al 2002 When acoustic conditions degrade such as in lectures or conference talks WER can increase to 40 45 Leeuwis et al 2003 although some reports suggest a 20 30 WER for lectures given in more artificial and better controlled conditions Rogina and Schaaf 2002 Kato et al 
2000 Figure 3 1 The transcript enhanced ePresence system Useful Transcriptions of Webcast Lectures 31 In this research manually and semi automatically generated transcripts were introduced into webcast archives and are investigating the influence of WER on the usability and usefulness of these archives Three research questions were asked Useful Transcriptions of Webcast Lectures 32 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS time synchronized with the video by boldfacing the current line of the transcript thus emulating a closed captioning system while fully displaying the transcript of the segment of lecture for the current slide The line breaks do not represent ends of sentences but rather correspond to pauses longer than 200ms 2 To further enhance the user s control over the lecture users can re synchronize the playback of the video by clicking on a line in the transcript In this chapter I will survey the existing research on how humans deal with imperfect transcripts 
and how to improve their interaction with online repositories of multimedia Section 3 1 I will then formulate several research questions and hypotheses related to the usefulness of ASR transcripts for webcast archives and the relation between transcript quality and users experience Section 3 2 introduce the design and setup of a human experiment user study3 for testing the hypotheses Section 3 3 and present the results of this experiment Sections 3 4 and 3 5 2 Since the lecture recordings employed in all the evaluations described in this dissertation were transcribed by several transcribers an arbitrary 200ms but easier to visually detect pause metric was chosen in order to ensure inter transcriber consistency Such a metric is preferable to automatic silence detection algorithms such as that of Placeway et al 1997 because the latter requires manual fine tuning to maintain consistency across speakers 3 Whenever used throughout this dissertation the term user study denotes an experiment with human subjects 
through which some hypotheses were tested Useful Transcriptions of Webcast Lectures 3 1 RELATED WORK 33 3 1 Related Work As more media archives become available research is starting to emerge on users strategies for navigating through such information rich repositories Studies on how archived webcasts are used Dufour et al 2005 and on the effectiveness of navigational tools for webcast archives Toms et al 2005 provide clues as to how users access information in webcasts Transcripts seem much needed to aid navigating through a webcast Dufour et al 2005 or accessing information in spoken media Wald and Basson 2003 Research is therefore needed to establish what is a satisfactory quality for archive transcripts and to develop better ASR systems that deliver transcripts with lower WERs Equally important since ASR techniques that achieve close to 0 WER are not likely to be available in the near future Whittaker and Hirschberg 2003 more studies are needed to understand users expectations from transcripts and to 
explore how imperfect transcripts should be integrated into a highly interactive webcast system Transcribing lecture presentation speech is a research topic still in its infancy The challenges met by the task of recognizing open domain speaker independent large vocabulary continuous and noisy speech are very hard to overcome While a significant amount of research effort have been spent on improving speech recognition for lectures and presentations Kato et al 2000 Rogina and Schaaf 2002 Leeuwis et al 2003 Park et al 2005 the quality of the transcripts typically WERs of 30 40 at most 20 in particular conditions is still below that for other domains such as broadcast news transcriptions For certain automated applications where transcripts obtained through Useful Transcriptions of Webcast Lectures 34 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS ASR are used by a machine e g travel reservation systems such as ATIS Ward and Issar 1995 a lower WER might not affect the system s 
performance as long as keywords are recognized accurately However when transcripts are to be used directly by humans the overall quality of the text could be more critical Unfortunately the research that investigates how humans deal with error ridden transcriptions and which accuracy rates can be deemed acceptable is scarce One of the few existing studies of users needs with respect to the ASR accuracy sought to assess users perception of improvements in recognition accuracy Van Buskirk and LaLomia 1995 The result of a Wizard of Oz simulation showed that humans are only able to perceive differences in WER greater than 5 10 when asked to directly rate the quality of transcriptions A previous study on handwriting recognition systems by the same authors LaLomia 1994 showed that users expectations of accuracy vary with how critical the domain of the application is participants were less willing to accept higher error rates for documents to be sent to their boss than for documents of personal use This shows that 
while users perception of transcript quality is very subjective it is also coarse grained Unfortunately this research does not measure the perception of the recognition accuracy for scenarios in which users perform specific information oriented tasks A study based on recognition accuracy that assessed human ability to use transcripts is presented in Stark et al 2000 Users performed summarizations and relevance judgements of audio materials from the HUB news corpus using transcripts of various WERs obtained by different state of the art ASR systems As expected the better the transcript Useful Transcriptions of Webcast Lectures 3 1 RELATED WORK 35 accuracy the better users performed on several measures such as time to solution solution quality amount of audio played and probability of abandoning the transcripts This study served as a motivation for the SCANMail system Whittaker et al 2002 a voicemail user interface that offers synchronized browsing skimming through a voicemail message and its automatically 
generated transcription While the SCANMail study revealed that users spent less time performing their tasks when they could browse through speech and text simultaneously their performances were lower for keywords that were not properly transcribed Also subjects were sometimes misled in their tasks by transcription errors assuming that transcripts accurately reflected the content of the voice message Another finding was that users were looking in the voice messages mostly for critical information such as phone numbers or names and that phone numbers especially needed to be recognized accurately It is to be expected that users performance when faced with an errorful transcript in the context of a speech browsing interface can be improved by providing additional information mining tools Indeed it is shown in Whittaker and Hirschberg 2003 in a similar context as Stark et al 2000 the retrieval of spoken news documents that when users are using a search tool to retrieve documents matching their query providing 
visual information extracted from transcripts about their search results can be more effective than displaying only the errorful transcript of the news story Thus appropriate choices for the design of multimedia browsing tool can offset some of the shortcomings of having imperfect transcripts Useful Transcriptions of Webcast Lectures 36 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS Unfortunately while these studies provide valuable insights into the users handling of errorful transcripts they do not study the relation between performance and WER nor do they provide insights into what level of WER is acceptable for a transcript to be included in a browsing interface Furthermore research is also needed to investigate how users compensate for errors in transcripts 3 2 The Research Questions and Hypotheses purpose of this research was to assess the usefulness of automatically generated transcripts for webcast archives and their influence on the usability of webcasts For this I 
have studied how partially correct transcripts would affect user performance and user perception of the system and thus would suggest an appropriate level of WER in which to aspire Specifically this research is proposing to test Performance Quality Hypothesis User performance will decrease with increased WER It is expected that users performance to be influenced by the accuracy of the transcripts the performance should increase as the quality of transcripts improves It is expected that transcripts become useful users perform tasks at the same level as or better than without transcripts when the WER of the transcripts is 25 or less Experience Quality Hypothesis The quality of the user s experience will decrease with increased WER It is expected that users perception4 of their experience in completing tasks is influenced by 4 User perception was not measured as a single value but represented instead as a series of indicators each with its own result as discussed in the next section Useful Transcriptions of 
Webcast Lectures 3 3 METHODS 37 the accuracy of the transcripts positive experiences should increase as the quality of the transcripts improves It is expected that users appreciate transcripts as a feature of the system when the WER of the transcripts is 25 or less 3 3 3 3 1 Methods Overview In order to test these hypotheses a within subjects study was designed in which participants were exposed to multiple levels of WER in their interaction in a typical webcast use 3 3 2 Independent Variables The independent variable in this study was the WER The WER of a transcript was computed as the average WERs of the sentences transcript lines of length at least 3 words 5 We assessed the effect of the WER at four levels WER 5 Most 1 and 2 word lines were just breathing noises or repetitions Useful Transcriptions of Webcast Lectures 38 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS acoustics and diverse speakers WER 3 3 3 Task Each participant completed a quiz consisting of five 
questions for each webcast viewed Each webcast was on a different lecture The 38 minute lectures came from a a third year undergraduate course The Design of Interactive Computational Media Participants were required to complete each quiz in 12 minutes which forced them to finish the quiz without listening to the entire lecture Of the five quiz questions at least one had the answer on the slide and at least two did not have the answers on a slide and were obscured by the errors in the transcripts The quizzes contained only factual questions specific to each of the lectures and answers were typically very short e g Who developed PICTIVE 3 3 4 Measures and Instruments As stated in the introduction to Section 3 2 the purpose of the research presented in this chapter was two fold first to assess the usefulness of automatically generated transcripts for webcast archives and how this usefulness is affected by transcription quality and second to investigate how transcripts of various levels of quality influence the 
usability of webcast archives Although usefulness and usability are closely related and sometimes it is difficult to clearly divide them Landauer 1996 defining these terms is needed in order to justify the choice of measures and instruments used to Useful Transcriptions of Webcast Lectures 3 3 METHODS 39 verify the hypotheses formulated in Section 3 2 Usefulness is a term that is rarely introduced formally in the HCI literature however it is often used to describe the degree to which a system performs the functions it was designed to perform It is sometimes referred to as utility defined as the question of whether the functionality of the system in principle ca do what is needed Grudin 1992 Usability is a key concept in HCI research and several authors proposed various definitions most of them having the interaction between users and system at their core the question of how well users can use a system s functionality Grudin 1992 or the quality of a system with respect to ease of learning ease of use and user 
satisfaction Rosson and Carroll 2002 Therefore to compare the effect of each level of WER from the perspective of these two important HCI concepts two types of data were collected Task performance data and User perception data The instruments used to collect these data are included in Appendix C Through task performance data we will assess the relation between WER and the usefulness of transcripts for archives of webcast lectures The analysis of user perception data will provide information related to users subjective experience when browsing and mining for information in archives of webcast lectures and through several of the indicators described in the following paragraphs to the influence that transcripts and their quality has on various aspects of the usability of webcast archives Useful Transcriptions of Webcast Lectures 40 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS Task Performance Data This was assessed by the accuracy of responses to a quiz on the presentation 
Each five question quiz had a maximum value of 10 points with two points for each correctly answered question The questions were designed in such a way that answers were unambiguous Therefore no partial points were awarded except for answers that were half complete which received one point Typically half complete answers were those for which participants found a partially correct answer caused by speech recognition errors in the transcript but did not verify its accuracy by listening to the corresponding audio stream In order to eliminate the effects that differences in difficulty among lectures may have on quiz scores the scores referred from this moment on as raw quiz scores were averaged across participants for each lecture and relative quiz scores defined as the difference between the raw quiz score and the lecture average were used For the four lectures used in the experiments the raw quiz averages were 4 10 5 62 6 18 and 6 67 Therefore relative quiz scores could range from 6 67 to 5 90 For example quiz 
scores on the most difficult lecture of 4 10 average could have ranged from 4 10 a raw score of 0 to 5 90 a raw score of 10 User Perception Data User perception was assessed using a series of indicators derived from two instruments a post quiz questionnaire completed after each quiz that assessed user perception of the task at a specific level of WER and a final post session questionnaire which reflected the influence of WER on Useful Transcriptions of Webcast Lectures 3 3 METHODS 41 users experience These instruments consisted of multiple choice questions and or indicated agreement disagreement with various statements user perception indicators include Perception of task difficulty Participants rated the difficulty of each quiz relative to a preliminary quiz Confidence in performance Participants assessed the correctness The of their answers to the quiz by choosing one of All correct Mostly correct Some correct Mostly wrong All wrong choices for the question I think my answers on the quiz were Perception of 
speech recognition errors Participants indicated their degree of agreement disagreement with two statements The errors in the transcript didn t stop me from solving the quiz and I was bothered by the errors in the transcript These statements were included only for tasks in which transcripts were present Another statement assessing directly their perception of errors I haven t noticed significant differences in the quality of the transcripts for different lectures was presented on the post session questionnaire Perception of usefulness of transcripts Participants indicated agreement disagreement with statements referring to transcripts as being helpful in solving the quiz better and in solving the quiz faster while on the post session questionnaire they indicated their agreement with the statement I would rather have transcripts with some errors than not have transcripts at all Perception of usability of transcript enhanced webcasts Participants through the post session questionnaire indicated which Useful 
Transcriptions of Webcast Lectures 42 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS We also feature they used to compensate for errors in the transcripts assessed the usability of making transcripts clickable both to compensate for errors in the transcript and as a general browsing feature Confidence in using the entire system Participants indicated the context in which they would choose to use the transcript enhanced ePresence webcast system The contexts ranged from very critical to less critical Prepare for an examination instead of going to classes Prepare for an examination in addition to going to classes Prepare for an assignment and Make up for a missed class For each context participants could choose Yes No or Only if transcripts have no errors The user perception data consist of ordinal and discrete values representing either choices on a rating scale or agreement disagreement with various statements In order to eliminate the differences in the lectures difficulty 
the post quiz raw data were translated into relative values in the same manner as the quiz scores Data collected from the post session questionnaires were not adjusted since these questionnaires addressed users overall experience with the enhanced ePresence system 3 3 5 System A Wizard of Oz like study was conducted in order to determine these relations as this simulation method is one of the most appropriate for studying the natural language based human computer interaction Bernsen et al 1998 Life et al 1996 Although Wizard of Oz s drawback resides in the need for a skilled human wizard this method is preferred instead of prototyping since the cost of building a full featured natural language Useful Transcriptions of Webcast Lectures 3 3 METHODS Variable Size in sentences of lecture corpus Modified lecture sentence lengths Number of added HUB 4 sentences Modified HUB 4 sentence lengths Values 20 50 100 200 all 1 5 7 original 0 650 all 1 5 7 original 43 Table 3 1 The variable used to control the training 
overfitting of the lecture language models prototype is often prohibitive However the proposed simulation method provides the convenience of Wizard of Oz setups while behaving like a true prototype system with no on line wizard intervention As the goal is to evaluate the user performance at four pre determined levels of WER see Section 3 3 1 for the rationale of choosing these levels it was important to maintain a realistic scenario for the Wizard of Oz simulation as it is recommended for studying natural language based human computer interaction 6 WERs that are usually reported in the literature for current broadcast news 25 and lecture speech 45 ASR systems Future work will take in consideration finer grained levels of WER Useful Transcriptions of Webcast Lectures 44 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS Lecture Number of sentences in lecture Variables values for WER Size in sentences of lecture corpus Modified lecture sentence lengths Number of added HUB 4 
sentences Modified HUB 4 sentence lengths 0 100 25 1 2 3 4 1280 928 811 972 45 25 45 25 45 25 45 20 200 20 100 20 50 20 original 5 original 5 original 5 original 5 650 0 650 0 650 0 650 1 1 1 1 Table 3 2 The training overfitting variables values for the target WERs of 25 and 45 Useful Transcriptions of Webcast Lectures 3 3 METHODS 45 particular to segments of lectures containing a variable number of sentences The following sections describe the design and setup of a WER controlled ASR system as well as details about the audio material used in testing 3 3 6 Acoustic Models The acoustic model AM that is part of the SONIC toolkit was used in our experiment The decision tree state clustered HMMs model is built on 30 hours of data from 283 speakers from the WSJ0 and WSJ1 subsets of the 1992 development set of the Wall Street Journal WSJ Dictation Corpus LDC 1994 The WSJ Dictation Corpus is a collection of microphone recordings 1 channel 16 bit 16KHz sampling rate of WSJ news texts read by journalists not 
necessarily with experience in dictation Both for the AMs and for the recognition process the acoustic vectors were represented using SONIC s default7 Perceptual Minimum Variance Distortionless Response PMVDR cepstral coefficients with a 39 dimensional feature vector 12 PMVDR parameters computed over 10ms audio frames and 20ms Hamming windows 3 3 7 Language Models Several alternatives exist for simulating the errors produced by ASR systems from a simple but unrealistic solution of replacing words in manual transcripts to complex algorithms that mimic the mismatches caused by both acoustic and language models such as in Schatzmann et al 2007 Since the focus of the ASR research presented in this dissertation was on 7 Overall we were pleased with SONIC s out of the box features pertaining to the acoustic part of the ASR process Useful Transcriptions of Webcast Lectures 46 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS improving LMs for lecture transcription all the experiments 
conducted as part of this research used speaker independent acoustic models As such for the experiment described in this Chapter ASR errors were simulated by controlling the LM training process In order to have a greater control of the overfitting process the training sentences were mixed with the transcripts of the 1997 LDC Broadcast News HUB 4 Corpus Stern 1997 Evaluation Set Although tri gram8 LMs were built on the training corpora further variability was introduced in the training process by altering the length of the training sentences this was achieved by concatenating all sentences in the corpus and then splitting them in new sentences of equal length A summary of the variables used to control the training overfitting process is presented in Table 3 1 The tri gram LMs were built in ARPA format using the CMU Cambridge Statistical Language Modeling toolkit Clarkson and Rosenfeld 1997 and converted to the SONIC binary format 3 3 8 Lexicon The pronounciation dictionary was build to cover all words found 
in the manual transcription thus there were no out of vocabulary items Individual lexicons were built for each segment of the lecture corpus on which LMs were trained The CMU Pronouncing Dictionary v 0 6 CMU 1998 was used to extract the pronounciations for the lecture words For technical words 8 A description of tri gram language models is given in Annex B Glossary of Technical Terms Useful Transcriptions of Webcast Lectures 3 3 METHODS 47 not in the dictionary9 the SONIC s sspell lexicon access tool was used to generate pronounciations using letter to sound predictions from a decision tree which we trained on the entire CMU Pronouncing Dictionary 3 3 9 The Recordings used for our study were collected in a large recordings amphitheatre style lecture hall 200 seating capacity using the AKG C420 head mounted directional microphone The lecturer is male early 60s and a native speaker of English The recordings were not intrusive and no alterations to the lecture environment or proceeding were made The 1 channel 
recordings were digitized using the TASCAM US 122 audio interface as uncompressed audio files with 16KHz sampling rate and 16 bit samples 3 3 10 Recognition The recognition was performed on each set of sentences using the language model that was trained on data consisting of or containing the same set For an individual lecture a set of models that produced the desired average WER was chosen such that all models in that particular set were trained using the same values for the training variables presented in Table 3 1 The variables values for the target WERs of 25 and 45 are outlined in Table 9 Only few technical words were not found in the CMU Pronouncing Dictionary Also as it will be shown in Chapter 4 if lectures are accompanied by slides the LMs can be easily adapted to include such words As such the realism of this simulation is not likely to be affected by the lack of OOV items Useful Transcriptions of Webcast Lectures 48 3 2 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST 
TRANSCRIPTS The SONIC decoder performs recognition in two passes The first pass decoding uses the specified AMs and LMs After the first pass is complete an unsupervised Maximum Likelihood Linear Regression MLLR of the AM is performed using the output of the first pass ASR hypotheses are labeled with confidence scores The second pass uses the MLLR adapted AM Since the recognition is performed in two passes each of them producing its own hypotheses we also considered using as one of the WER controlling variables the pass from which we selected the ASR output However as mentioned in Pellom 2001 SONIC s MLLR adaptation in the second pass usually produces an output of a slightly lower WER Thus we found that the output of the first pass was always a better choice for our purpose Besides allowing for a greater control of the WER variable the method used to generate lecture transcripts ensured that users were exposed to transcripts generated by a real ASR system Transcripts with these levels of WER as well as no 
transcript were integrated into an existing webcasting system that additionally provided the following components video of the presentation slides table of contents and timeline This setup allowed us to design an ecologically valid experiment as in a Wizard of Oz simulation without the need for the on line intervention of a human wizard 3 3 11 Experimental Design A repeated measures within subjects design was followed each participant completed four quizzes one for each level of the independent variable Each quiz was administered on a different lecture Useful Transcriptions of Webcast Lectures 3 3 METHODS 49 A Latin square design of size four was chosen to randomize the order in which participants were exposed to the four levels of the independent variable Kirk 1995 For the 48 participants 12 squares were used The squares were designed such that each level of the independent variable was matched with one of the four lectures an equal number of times and such that each of the four lectures appeared in every 
position in the sequence given to the participants 3 3 12 Participants The study was conducted using 48 students 26 female and 22 male at the undergraduate level recruited from various disciplines 3 3 13 Procedures Participants first completed a preliminary quiz that consisted of the questions from all four quizzes used in the experiment as well as filler questions to eliminate the potential for confounding effects that might have been caused by a previous exposure to the course lectures used in the study Only two participants correctly answered questions on the preliminary quiz and only one question each One of them answered the same question correctly on the quiz during the Useful Transcriptions of Webcast Lectures 50 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS data Next each of the four quizzes and the corresponding lectures were presented to participants Upon completion of each quiz participants were assigned a very brief post quiz questionnaire to assess user 
perception Breaks were permitted between quizzes After all quizzes were completed a post session questionnaire collected additional comparative user perception data and demographic information 3 3 14 Data Analysis The most suitable statistical test for within subjects design with various confounding effects and multiple level independent variables is the repeated measures ANOVA Howell 1999 which can be carried out in SPSS through the General Linear Model Repeated Measures procedure SPSS 2005 All tests were run using a significance level of 05 as the size of the null hypothesis rejection region For the ANOVA procedure the independent variable WER was used with its four levels WER 0 WER 25 WER 45 and WER NT Although we tested the data for normalcy a non parametric distribution free test Friedman s Rank Test for Correlated Samples Howell 1999 was also run and 2 scores were computed in order to confirm the validity of the F scores obtained through ANOVA Beside the tests for statistical significance simple 
descriptive statistics are also presented for each level of the WER variable Useful Transcriptions of Webcast Lectures 3 4 RESULTS TASK PERFORMANCE 51 3 4 Results Task Performance Synopsis Transcripts of WER 0 led to best Task performance followed in decreasing order by WER 25 WER NT and WER 45 The Performance Quality Hypothesis was tested through the ANOVA procedure Also a trend analysis was performed in order to estimate the nature of the relation between the scores corresponding to each level of WER As indicated in Howell 1997 and Howell 2002 for independent variables with ordinal values such as the WER variable trend analysis is more meaningful than multiple pairwise ANOVAs among levels of the independent variable in revealing the kind of relationship that exists between the independent and the dependent variable As the WER variable has a mixture of ordinal 0 25 and 45 error rates and categorical pairwise ANOVA comparisons were performed between WER NT and each of WER 0 WER 25 and WER 45 to confirm the 
findings of the trend analysis 3 4 1 Performance Quality Hypothesis Synopsis Users performance is indeed influenced by WER Also WER 25 is above the WER threshold for achieving the same performance as no Useful Transcriptions of Webcast Lectures 52 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS WER 0 25 NT 45 Mean Score 0 907 0 071 0 373 0 605 ANOVA Friedman F 3 141 7 264 p 001 2 F 3 18 325 p 001 Table 3 3 Mean relative quiz scores for each level of WER and tests of significance over all levels of WER transcripts However the increase in performance does not diminish as quality improves Instead the trend analysis detailed below suggests that performance varies linearly with the transcript s quality The ANOVA using all levels of WER shows a significant relation between quiz scores and quality of transcript Table 3 3 The results are also confirmed by the distribution free test Friedman s Rank test The trend analysis carried out on the ordinal values of WER shows a significant 
linear relation Table 3 4 WER 0 having the highest score and WER 45 the lowest In order to assess whether WER 25 leads to a better performance than having no transcripts we consider WER NT as an ordinal value being equivalent to a transcript of unknown WER The trend analysis Table 3 4 reveals that the quiz scores for WER NT fall between those for WER 45 and those for WER 25 Table 3 3 shows the average scores for each value of WER the relation still being best approximated as linear A set of multiple pairwise comparisons Table 3 5 was also carried out between the categorical value of WER WER NT and each of the Useful Transcriptions of Webcast Lectures 3 4 RESULTS TASK PERFORMANCE 53 WER 0 25 45 0 25 NT 45 Trend Linear F 1 47 20 133 p 001 Linear F 1 47 23 477 p 001 Table 3 4 Trend analyses over the ordinal values of WER and over all values of WER WER 0 25 45 Comparison with WER NT F 1 47 18 498 p 001 F 1 47 1 428 p 238 F 1 47 405 p 527 Table 3 5 Multiple comparisons between WER NT and each of the ordinal 
levels of WER Useful Transcriptions of Webcast Lectures 54 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS ordinal levels WER 0 WER 25 and WER 45 While the difference in means can be attributed to the quality of transcripts for WER 0 when compared to WER NT the differences between WER 25 and WER NT and between WER 45 and WER NT are not significant This confirms the trend analysis findings that the performance for WER NT is close to both WER 25 and WER 45 falling in between them and significantly lower than the performance for WER 0 3 4 2 Performance Quality Hypothesis Breakdown by Quiz Question Type A similar analysis was also performed on a OnSlide NotOnSlide breakdown of quiz scores Some of the quiz questions typically 1 or 2 out of 5 for each quiz could be answered without listening to the lecture the answer was found on the slides Therefore a separate analysis was carried out for quiz scores that summed up only the questions with answers on slides OnSlide scores and 
independently for those that required a thorough listening of the lecture or reading of transcripts in order to answer the questions NotOnSlide scores When considering only the OnSlide scores there are no significant effects of having different values for the WER variable However the differences in WER values have a significant effect on the NotOnSlide scores Table 3 6 The trend analysis Table 3 7 clearly shows a linear relation between WER values 0 25 and 45 and quiz NotOnSlide scores Interestingly the scores Table 3 6 for WER NT are now marginally lower than those for WER 45 Indeed the trend analysis over all levels of WER Table 3 7 indicates a linear Useful Transcriptions of Webcast Lectures 3 4 RESULTS TASK PERFORMANCE 55 WER 0 25 45 NT Mean Score 0 287 0 024 0 121 0 140 ANOVA Friedman F 3 141 8 473 p 001 2 F 3 18 175 p 001 Table 3 6 Mean relative NotOnSlide scores for each level of WER and tests of significance over all levels of WER WER 0 25 45 0 25 NT 45 Trend Linear F 1 47 18 139 p 001 Linear F 1 47 
29 293 p 001 Quadratic F 1 47 4 010 p 051 Table 3 7 Trend analyses over the ordinal values of WER and over all values of WER for NotOnSlide scores WER 0 25 45 Comparison with WER NT F 1 47 27 996 p 001 F 1 47 1 447 p 235 F 1 47 034 p 855 Table 3 8 Multiple comparisons between WER NT and each of the ordinal levels of WER for NotOnSlide scores Useful Transcriptions of Webcast Lectures 56 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS WER 0 25 NT 45 0 635 Mean Score 0 812 0 261 0 438 ANOVA Friedman Trend F 3 117 5 567 p 001 2 F 3 12 570 p 006 Linear F 1 39 18 207 p 001 Table 3 9 Mean relative scores for each level of WER across novice users and tests of significance and trends over all levels of WER relation between WER values and scores with a slight quadratic component explained by the close means for WER NT and WER 45 However in terms of post hoc analysis between WER NT and the ordinal levels of WER Table 3 8 the effects of WER 25 and WER 45 are as expected still not 
significantly different than those of WER NT The analysis of NotOnSlide scores indicates that the WER threshold for which transcripts yield better performance than having no transcripts lies in the upper part of the interval 3 4 3 Performance Quality Hypothesis Breakdown by Demographic Information While ANOVA tests and trend analyses allow inferences about the data collected through the experiments in many cases it is also important to take Useful Transcriptions of Webcast Lectures 3 4 RESULTS TASK PERFORMANCE 57 Figure 3 2 The histogram of the relative quiz scores for WER 25 Useful Transcriptions of Webcast Lectures 58 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS Figure 3 3 The histogram of the relative quiz scores for WER 25 only for subjects that haven t used the system or a similar one before Useful Transcriptions of Webcast Lectures 3 4 RESULTS TASK PERFORMANCE 59 a closer look at simple descriptive statistics Howell 1999 In particular for our experiment examining 
the histograms of quiz scores for each level of WER leads to some interesting observations Although the distributions of quiz scores for each value of WER can be approximated as normal the histogram for WER 25 Figure 3 2 shows an almost bi modal distribution with scores between 3 and 2 occurring 14 times while scores between 1 and 2 occur 12 times In order to determine the cause for having scores distributed around 2 poles for WER 25 we looked at the demographic information collected through the post session questionnaire The demographic information consists of familiarity with webcast systems e g using the system or a similar one before estimated number of hours of daily internet usage interaction with media content on internet field and year of study enrollment in the course where the recordings of lectures come from The histograms for each demographic factor were analyzed and only the used a similar system before factor 8 subjects out of 48 produced a change in the shape of the histogram Figure 3 3 shows 
the histogram for WER 25 with these 8 subjects removed The distributions for the other levels of WER are not affected by this factor The same analyses that were carried out for testing the Performance Quality Hypothesis were also performed using the 40 subjects that never used a similar system before novice users While there is still a significant effect of the WER variable on quiz scores Table 3 9 and a linear trend can also be observed among all levels of WER the mean scores for WER 25 for novice users are higher than for WER 25 across Useful Transcriptions of Webcast Lectures 60 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS all participants while scores for WER 0 and WER 45 remain relatively unchanged A possible explanation of this is that similar webcast systems as well as previous versions of the ePresence system do not include any transcripts in the interface thus participants that used such systems were not accustomed to handling transcripts While perfect 
transcripts WER 0 equally helped such users and novice users and poor quality transcripts WER 45 lowered the performance for experienced users and novice users alike transcripts that are useful but not perfect WER 25 required participants to employ strategies to compensate for errors in transcripts that might be more easily to be developed by novice users than by more experienced users as novice users have no prior expectations about the system 3 5 Results User Perception Synopsis Transcripts of WER 0 led to best user experience followed in decreasing order by WER 25 WER NT and WER 45 As previously mentioned the user perception data were collected through post quiz questionnaires post quiz perception data and through the post session questionnaire post session user perception data The post quiz data were analyzed through Repeated Measure ANOVAs F scores in the same manner as the relative quiz scores Trend analyses were also carried out as well as multiple comparisons between WER NT and each of the ordinal 
levels of WER Since the post session user perception data were collected at the end of the Useful Transcriptions of Webcast Lectures 3 5 RESULTS USER PERCEPTION WER Confidence in perform Mean level ANOVA 0 25 NT 45 61 0 220 0 026 0 009 0 238 F 3 141 5 369 p 001 0 339 0 151 0 245 0 245 F 3 141 6 201 p 001 Percep task Mean level difficulty Table 3 10 ANOVA Mean relative perception of difficulty and confidence in performance levels for each level of WER and tests of significance over all levels of WER Lower values mean increased confidence choice 1 on questionnaire indicated being very confident and perception of an easier task choice 1 indicated a very easy task study and refer to users overall experience and as such are not influenced by the independent variables no tests of statistical significance needed to be performed Instead simple descriptive statistics are used to present this post experiment analysis of user perception 3 5 1 Experience Quality Hypothesis Synopsis Users experience is indeed influenced 
by WER Also WER 25 is above the WER threshold at which users welcome transcripts as a feature of the system However the increase in user experience does not always slow down as quality improves We found that some user perception data perception of task difficulty and confidence in performance exhibit only a linear relation with WER while other data perception of transcripts usefulness and perception of errors in transcripts show an increase in user Useful Transcriptions of Webcast Lectures 62 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS WER Confidence in perform Percep task difficulty 0 25 45 0 25 NT 45 F 1 47 12 006 p 001 0 25 45 F 1 47 10 735 p 001 0 25 NT 45 F 1 47 12 857 p 001 Table 3 11 Trend analyses over the ordinal values of WER and over all values of WER for perception and confidence levels experience as quality improves but seem to level off at lower values of WER Perception of task difficulty and confidence in performance These were the user perception data 
collected at all levels of WER The ANOVA shows that WER affects users experience both for perception of task difficulty and for confidence in performance Table 3 10 The increase in users experience over the ordinal values of WER is linear both for confidence in performance and perception of difficulty Table 3 11 For confidence in performance multiple comparisons pairwise ANOVAs between WER NT and each ordinal level of WER show a significant effect Table 3 12 between WER NT and WER 0 and between WER NT and WER 45 but not between WER NT and WER 25 Indeed the mean relative level of confidence for WER NT is very close to that for WER 25 which is confirmed by the trend analysis of all levels of WER This is still a linear relation since the confidences for WER 25 and WER NT are very close when compared to those for WER 0 and for Useful Transcriptions of Webcast Lectures 3 5 RESULTS USER PERCEPTION WER Confidence in perform 0 25 45 Percep task difficulty 0 25 45 Comparison with WER NT F 1 47 4 399 p 041 F 1 47 0 
126 p 724 F 1 47 4 642 p 036 F 1 47 12 242 p 001 F 1 47 4 797 p 034 F 1 47 0 000 p 1 00 63 Table 3 12 Multiple comparisons between WER NT and each of the ordinal levels of WER for perception and confidence levels WER 45 For perception of difficulty pairwise comparisons Table 3 12 reveal significant effects between WER NT and WER 0 and between WER NT and WER 25 but not between WER NT and WER 45 Perceived difficulty levels for WER NT and WER 45 are equal thus the trend analysis over all values still shows a linear relation Therefore it can be concluded that WER 25 is at least equal or even better in improving users experience as having no transcripts Perception of speech recognition errors Participants indicated their agreement disagreement with two statements that appeared on post quiz questionnaires that were administered only after quizzes where transcripts were present The errors in the transcript didn t stop me from solving the quiz and I was bothered by the errors in the transcript For both questions the 
level of agreement was significantly Useful Transcriptions of Webcast Lectures 64 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS influenced by WER F 2 94 7 060 p 001 and F 2 94 12 212 p 001 respectively A trend analysis shows that the relation between WER and participants perception of error rates is linear for both questions F 1 47 12 746 p 001 and F 1 47 20 699 p 001 respectively with users being more aware of the errors in transcripts of higher WER The perception of errors in transcripts was also assessed through the post session questionnaire Participants indicated their agreement with the statement I haven t noticed significant differences in the quality of the transcripts for different lectures 64 of subjects disagreed or strongly disagreed with the statement while only 19 of subjects agreed or strongly agreed with it This further shows that participants were fully aware of the differences in transcripts quality levels across sessions Perception of transcripts 
helpfulness Participants indicated their agreement with two statements from post quiz questionnaires administered only when transcripts were present Transcripts helped me solve the quiz faster and Transcripts helped me solve the quiz better For both questions the level of agreement was significantly influenced by WER However the trend analysis shows Table 3 13 both a linear and a quadratic component of the relation between perception of transcripts helpfulness and WER transcripts of WER 0 being perceived as more helpful than those of WER 25 which in turn are more helpful than those of WER 45 The quadratic component is explained by users perception of helpfulness for WER 25 being closer to that for WER 0 than to the perception for WER 45 Table 3 13 Useful Transcriptions of Webcast Lectures 3 5 RESULTS USER PERCEPTION WER Solved faster Mean Score ANOVA Trend linear quadratic Solved better Mean Score ANOVA Trend linear quadratic 0 25 45 0 627 65 0 345 0 282 F 2 94 20 164 p 001 F 1 47 24 790 p 001 F 1 47 11 594 
p 001 0 283 0 240 0 523 F 2 94 14 721 p 001 F 1 47 18 884 p 001 F 1 47 8 051 p 006 Table 3 13 Mean relative perception of helpfulness levels for each level of WER and tests of significance and trends over all levels of WER Lower values mean increase helpfulness choice 1 on questionnaire indicated transcripts helped solved the quiz faster better Besides the statements from the post quiz questionnaires users perception of usefulness was also assessed through one question on the post session questionnaire by indicating their agreement with the statement I would rather have transcripts with some errors than not having transcripts at all 91 of subjects indicated agreed or strongly agreed their preference for having access to transcripts even if their quality is not perfect This further demonstrates that 25 error rate is acceptable from the users perspective Useful Transcriptions of Webcast Lectures 66 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS Perception of usability of 
transcript enhanced webcasts The post session questionnaire also asked participants to indicate which features they used to compensate for the errors in the transcripts by indicating their agreement with statements such as When transcripts seemed to be full of errors I used X to find the answer with X being each of slides audio playback table of contents and timeline Slides audio playback and table of contents were equally favoured by users about 65 of subjects agreed or strongly agreed with each as navigational tools useful in compensating for speech recognition errors in transcripts timeline was used by only 18 Of these the table of contents was the highest rated 31 as the first choice strongly agreed that it helped compensate for transcripts errors followed by the audio playback 23 Participants also indicated over 80 agreed or strongly agreed that being able to play individual lines from transcripts both made the tasks easier to accomplish and was useful when transcripts had errors Participants were also 
asked on the post session questionnaire to rate all features of the system from an overall usefulness perspective About 95 of subjects rated all features except the timeline as useful or very useful the timeline was rated as useful or very useful only by 59 of the subjects The table of contents was rated the highest for first choice only very useful for 79 of the subjects followed by transcripts 62 This analysis leads to the conclusion that appropriate navigational tools improve users experience when errorful transcripts are present Useful Transcriptions of Webcast Lectures 3 6 LIMITATIONS AND GENERALIZATIONS Confidence in using the system 67 Users overall confidence in using the system was also assessed with respect to the importance of the application where the system is to be used When asked if they would use the system to prepare for an examination instead of going to classes 33 of respondents chose no while 37 of them indicated only if transcripts have no errors Unsurprisingly their confidence changed 
when asked if they would use the system to prepare for an examination in addition to going to classes 75 opted for an unconditional yes With respect to less critical tasks preparing for an assignment 72 indicated they would use the system while 21 conditioned it by having transcripts with no errors Meanwhile using a system to make up for a missed class would not demand accurate transcripts 93 would use the system for such a task only 4 conditioning it by having perfect transcripts This shows that transcripts quality is more critical in some applications than in others 3 6 Limitations and Generalizations The experiment described in this chapter was carried out under an information mining scenario in which students used a webcast system to answer quiz questions about the content of lectures The main limitation of this task is in the types of information sources that are integrated into the webcast archives specifically the existence of a structured table of contents and of the slides used in the presentation 
Additional limitations may have been caused by the information mining aspects of the task that was carried out as well as by the choice of Useful Transcriptions of Webcast Lectures 68 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS time constrained factoid quizzes as a metric of user performance Similar research such as that of Whittaker et al 1999 and of Whittaker and Hirschberg 2003 explore the trade off between accuracy and time to completion as a measure of user performance when dealing with imperfect transcripts under tasks that also include comprehension e g writing summaries While our choice of the task was based not only on its typicality for educational activities it was also based on practical reasons comprehension tasks have more confounding variables and are more difficult to assess quantitatively as in the case of judging summaries Penn and Zhu 2008 Within the scope of information mining tasks other measures besides quiz scores can be employed such as the time 
to completion used by Whittaker et al 1999 or the rate of correct answers per second used by Lonsdale et al 2006 such measures however are not significantly different than the scores of time limited quizzes Under similar conditions it is plausible that the findings of this research especially the fact that WER is correlated with users performance and experience can be generalized to other information mining and comprehension tasks as well as presentation environments although the quality threshold is likely to be lower for more information critical talks such as possibly some business presentations 3 7 Summary and Discussion One of the major drawbacks for the users of audio video archives such as those of webcast lectures and presentations is the difficulty Useful Transcriptions of Webcast Lectures 3 7 SUMMARY AND DISCUSSION 69 in performing operations typically associated with archived text such as scanning and browsing While manual transcription is a very expensive and time consuming task speech 
recognition systems can provide an alternative solution However for lecture and presentation speech the poor accuracy of automatically generated transcripts makes their use questionable In this research users expectations for transcription accuracy in webcast archives was investigated and how the quality of the transcripts affects the usability and usefulness of the archives was measured Also the investigation looked at what other navigational tools table of contents slides etc users employ to compensate for errors in the transcripts For this an ecologically valid experiment was designed where 48 subjects used a fully featured webcast browsing tool while answering quizzes based on archives of webcast lectures The analysis of the task performance data revealed that users performance correlates with the accuracy of speech recognition For transcripts with a word error rate equal to or less than 25 users task performance was better than that of using no transcripts Word error rate also influenced users 
experience as shown by the analysis of the user perception data Error rates of 25 led to users experience above that achieved when using no transcripts When exposed to transcripts with WER of 45 both task performance and user experience were worse in most cases10 than if no transcripts had been provided 10 The usefulness threshold for transcript quality is in the proximity of 45 WER when the information users are searching for is found only in the audio channel and not present in any accompanying sources such as the slides Useful Transcriptions of Webcast Lectures 70 CHAPTER 3 THE ACCEPTABLE WORD ERROR RATE OF MACHINE GENERATED WEBCAST TRANSCRIPTS As current lecture dedicated ASR systems deliver in realistic conditions WERs of 45 or even greater more research is needed in order to reduce this to the acceptable level of 25 as determined by the study presented here As such in Chapter 4 a solution for improving lecture ASR is proposed that takes steps toward reaching this acceptable transcript quality threshold 
Useful Transcriptions of Webcast Lectures Chapter 4 Improving Automatic Speech Recognition for Webcast Lectures Automatically transcribing lecture speech is one of the most challenging areas of ASR research One direction of research where significant efforts are being spent is the improvement of Language Models LMs for lecture transcription The main goal of this direction is finding appropriate methods of modelling the dual nature of lecture speech it is characterized as large vocabulary continuous speech speaker independent and as topic and domain specific Usually for lecture transcription tasks only small amounts of training data are available compared with for example broadcast news transcription Typically solutions for this type of problem were sought by building separate LMs targeting the reduction of WER independently for each of the traits specific to lecture speech while the recognition was performed using an 71 72 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES interpolated LM 
Rosenfeld 2000 One of the disadvantages of previous approaches is the more complicated process of determining and acquiring the most appropriate corpora for building two or more separate models Other shortcomings include the need to fine tune the interpolation or hypothesis space combination parameters and the difficulties in automatically or manually extracting reliable topic specific keywords from additional knowledge sources e g lecture slides The approach proposed in this chapter eliminates the need for multiple models yet achieves similar or better WER reductions Our method exploits the slides used in the lecture or presentation to be transcribed by using the entire content of the slides as web search queries it retrieves web corpora that can be used to directly train a single LM suitable for both the conversational and the topic specific styles of lectures In this chapter I will first present a survey of the research on language model adaptation as a solution to improve ASR for dedicated domains 
Section 4 1 followed by a review of existing work on lecture ASR and related domains Section 4 2 I will then introduce a technique for improving the ASR process for lectures and presentations and discuss several web retrieval and training alternatives Section 4 3 Before concluding this chapter I will also compare the best solution with current interpolation based LM adaptation methods Section 4 4 Useful Transcriptions of Webcast Lectures 4 1 EXISTING RESEARCH ON ADAPTING AND BUILDING LANGUAGE MODELS FOR DEDICATED DOMAINS 73 4 1 Existing Research on Adapting and Building Language Models for Dedicated Domains It is an undeniable fact that automatic speech recognition systems are improving constantly nowadays some reaching accuracies close to 100 in user dependent limited vocabulary dedicated applications One of the reasons for such improvements resides in the increased availability of resources e g extremely large amounts of recordings for such applications that support the development of accurate language and 
acoustic models Unfortunately the availability of such resources is limited in the large vocabulary continuous speech lecture domain and applications targeting new domains face some of the challenges existing systems had to cope with in the past As a result increasingly significant research has been directed toward porting existing systems to new domains mainly by adapting their LMs without having to pay the high price of collecting the required amount of data As mentioned in Chapter 3 the WER of current systems is still higher than the minimum level for which transcripts are accepted by humans During recent years several approaches were proposed to improve the ASR systems used in lecture transcription Although some significant improvements can be achieved through acoustic model adaptation if manual transcripts of the same lecturer are available Park et al 2005 Useful Transcriptions of Webcast Lectures 74 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES Kato et al 2000 Of potential 
interest to the task of lecture presentation speech recognition is the domain of broadcast news transcription These two domains share some important aspects such as speaker independence large vocabulary less than optimal acoustic conditions for lectures caused by the lecture room and for broadcast news caused by external disturbances such as background music for studio broadcasts or background noises for outside broadcasts or the occurrence of speaker interruptions One of the more recent attempts to port broadcast news recognition systems is presented in Giuliani and Federico 2001 Bertoldi et al 2002 In this research a broadcast news transcription system for Italian is ported to the domain of spontaneous dialogue systems The acoustic model HMMs is adapted through Maximum Likelihood Linear Regression MLLR The language model is adapted through recursive interpolation Federico and Bertoldi 2004 by combining trigrams from the source model with trigrams computed on relatively small texts from the target domain 
Another adaptation example is provided by Ariki et al 2003 in which the target is a recognition system for sports broadcast speech The language model was initially built on a collection of documents gathered from the World Wide Web by selecting documents related to a particular sport A smaller adaptation corpus was also built using manual transcription of sport event recordings The final language model was built by interpolating the initial and adaptation language models a technique that increased the accuracy on two test sets by 16 8 and 11 1 respectively no details are given on the baseline WER Useful Transcriptions of Webcast Lectures 4 1 EXISTING RESEARCH ON ADAPTING AND BUILDING LANGUAGE MODELS FOR DEDICATED DOMAINS 75 Adapting existing models to the task of lecture presentation speech recognition was also the solution chosen in Spontaneous Speech Corpus and Processing Technology Kawahara et al 2001 a large project aimed at the development inter alia of a corpus of spontaneous speech consisting of 
lectures and oral presentations Broadcast news is not the only source for building language models for adaptation to particular domains As presented in Bacchiani and Roark 2004 metadata can be used to condition the language model in contrast to other approaches where the language model is conditioned by external knowledge sources such as the topic Metadata refers to any type of knowledge external to the speech signal for example the record of a customer calling a company s customer service The authors use caller IDs as metadata for their experiments with the SSNIFR corpus voice mail recordings at a customer care centre They report up to 27 WER for speech recognition on the SSNIFR corpus using metadata adapted models compared to 30 WER using models trained on the SSNIFR corpus When adapting language models to a new domain a variety of other sources of information can be used For example the World Wide Web is employed as a source for building topic specific corpora and language models in several topic 
dependent recognition tasks such as call routing applications by extracting relevant semantic information from web data as in Hakkani Useful Transcriptions of Webcast Lectures 76 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES building web queries of keyword centered N grams extracted from existing transcriptions as in Sarikaya et al 2005 However such approaches require the availability of either manual transcripts costly or impractical or of an automatic keyword extraction procedure not necessarily guaranteed to yield the most relevant keywords to a specific topic A further example of using relevant textual information to build topic specific LMs is provided independently by Seymore and Rosenfeld 1997 and Bigi and De Mori 2004 where the output from a first pass of an ASR system through the speech document is used as the main source of information for LM adaptation Seymore and Rosenfeld 1997 used the ASR output to detect a set of topics from a pool of 5000 topics that describe segments 
of the speech document to be transcribed Each detected topic is then used to extract that topic related part of the training corpus and build language models dedicated to the detected topic Such topic based adaptation reduces the WER from 40 to 35 ASR output can also be used to reduce the Out Of Vocabulary OOV rate which is the focus of the work presented in Bigi and De Mori 2004 The authors employ Information Retrieval IR techniques based on the Kullback Leibler Distance a measure of divergence between probability distributions to obtain relevant documents based on the hypothesized transcription Such documents are then used mostly to build better vocabularies for a second ASR pass While such technique can be exploited in LM adaptation the main benefits are a 28 reduction of the OOV rate The OOV rate is also the focus of Schwarm et al 2004 as new words needed by the target domain to which the language model is adapted even Useful Transcriptions of Webcast Lectures 4 1 EXISTING RESEARCH ON ADAPTING AND 
BUILDING LANGUAGE MODELS FOR DEDICATED DOMAINS 77 if added to the new vocabulary cannot always be integrated into the new language models This is mainly caused by a lack of sufficient training data from the new domain which typically leads to poor WER for vocabulary items that are specific to the target domain In particular the authors are concerned with such words in the context of automatic meeting transcription For this a variety of information sources are used ranging from e mails sent to the research groups which are the target of the meeting to papers related to the topic of the meeting Using linear interpolation of a general purpose conversational language model with a domain specific model a 9 overall reduction in WER and a 61 reduction in WER for new vocabulary items can be achieved Underlying problems related to LM adaptation are also addressed in Useful Transcriptions of Webcast Lectures The authors 78 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES report a WER of between 15 
5 and 12 5 although no baseline WER is mentioned 4 2 Previous Work on Automatic Speech Recognition for Lectures Transcribing lecture presentation speech is a research topic still in its infancy The challenges met by the task of recognizing open domain speaker independent large vocabulary continuous and noisy speech are very hard to overcome While a significant amount of research effort has been spent on improving speech recognition for lectures and presentations the quality of the transcripts typically WERs of 30 40 at most 20 in particular conditions is still below that of other domains such as broadcast news transcriptions An important effort directed at lecture transcription has been carried out by several Japanese research groups as part of the larger project The Corpus of Spontaneous Japanese The approach proposed in the context of this project Kato et al 2000 was to train the language model on corpora consisting of lectures and presentation transcriptions 837 000 words were collected for this purpose 
with a vocabulary of 32 000 unique tokens The language model is a tri gram model trained on this collection The model is topic independent all words related to a specific lecture topic having been removed It is assumed that the remaining words especially those which frequently appear across lectures are related to various acts of the presentation ranging from filler words such as hesitations to commands for a Useful Transcriptions of Webcast Lectures 4 2 PREVIOUS WORK ON AUTOMATIC SPEECH RECOGNITION FOR LECTURES 79 voice operated projector The language model is then adapted to a specific topic for recognition of individual lectures The adaptation is performed through linear interpolation of the general model with a topic dependent model trained on a collection of technical papers indicated by the instructor of each lecture as bibliographical and background material The reported improvement ranges from 18 to 26 WER for an artificial evaluation with 3 speakers each giving a 10 minute technical presentation 
Another approach also relying on presentation slides to adapt the language model was followed by Rogina and Schaaf 2002 wherein all content words extracted from the slides are ranked according to their 1 A brief definition of more formal and detailed description can be found in Manning and Useful Transcriptions of Webcast Lectures 80 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES algorithm and achieves alignment error rates of 27 31 the authors did not compare this to using manually generated transcripts Significant progress has been recorded lately in improving the recognition of continuous large vocabulary speaker independent speech such as broadcast news mostly due to the increased availability of large annotated corpora However for the domain of lecture presentation transcription such necessary resources are very scarce For this reason E Leeuwis and colleagues 2003 used the TED corpus combined with the American English newspaper Wall Street Journal dictation corpus The TED 
Translanguage English Database corpus released by ELRA and LDC in 2002 is a collection of 188 manually transcribed recordings of talks given in English at the Eurospeech conference in 1993 Both the acoustic and language models were adapted The acoustic model was adapted from the Wall Street Journal on the TED corpus using Maximum Likelihood Linear Regression MLLR Leggetter and Woodland 1995 The language model was adapted through recursive interpolation and non linear smoothing from a 15 million word corpus of scientific papers and a 300 000 word corpus of conversational speech transcripts to the 55 000 word TED training corpus The language model was then adapted to a specific speaker by using various parts of the paper presented Word error rates were reported as 44 2 adaptation using the title 43 9 using the abstract and 43 8 using the entire text of the paper using Probabilistic Latent Semantic Analysis Hofmann 1999 to perform the adaptation Using the entire text and a mixture model obtained through 
interpolation at the level of relative frequencies achieved a 39 2 error rate Useful Transcriptions of Webcast Lectures 4 2 PREVIOUS WORK ON AUTOMATIC SPEECH RECOGNITION FOR LECTURES 81 As webcast of lectures are becoming increasingly common new research is emerging on automatic transcription of lectures Such a project was recently started by the Spoken Language Systems Group at MIT Glass et al 2004 Park et al 2005 Hsu and Glass 2006 The goals of this project are to improve upon the ASR process for lectures mainly for the purpose of document indexing As part of this project a lecture browsing interface Cyphers et al 2005 and illustrated in Figure 2 2 from Chapter 2 was developed that assists users in querying a lecture for information This interface is not a multimedia visualization tool instead being quite similar to the story retrieval tool of Whittaker and Hirschberg 2003 The ASR part of the project is focused on reducing the OOV rate and the WER The OOV rate is reduced by finding the appropriate 
adaptation Useful Transcriptions of Webcast Lectures 82 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES a more significant role WER with AM only adaptation on the same corpus is 41 2 This can be explained by the large amount of material from the same lectures used in the AM adaptation which unfortunately is not always a practical solution 4 3 Web Based Language Modelling for Automatic Lecture Transcription Significant research efforts are dedicated to the improvement of LMs for lecture transcription the main goal being finding appropriate methods of modelling the dual nature of lecture speech it is characterized as large vocabulary continuous speech speaker independent and as topic and domain specific Typically solutions for this problem were sought by building separate LMs targeting the reduction of WER independently for each trait while the recognition was performed using either interpolated LMs or separate models followed by a combination of the resulting hypothesis spaces One of the 
disadvantages of previous approaches is the more complicated process of determining and acquiring the most appropriate corpora for building two or more separate models such as in Kato et al 2000 or Leeuwis et al 2003 Other shortcomings include the need to fine tune the interpolation or hypothesis space combination parameters as well as the difficulties in automatically or manually extracting reliable topic specific keywords from additional knowledge sources e g lecture slides as in Rogina and Schaaf 2002 In contrast the approach proposed in this Useful Transcriptions of Webcast Lectures 4 3 WEB BASED LANGUAGE MODELLING FOR AUTOMATIC LECTURE TRANSCRIPTION 83 Chapter eliminates the need for multiple models and for keyword extraction procedures yet achieves similar or better WER reductions It exploits the slides used in the lecture or presentation to be transcribed by using the entire content of the slides as web search queries it retrieves web corpora that can be used to directly train a single LM suitable for 
both the conversational and the topic specific styles of lectures This section presents the implementation details of the proposed method discuss several web retrieval and training alternatives and compare the best solution with current interpolation based LM adaptation methods 4 3 1 General Algorithm The fundamental principle of the proposed web based language modelling method consists in treating the entire content of the lecture presentation slides as the source of external knowledge used in building the dedicated LMs As it will be shown in this Section this approach eliminates the need to build both topic dependent and general purpose LMs Figure 4 1 describes the algorithm used to collect the web based corpora used in training lecture specific LMs It assumes every lecture is accompanied by slides which are mostly organized in bullet form one idea constitutes a line on the slide however every line on the slides is treated as a separate web query even if part of a larger text Figure 4 2 shows several 
examples of typical queries based on lines from slides There is no pre processing of the slides each web query being an exact copy of a slide line Not all lines consist exclusively of topic specific keywords since many slide bullets do not contain any keywords while some lines are artifacts of the slide conversion process Useful Transcriptions of Webcast Lectures 84 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES which ensures that the corpora retrieved using such queries appropriately matches both the topic specific and conversation style of a lecture Figure 4 3 illustrates the differences in the type of documents that are retrieved for different queries for every slide Si in a lecture for every line Li j on slide Si define Ti j as the text of line Li j run web search with query Q Ti j retrieve most relevant first N documents in PDF format convert retrieved documents to text corpora Ci j Figure 4 1 The algorithm for using the lecture slides and the World Wide Web to build corpora on 
which lecture specific LMs are trained 4 3 2 Corpora Adjustment Several parameters can be adjusted both during corpora retrieval and language modelling Number of documents to be retrieved for each slide line N in Figure 4 1 which will be the main factor influencing the size of the final corpus Percentage of non dictionary words permitted corpus filtering For each retrieved document Ci j sentences or lines in Ci j for which Useful Transcriptions of Webcast Lectures 4 3 WEB BASED LANGUAGE MODELLING FOR AUTOMATIC LECTURE TRANSCRIPTION 85 Figure 4 2 Examples of slide bullets used as web queries These apparently un related examples are from the same lecture Conceptual Design of the third year Computer Science undergraduate course The Design of Interactive Computational Media the number of words not in dictionary exceeds a desired threshold are removed from the corpus This is useful for preserving the integrity of the LM with respect to the pronunciation dictionary 4 3 3 LM and ASR Scope Alternatives Once all 
corpora Ci j are collected and filtered LMs can be built using the entire collection or a slide specific selection Three alternatives are proposed one LM for the entire lecture one LM for each slide and one LM for each cluster of slides For the latter two the slides must be time indexed by recording the time of each change of slides One LM per lecture in order to obtain a single model M for the entire lecture all collected corpora will be joined in a single corpus C i j Ci j on which M will be trained One LM per slide for every slide Si of a lecture a corresponding LM Mi Useful Transcriptions of Webcast Lectures 86 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES Figure 4 3 Examples of documents retrieved from the World Wide Web relevant to two of the queries in Figure 4 2 Useful Transcriptions of Webcast Lectures 4 4 EMPIRICAL EVALUATION will be trained for every Ci 87 Ci j During the recognition process j a separate model Mi will be used for the audio segment of the lecture 
corresponding to the time span of slide Si One LM per cluster of slides assuming slides are numbered chronologically2 for each slide Si a cluster of slides of range r is defined as S i r k Sk i r k i r The related corpora are k also clustered in the same manner C i r where Ck j Ck i r k i r Ck j Thus individual LMs M i r are separately trained on corpora clusters C i r and subsequently used during recognition for the audio segments associated with the time span of S i r 4 4 Empirical Evaluation An extensive evaluation of the approach proposed in Section 4 3 1 was carried out in which several combinations of corpora and LM scope parameters were tested as the proposed method was not refined during the evaluation no developmental iteration was performed The evaluation was carried out using the SONIC toolkit Pellom 2001 We used the acoustic model that is part of the toolkit3 built on 30 hours of data from 283 speakers from the WSJ0 and WSJ1 subsets of the 1992 development set of the Wall Street Journal WSJ 
Dictation Corpus LDC 1994 2 If the same slide is displayed more than once during a lecture e g it s re visited by the lecturer the multiple occurrences of that slide are treated as separate slides and numbered accordingly 3 The AM related parameters were the same as those described in detail in Section 3 3 Useful Transcriptions of Webcast Lectures 88 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES For all the LMs used web based as well as baseline models a pronunciation dictionary was custom built to include all words appearing in the corpus on which the LM was trained The pronunciations were extracted from the 5K word WSJ dictionary included with the SONIC toolkit and from the 100K word CMU pronunciation dictionary CMU 1998 For all models one non dictionary word was allowed per line of corpus only for lines longer than four words using the CMU CAM Language Modelling Toolkit Clarkson and Rosenfeld 1997 with a training vocabulary size of 40K words the out of vocabulary rate was low for 
all 4 4 1 Test Data The test data consist of four lectures of approximately 50 minutes each recorded in different weeks of the same course The recordings used for our evaluation were collected in a large amphitheatre style lecture hall 200 seating capacity using the AKG C420 head mounted directional microphone English The lecturer is male early 60s and a native speaker of The recordings were not intrusive and no alterations to the lecture environment or proceeding were made The 1 channel recordings were digitized using the TASCAM US 122 audio interface as uncompressed audio files with 16KHz sampling rate and 16 bit samples The audio recordings were manually segmented at pauses longer than 200ms Useful Transcriptions of Webcast Lectures 4 4 EMPIRICAL EVALUATION 89 4 4 2 Web Based Lecture Models For each of the four lectures all LM training options described in Section 4 3 3 were considered with a range r 1 for the cluster option In terms of the number of retrieved documents Section 4 3 2 for each LM training 
option three values were allowed for N 10 20 and 30 documents per bullet the actual number was in some cases slightly lower than N due to web retrieval and PDF conversion errors The Google APIs http code google com were used for returning the URLs of the web documents relevant to each query the search was limited to documents in English 4 4 3 Baseline Models The transcripts of the Switchboard SWB corpus Godfrey et al 1992 were used for training the baseline model SWB LM The SWB corpus is a large collection of about 2500 scripted telephone conversations between approximately 500 English native speakers making it a suitable choice among general purpose LMs for the conversational style of lectures as also suggested in Park et al 2005 In order to compare the proposed web based modelling with the typical approach of using interpolation based optimizations for domain specific ASR two more baseline LMs were built for each of the four lectures a topic specific model and one built through the typical approach of 
adapting a general purpose model to a specific topic For these a set of keywords relevant to each lecture was manually extracted4 from the slides by the 4 While several automatic both supervised and unsupervised keyword extraction algorithms exist Turney 2000 they do not produce entirely accurate results a Useful Transcriptions of Webcast Lectures 90 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES teaching assistant associated with the course A query was constructed with the selected keywords and 200 relevant web documents were retrieved on which a language model KEYW LM was trained Finally the KEYW LM was statically interpolated with an interpolation weight 0 5 with the SWB LM to generate the third baseline LM Each of the baseline LMs was accompanied by a lexicon constructed in the manner described at the beginning of this section LM scope Docs per slide bullet 10 Lecture 20 30 10 Slide 20 30 10 Cluster 20 30 1 42 34 41 71 42 03 51 02 48 37 47 38 46 94 46 17 45 21 Lecture 2 41 38 40 70 
41 01 51 25 49 06 47 63 46 99 46 24 46 31 3 44 19 44 18 43 73 50 63 49 79 49 04 49 02 49 43 48 36 4 48 35 47 56 46 95 57 29 55 26 54 60 54 41 52 89 52 65 Table 4 1 Web based lecture LM the WERs corresponding to the training options described in Section 4 3 3 human based assessment of such algorithms producing relevance scores in between 60 and 80 As such the evaluation of LMs was restricted to using manually extracted keywords in order to avoid confounding this evaluation with the keyword extraction algorithm Furthermore keyword selection is regarded by other disciplines such as Information Studies e g Gross and Taylor 2005 as a subjective and task dependent process that cannot always be quantitatively compared to automatic methods Useful Transcriptions of Webcast Lectures 4 4 EMPIRICAL EVALUATION Lecture LM SWB KEYW SWB KEYW LECT 1 2 3 4 91 47 26 48 08 48 71 50 48 44 04 45 39 46 39 50 39 41 11 43 43 42 64 46 39 41 71 40 70 43 73 46 95 Table 4 2 The WERs corresponding to the best web based lecture models 
LECT compared to the baseline model built on general purpose conversational texts SWB to the baseline model built using manually extracted keywords defining the lecture topic KEYW and to the baseline obtained through interpolation SWB KEYW 4 4 4 Results WER Reduction Table 4 1 presents the WER for each of the four lectures on ASR runs using the LMs described in this section The lowest WER among the web based LMs is achieved for training over the corpus relevant to the entire lecture where the number of documents retrieved for each slide bullet ranges from 20 to 30 Comparatively as described in Table 4 2 and illustrated in Figure 4 4 the baseline model SWB LM yields WERs higher on average by relatively 11 while the average difference between the web based LMs and the best model trained with manual supervision SWB KEYW is less than 1 Useful Transcriptions of Webcast Lectures 92 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES LM scope Docs per slide bullet 1 Lecture 2 3 Precision 4 Lecture 
Lecture SWB KEYW 20 30 93 41 92 79 91 39 93 01 93 99 91 64 91 33 87 22 89 88 90 60 92 02 93 01 90 20 91 26 92 78 87 70 87 90 83 24 85 71 85 52 SWB KEYW Recall Lecture Lecture SWB KEYW SWB KEYW 20 30 81 04 80 26 57 92 79 48 77 14 81 94 81 44 51 55 75 52 74 48 71 49 71 49 57 02 69 01 69 01 67 72 68 99 48 73 64 56 59 81 Table 4 3 Precision and Recall scores for keyword detection when using the best training options as indicated in Table 4 1 for the web based models compared to the baseline model built on general purpose conversational texts SWB to the baseline model built using manually extracted keywords defining the lecture topic KEYW and to the baseline obtained through interpolation SWB KEYW Useful Transcriptions of Webcast Lectures 4 4 EMPIRICAL EVALUATION 93 Figure 4 4 Average WER scores across all lectures for the baseline model built on general purpose conversational texts SWB the interpolation based optimization SWB KEYW and the best web based LMs LECT 4 4 5 Results Precision Recall of Keywords Text 
transcripts are often used in automatic information retrieval tasks e g using queries to search a webcast lecture repository for a particular topic However one of the challenges associated with such retrievals is the accuracy of keyword transcription during the ASR process For this the proposed web based modelling was also evaluated for lectures through the Precision and Recall5 of the transcribed keywords measured against the manual transcripts Since no general agreement exists on how to automatically identify such keywords a manually generated list of keywords was used an approach similar to that taken in Park et al 2005 where 5 A definition of Precision and Recall is given in Appendix B Glossary of Technical Terms Useful Transcriptions of Webcast Lectures 94 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES Figure 4 5 Average Precision and Recall scores for keyword detection across all lectures when using the baseline model built on general purpose conversational texts SWB the 
interpolation based optimization SWB KEYW and the best web based LMs LECT Useful Transcriptions of Webcast Lectures 4 5 LIMITATIONS AND GENERALIZATIONS 95 the course textbook s index was used For each lecture the list was set to an arbitrary length equal to 1 of the number of words in the manual transcript of each lecture and words on the list were selected by the teaching assistant associated with the course from all words appearing on slides Table 4 3 compares the Precision and Recall scores of the two best web based models with that of the baseline and the keyword based models As can be observed and also illustrated in Figure 4 5 Precision scores are higher for the web based models than for the Switchboard models and similar to those for the keyword interpolated Switchboard model while Recall scores are higher for web based models even than those for the manually supervised SWB KEYW model 4 5 Limitations and Generalizations The solution proposed in this chapter relies on the textual content of lecture 
slides and related documents retrieved from the World Wide Web to improve the accuracy of lecture ASR systems Therefore it is expected that its applicability will be limited6 to classes that are accompanied by slides containing most of the information in textual natural language form Furthermore it is expected that the lecture content concepts examples etc delivered by the instructor will be contained within the content summarized by the slides although as shown by the evaluation Section 4 4 4 the order in which the content is delivered does not need to be the same as that of the 6 Section 7 2 1 discusses various proposals for future work that include enlarging the applicability of the proposed solution to other domains and conditions Useful Transcriptions of Webcast Lectures 96 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES slides The extremely large number and diversity of documents that can be retrieved from the World Wide Web suggest that relevant material can always be found for 
any lecture However there might be lectures on more obscure topics for which less relevant training data is retrieved In such cases it is expected that the proposed web based LM solution will still produce adequate conversational models as mentioned in this Chapter the non topic parts of the lecture slides are responsible for the conversational aspect of the LM while the Recall of topic specific keywords will be lower 7 Future work should explore combining the proposed web based lecture LM with Information Extraction methods such as in Sarikaya et al 2005 to filter the retrieved web corpora for more topic relevant documents when the lecture topic is less popular Despite the limitations mentioned above the proposed LM solution can be generalized to other domains For example in contrast to the approaches for improving lecture ASR that will be introduced in Chapters 5 and 6 the web based lecture LM can be applied to tasks requiring a faster recording to transcript time This can be as low as the real time factor 
of the ASR system employed if lecture slides are available before the lecture is recorded thus allowing for the web corpus to be retrieved and the LM trained on it Furthermore the proposed LM solution could serve as the basis of LM methods that can be generalized to other domains For example if a 7 As suggested earlier in this Chapter the high Recall values for the topic keywords of the web based lecture LM is due to the straightforward slide to query conversion resulting in many web queries containing only topic keywords Useful Transcriptions of Webcast Lectures 4 6 SUMMARY AND DISCUSSION 97 lecture or presentation does not have well structured slides the web based LM approach can be enhanced with topic modelling to identify areas on slides or on other presentation materials that are promising candidates for serving as web queries 4 6 Summary and Discussion A novel algorithm for corpora building and language modelling aimed at improving the accuracy of automatic lecture transcription was introduced in this 
chapter The proposed approach uses the entire content of the slides presented in a lecture to build a language model that captures both the conversational style and the topic specific content of that particular lecture This leads to a reduction in WER of up to 11 relative to baseline algorithms using general purpose language models as shown through an empirical evaluation carried out over several recordings of university lectures In contrast to existing optimizations that rely on the interpolation of very large general purpose LMs with smaller topic specific models the proposed approach eliminates the need for multiple models By using the entire content of the presented slides as web queries only a single web based corpus needs to be collected on which a language model is trained The WER reductions are similar or better than those of existing methods based on interpolating general purpose with topic specific models as well as improved Precision and Recall scores for keyword identification One of the 
drawbacks of interpolation based optimizations is selecting the keywords that define the topic of the recording to be transcribed Useful Transcriptions of Webcast Lectures 98 CHAPTER 4 IMPROVING AUTOMATIC SPEECH RECOGNITION FOR WEBCAST LECTURES This is needed to build an accurate topic specific LM however manual intervention or fine tuning of an automated algorithm is usually required to accurately define such keywords Beside allowing for an entirely automated lecture specific modelling the approach proposed in this chapter does not rely on identifying keywords for each lecture topic Therefore this unsupervised method is suitable for integration into webcast archive systems such as ePresence where the process of publishing a recording must be fully automated in order to maintain its cost effectiveness Despite the improvements to the ASR performance for lecture transcriptions brought by the research presented here there still exists a significant gap between the desirable and actual WER the lecture specific 
Web based modelling leads to average WERs of 43 below the usefulness threshold of 45 for some tasks as discussed in Section 3 4 2 but above the desired threshold of 25 determined for all tasks described in Chapter 3 As such Chapter 5 presents an HCI in particular user collaboration based solution aimed at reducing and in some cases even eliminating this gap Useful Transcriptions of Webcast Lectures Chapter 5 Wiki editing of Webcast Transcripts As manually transcribing the increasing amount of available on line recordings is an expensive solution such task would ideally be accomplished by an Automatic Speech Recognition ASR system However due to adverse acoustic and linguistic characteristics large vocabulary speaker independent continuous speech imperfect recording conditions currently available ASR systems do not perform satisfactorily in domains such as lectures or conference presentations As it was shown in Chapter 3 The Acceptable Word Error Rate of Machine Generated Webcast Transcripts and in Munteanu 
et al 2006a when using a fully featured webcast browsing tool users task performance and perception of difficulty was better than using no transcripts at all only for transcripts with Word Error Rates WERs equal to or less than 25 It was also shown that for most browsing scenarios users prefer having 99 100 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS transcripts even if their quality is less than optimal Unfortunately most recognition systems achieve WERs of about 40 45 in the acoustically and linguistically challenging context of lecture recordings Leeuwis et al 2003 Munteanu et al 2007 Moreover it is expected that such systems will not reach perfect or near perfect accuracy in the near future Whittaker and Hirschberg 2003 In order to achieve useful transcripts of archived webcast lectures in this chapter I am proposing and evaluating an alternative tool to reduce current Word Error Rate WER levels of 40 45 to the desired 25 or better For this I have developed an interactive tool that extends the 
ePresence webcast system s functionality by facilitating in a wiki manner the collaboration between users mainly students attending such lectures in correcting the ASR produced errors in the lecture transcripts The editing tool is seamlessly integrated into the regular archive viewing mode of our webcast system allowing users to make corrections on the fly while viewing an archived webcast This tool was evaluated in a field study showing that it provides a feasible solution for improving the quality of webcast lecture transcripts In this chapter I will first briefly survey other research efforts directed at improving the quality of ASR transcripts for lectures as well as examples of research where human collaboration was employed to correct or compensate for various computational shortcomings Section 5 1 I show how transcripts can be integrated into webcast archives Section 5 2 and present the design and implementation of the wiki editing tool Section 5 3 I then describe the evaluation of the editing tool 
through a field study Sections 5 4 and 5 5 followed by an analysis of the collected data Section 5 6 and the re design Useful Transcriptions of Webcast Lectures 5 1 RELATED RESEARCH 101 and re evaluation of the tool based on the findings of the field study Section 5 7 5 1 Related Research The most commonly used measure for the quality of a speech recognition system is the word error rate WER lecture speech typical WER for lecture speech can reach rates as high as 50 when general purpose ASR systems are used Park et al 2005 Munteanu et al 2007 Despite significant research efforts Leeuwis et al 2003 Munteanu et al 2007 WERs are still greater than 40 under unsupervised un controlled training conditions with further reductions of WER only achieved by strictly controlling the ASR training conditions Furui 2005b Useful Transcriptions of Webcast Lectures 102 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS exists that address the cost of this approach for reducing the WER of transcripts Human intervention however has 
been successfully used to correct ASR errors such as in the voicemail transcript editing system introduced by Burke et al 2006 which proposes solutions to assist users in correcting transcripts when input capabilities are restricted such as in mobile environments In areas collaboration various other scientific computer supported has emerged as an alternative to single user intervention For example it was shown in two separate studies von Ahn and Dabbish 2004 Volkmer et al 2005 that the task of indexing and labelling a large collection of images for query based carried out retrieval using can be web based Our transcript enhanced collaboration Collaboration has Figure 5 1 also been successfully applied ePresence system displaying a screen to various other tasks from capture of the system with transcripts controlling a mechanical robot of 45 WER over the Internet Goldberg et al 2000 to open source software development Crowston et al 2004 and to geographic information mapping Li and Coleman 2002 Recent years 
have Useful Transcriptions of Webcast Lectures 5 2 ENHANCING WEBCASTS WITH TRANSCRIPTS 103 also witnessed an increase in online collaborative writing mainly in the form of wikis from large scale encyclopedias http wikipedia org to classroom projects Forte and Bruckman 2006 5 2 Enhancing Webcasts with Transcripts For this research the transcript enhanced version illustrated in Figure 5 1 of the ePresence webcast system was employed As described in Chapter 3 the ePresence system allows users to control the playback and interaction with a video archive as well as to access the slides used in the recorded presentation or lecture The interactive clickable transcripts are synchronized with the playback emulating a closed captioned system while fully displaying the transcript of the segment of lecture for the current slide complete description of this system is given in the introductory part of Chapter 3 and detailed in Figure 3 1 on page 30 5 3 Managing Imperfect Transcripts Current ASR systems deliver transcripts 
of webcast lectures and presentations of 40 45 WER while the necessary WER threshold is 25 as shown in Munteanu et al 2006a The collaborative editing tool that we developed for our webcast interface allows users to correct and edit the transcripts It extends the basic functionality of the system without burdening the user at the same time During regular playback of a webcast archive users can right click Useful Transcriptions of Webcast Lectures 104 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS on any transcript line not necessarily the one currently being played back and an edit box Figure 5 2 is displayed allowing users to make corrections to the selected line 1 This line becomes highlighted in red which potentially differentiates it from the current line which is bold faced Besides colour highlighting the edit box is popped up on the screen about two transcript lines above the selected line to maintain a visual connection with the transcript context Figure 5 2 Wiki like editing of imperfect transcripts To 
avoid editing conflicts a server side locking mechanism prevents users from simultaneously editing the same line When trying to edit a locked line users are informed that the line is being edited by a different user and that 1 While many systems that allow user correction of ASR transcripts employ text shading as a method of conveying information about possible errors as in Burke et al 2006 such methods rely mainly on ASR own confidence scores and are not always reliable Since other studies e g Vertanen and Kristensson 2008 suggest that ASR confidence visualization does not always help users As such no ASR confidence information was included in the proposed system Useful Transcriptions of Webcast Lectures 5 3 MANAGING IMPERFECT TRANSCRIPTS 105 a browser refresh might be needed to update the transcript webcasts need accurate time synchronization between all components so regularly checking for transcript updates is not possible This on the fly editing mode has the advantage of being light weight on the 5 3 1 
Features of the Transcript Edit Tool Edit area users can freely make corrections to the transcript line displayed in the edit box Suggestion drop down when right clicking on words in the edit box a list of possible replacement words is displayed These are choices under consideration by the ASR system during the recognition process and extracted from the word lattices produced by the ASR Useful Transcriptions of Webcast Lectures 106 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS deletions insertions and substitutions Also editing access can be restricted to certain users up to the level of transcripts corresponding to certain slides which is useful for defining a collaboration model of students lecture transcript editing 5 4 A Field Study A field study was carried out to assess the feasibility of the proposed tool and to obtain further insight into how users accept and manage lectures with imperfect transcripts In this section we describe the setup of this in situ evaluation while the following section will 
present the results and recommendations arising from the field study 5 4 1 Research Objectives As we propose a collaborative tool that engages users to edit and correct imperfect transcripts of webcast lectures we devised and conducted a field study whose main objectives were to assess The feasibility of the interface for wiki editing of webcast transcripts as a solution for completing the task of improving the quality of computer generated transcripts User experience when using the transcript enhanced webcast system and the editing tool encompassing several components users acceptance of such interface transcript quality s influence on user experience the attitude towards using it in a real lecture setting and other Useful Transcriptions of Webcast Lectures 5 5 METHODS 107 indirect benefits gained by users such as better lecture comprehension Moreover users confidence in using the system was also measured 2 Appropriate motivational schemes for increasing users students involvement and effectiveness in wiki 
editing of webcast lecture transcripts General user feedback on the interface design elements that can be improved in order to maximize the benefits of transcript editing such as to improve the amount of edits that can be done in a short period of time 5 5 Methods The field study was carried out in the context of a real classroom over a 13 week semester consisting of 21 lectures each approximately one hour long The lectures are part of the same course third year Computer Science course Although the recordings took place in classroom all measures were taken such that the recording did not interfere with the regular lecture proceeding 26 students were enrolled in the course while typical classroom attendance was approximately 15 students every week The lecture recordings were made available online using our webcast system within a day of the lecture date 2 Since our previous laboratory based experiment Munteanu et al 2006a also investigated users acceptance of machine generated transcripts as an enhancement of 
webcast systems a similar assessment was conducted for the present field study in a real life setting Useful Transcriptions of Webcast Lectures 108 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS 5 5 1 System Our webcast interface is entirely web based and the recordings can be accessed through most browsers Internet Explorer and Mozilla Firefox on several platforms Windows Linux Mac OS without installing additional software beside the Real Player plugin which is freely available for a variety of browsers and platforms For every recorded lecture the webcast system gives users full control of the archive mainly through the display of the slides used in lectures and the video recording through interaction with the TOC at the left of the screen which contains chapter headings and the title of the slides and through the timeline an interactive clickable fine grained time progress indicator Textual transcripts of the recording are displayed below the slide The lines of text are time synchronized with the video by 
boldfacing the current line of the transcript thus emulating a closed captioning system while fully displaying the transcript of the segment of lecture for the current slide The line breaks do not represent ends of sentences but rather correspond to pauses longer than 200ms To further enhance the user s control over the lecture users can re synchronize the playback of the video by clicking on a line in the transcript The transcripts were editable as described in Section 5 3 Transcripts were obtained using the SONIC ASR toolkit3 Pellom 2001 The lecturer is male late 30s native but accented speaker of English Due to the high speaking rate accent and speaking style the WER was between 50 and 60 3 The AM and LM used for this task were those described in detail in Section 3 3 Useful Transcriptions of Webcast Lectures 5 5 METHODS 109 5 5 2 Task and Procedures Participants were provided access to the web based interface for accessing webcast archives No restrictions were imposed on how the participants watched the 
recorded lectures They were encouraged to make use of these as an additional course material and as thus it was linked from the course website With respect to editing the lecture transcripts users were also left the choice of which lectures and which parts of lecture to correct Since the purpose of the study was to investigate the use of the wiki editing tool in a real situation no further requirements beside watching lectures and editing transcripts were formulated during the course of the in situ evaluation All participants were required to complete a questionnaire and brief interview after the course was completed The first 9 of the total of 21 lectures were freely accessible to all students in the class while the rest were available only to the participants of the field study Transcript editing was restricted to users registered as participants in the study The remaining 12 lectures could be accessed unlocked by participants through a credit gaining scheme for each user the number of words edited was 
recorded as credits that can be exchanged for access to the restricted lectures In a previously run pilot study we have determined that students correct an average of 300 erroneous words per hour In agreement with the course lecturer4 the required amount of participants involvement was set to 4 hours during the entire semester beside normally watching the lecture recording As such the amount of credits needed to unlock access to one lecture 4 The authors were not affiliated with the course in which the study took place Useful Transcriptions of Webcast Lectures 110 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS was set to 80 words thus allowing full access to all lectures in exchange of 4 hours of transcript editing As an additional incentive for editing the transcripts students received a small course grade bonus and a modest financial compensation according to the amount of transcript corrections significant only for those dedicating more than 4 hours 5 5 3 Participants The study was conducted with 15 
participants all third year Computer Science undergraduate students enrolled in the course that was recorded participation in the study was not compulsory Two participants had previous experience with the webcast system as it is used without transcripts for other courses However due to the intuitiveness of the webcast interface controls no training was required for the other participants beside a brief explanation of the system s web based controls Moreover all participants indicated they are familiar with various forms of Internet based media 5 5 4 Instruments and Measures In order to answer the research questions that motivated this field study four types of data were collected task completion data user experience data involvement and motivation data and general user feedback The instruments used to collect these data are included in Appendix C Useful Transcriptions of Webcast Lectures 5 5 METHODS Task completion 111 One of the objectives of this field study is to assess the feasibility of the wiki editing 
tool as a solution for improving the quality of the lecture transcripts As such we have collected data indicating what percentage of the lecture transcripts were corrected by users As it will be shown in Section 5 6 1 the task completion is assessed through the percentage of edited transcript lines sentences as well as through the relative WER reductions a commonly used measure of ASR accuracy User experience A post study questionnaire was used as the instrument for collecting user experience data The questionnaire consisted of multiple choice questions and indicated agreement disagreement with various statements The user experience was assessed through a series of indicators User acceptance This indicator measured students willingness to use the transcript enhanced webcast system through statements such as Being able to access lectures through the webcast system helps me better review the course material I would like to see the system used for more classes I didn t need the lecture archives the slides and 
examples on the prof s page are enough and I only need to review parts of lectures occasionally I don t need transcripts for that Transcript quality s influence on user experience was assessed through the answers to several statements I would rather use this system without transcripts Having transcripts for every lecture means I don t have Useful Transcriptions of Webcast Lectures 112 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS to attend classes anymore I think the quality of the transcripts was good enough for what I needed Attitudes toward wiki editing Measuring task completion rates as the percentage of corrected words provides an objective assessment of the feasibility of the wiki editing solution However a subjective evaluation from the users perspective is also needed For this we have collected data related to users attitudes toward the editing tool mainly focused on determining if students perceive the editing tool as a useful addition to the webcast system and if they are willing to use it Several 
statements on the questionnaire were used for this such as Being able to correct errors in the transcripts really improved access to the course material I think I also benefited from other users editing of the transcripts I would gladly help the class by editing transcripts for lectures using the webcast system I would have rather payed to access perfect transcripts than do my part of the editing and I rather go to class and take notes than edit transcripts Perception of indirect benefits Beside determining the attitudes toward using the wiki editing tool we queried students about how they perceived the educational benefits of the editing tool mainly motivated by the hypothesis that more exposure to the lecture material would be beneficial For this we have asked questions such as I don t think that my editing of the transcripts helped me better prepare for the course When editing the transcripts I payed more attention to the lecture and I think editing the transcripts helped me better understand the course 
material Useful Transcriptions of Webcast Lectures 5 5 METHODS 113 Confidence in using the system Similar to our previous study Munteanu et al 2006a and to compare differences between the use of transcripts in an artificial experiment with that of a real lecture setting participants indicated the context in which they would choose to use the transcript enhanced webcast system The contexts ranged from very critical to less critical Prepare for an examination instead of going to classes Prepare for an examination in addition to going to classes Prepare for an assignment and Make up for a missed class For each context participants could choose Yes No or Only if transcripts have no errors In addition to these a new choice was added to the possible answers Only if everyone is helping correct the transcripts Involvement and motivation In order to determine if the wiki editing tool is an appropriate solution for correcting the automatic transcription errors it is important to investigate how much time students are 
willing to dedicate for improving the lecture transcripts and how to better motivate them Multiple choice questions on the study completion questionnaire were used to collect data about the number of hours spent weekly for transcript editing the amount of time willing to spent and students estimate of the amount of time others would be willing to spend Ideally students would voluntarily edit the transcripts for the benefit of the entire class However this might not be a realistic expectation for smaller size classrooms Therefore we have also investigated possible Useful Transcriptions of Webcast Lectures 114 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS motivational schemes Beside the edit for access scheme employed during the field study we have also asked students to indicate their preference for others such as cost increases making transcript editing part of course requirements and edit for access combined with course bonus marks for editing more than the required minimum Preference for each of the scheme 
was indicated by choosing one of Fair Maybe or Not a fair deal options General user feedback We have invited users to also provide free form feedback as answers to an interview like questionnaire suggesting as possible dimensions features of the webcast system features of the editing tool positive negative impressions of the entire system and general comments Figure 5 3 The percentage of edited transcript lines and relative WER reduction for each of the 21 lectures after all transcripts were corrected Useful Transcriptions of Webcast Lectures 5 6 RESULTS 115 5 6 5 6 1 Results Task completion The improvements in transcript quality through collaborative editing are measured at sentence level through the percentage of corrected transcript lines but also in terms of relative WER reductions As Figure 5 3 shows most lectures had a significant number of sentences corrected for example 16 of the 21 lectures had more than 75 of transcript lines corrected On average 84 of all transcript lines were edited This resulted 
in an average relative WER reduction of 53 In order to facilitate users editing of transcripts no restrictions5 were imposed with respect to what users were allowed to type in the edit box resulting in inconsistencies6 between the 12 participants such as abbreviations formulas proper names even spelling errors that do not influence the text 5 The lack of restrictions was mainly motivated by the quest to understand the pattern of edits carried out by users however as explained in Section 5 7 1 users preferred to re transcribe entire lines 6 There were no spurious edits as all transcripts corrections were logged under a numeric user ID that was uniquely assigned to each participant However future versions of this system will include a spell checker and other vocabulary based editing restrictions in order to ensure consistency across users contributions Useful Transcriptions of Webcast Lectures 116 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS However while the 25 threshold was computed over uniformly 
distributed imperfect sentences the wiki editing of lecture transcripts creates a slightly uneven distribution of corrected sentences as illustrated in Figure 5 3 by the non linear variations in the percentage of corrected lines and WER reductions Strongly Q1 Q2 Q3 Q4 Agree 25 63 64 0 0 Agree Neutral 58 33 27 27 33 33 8 33 16 67 9 09 8 33 16 67 Strongly Disagree Disagree 0 0 50 66 67 0 0 8 33 8 33 Q1 Being able to access lectures through the webcast system helps me better review the course material Q2 I would like to see the system used for more classes Q3 I only need to review parts of lectures occasionally I don t need transcripts for that Q4 I didn t need the lecture archives the slides and examples on the prof s page are enough Table 5 1 User acceptance of the transcript enhanced webcast system Useful Transcriptions of Webcast Lectures 5 6 RESULTS 117 5 6 2 User Experience As previously mentioned user experience data was collected through a questionnaire administered at the end of the semester Students 
indicated their level of agreement with several statements related to the indicators described in Section 5 5 4 User acceptance Participants indicated their agreement with four questions relevant to their willingness to use the transcript enhanced webcast system As Table 5 1 shows most users consider it a necessary addition to traditional lecture preparation materials Strongly Q1 Q2 Q3 Agree 0 0 0 Agree Neutral 16 67 16 67 33 33 25 16 67 25 Strongly Disagree Disagree 33 33 41 67 33 33 25 25 8 33 Q1 I would rather use this system without transcripts Q2 Having transcripts for every lecture means I don t have to attend classes anymore Q3 I think the quality of the transcripts was good enough for what I needed Table 5 2 Users attitudes toward imperfect transcripts Useful Transcriptions of Webcast Lectures 118 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS The influence of transcript quality Table 5 2 shows users responses to questions assessing their acceptance of imperfect transcripts While users indicate they 
prefer having transcripts significant disagreement with Q1 these are shown to not provide the same experience as attending the lecture disagreement with Q2 the quality of the transcripts continuing to pose challenges Q3 It should be noted however that responses to Q3 could also be the result of users being exposed to transcripts of varying level of correctness as access to lecture archives as well as transcript editing was not uniformly distributed over the entire semester Attitudes toward wiki editing One of the areas that was the main focus of this field study is the users attitude toward wiki editing as a viable solution for improving the quality of the transcripts As illustrated in Table 5 3 students manifested a positive attitude toward the system not only as a transcript correction tool but as an enhancement of the classroom experience as well Perception of indirect benefits One of the hypotheses that motivated the field study was that of user editing of transcripts not only as a solution for improving 
the accuracy of the transcripts but as having the added benefit of providing students with more exposure to the lecture materials Indeed as Table 5 4 indicates most users perceive this as a benefit by disagreeing with Q1 and agreeing with Q2 although such benefits do not seem to stem from increased attention to the lecture slight preference for disagreement with Q3 Useful Transcriptions of Webcast Lectures 5 6 RESULTS 119 Strongly Q1 Q2 Q3 Q4 Q5 Agree 8 33 25 8 33 0 8 33 Agree Neutral 58 33 50 83 33 0 25 33 33 8 33 8 33 16 67 16 67 Strongly Disagree Disagree 0 16 67 0 33 33 50 0 0 0 50 0 Q1 Being able to correct errors in the transcripts really improved access to the course material Q2 I think I also benefited from other users editing of the transcripts Q3 I would gladly help the class by editing transcripts for lectures using the webcast system Q4 I would have rather payed to access perfect transcripts than do my part of the editing Q5 I rather go to class and take notes than edit transcripts Table 5 3 
Users attitudes toward wiki editing Useful Transcriptions of Webcast Lectures 120 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS Strongly Q1 Q2 Q2 Agree 8 33 16 67 0 Agree Neutral 16 67 41 67 16 67 16 67 25 41 67 Strongly Disagree Disagree 41 67 0 25 16 67 16 67 16 67 Q1 I don t think that my editing of the transcripts helped me better prepare for the course Q2 I think editing the transcripts helped me better understand the course material Q3 When editing the transcripts I payed more attention to the lecture Table 5 4 Users perception of the indirect benefits of wiki editing of transcripts Useful Transcriptions of Webcast Lectures 5 6 RESULTS 121 Only if Yes transcripts have no errors Q1 Q2 25 75 16 67 0 8 33 8 33 Only if everyone is helping correct the transcripts 0 8 33 0 0 58 33 16 67 50 8 33 No Q3 41 67 Q4 83 33 Q1 Would you consider using the ePresence system to prepare for an examination instead of going to classes Q2 Would you consider using the ePresence system to prepare for an examination in 
addition to going to classes Q3 Would you consider using the ePresence system to prepare an assignment Q4 Would you consider using the ePresence system to make up for a missed class Table 5 5 Users confidence in using the system as a relation of transcript quality Useful Transcriptions of Webcast Lectures 122 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS Confidence in using the system Similar to our study that determined the minimum WER level for useful transcripts Munteanu et al 2006a we have assessed users overall confidence in using the system with respect to the application where the system is to be used The responses presented in Table 5 5 are consistent with the findings of our previous 5 6 3 Users Involvement and Motivation Out of the total fifteen participants two chose to contribute more than the required 4 hours of transcript editing The combined contributions of these two participants accounted for approximately 75 of the transcript corrections Ten more participants contributed the full required 
amount of editing thus gaining access to the entire archive of lectures Three participants contributed less than the required amount and did not respond to the questionnaires Users level of involvement was also assessed through the final questionnaire Participants indicated that they are willing to spend an average of approximately 50 minutes a week for transcript editing two users indicating 2 hours while other two indicating only 15 minutes However when asked about other students willingness to edit transcripts most of the Useful Transcriptions of Webcast Lectures 5 6 RESULTS 123 twelve responses estimated 15 minutes four responses or no time also four responses 7 Participants were also asked to estimate how much time they spent weekly editing transcript during the field 5 6 4 General User Feedback Given the particularities of this field study mainly evaluating a completely new concept wiki editing of imperfect transcripts in a real setting students attending lectures during an entire semester participants 
feedback is one of the most important aspects of the data collection Beside various comments related to the web interface such as video player plugin most of users feedback was focused on the editing tool and on the integration of transcripts While the shortcomings of the editing tool and the measures taken 7 A plausibility argument on the cost effectiveness of ASR improvements through wiki enabled user corrections is given in Section 7 2 7 Useful Transcriptions of Webcast Lectures 124 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS to address them are discussed in the following section the integration of transcripts into the webcast system generated several suggestions from users Both our previous study Munteanu et al 2006a and the analysis of the data collected during this field study show that transcripts are regarded as a necessary addition to the webcast system even if they are of lower quality However the participants indicated that the sheer amount of transcripts make them difficult to be skimmed 
through This suggest a more compact representation either as a summary or another high level representation of the lecture content may be more appropriate especially for lectures that are accompanied by information rich slides or discussing topics such as a programming language that are easily accessible elsewhere 5 7 5 7 1 Interface Re design and Re evaluation Assessment of Current Design In terms of the wiki editing the feedback provided by participants highlighted the need for a more versatile editing tool Due to the initial recording segmentation process by pauses of 200ms as described in Section 5 2 and given the fast speaking rate of the lecturer some lines of transcript were spanning longer audio segments 15 20 seconds This resulted in difficulties when correcting the transcripts and as such users suggested better playback control for the editing interface The second significant suggestion involved users approach to correcting transcripts Both through the open form responses and through specific 
questions on the final questionnaire it emerged that our initial model of Useful Transcriptions of Webcast Lectures 5 7 INTERFACE RE DESIGN AND RE EVALUATION 125 editing on the fly is mainly applicable to occasional corrections of the transcripts However in the context of the large scale editing needed to correct entire lectures an editing tool that incorporates transcript navigation is needed 7 of the 12 participants indicated they prefer to consecutively edit several transcript lines without switching to lecture watching Therefore a second editing tool was integrated with the interface that incorporates multi line transcript editing with slide navigation and better playback control 5 7 2 Extended Editing Mode The on the fly editing tool is activated by right clicking on transcript lines during regular webcast viewing and does not obscure the webcast interface while active being implemented as a small pop up window In contrast the extended editing tool replaces the webcast viewing mode when activated as it 
is designed for longer editing tasks For this users can click on the Edit this slide button from the slide navigation panel on the right of the webcast interface The extended editing mode Figure 5 4 allows full editing of a transcript line similarly to the editing box In addition the extended mode provides enhanced features for Transcript navigation users can navigate through all transcript lines corresponding to the current slide Enhanced audio playback the audio segment associated with the line of text being edited is controlled through the visually intuitive playback panel of the Real Player plugin RealNetworks 2004 Useful Transcriptions of Webcast Lectures Users can 126 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS Figure 5 4 The extended editing mode allowing for full control of the audio playback and for editing of consecutive transcript lines play pause stop the audio segment fast forward through it or use the slider for more accurate positioning This allows for better visual synchronization between 
the length of text and the audio segment as requested by participants after the first field study Enhanced editing the text area is no longer limited to a single row facilitating editing of longer transcript lines Users are also provided with an Undo options The Save button will instantly update the changes in the original transcripts stored on the server and since multiple lines of text can be edited in the extended mode will also advance to the next transcript line Return to archive exits the extended editing mode and returns the user to the webcast viewing mode Locking mechanism similar to the pop up editing tool a server side locking mechanism prevents simultaneous corrections to the same line For the Useful Transcriptions of Webcast Lectures 5 7 INTERFACE RE DESIGN AND RE EVALUATION 127 extended editing mode the lock is engaged only when the users click inside the text edit area avoiding unnecessary locks when users do not edit the current line When advancing to a locked line the edit buttons Save and 
Undo and text area are gray shaded and inactive Clicking on the text area of a previously locked line or of a line that became locked after the user advanced to it activates a prompt informing the user that the line is currently edited by a different user 5 7 3 Evaluation of the Re designed System The re designed webcast system enhanced with the extended editing tool was deployed for use by students in a different motivational scheme was modified the incentives to correct the transcript were limited to course bonus marks instead of both marks and financial compensation As this follow up study took place during the Summer term when students typically work full time and enroll in at most one course and due to the weaker incentives enrollment in the user study was lower than in the initial study Moreover the overall interest in viewing the lecture archives was significantly lower in part explained by the very informative lecture slides and the comprehensive reading package Five students from a class of 30 
participated in the follow up study and four of them completed the final questionnaires Useful Transcriptions of Webcast Lectures 128 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS Since the focus of this re evaluation was to test the changes in the design of the transcript editing mode we will discuss here the main differences in collected user experience data in motivation data and in general user feedback User Experience With respect to the wiki editing of imperfect webcast transcript data collected from two indicators of user experience was analyzed attitudes toward wiki editing and perception of indirect benefits In terms of attitudes toward wiki editing the four participants in the follow up study responded mostly as in the initial study as illustrated in Table 5 6 Compared to the initial data described in Table 5 3 a slight shift toward positive attitudes was observed for Q2 and Q3 no Disagree responses for Q2 and only Agree responses for Q3 However the responses for Q1 were divided one of the four 
participants indicating disagreement with Q1 and the rest indicating agreement a difference from the initial study that can be explained by the fewer overall corrections of the lecture transcripts Differences were also observed for Q48 and Q5 where most responses were centered around the neutral answers a possible consequence of the part time nature of summer courses consisting mainly of full time working students The analysis of perception of indirect benefits data showed more divided responses than in the initial study Two of the four participants indicated they saw an overall learning benefit from transcript editing This difference can also be attributed to the higher availability of lecture preparation 8 One of the participants did not answer Q4 Useful Transcriptions of Webcast Lectures 5 7 INTERFACE RE DESIGN AND RE EVALUATION 129 Strongly Q1 Q2 Q3 Q4 Q5 Agree 0 25 0 0 0 Agree Neutral 75 50 100 0 25 0 25 0 66 67 50 Strongly Disagree Disagree 25 0 0 33 33 25 0 0 0 0 0 Q1 Being able to correct errors in 
the transcripts really improved access to the course material Q2 I think I also benefited from other users editing of the transcripts Q3 I would gladly help the class by editing transcripts for lectures using the webcast system Q4 I would have rather payed to access perfect transcripts than do my part of the editing Q5 I rather go to class and take notes than edit transcripts Table 5 6 Users attitudes toward wiki editing with the re designed system Useful Transcriptions of Webcast Lectures 130 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS materials than during the initial field study Motivation Although we expected students preference for motivational scheme to be significantly different in the follow up study due to the different course setting participants indicated the same preference 75 for using the credit earned for transcript editing to gain access to lectures Moreover all respondents supported the same scheme that also allows receiving of course bonus marks for additional editing General User 
Feedback Our analysis of participants free form feedback and of their responses to questions of preference for editing modes indicated a positive response to the introduction of the extended editing tool Compared to the initial study the same preference for editing larger ranges of transcript was manifested three of the four participants indicated a strong preference for this while one response indicated a preference for a mixed mode One participant explicitly commended it for allowing fast transcript corrections No other negative comments were collected regarding the editing modes suggesting that providing both alternatives for editing on the fly and extended is the appropriate solution for facilitating transcript editing Useful Transcriptions of Webcast Lectures 5 8 LIMITATIONS AND GENERALIZATIONS 131 5 8 Limitations and Generalizations The research presented here and the findings of the field studies are mostly applicable within the scope of university lectures and rely on the availability of specific 
incentives to elicit users contributions Some contributions of this research can be generalized to other environments such as large and popular online repositories of information media where either explicit motivation e g gaining access to recordings receiving financial compensations etc or intrinsic motivation e g users altruism is present However due to the high importance of the motivation for the success of the proposed solution more investigation is needed to determine the applicability of these findings to other domains and under different assumptions 5 9 Summary and Discussion The usefulness and usability of webcast archives of lectures and academic presentations can be significantly improved by the integration of text transcripts Unfortunately manual transcription can often be logistically or cost ineffective At the same time ASR systems for lectures yield error rates of 40 45 under realistic conditions below the 25 threshold of usefulness as determined in Munteanu et al 2006a and presented in 
Chapter 3 As a solution to bridging the WER gap we have developed a collaborative tool that extends the basic functionality of a transcript enhanced webcast system by engaging users to collaborate in transcript editing and correction for webcast lectures and presentations We have evaluated this tool through a field study carried out in the context of a real classroom and have shown Useful Transcriptions of Webcast Lectures 132 CHAPTER 5 WIKI EDITING OF WEBCAST TRANSCRIPTS that this is a feasible solution for alleviating the ASR errors of webcast lecture transcripts The editing tool was evaluated iteratively by integrating it with the other educational resources available to the students of two Computer Science courses We have analyzed not only the improvements in transcript quality brought by the editing tool but also looked at how students are making use of transcripts in general and of the wiki editing tool and what is their attitude toward such enhancements of webcast systems We have found that wiki 
editing is well received by webcast users and that students are willing to contribute to the improvement of lecture transcripts Our study revealed that access to transcript editing must be facilitated by providing both the option of on the fly editing and that of mass editing Since providing various academic or even financial incentives for transcript editing may not always be feasible particularly outside the classroom environment there might be cases where the solution described in this chapter will lead to only a part of the transcripts being corrected In Chapter 6 I will propose to combine the HCI and ASR based efforts to improve lecture transcript quality by introducing a method that exploits users corrections of the early parts of a lecture to further improve the ASR system Useful Transcriptions of Webcast Lectures Chapter 6 Automatic Speech Recognition for Webcast Lectures Learning from Wiki enabled Transcript Corrections Improving access to archives of webcast lectures is a task that by its very 
nature requires research efforts common to both Automatic Speech Recognition ASR and Human Computer Interaction HCI One of the main challenges to integrating text transcripts into archives of webcast lectures is the poor performance of ASR systems when transcribing lectures This is in part caused by the mismatch between the language used in a lecture and the predictive language models employed by the ASR system The solution proposed in Chapter 4 and in Munteanu et al 2007 addresses this issue through an information retrieval technique that exploits lecture 133 134 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS slides by automatically mining the World Wide Web for documents related to the presentation topic and using these to build a better matching language model Despite the improvements to ASR performance on lecture transcriptions 11 relative WER reduction brought by the proposed lecture specific language modelling there still exists a significant gap between the actual WER and the 
established quality threshold of 25 presented in Chapter 3 and in Munteanu et al 2006a To and in Munteanu et al 2008a facilitates the editing of ASR generated transcripts and under certain conditions e g proper student motivation leads to entirely corrected transcripts One of the research questions that remains open is determining the appropriate motivational method for eliciting the necessary user participation that leads to satisfactory correction of webcast transcripts in more general scenarios particularly outside the classroom environment However research evidence suggests that further WER reductions are possible in subsequent lectures when manual transcripts of large enough number of earlier lectures in a series are used to re train an ASR system Glass et al 2007 For this the corrected transcripts can be employed to address one of the main problems that ASR systems face in large vocabulary and unconstrained domains such as lectures the lack of previously collected data on the same topic and from Useful 
Transcriptions of Webcast Lectures 135 the same speaker Our previous work on improving the quality of lecture transcripts found that users of web based lecture viewing systems can successfully contribute to the reduction of WER Although it is possible to entirely correct the transcripts for a lecture this usually requires additional incentives We have observed Chapter 5 Munteanu et al 2008a however that manually transcribing the first 10 to 15 minutes of each lecture is a goal that can be easily achieved In this chapter we will look at exploiting this finding to further improve the quality of lecture ASR systems A solution for improving the quality of automatically generated webcast lecture transcriptions is introduced in this chapter In contrast to the approach of Glass et al 2007 that uses the manual transcripts of the lectures from the first half of a semester long course the proposed solution makes use of a minimal amount of manual transcripts corresponding to as little as the first ten minutes of each 
hour long lecture These transcripts are obtained through user collaboration enabled by the wiki editing system described in Chapter 5 The WER reductions exhibited by the empirical evaluation demonstrate that wiki editing of imperfect webcast transcripts not only directly improves the quality of the transcripts but can be successfully used to further improve the ASR output despite the significantly small size of the user corrected transcripts In this chapter I will review the existing work on improving ASR for lectures by making use of existing manual transcripts Section 6 1 and present a brief introduction Section 6 2 to Transformation Based Learning TBL a method used in various Natural Language Processing tasks to Useful Transcriptions of Webcast Lectures 136 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS correct the output of a stochastic model I will then introduce a TBL based solution for improving ASR transcripts for lectures Section 6 3 followed by a description of an empirical 
evaluation of the proposed solution and an analysis of its results Section 6 4 6 1 Related Work Several ASR research directions are focused on improving the quality of lecture transcripts from better acoustic modelling Park et al 2005 Useful Transcriptions of Webcast Lectures 6 1 RELATED WORK 137 lectures instead of completely transcribing the entire course In contrast if a university aims to improve access to its potentially large collection of online lectures by offering transcripts for each lecture the costs of manually producing such transcripts would be prohibitive Therefore as argued in Hazen 2006 any ASR improvements that rely on manual transcripts need to offer a balance between the cost of producing those transcripts and the amount of improvement i e WER reductions Since manual transcripts are a costly resource several research efforts have looked at more efficiently exploiting limited amounts of them One such approach is active learning where the goal is to select or generate a subset of the 
available data for which the manual transcripts would be the best candidate for ASR adaptation or training Riccardi and Hakkani Tur 2005 Huo and Li 2007 Usually the goal of this research is to reduce the size of the training data without an increase in example by treating the ASR output as being in a foreign language and trying to translate that into the target language through the noisy channel method used in Machine Translation Ringger and Allen 1996 This reduced WER from 41 to 35 on a corpus of train dispatch dialogues In other examples post ASR improvements are achieved by combining either the Useful Transcriptions of Webcast Lectures 138 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS transcripts or the word lattices from which transcripts are extracted of two complementary ASR systems a technique first perfected by NIST s ROVER system Fiscus 1997 with a 12 relative WER reduction and subsequently widely employed throughout the ASR community Most of the ASR research surveyed here and in 
particular for difficult tasks such as lectures requires large quantities of manually annotated training data Only a few approaches e g Munteanu et al 2007 try to solve the problem in a realistic scenario such as in lectures for which collecting extensive training data is not practical In this Chapter an enhancement of Transformation Based Learning detailed in Section 6 2 is proposed for automatically correcting ASR transcripts Our approach uses minimal training data that can be easily obtained such as through the wiki editing system proposed in Munteanu et al 2006d 2008a and described in Chapter 5 Transformation Based Learning TBL has been successfully applied to other Natural Language Processing NLP tasks such as Part of Speech tagging Brill 1992 In recent years TBL has received increasing attention in the ASR community being used for example to improve the word lattices from which the transcripts are selected Mangu and Padmanabhan 2001 resulting in a relative WER reduction of 5 over test data from a 
general purpose conversational corpus SWITCHBOARD using a language model that is trained over a diverse combination of broadcast news and telephone conversation transcripts Unfortunately such approaches are not suitable for cases in which the particular combination of acoustic and language models leads to word lattices that have a limited number of paths through them Useful Transcriptions of Webcast Lectures 6 1 RELATED WORK 139 as anecdotally evidenced during our evaluation of the web based lecture modeling described in Chapter 4 An alternative is to directly correct the ASR transcripts through n gram word level editing rules such as in Peters and Drexel 2004 which reports a 9 6 relative WER reduction on a professional dictation corpus with a 35 initial WER What is shown in this chapter is that a true WER calculation is so valuable that a manual transcription of only about 10 minutes of a one hour lecture is necessary to learn the TBL rules and that this smaller amount of transcribed data in turn makes the 
true WER calculation computationally feasible With this combination we achieve a greater average relative error reduction 12 9 than that reported by Peters and Drexel 2004 on their dictation corpus and a relative reduction over three times greater than that of our reimplementation of their heuristics on our lecture data 3 6 This is on top of the average 11 RER from language model adaptation on the same data We also achieve this reduction from TBL without the obligatory round of development set parameter tuning required by their heuristics and in a manner that is robust to perplexity Less is more Section 6 2 briefly introduces Transformation Based Learning TBL a method used in various Natural Language Processing tasks to correct the output of a stochastic model and then introduces a TBL based solution for improving ASR transcripts for lectures Section 6 3 describes the proposed TBL approach to lecture transcript correction while Section 6 4 presents the setup of the experimental evaluation and analyses its 
results Useful Transcriptions of Webcast Lectures 140 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS 6 2 Transformation Based Learning Historically research in Natural Language Processing can be divided into two major approaches statistical driven and rule based with statistical approaches being more recently predominant Roche and Schabes 1995 However research has emerged that successfully combined the two approaches such as Brill s Part of Speech POS tagger Brill 1992 Brill s tagger introduced the concept of Transformation Based Learning TBL The fundamental principle of TBL is to employ a set of rules to correct the output of a stochastic model In contrast to traditional rule based approaches where rules are manually developed TBL rules are automatically learned from training data The training data consist of sample output from the stochastic model aligned with the correct instances For example in Brill s tagger the system assigns POSs to words in a text which are later corrected by TBL 
rules These rules are learned from manually tagged sentences that are aligned with the same sentences tagged by the system Typically rules take the form of context dependent transformations for example change the tag from verb to noun if one of the two preceding words is tagged as a determiner Brill 1992 An important aspect of TBL is the rule scoring and ranking While the evidence from the training material can suggest a certain transformation rule there is no guarantee that such a rule will indeed correct and improve an NLP system s output In order to ensure that rules will not be used to falsely transform correct output a scoring function is used to rank rules From all the rules learned during training only those scoring higher than a certain threshold are retained For a particular task the scoring function Useful Transcriptions of Webcast Lectures 6 2 TRANSFORMATION BASED LEARNING ideally reflects an objective quality function 141 Since Brill s tagger was first introduced the TBL approach has been used 
for other NLP applications One recent example is the system described by Peters and Drexel 2004 in which TBL rules are employed to directly correct ASR output a graphical illustration of TBL used for ASR corrections is presented in Figure 6 1 in Section 6 3 In this case the rules consist of word level transformations that correct sequences of words n grams One of the main challenges is the heavy computational requirements of the rule scoring function Roche and Schabes 1995 Ngai and Florian 2001 This is particularly prevalent in applications of TBL to ASR corrections where large training corpora are needed Therefore while the objective function for improving the ASR transcript is the WER reduction the use of this for scoring the TBL rules is prohibitive For example Peters and Drexel 2004 had access to a very large amount of manually transcribed data so large in fact that the computation of true WER in the TBL rule selection loop was computationally infeasible and so they used a set of faster heuristics 
instead In Section 6 3 a solution for correcting ASR transcripts for lectures is presented that makes use of minimal amounts of training data for short one time recordings and as such can employ the WER as the rule scoring function Useful Transcriptions of Webcast Lectures 142 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS 6 3 Minimally Trained Transformation Based Learning for Webcast Transcription The underlying concept behind transformation based learning is the use of a scoring function to rank rules for correcting the output of a predictive model For ASR systems such rules are typically applied to either the compact search space from which the transcript is generated i e word lattices or directly to the first best path through this space the ASR transcript itself In both cases a large training data size is required to robustly learn transformation rules This requirement is more critical for ASR where the large vocabulary can lead to potentially many more errors than for example POS 
tagging where the set of tags is quite limited As such TBL applications to ASR traditionally employed heuristic approximations of transcript quality improvements in order to score rules While the objective function for improving the ASR transcript is WER reduction the use of this for scoring TBL rules can be computationally prohibitive over large data sets Peters and Drexel 2004 address this problem by using an heuristic approximation to WER instead and it appears that their approximation is indeed adequate when large amounts of training data are available Our approach stands at the opposite side of this trade off restrict the amount of training data to a bare minimum so that true WER can be used in the rule scoring function As it happens the minimum amount of data is so small that we can automatically develop highly domain specific language models for single 1 hour lectures We show here that the rules selected by this function lead to a significant WER reduction for individual Useful Transcriptions of 
Webcast Lectures 6 3 MINIMALLY TRAINED TRANSFORMATION BASED LEARNING FOR WEBCAST TRANSCRIPTION 143 lectures even if a little less than the first ten minutes of the lecture are manually transcribed This combination of domain specificity with true WER leads to the superior performance of the present method at least in the lecture domain we have not experimented with a dictation corpus Another alternative would be to change the scope over which TBL rules are ranked and evaluated but it is well known that globally scoped ranking over the entire training set at once is so useful to TBL based approaches that this is not a feasible option one must either choose an heuristic approach such as that of Peters and Drexel 2004 or reduce the amount of training data to learn sufficiently robust rules 6 3 1 TBL Algorithm for ASR Output Correction As our proposed TBL adaptation operates directly on ASR transcripts we employ an adaptation of the specific algorithm proposed by Peters and Drexel 2004 which is schematically 
represented in Figure 6 1 This in turn was adapted from the general purpose algorithm introduced in Brill 1992 6 3 2 Rule Discovery for Lecture Transcripts The TBL rules are represented as contextual replacement rules to be applied to ASR transcripts The rules are learned by performing a word level alignment between corresponding utterances in the manual and ASR transcripts of training data and extracting the mismatched word sequences anchored by matching words that serve as contexts The rule discovery algorithm outlined in Figure 6 2 is applied to every instance of Useful Transcriptions of Webcast Lectures 144 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS Figure 6 1 The TBL algorithm for correcting ASR output Transformation rules are learned from the alignment of manually transcribed text T with automatically generated transcripts TASR of training data ranked according to a scoring function S and applied to the ASR output TASR of test data Useful Transcriptions of Webcast Lectures 6 3 
MINIMALLY TRAINED TRANSFORMATION BASED LEARNING FOR WEBCAST TRANSCRIPTION 145 mismatching word sequences between the utterance level aligned manual and ASR transcripts For every matching sequence of words a set of transformation contextual replacement rules is generated The set contains the original matching sequence and its replacement part by itself and together with three alternatives where it is surrounded by the left right or both anchor context words In addition all possible splices of the matching sequence and the surrounding context words are also considered 1 Rules are represented as replacement expressions in a sed like syntax e g every match of the left side of the expression in the automatically generated transcript is to be replaced with the correct instance on the right side Rules that would result in arbitrary insertions of single words e g w1 are discarded An example of a rule learned from transcripts is presented in Figure 6 3 6 3 3 Scoring Function for the TBL rules As described in Section 
6 2 the scoring function that ranks rules is the main component of any TBL algorithm The goal of this function is to select rules that when applied to unseen test data will lead to a reduction in the error rate Typically for speech recognition tasks the goal is the reduction of Word Error Rate WER Ideally TBL rules would be scored by a function that directly correlates with WER However for large vocabulary 1 The splicing preserves the original order of the word level utterance output of a typical dynamic programming implementation of the edit distance algorithm Gusfield 1997 For this word insertion and deletion operations are treated as insertions of blanks in either the reference manual or ASR transcript Useful Transcriptions of Webcast Lectures 146 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS for every sequence of words c0 w1 wn c1 in the ASR output that is deemed to be aligned with a corresponding sequence c0 w1 wm c1 in the manual transcript add the following contextual replacements 
to the set of discovered rules c0 w 1 w n c1 c0 w 1 w m c1 c0 w 1 w n c0 w 1 w m w 1 w n c1 w 1 w m c1 w1 wn w1 wm for each i such that 1 i min n m add the following contextual replacements to the set of discovered rules c0 w 1 w i c0 w 1 w i c1 w i w n c1 w i w m c1 w 1 w i w 1 w i c1 wi wn wi wm Figure 6 2 The discovery of transformation rules as part of the TBL algorithm described in Figure 6 1 Useful Transcriptions of Webcast Lectures 6 3 MINIMALLY TRAINED TRANSFORMATION BASED LEARNING FOR WEBCAST TRANSCRIPTION 147 Utterance align ASR output and correct transcripts ASR Correct the okay one and you come and get your seats ok why don t you come and get your seats Insert sentence delimiters to serve as possible anchors for the rules ASR Correct s the okay one and you come and get your seats s s ok why don t you come and get your seats s Extract the mismatching sequence enclosed by matching anchors ASR Correct s the okay one and you s ok why don t you Output all rules for replacing the incorrect ASR sequence 
with the correct text using the entire sequence a or splices b with or without surrounding anchors a a a a b b b b b b b b the okay one and the okay one and you s the okay one and s the okay one and you the okay s the okay one and one and you the okay one s the okay one and and you ok why don t ok why don t you s ok why don t s ok why don t you ok s ok why don t why don t you ok why s ok why don t don t you Figure 6 3 An example of the rule discovery mechanism from Figure 6 2 Useful Transcriptions of Webcast Lectures 148 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS ASR such as broadcast news transcriptions especially for tasks covering multiple speakers and several sub topics the size of the training corpus must be large in order to avoid selecting rules that are scoring high only locally Thus various heuristic approximations are employed for which rules will be assigned scores that can be globally optimized in a computationally feasible manner Furthermore a development corpus is needed 
on which several parameters of the heuristic approximation are fine tuned As outlined in both the introduction to this Chapter and Section 6 2 for the task of transcribing lectures the availability of training data is very limited Furthermore it is often impractical to manually transcribe large parts of lectures in order to establish a development corpus particularly when the goal is the transcription of a single one hour lecture given by a one time speaker such as the case of invited presentations However as demonstrated in Chapter 5 and in Munteanu et al 2008a manual transcripts of the first 10 15 minutes in a lecture can be easily obtained Assuming a relatively small size for the available training data a TBL scoring function that directly correlates with WER can be conducted globally over the entire training set In keeping with TBL tradition however rule selection itself is still greedily approximated Our scoring function is formally described in Equation 6 1 and an example of the rule scoring process is 
given in Figure 6 4 SW ER r TASR T W ER TASR T W ER r TASR T where r TASR is the result of applying rule r on text TASR is more formally introduced in Section 6 3 4 6 1 Useful Transcriptions of Webcast Lectures 6 3 MINIMALLY TRAINED TRANSFORMATION BASED LEARNING FOR WEBCAST TRANSCRIPTION 149 Figure 6 4 An example of rule scoring and selection As outlined in Figure 6 1 rules that occur in the training sample more often than an established threshold will be ranked according to the scoring function The ranking process is iterative in each iteration the highest scoring rule rbest is selected In subsequent iterations the training data TASR will be replaced with the result of applying the selected rule on it TASR rbest TASR and the remaining rules will be scored on the modified training text This ensures that the scoring and ranking of remaining rules takes into account the changes brought by the application of the currently selected rule to the training transcript The iterations stop when the scoring function 
reaches zero none of the remaining rules improves the objective function WER on the training data Useful Transcriptions of Webcast Lectures 150 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS 6 3 4 Rule Application During the training phase the best scoring rule in each iteration is selected as described in Section 6 3 3 On testing unseen data rules are applied to ASR transcripts in the same order in which they were selected As shown in Figure 6 2 and discussed in Section 6 3 2 we have chosen a sed like representation for the replacement rules Therefore rule application as exemplified in Figure 6 5 is a straightforward process for each rule r w1 wn w1 wm the new transcript r TASR is obtained by replacing all instances of the n gram w1 wn appearing in the ASR transcript TASR with the n gram w1 wm ASR the okay let s start Insert sentence delimiters ASR s the okay let s start s Identify a rule that matches the original ASR text s the okay s ok Apply the rule replace the incorrect ASR sequence 
with the correct text ASR corrected s ok let s start s Remove sentence delimiters ASR corrected ok let s start Figure 6 5 An example of rule application Useful Transcriptions of Webcast Lectures 6 4 EMPIRICAL EVALUATION 151 6 4 Empirical Evaluation The proposed TBL based approach to ASR transcript correction was evaluated over several lecture recordings Several combinations of TBL parameters were tested As the proposed method was not refined during the evaluation and since one of the goals of our proposed approach is to eliminate the need for developmental data sets the available data were partitioned only into training and test sets 2 The empirical evaluation was conducted using the SONIC toolkit Pellom 2001 We used the acoustic model that is part of the toolkit built on 30 hours of data from 283 speakers from the WSJ0 and WSJ1 subsets of the 1992 development set of the Wall Street Journal WSJ Dictation Corpus LDC 1994 6 4 1 Evaluation Data The evaluation data consist of a total of eleven lectures of 
approximately 50 minutes each recorded in three separate courses each taught by a different instructor For each course the recordings were performed in The recordings were collected in a different weeks of the same course large amphitheatre style lecture hall 200 seats using the AKG C420 head mounted directional microphone The recordings were not intrusive no alterations to the lecture environment or proceedings were made The mono recordings were digitized using the TASCAM US 122 interface as 2 With the exception of one hour of lecture recording used as a testbed during code development and debugging Useful Transcriptions of Webcast Lectures 152 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS uncompressed audio files with a 16KHz sampling rate and 16 bit samples The audio recordings were segmented at pauses longer than 200ms manually for one instructor and automatically3 for the other two The evaluation data are described in Table 6 1 Four evaluations tasks were carried Evaluation task name 
Instructor Gender Age Segmentation Number of lectures Lecture topic R 1 R Male R 2 G 1 G Male Mid 40s automatic 3 Software design ICSISWB K 1 K Female Early 40s automatic 4 Unix programming WSJ 5K Early 60s manual 4 Interactive media design Language model WSJ 5K WEB LECT Table 6 1 The evaluation data 3 The automatic segmentation was performed using an implementation of the silence detection algorithm described in Placeway et al 1997 manually fine tuned for every instructor in order to detect all pauses longer than 200ms while allowing a maximum of 20 seconds in between pauses 4 The AM related parameters were the same as those described in detail in Section 3 3 Useful Transcriptions of Webcast Lectures 6 4 EMPIRICAL EVALUATION 153 6 4 2 Language Models The four evaluations were carried out using the language models mentioned in Table 6 1 either custom built for a particular topic or the baseline models included in the SONIC toolkit as follows WSJ 5K is the baseline model of the SONIC toolkit It is a 5K word 
model built using the same corpus as the base acoustic model included in the toolkit 30 hours of data from 283 speakers from the WSJ0 and WSJ1 subsets of the 1992 development set of the Wall Street Journal WSJ Dictation Corpus ICSISWB is a 40K word model created through the interpolation of LMs built on the entire transcripts of the ICSI Meeting corpus and the Switchboard corpus The ICSI Meeting corpus consists of recordings of university based multi speaker research meetings totaling about 72 hours from 75 meetings Janin et al 2003 The Switchboard SWB corpus Godfrey et al 1992 is a large collection of about 2500 scripted telephone conversations between approximately 500 English native speakers suitable for the conversational style of lectures as also suggested in Park et al 2005 WEB LECT is a LM built for each particular lecture using information retrieval techniques that exploit the lecture slides to automatically mine the World Wide Web for documents related to the presented topic and uses these to build 
a language model that better matches the lecture topic 5 A pronunciation dictionary was custom built to 5 A complete description of this LM solution can be found in Munteanu et al 2007 Useful Transcriptions of Webcast Lectures 154 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS include all words appearing in the corpus on which the LM was trained The pronunciations were extracted from existing initial dictionaries the 5K word WSJ dictionary included with the SONIC toolkit and the 100K word CMU pronunciation dictionary CMU 1998 Only one non dictionary word per line of corpus was allowed for lines longer than four LMs were trained using the CMU CAM Language Modelling Toolkit Clarkson and Rosenfeld 1997 with a training vocabulary size of 40K words 6 4 3 Training and Test Data Partitioning As previously mentioned in the survey of related work Section 6 1 most approaches to ASR improvement that operate on the transcripts themselves such as TBL based corrections need both a training and a 
development corpus Furthermore these are often up to five times larger than the test corpus on which the methods are evaluated and both the test and the training set must be subsets of the same cohesive corpus While such requirements can be favourably met for larger scale applications e g broadcast news transcription they are rarely satisfied for lecture transcription Despite the challenges facing automatic lecture transcription even in the more realistic scenario of transcribing a single lecture one rather important and in Chapter 4 Useful Transcriptions of Webcast Lectures 6 4 EMPIRICAL EVALUATION 155 condition is met it can be assumed that a one hour lecture given by the same instructor will exhibit a strong cohesion both in topic and in speaking style between its parts Therefore in contrast to typical TBL solutions we have decided to evaluate our TBL based approach by partitioning each 50 minute lecture into a training and a test set where the training set is smaller than the test set As mentioned in the 
introductory part of this chapter in a real life scenario a feasible solution exists to obtain manual transcripts for the first 10 to 15 minutes of a lecture As such the evaluation was carried out with two values for the training size the first fifth T S 20 and the first third T S 33 part of the lecture being manually transcribed 6 4 4 Scoring Functions Three scoring functions were assessed through the experimental evaluation the proposed TBL scoring function as well as two baseline functions XER is the baseline used in this evaluation Since our TBL solution is an extension of the solution proposed in Peters and Drexel 2004 this baseline is a simple implementation of the heuristic approximation based TBL approach introduced there The scoring function for this baseline is the expected error reduction XER Useful Transcriptions of Webcast Lectures 156 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS text being unnecessarily corrected by a XER NoS An analysis of the rules selected by both TBL 
implementations revealed that using the XER approximation leads to several single word rules being selected such as rules removing all instances of frequent stop words such as the and for or pronouns such as he Therefore an empirical improvement XER NoS of the baseline was implemented that beside pruning rules below the RT threshold omits such single word rules from being selected SW ER is the proposed TBL scoring function formally introduced in Equation 6 1 This function directly correlates with WER and scores rules gloally over the entire training set as described in Section 6 3 3 6 4 5 Results Beside the training size parameter during all evaluation sessions a second parameter was also considered the rule pruning threshold RT As described in Section 6 3 1 from all the rules learned during the rule discovery step only those that occur more often than the threshold are scored and ranked This parameter can be set as low as 1 consider all rules or 2 consider all rules that occur at least twice over the 
training set For larger scale tasks this threshold serves as a pruning mechanism in order to reduce the computational burden of scoring several thousand rules especially when the scoring function is not a simple approximation of the objective Useful Transcriptions of Webcast Lectures 6 4 EMPIRICAL EVALUATION 157 function However a larger threshold could potentially lead to discrediting low frequency but high scoring rules Therefore due to the small size of the training data for lecture TBL the lowest threshold was set to RT 2 When a development set is available several values for the RT parameter can be tested and the optimal one chosen for the evaluation task In our case a development set is not available As such we have tested two more values for the rule pruning threshold RT 5 and RT 10 Tables 6 2 6 3 and 6 4 present the evaluation result for instructors R and G The transcripts were obtained through ASR runs using three different language models The TBL implementation with our scoring function SW ER 
brings relative WER reductions ranging from 10 5 to 18 0 with an average of 12 9 These WER reductions are greater than those produced by the XER baseline approach It is not possible to provide confidence intervals since the proposed method does not tune parameters from sampled data which we regard as a very positive quality for such a method to have Our speculative experimentation with several values for T S and RT however leads us to conclude that this method is significantly less sensitive to variations in both the training size T S and the rule pruning threshold RT than earlier work making it suitable for application to tasks with limited training Useful Transcriptions of Webcast Lectures 158 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS TBL follows upon benefits words with higher information content more It is possible that the favour observed in TBL with SW ER towards lower information content is a bias produced by the preceding round of language model adaptation but regardless it 
provides a much needed complementary effect This can be observed in Tables 6 2 and 6 3 in which TBL produces nearly the same relative error reductions in either table for any lecture The empirical improvement XER NoS of the baseline function XER slightly improves the performance of the approximation based TBL for some values of the RT and T S parameters as outlined in Tables 6 2 6 3 and 6 4 However it still does not consistently match the WER reductions of the globally scoped scoring function While the evaluation presented in Table 6 3 is carried out over the same lectures as those in Table 6 2 it is worth noting that the LM employed here WEB LECT is a lecture specific model as described in Section 6 4 2 and in Munteanu et al 2007 Despite this being already optimized for each task the TBL with SW ER scoring is able to bring further WER reductions resulting in average WERs of 40 over the four lectures from instructor R Although the empirical evaluation shows positive improvements in transcript quality through 
TBL in particular when using the SW ER scoring function an exception is illustrated in Table 6 5 The recordings for this evaluation were collected from a course on Unix programming and lectures were highly interactive Instructor K used numerous examples of C or Shell code many of them being developed and tested in class While the keywords from a programming language can be easily added to the ASR lexicon the pronunciation of such abbreviated forms especially for Shell Useful Transcriptions of Webcast Lectures 6 5 LIMITATIONS AND GENERALIZATIONS 159 programming and of mostly all variable and custom function names proved to be a significant difficulty for the ASR system This combined with a high speaking rate and often inconsistently truncated words led to few TBL rules occurring even above the lowest RT 2 threshold despite many TBL rules being initially discovered Anecdotal evidence collected from users of the transcript correction system showed they often paraphrased the speaker due to a lack of guidelines 
for how to deal with such difficulties in transcribing the recordings As previously mentioned one of the drawbacks of global TBL rule scoring is the heavy computational burden However the empirical evaluation conducted here showed an average learning time of one hour per one hour lecture reaching at most three hours6 for a threshold of 2 when training over transcripts for one third of a lecture Therefore it can be concluded that despite being computationally more intensive than a heuristic approximation a TBL system using a globally scoped WER correlated scoring function is not only a better performing but also a feasible solution to improving the quality of lecture transcripts when manual transcripts for the beginning of each lecture are present 6 5 Limitations and Generalizations The TBL scoring function proposed in this Chapter is mostly applicable to the improvement of single e g one hour lectures delivered by one 6 It should be noted that in order to preserve compatibility with other software tools the 
code developed for this evaluation was not optimized for speed It is expected that a dedicated implementation would result in even lower run times Useful Transcriptions of Webcast Lectures 160 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS Instructor R Language model WSJ 5K Lecture TS Initial WER RT 10 XER RT 5 RT 2 RT 10 XER NoS RT 5 RT 2 RT 10 SWER RT 5 RT 2 20 50 48 49 97 50 01 49 87 47 25 49 03 52 21 45 18 44 82 44 04 1 33 50 93 49 82 50 07 51 75 46 82 48 78 53 47 44 58 43 82 43 99 20 51 31 49 27 49 99 49 52 49 98 47 37 49 31 49 06 46 73 45 81 2 33 51 90 49 77 51 13 51 13 48 72 51 25 52 29 45 97 45 52 45 16 20 50 28 46 85 48 39 47 13 48 44 47 84 50 85 46 49 45 64 44 35 3 33 49 23 48 08 47 37 47 31 45 21 44 07 49 41 45 30 43 18 41 49 20 54 39 52 17 50 91 52 70 51 37 49 54 50 63 49 60 47 79 46 89 4 33 54 04 50 58 49 62 50 56 49 73 48 97 51 81 47 95 46 74 44 28 Table 6 2 The WER values for instructor R for which the ASR output using the WSJ 5K language model is corrected by TBL rules that 
are scored by the approximation functions XER and XER NoS as baselines and by the proposed globally scoped non heuristic scoring function SW ER Useful Transcriptions of Webcast Lectures 6 5 LIMITATIONS AND GENERALIZATIONS 161 Instructor R Language model WEB LECT Lecture TS Initial WER TBL with 1 20 45 54 42 91 43 45 43 26 43 51 44 96 46 72 41 98 40 97 40 67 33 20 2 33 43 87 43 81 44 37 44 66 41 98 40 52 45 87 40 75 39 08 38 07 20 46 69 46 78 46 90 43 77 44 66 44 66 40 44 44 66 44 66 40 00 3 33 47 14 45 35 42 12 45 12 46 59 41 74 44 32 45 27 40 84 40 08 20 49 78 46 92 47 34 61 54 47 24 47 23 61 84 47 24 45 27 43 31 4 33 49 38 49 65 46 04 60 40 46 30 44 35 64 40 45 85 42 39 41 52 45 85 43 36 43 90 43 81 45 46 42 97 42 98 48 16 41 44 40 56 42 44 42 65 44 19 42 11 40 01 44 79 42 11 38 85 RT 10 RT 5 RT 2 RT 10 RT 5 RT 2 RT 10 RT 5 RT 2 XER scoring TBL with XER NoS scoring TBL with SWER scoring 40 47 38 00 Table 6 3 The WER values for instructor R for which the ASR output using the WEB LECT language model is 
corrected by TBL rules that are scored by the approximation functions XER and XER NoS as baselines and by the proposed globally scoped non heuristic scoring function SW ER The average WER when using the SW ER function and the training parameter RT 2 is 40 Useful Transcriptions of Webcast Lectures 162 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS Instructor G Language model ICSISWB Lecture TS Initial WER TBL with 1 20 50 93 46 63 48 34 54 05 49 54 49 54 59 00 46 63 46 63 44 48 33 50 75 49 38 49 75 56 84 49 38 49 31 59 28 46 53 45 60 44 30 20 54 10 49 93 49 32 52 01 54 10 56 70 57 61 49 80 47 75 47 46 2 33 53 93 48 61 48 81 49 11 53 93 55 50 55 03 48 44 47 23 47 02 20 48 79 49 52 49 58 50 37 48 79 48 51 50 41 45 83 44 76 43 60 3 33 49 35 50 43 49 26 51 66 48 24 48 42 52 67 45 42 44 44 44 13 RT 10 RT 5 RT 2 RT 10 RT 5 RT 2 RT 10 RT 5 RT 2 XER scoring TBL with XER NoS scoring TBL with SWER scoring Table 6 4 The WER values for instructor G for which the ASR output using the ICSISWB language 
model is corrected by TBL rules that are scored by the approximation functions XER and XER NoS as baselines and by the proposed globally scoped non heuristic scoring function SW ER Useful Transcriptions of Webcast Lectures 6 5 LIMITATIONS AND GENERALIZATIONS 163 Instructor K Language model WSJ 5K Lecture TS Initial WER TBL with 1 20 33 20 2 33 20 3 33 20 4 33 44 31 44 06 46 12 45 80 51 10 51 19 53 92 54 89 RT 10 44 31 44 06 46 12 46 55 51 10 51 19 53 92 54 89 RT 5 RT 2 44 31 44 87 46 82 47 47 51 10 51 19 53 96 55 56 47 46 55 21 50 54 51 01 52 60 54 93 57 48 60 46 XER scoring TBL with RT 10 44 31 44 06 46 12 46 55 51 10 51 19 53 92 54 89 RT 5 RT 2 44 31 44 87 46 82 47 47 51 10 51 19 53 96 55 56 46 43 54 41 50 54 51 01 53 01 55 02 57 47 60 02 XER NoS scoring TBL with RT 10 44 31 44 06 46 12 45 80 51 10 51 19 53 92 54 89 RT 5 RT 2 44 31 44 05 46 11 45 88 51 10 51 19 53 92 54 89 44 34 44 07 46 03 45 89 50 96 50 93 54 01 55 16 SWER scoring Table 6 5 The WER values for instructor K for which the ASR output using 
the WSJ 5K language model is corrected by TBL rules that are scored by the approximation functions XER and XER NoS as baselines and by the proposed globally scoped non heuristic scoring function SW ER Useful Transcriptions of Webcast Lectures 164 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS instructor The ASR system used to transcribe such lectures must have pronunciation dictionaries that cover topic specific keywords Since the experimental evaluation showed reductions in WER when using this scoring function over eleven hours of recordings from two different instructors it can be expected that the results of this research are generalizable to most lectures for which the manual transcripts of the first ten minutes of the lecture are available However as pointed out by the results on one particular instructor lecturer K the proposed method and in fact any other TBL solution will not produce significant WER reductions for lectures characterized by the frequent usage of numerous technical 
terms for which pronunciations are not available such as the name of variables or functions in C or Shell script A visual inspection of the rules selected by the proposed scoring function reveals that most of these rules repair ASR errors pertaining to a particular instructor s speaking style and to the genre of lecture speech instead of topic specific errors As such the applicability of this research can be extended to other domains where the transcripts of the first 10 or 15 minutes of an approximately one hour recording are available provided that these introductory parts of the recording are representative of the speaker s speaking style that such recordings are from a single speaker and that they belong to a genre characterized by a limited number of speaking styles 6 6 Summary and Discussion One of the challenges to reducing the WER of ASR transcriptions of lecture recordings is the lack of manual transcripts on which to train various ASR Useful Transcriptions of Webcast Lectures 6 6 SUMMARY AND 
DISCUSSION 165 improvements In particular for one hour lectures given by different lecturers such as for example invited presentations it is often impractical to manually transcribe parts of the lecture that would be useful as training or development data However transcripts for the first 10 minutes of a particular lecture can easily be obtained In this chapter a solution is presented that improves the quality of ASR transcripts for Useful Transcriptions of Webcast Lectures 166 CHAPTER 6 ASR FOR WEBCAST LECTURES LEARNING FROM WIKI CORRECTIONS threshold practically scoring all rules that occur at least twice with limited impact on the computational burden of learning the transformation rules The ASR improvement proposed in this chapter can be combined with other lecture specific optimizations resulting in average WERs of 40 over the lectures where such combinations were evaluated Specific WERs on these lectures ranged from 38 to no more than 43 This brings the lecture WER closer to the levels for which 
transcripts become a useful addition to webcast archives under certain tasks as mentioned in Section 3 4 2 It shows that combining HCI and ASR efforts is a promising path toward delivering useful lecture transcripts of WERs below 25 Useful Transcriptions of Webcast Lectures Chapter 7 Contributions Conclusions and Future Work Improving access to archives of webcast lectures is a task that by its very nature requires research efforts drawn from several disciplines such as Human Computer Interaction Natural Language Processing in particular Automatic Speech Recognition and Computer Supported Collaborative Work In this dissertation I have measured the threshold of quality for integrating transcripts into webcast archives and proposed solutions for improving the usefulness of automatically generated transcripts of webcast lectures and academic presentations I have shown that such improvements are possible through just such an interdisciplinary approach integrating novel speech recognition techniques with the 
development of an interactive collaborative interface Having determined the acceptable quality of automatically generated transcripts to be included in webcast lectures archives I showed that 167 168 CHAPTER 7 CONTRIBUTIONS CONCLUSIONS AND FUTURE WORK users performance and transcript quality perception are affected by the Word Error Rate With this precisely determined goal in mind for ASR performance two solutions were proposed that aimed to bring the quality of ASR transcripts to these desired levels The two methods independently address aspects of usability and transcript quality for lecture webcasts from the perspective of HCI and ASR respectively The ASR specific improvement makes use of an information retrieval technique that exploits lecture slides by automatically mining the World Wide Web for documents related to the presented topic and then uses these to build a better fitting language model for lecture transcription The HCI and CSCW based solution consists of a collaborative tool that extends a 
webcast system s functionality by allowing users to edit and correct the webcast transcripts in a wiki like manner Lastly a joint HCI and ASR method was introduced in that further improves the quality of webcast lecture transcripts by making use of the transcript corrections collected through the wiki editing tool to learn a set of transformation rules that improves the performance of the ASR system 7 1 Contributions How Good is Good Enough and What to Do When It Isn t 7 1 1 ASR how good is good enough Through an extensive user study I have investigated the user needs for transcription accuracy in webcast archives For this a within subjects study was designed in which 48 participants were exposed to multiple levels of Useful Transcriptions of Webcast Lectures 7 1 CONTRIBUTIONS HOW GOOD IS GOOD ENOUGH AND WHAT TO DO WHEN IT ISN T 169 WER in their interaction in a typical webcast use scenario students writing a quiz based on a lecture The results showed that users performance and transcript quality perception 
is linearly affected by WER with transcripts of WER equal to or less than 25 being useful and accepted by users of webcast archives This was determined by assessing users performance in a question answering task their perception of transcript quality as well as users confidence in their performance and their perceived level of task difficulty The study also revealed that for most browsing scenarios users prefer having transcripts even if the quality of those transcripts is less than optimal 7 1 2 What to do when ASR is not good Spoken Language Processing and in particular Automatic Speech Recognition have long focused on methods and technologies that would ultimately mimic human performance when transcribing speech to text In this vein I have worked on improving the accuracy of ASR systems for lectures by building statistical predictive language models specific to the topic and genre presented One of the common challenges when transcribing lectures is the mismatch between the language used in a lecture and 
the predictive language models employed by the ASR system In my dissertation I have proposed a solution that addresses this issue through an information retrieval technique that exploits lecture slides by automatically mining the World Wide Web for documents related to the presented topic and using these to build a better fitting language model Such an approach overcomes Useful Transcriptions of Webcast Lectures 170 CHAPTER 7 CONTRIBUTIONS CONCLUSIONS AND FUTURE WORK the need to manually fine tune the process of interpolating two models a large topic independent one and a smaller topic specific model and avoids relying on often imprecise algorithms or manual procedures for extracting keywords needed to build topic specific models The solution proposed in this dissertation achieves a relative WER reduction of up to 11 7 1 3 What to do when ASR is not good Despite improvements to the ASR performance for lecture transcriptions there still exists a significant gap between the desirable and actual WER To reduce 
and possibly eliminate this gap I have developed and evaluated an iterative design of a collaborative tool that in a wiki like manner allows users of webcast archives to edit and correct the transcripts on the fly while viewing an archived webcast The editing tool was evaluated iteratively by integrating it with other educational resources available to the students in two Computer Science courses The field studies showed that this is a feasible solution for alleviating the errors of ASR webcast lecture transcripts when fully engaged and properly motivated students completely corrected the transcripts for almost all lectures in the course I have also found that wiki editing is well received by webcast users and that students see the ability to correct transcripts as an enhancement of the classroom experience and are willing to contribute to the improvement of lecture transcripts Useful Transcriptions of Webcast Lectures 7 1 CONTRIBUTIONS HOW GOOD IS GOOD ENOUGH AND WHAT TO DO WHEN IT ISN T 171 7 1 4 What to 
do when ASR is not good Although engaging users to collaboratively correct the ASR transcripts of webcasts can lead to WERs of close to zero it is not always feasible to rely exclusively on this approach for improving the quality of webcast transcripts However in most cases such as webcasts of lectures transcripts for the first 10 minutes of an hour long presentation can be easily obtained As such in my dissertation I have proposed a joint HCI and ASR based approach in which partially corrected transcripts can be employed to address one of the main problems that ASR systems face in large vocabulary and unconstrained domains such as lectures the lack of previously collected data on the same topic and from the same speaker The proposed method employs users transcript corrections to learn word level transformation based rules that attempt to replace parts of the ASR transcript with possible corrections This leads to WER reductions of between 10 and 18 relative to initial values with an average of close to 13 
even on transcripts obtained by using language models already optimized for the lecture topic Such results show that wiki editing of imperfect webcast transcripts not only directly improves the quality of the transcripts but can be successfully used to further improve the ASR output By pursuing this approach the user is no longer be a simple recipient of an ASR system s output but rather an active contributor to the system s internal operation Useful Transcriptions of Webcast Lectures 172 CHAPTER 7 CONTRIBUTIONS CONCLUSIONS AND FUTURE WORK 7 2 Future Work As traditional forms of sharing and preserving information are continuously being replaced by online multimedia it becomes increasingly important to provide users with efficient and usable information access tools Future work should continue on the interdisciplinary path of advancing Human Computer Interaction and Spoken Language Processing research to improve the usability and usefulness of information rich media in particular within educational 
environments 7 2 1 Enlarging the Scope of Current Research As mentioned in the Limitations and Generalizations Sections 3 6 4 5 5 8 and 6 5 of this dissertation the research presented here was conducted within the scope of university lectures in which the instructors follow very detailed slides While certain aspects can be generalized as discussed in the aforementioned Sections one direction that future research should explore is the applicability of this research to completely different settings for example measuring the acceptable transcript quality of presentations in information critical domains such as business meetings or investigating topic specific language modelling approaches for lectures and presentations that do not use slides or that make use of other visual aids such as interactive examples Useful Transcriptions of Webcast Lectures 7 2 FUTURE WORK 173 7 2 2 User Motivated Measure of Transcript Quality One of the first steps toward the goal of improving users experience when accessing media 
archives is the development of a user motivated measure of transcript quality WER is the de facto standard for evaluating Automatic Speech Recognition but it is exclusively based on a blind comparison with a reference text This bears no resemblance to how imperfect ASR output is used by humans where various aspects of text readability become relevant as shown by Jones et al 2003 Some researchers have started to question the adequacy of WER in language understanding tasks such as automatic knowledge extraction from ASR transcripts Wang and Chelba 2003 Unfortunately no similar research was conducted to our knowledge on tasks involving humans As such one of the goals for future research is to formulate a scientific measure of the usability of transcripts This putative computationally feasible measure would be based on text features that correlate with human indicators of success in reading and comprehension tasks particularly in the context of using transcripts of information rich webcasts such as those of 
lectures and presentations 7 2 3 Refining the Acceptable Quality of ASR Transcripts The study presented in Chapter 3 is limited to one specific task quiz answering under strict time constraints and to an undergraduate level student population most of whom used such a system for the first time While the findings of this research can be generalized as indicated by the Useful Transcriptions of Webcast Lectures 174 CHAPTER 7 CONTRIBUTIONS CONCLUSIONS AND FUTURE WORK post session questionnaire to various academic activities such as making up for a missed class or preparing an assignment future work must extend this study to a broader pool of participants such as corporate webcast users for which accuracy might be more critical and to more diverse tasks and conditions such as presentations for which no slides are available With respect to more accurately determining the relation between transcript quality and usefulness of webcasts experiments should also be carried out with finer grained levels of WER e g in 
increments of 5 around the threshold of 25 determined in Chapter 3 The trend analysis of the data collected in the experiments described in Chapter 3 showed that there is a statistically significant effect on users performance and experience caused by the different experimental conditions in the order WER 0 WER 25 no transcript and WER 45 with the exception of situations where the information users searched for was not present on lecture slides in which case there was an ordinal inversion of the no transcript and the WER 45 conditions In terms of pairwise comparisons between the immediate values of WER 25 no transcript and 45 not all measures indicated a statistically significant categorical difference As such it might not be possible to determine an accurate gold standard for the quality of webcast transcripts experiments with finer grained levels of WER however could reveal with statistical significance an interval in which such a true WER usefulness threshold lies Finally this research must be extended to 
include any user motivated text quality measures as proposed in Section 7 2 2 Useful Transcriptions of Webcast Lectures 7 2 FUTURE WORK 175 7 2 4 Higher Level Text Based Representations of Lectures and Presentations ASR systems are not likely to improve significantly in the near future and thus transcripts of webcasts may not reach the same usefulness levels as perfect manually generated transcripts As such although several experimental findings including those presented in this dissertation highly commend the importance of transcripts for webcast systems a higher level text based representation but more comprehensive than a table of contents of lecture or presentation content is also needed Future work should look at enhancing current webcast interfaces with other more compact textual representations of lectures and presentations such as summaries hierarchical tables of contents or slide annotations 7 2 5 ASR Based Search Tools for Webcast Archives Existing literature as well as the research evidence 
presented in this dissertation indicate that transcripts are a much needed tool in improving users browsing and information mining through webcast archives However certain tasks would benefit from the availability of a reliable search function for example retrieving a recording from a large collection of webcasts based on a user specified topic description i e document retrieval or locating a point in the archive of a lecture webcast when an instructor mentioned a particular concept i e utterance retrieval These operations are not possible without searching through and indexing the audio channel of the archived webcasts There is an increased interest in the ASR community Useful Transcriptions of Webcast Lectures 176 CHAPTER 7 CONTRIBUTIONS CONCLUSIONS AND FUTURE WORK in improving the spoken document utterance retrieval and indexing in particular on performing these operations without the need for accurate transcripts Saraclar and Sproat 2004 Hori et al 2007 and within the context of large scale Internet 
based repositories of audio recordings Zhou et al 2006 Beside the challenges of improving the speech searching and indexing algorithms this area presents some interesting opportunities for future HCI research as well while many solutions were proposed and successfully adopted for the problem of displaying in a usable manner results of queries over text collections searching through repositories of and within information rich mixed media presentations is still an open research problem the MIT lecture browser being one of the few examples of webcast systems that allows users to interact with ASR based search results as illustrated in Figure 7 1 Figure 7 1 The ASR based search interface of the MIT lecture browser Useful Transcriptions of Webcast Lectures 7 2 FUTURE WORK 177 7 2 6 Further ASR Improvements for Webcast Lecture Transcription Since one of the typical sources of recognition errors is large vocabulary and language model size future work should look at improving the web based retrieval of relevant 
corpora introduced in Chapter 4 Corpus and text similarity measures could be considered that select from the retrieved web corpus only those documents that better maximize the match between the retrieved corpus and the conversational style of a lecture and of a particular lecturer Some simple lexical measures of corpus similarity1 were already employed with moderate but promising success such as assessing the effect on the perplexity of a language model brought by the removal of documents from the training corpus for a broadcast news transcription task Klakow 2000 Given the less scripted nature of lectures it is expected that instructors particular speaking styles will be reflected more in their lecture speech As such the suitability of syntactic similarity measures used in authorship attribution especially for short texts as in Hirst and Feiguina 2007 should also be investigated As was shown in Chapter 6 transformation based TBL rules that correct the ASR transcripts and reduce the WER by up to 18 can be 
learned on manual transcripts corresponding to as little as 10 minutes of a one hour lecture Future research should consider other methods of improving ASR systems that make use of only small amounts of training data Among these methods are TBL approaches that operate at the word lattice level or at the 1 An extended survey of corpus similarity measures can be found in Kilgarriff and Rose 1998 Useful Transcriptions of Webcast Lectures 178 CHAPTER 7 CONTRIBUTIONS CONCLUSIONS AND FUTURE WORK phonetic transcription level 7 2 7 Maximizing the Trade off between User Editing and ASR Improvements The method proposed in Chapter 6 improves the ASR for a lecture by exploiting the manual transcripts of the first 10 minutes A possible direction of future research is the exploration of various trade offs between the amount of manual transcripts used and the reductions in WER This is particularly relevant in the context of keeping the users in the ASR editing loop e g combining user edits with TBL in a cycle converging to 
lower WER values Previous research such as that of Glass et al 2007 shows that manually transcribing one half of the recordings in a course series can through a combination of AM and LM adaptation reduce the WER on the remaining lectures below 20 While such a low WER value is certainly desirable the financial proposition of manually transcribing lectures might not warrant it For example as a plausibility argument if a typical base cost of a course is 20 000 10 000 in instructor fees and another 10 000 in University overhead and considering a standard student work pay rate of 12 50 hour transcribing 20 hours of lectures in a 40 hour semester would increase the entire cost of the course by 12 5 or 2500 However if the users are kept in the ASR improvement loop e g the ASR system improves constantly as users correct the transcripts this cost can be reduced further For this future work should focus on developing better editing interfaces that would facilitate faster correction of ASR errors by webcast users 
Assuming that the threshold at which correcting errors is faster than re transcribing is also at Useful Transcriptions of Webcast Lectures 7 2 FUTURE WORK 179 25 WER the mark up costs of providing useful transcriptions for webcast lectures can be reduced to below 10 of the total course costs Furthermore if the course is to be taught again by the same instructor in subsequent years the initial WER will potentially be lower if for example the previous speaker adapted AMs are used As such the number of wiki enabled user corrections required for useful transcripts can be even lower 7 2 8 Other Collaborative Approaches to Webcast Usability Improvement Another research question that remains open is that of determining the appropriate motivations for subjects to participate in collaborative transcript correction Combining lecture access limitations with academic and financial incentives yielded sufficiently many contributions from students even in a smaller sized class Weaker incentives coupled with comprehensive 
lecture materials slides readings resulted in both significantly fewer contributions and reduced interest in archived lecture viewing Evaluating the wiki editing concept in a larger sized class with reduced availability of course materials should also be considered The ePresence system is used not only for webcast lectures but for other genres of presentations which are archived and available through the ePresence tv media portal Many of these archives are accessed by several thousand viewers Such communities have already overcome the issue of reaching a critical mass of contributors and they typically offer intrinsic motivations to contribute Kuznetsov 2006 Future development efforts should be dedicated to integrating the wiki editing interface into the Useful Transcriptions of Webcast Lectures 180 CHAPTER 7 CONTRIBUTIONS CONCLUSIONS AND FUTURE WORK ePresence portal Besides investigating the use of automatically generated textual projections of lectures such as summaries future work should expand the scope 
of collaborative solutions to encompass all text based representations of webcast archives In the lecture domain for example course materials such as class handouts are a common textual resource that accompanies lectures and students typically enhance its content through handwritten notes annotations taken during lectures Natural language processing can be combined with collaborative approaches to generate handout annotations 7 3 In Conclusion In this dissertation I have shown that an interdisciplinary approach combining research in Automatic Speech Recognition and Human Computer Interaction can improve the access to and the interaction with online archives of webcast lectures and academic presentations For this I have proved that integrating automatically generated transcripts of Word Error Rate of 25 or less into archives of webcast lectures enhances the usability of archives and results in improved usefulness of archived webcast lectures I have also shown that significant transcript quality improvements 
toward the acceptable Word Error Rate can be achieved achieved if speech recognition techniques specifically addressed at increasing the accuracy of webcast transcriptions are integrated with the development of an interactive collaborative interface that facilitates users editing of machine generated transcripts Useful Transcriptions of Webcast Lectures Bibliography Ariki et al 2003 Y Ariki T Shigemori T Kaneko J Ogata and M Fujimoto Live speech recognition in sports games by adaptation of acoustic model and language model In Proceedings of Eurospeech pages skimming recorded speech Interaction 4 1 ACM Transactions on Computer Human Bacchiani and Roark 2004 M Bacchiani and B Roark conditional language modeling Meta data In Proceedings of the International Conference on Acoustics Speech and Signal Processing Montreal Canada May 2004 Baecker et al 2003 R Baecker G Moore D Keating and A Zijdemans Reinventing the lecture Webcasting made interactive In Proceedings of HCI International volume 1 pages 182 
BIBLIOGRAPHY archives In Proceedings of CASCON pages Recognition Theory and C Implementation John Wiley Sons New York 1999 augmentation and language model adaptation using singular value decomposition In Pattern Recognition Letters volume 25 pages distributed groups In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems pages Useful Transcriptions of Webcast Lectures BIBLIOGRAPHY Birnholtz 2006 J Birnholtz Back to school 183 Design principles for improving webcast interactivity from face to face classroom observation In Proceedings of the ACM SIGCHI Conference on Designing Interactive Proceedings of the Third Conference on Applied Natural Language Processing pages ACM Transactions on Computer Human Interaction 11 2 Useful Transcriptions of Webcast Lectures 184 BIBLIOGRAPHY Chelba et al 2007 C Chelba J Silva and A Acero Soft indexing of speech content for search in spoken documents Computer Speech and Language 21 3 http www speech cs cmu edu 1998 Crowston et al 2004 K Crowston H 
Annabi J Howson and C Masango Effective work practices for software engineering Free libre open source software development In Proceedings of the ACM workshop on Interdisciplinary software engineering for automatic transcription and browsing of audio visual presentations Technical report MIT CSAIL Research Abstract 2005 Useful Transcriptions of Webcast Lectures BIBLIOGRAPHY 185 factors in computing systems pages Useful Transcriptions of Webcast Lectures Spoken Dialogues with Computers 186 BIBLIOGRAPHY IEEE Workshop on Automatic Speech Recognition and In Proceedings of the Seventh International ISCA INTERSPEECH Conference 2006 Furui 2005a S Furui Recent progress in corpus based spontaneous speech recognition IEICE Transactions on Information and Systems E88 D 3 Recent progress in corpus based spontaneous speech recognition IEICE Transactions on Information and Systems 3 88 Useful Transcriptions of Webcast Lectures BIBLIOGRAPHY Gauvain et al 2002 J L Gauvain L Lamel and G Adda LIMSI broadcast news 
transcription system 37 187 The Speech Communications Giuliani and Federico 2001 D Giuliani and M Federico Unsupervised language and acoustic model adaptation for cross domain portability In Proceedings of the ISCA ITR Workshop on Adaptation Methods for Speech Recognition Sophia Antipolis France August 2001 Glass et al 2004 T J Glass J R amd Hazen L Hetherington and C Wang Analysis and processing of lecture audio data Preliminary investigations In Proceedings of the HLT NAACL Workshop on Interdisciplinary Approaches to Speech Indexing and Retrieval pages lecture processing project In Proceedings of the Tenth ISCA European Conference on Speech Communication and Useful Transcriptions of Webcast Lectures 188 BIBLIOGRAPHY Godfrey et al 1992 J J Godfrey E C Holliman and J McDaniel SWITCHBOARD Telephone speech corpus for research and development In Proceedings of the IEEE Conference on Acoustics Speech and Signal development context Interacting with Computers 4 2 Bootstrapping language models for spoken dialog 
systems from the World Wide Web In Proc IEEE Conf on Acoustics Speech and Signal Useful Transcriptions of Webcast Lectures BIBLIOGRAPHY 189 Informedia at TRECVID 2003 Analyzing and searching broadcast news video In Proceedings of VIDEO TREC 2003 The Twelfth Text Retrieval Conference Gaithersburg Maryland USA November 2003 Hazen 2006 T J Hazen Automatic alignment and error correction of human generated transcripts for long speech recordings In Proceedings of the Ninth International ISCA Conference on Spoken Language Useful Transcriptions of Webcast Lectures 190 BIBLIOGRAPHY Howell 1997 D C Howell Statistical Methods for Psychology Duxbury Press USA 1997 Howell 1999 D C Howell Fundamental Statistics for the Behavioural Sciences Duxbury Press USA 1999 Howell 2002 D C Howell Multiple comparisons with repeated measures http www uvm edu dhowell 2002 Hsu and Glass 2006 B J Hsu and J Glass Style topic language model adaptation using HMM LDA In Proc ACL Conf on Empirical Methods in Natural Language Useful 
Transcriptions of Webcast Lectures Spoken Belgium BIBLIOGRAPHY 191 Jones et al 2003 D A Jones F Wolf E Gibson E Williams E Fedorenko D A Reynolds and M Zissman readability of automatic speech to text transcripts Measuring the In Proceedings of the Eight European Conference on Speech Communication and transcription of lecture speech using topic independent language modeling In ICSLP volume 1 pages onautomatic Speech Recognition and Understanding 2001 Kawahara 2004 T Kawahara Spoken language processing for audio In Processing of the archives of lectures and panel discussions International Conference on Informatics Research for Development of Knowledge Society Behavioural Sciences Brooks Cole Publishing 1995 Useful Transcriptions of Webcast Lectures 192 Klakow 2000 D Klakow BIBLIOGRAPHY Selecting articles from the language model training corpus In Proc IEEE Conf on Acoustics Speech and Signal DARPA CSR The Linguistic Data Consortium LDC94S13 1994 Leeuwis et al 2003 E Leeuwis M Federico and M Cettolo Language 
modeling and transcription of the TED corpus lectures In Proceedings of International Conference on Acoustics Speech and Signal Proceedings of the Joint International Symposium and Exhibition on Geospatial Theory Processing and Applications pages Useful Transcriptions of Webcast Lectures BIBLIOGRAPHY 193 Life et al 1996 A Life I Salter J N Temem F Bernard S Rosset S Bennacef and L Lamel Data collection for the MASK kiosk Woz vs prototype system In Proceedings of the International Conference on Speech and Language Processing pages Useful Transcriptions of Webcast Lectures 194 BIBLIOGRAPHY Munteanu et al 2006c C Munteanu G Penn R Baecker and Y Zhang Automatic speech recognition for webcasts How good is good enough and what to do when it isn t In Proceedings of the Eight International Conference on Multimodal Interfaces ICMI pages for usefulness usability transcript enhanced webcasts In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems pages Proceedings of the ACM SIGCHI Conference 
on Human Factors in Computing Useful Transcriptions of Webcast Lectures BIBLIOGRAPHY Nanjo and Kawahara 2003 H Nanjo and T Kawahara 195 Unsupervised language model adaptation for lecture speech recognition In Proc ISCA IEEE Workshop on Spontaneous Speech Processing and learning in the fast lane In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational processing of audio lectures for information retrieval Vocabulary selection and language modeling In Proceedings of the IEEE Conference on Acoustics Speech and Signal evaluation baselines for speech summarization In Proceedings of ACL 08 HLT pages Useful Transcriptions of Webcast Lectures 196 BIBLIOGRAPHY Peters and Drexel 2004 J Peters and C Drexel Transformation based error correction for speech to text systems In Proc ISCA Conf on Spoken Language http www realnetworks com support education production html Riccardi and Hakkani Tur 2005 G Riccardi and D Hakkani Tur Active learning Theory and applications to 
automatic speech recognition IEEE Transactions on Speech and Audio Processing 13 4 Useful Transcriptions of Webcast Lectures BIBLIOGRAPHY 197 Ringger and Allen 1996 E K Ringger and J F Allen Error correction via a post processor for continuous speech recognition In Proc IEEE Conf on Acoustics Speech and Signal Technical report Wainhouse Research Whitepapers 2004 Roche and Schabes 1995 E Roche and Y Schabes part of speech tagging with finite state transducers Linguistics 21 2 International Conference on Multimodal where do we go from here 88 8 Human Computer Interaction Morgan Kaufmann 2002 Rowe et al 2001 L A Rowe D Harley P Pletcher and S Lawrence Bibs A lecture webcasting system Technical report Berkeley Multimedia Research Center University of California Berkeley USA June 2001 Useful Transcriptions of Webcast Lectures 198 BIBLIOGRAPHY Saraclar and Sproat 2004 M Saraclar and R Sproat Lattice based search for spoken utterance retrieval In Proceedings of the HLT NAACL Workshop on Interdisciplinary Approaches 
to Speech Indexing and Retrieval pages language model development using external resources for new spoken dialog domains In Proc IEEE Conf on Acoustics Speech and Signal 7 3 Useful Transcriptions of Webcast Lectures BIBLIOGRAPHY 199 Seymore and Rosenfeld 1997 K Seymore and R Rosenfeld Large scale topic detection and language model adaptation Technical Report CMU CS 97 152 School of Computer Science Carnegie Mellon University 1997 SPSS 2005 SPSS Spss 13 0 http www spss com 2005 Stark et al 2000 L Stark S Whittaker and J Hirschberg Asr satisficing The effects of asr accuracy on speech retrieval In Proceedings of International Conference on Spoken Language Processing 2000 Stern 1997 R Stern Specification of the 1996 hub 4 broadcast news evaluation In Proceedings of the DARPA Speech Recognition Workshop Chantilly Virginia USA February 1997 Toms et al 2005 E G Toms C Dufour J Lewis and R M Baecker Assessing tools for use with webcasts In Proceedings of the Joint Conference on Digital inter annotator agreement In 
Proceedings of the SENSEVAL Workshop Useful Transcriptions of Webcast Lectures 200 BIBLIOGRAPHY Evaluating Word Sense Disambiguation Programs Herstmonceux Castle Sussex England September 1998 Vertanen and Kristensson 2008 K Vertanen and P O Kristensson On the benefits of confidence visualization in speech recognition In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems pages recognition in university classrooms In Proc of the International ACM SIGCAPH Conference on Assistive Technologies pages Useful Transcriptions of Webcast Lectures BIBLIOGRAPHY 201 Weintraub et al 1996 M Weintraub K Taussig K Hunicke Smith and A Snodgrass Effect of speaking style on LVCSR performance In Proceedings of the International Conference on Spoken Language Processing pages or listen Discovering effective techniques for accessing speech data In Proceedings of the Human Computer Interaction Conference pages interfaces to support retrieval from speech archives In Proceedings of the Twenty Second ACM 
SIGIR Conference on Research and Development in Information Retrieval pages Useful Transcriptions of Webcast Lectures 202 recognition BIBLIOGRAPHY In Proc IEEE Conf on Acoustics Speech and Signal spoken document retrieval for the internet Lattice indexing for large scale web search architectures In Proceedings of the HLT NAACL Workshop on Interdisciplinary Approaches to Speech Indexing and Retrieval pages Useful Transcriptions of Webcast Lectures Appendix A Abbreviations AM Acoustic Model ANOVA ANalysis Of VAriance ASR Automatic Speech Recognition CSCW Computer Supported Cooperative Collaborative Work HCI Human Computer Interaction HMM Hidden Markov Model LM Language Model LVCS Large Vocabulary Continuous Speech MLLR Maximum Likelihood Linear Regression NLP Natural Language Processing OOV Out Of Vocabulary rate 203 204 APPENDIX A ABBREVIATIONS PMVDR Perceptual Minimum Variance Distortionless Response POS Part Of Speech SVD Singular Value Decomposition TOC Table Of Contents TBL Transformation Based Learning 
WER Word Error Rate Useful Transcriptions of Webcast Lectures Appendix B Glossary of Technical Terms Acoustic Model a statistical model of how specific words are 206 APPENDIX B GLOSSARY OF TECHNICAL TERMS Woodland 1995 N gram Model see Language Model Out Of Vocabulary the percent of vocabulary items in the actual utterance not appearing in the language model Perceptual Minimum Variance Distortionless Response an algorithm for extracting acoustic features from the speech signal For more details see Yapanel and Dharanipragada 2003 Precision Given a number of keywords S transcribed by an ASR system the precision with respect to these keywords is N S where N is the number of keywords correctly transcribed by the system Succintly precision true positives true positives f alse positives few false positives e g in the context of a keyword based search there will be less search results that actually do not contain the searched keyword Pronunciation dictionary sometimes referred to as lexicon contains all possible 
phonetic transcription of the words that an ASR system is expected to encounter Recall Given a total number of keywords T identified in the manual transcript the recall of the ASR system with respect to these keywords is N T where N is the number of keywords correctly transcribed by the system Succintly recall true positives true positives f alse negatives score means few false negative e g in the context of a keyword based Useful Transcriptions of Webcast Lectures 207 search more of the transcript lines that actually contain the search keyword will be included in the search results Singular Value Decomposition a method for factorization of rectangular matrices In NLP it is often used for dimensionality reduction tasks such as latent semantic indexing Hofmann 1999 Supervised training typically refers to ASR training for which manually annotated data is provided Often during supervised training other forms of manual intervention are also present such as making informed decisions about training data selection 
or fine tuning parameters information retrieval measure useful for determining the terms that are salient to a particular document in a collection e g terms that appear often in that document while being infrequent or not appearing at all in other documents It can also be used to score the relevance of documents with respect to a query term Tri gram Model an n gram model with n 3 see Language Model Unsupervised training the opposite of supervised S D I L where S D I Useful Transcriptions of Webcast Lectures 208 APPENDIX B GLOSSARY OF TECHNICAL TERMS are the number of substitutions deletions and insertions needed to transform the output sentence into the correct sentence and L is the number of words in the correct sentence Useful Transcriptions of Webcast Lectures Appendix C Instruments Used in Experiments 209 210 APPENDIX C INSTRUMENTS USED IN EXPERIMENTS Figure C 1 Consent Form page 1 for the experiment in Chapter 3 Useful Transcriptions of Webcast Lectures 211 3 5 6 0 1 1 2 44 05 7 8 1 
99999999999999999999999999999 99999999999999999999999999999 999999999999 8 9999999999999999999999999999 8 9999999999999999999999999 8 Figure C 2 Consent Form page 2 for the experiment in Chapter 3 Useful Transcriptions of Webcast Lectures 212 APPENDIX C INSTRUMENTS USED IN EXPERIMENTS http test kmdi utoronto ca cosmin Figure C 3 Consent Form page 1 for the main study in Chapter 5 Useful Transcriptions of Webcast Lectures 213 0 3 7 9 4 5 1 0 2 6 6 8 3 9 2 0 4 1 0 0 0 4 2 0 3 Figure C 4 Consent Form page 2 for the main study in Chapter 5 Useful Transcriptions of Webcast Lectures 214 APPENDIX C INSTRUMENTS USED IN EXPERIMENTS http test kmdi utoronto ca cosmin 1 1 0 Figure C 5 Consent Form page 1 for the study following the interface re design in Chapter 5 Useful Transcriptions of Webcast Lectures 215 0 3 7 9 4 5 1 0 2 6 6 8 3 9 2 0 4 1 0 0 0 4 2 0 3 Figure C 6 Consent Form page 2 for the study following the interface re design in Chapter 5 Useful Transcriptions of Webcast Lectures 216 APPENDIX C INSTRUMENTS 
USED IN EXPERIMENTS What is a metaphor What does a prototype give you a sense of Why does the summative evaluation has this name In what kind of respondend strategy are we asking people about themselves and their work What kind of design is grounded in cognition Where was the mobile device shown as an interface sketch example intended to be used How many hours are needed for a careful analysis of one hour of audio video recording Figure C 7 The introductory quiz page 1 administered before the start of the experiment in Chapter 3 Useful Transcriptions of Webcast Lectures 217 How many usability principles guidelines does Nielsen s heuristic have What other program beside KidPix is given as an example of mental models of painting Who developed Pictive Why is it difficult to get people to think aloud What is the name of the colleague from xerox park that did the airport video project What company has become the leading vendor of personal financial management software What drawing package was used to create some 
of the ePresence interface prototypes What user related concept is described as more in vogue now Figure C 8 The introductory quiz page 2 administered before the start of the experiment in Chapter 3 Useful Transcriptions of Webcast Lectures 218 APPENDIX C INSTRUMENTS USED IN EXPERIMENTS What lab had the slogan demo or die In participatory design the users participate on the design team as what What do you ask yourself in information design What do you actually do when doing user testing What does the HCI research suggests from an engineering point of view Figure C 9 The introductory quiz page 3 administered before the start of the experiment in Chapter 3 Useful Transcriptions of Webcast Lectures were used in a latin square setup as described in Section 3 3 Chapter 3 Four quizzes with questions selected from the introductory quiz Figure C 10 An example of the quiz administered during the experiment in What concept is given to us by the visual psychologists Useful Transcriptions of Webcast Lectures What are 
the most effective icons What is a relevant example of a good choice of imagery Where and when were the Gestalt principles first formulated What controls are a relevant example of the good use of both psychology principles and graphic design 219 220 APPENDIX C INSTRUMENTS USED IN EXPERIMENTS Figure C 11 The questionnaire administered after a session with a level of WER of either 0 25 or 45 from the experiment in Chapter 3 Useful Transcriptions of Webcast Lectures 221 Figure C 12 The questionnaire administered after a session with a level of WER of NT no transcripts from the experiment in Chapter 3 Useful Transcriptions of Webcast Lectures 222 APPENDIX C INSTRUMENTS USED IN EXPERIMENTS 1 0 121 3 4 2 5 5 5 1 0 2 2 4 Figure C 13 The questionnaire page 1 administered at the end of the experiment in Chapter 3 Useful Transcriptions of Webcast Lectures 223 Figure C 14 The questionnaire page 2 administered at the end of the experiment in Chapter 3 Useful Transcriptions of Webcast Lectures 224 APPENDIX C INSTRUMENTS 
USED IN EXPERIMENTS Figure C 15 The questionnaire page 3 administered at the end of the experiment in Chapter 3 Useful Transcriptions of Webcast Lectures 225 Figure C 16 The questionnaire page 4 administered at the end of the experiment in Chapter 3 Useful Transcriptions of Webcast Lectures 226 APPENDIX C INSTRUMENTS USED IN EXPERIMENTS 0 1 1 2 3 1 3 2 4 5 Figure C 17 The web based questionnaire page 1 administered at the end of the field study in Chapter 5 Useful Transcriptions of Webcast Lectures 227 Figure C 18 The web based questionnaire page 2 administered at the end of the field study in Chapter 5 Useful Transcriptions of Webcast Lectures 228 APPENDIX C INSTRUMENTS USED IN EXPERIMENTS 0 0 1 2 5 5 0 5 2 5 3 4 1 1 3 4 1 1 Figure C 19 The web based questionnaire page 3 administered at the end of the field study in Chapter 5 Useful Transcriptions of Webcast Lectures 229 Figure C 20 The web based questionnaire page 4 administered at the end of the field study in Chapter 5 Useful Transcriptions of Webcast 
Lectures 230 APPENDIX C INSTRUMENTS USED IN EXPERIMENTS Useful Transcriptions of Webcast Lectures 
24	a	A probabilistic framework for landmark detection based on phonetic features for automatic speech recognitiona Amit Junejab and Carol Espy Wilson Department of Electrical and Computer Engineering University of Maryland College Park Maryland 20742 Received 15 November 2007 accepted 20 November 2007 A probabilistic framework for a landmark based approach to speech recognition is presented for obtaining multiple landmark sequences in continuous speech The landmark detection module uses as input acoustic parameters APs that capture the acoustic correlates of some of the manner based phonetic features The landmarks include stop bursts vowel onsets syllabic peaks and dips fricative onsets and offsets and sonorant consonant onsets and offsets Binary classifiers of the manner phonetic features syllabic sonorant and continuant are used for probabilistic detection of these landmarks The probabilistic framework exploits two properties of the acoustic cues of phonetic features 1 sufficiency of acoustic cues of a phonetic 
feature for a probabilistic decision on that feature and 2 invariance of the acoustic cues of a phonetic feature with respect to other phonetic features Probabilistic landmark sequences are constrained using manner class pronunciation models for isolated word recognition with known vocabulary The performance of the system is compared with 1 the same probabilistic system but with mel frequency cepstral coefficients MFCCs 2 a hidden Markov model HMM based system using APs and 3 a HMM based system using MFCCs I INTRODUCTION In a landmark based automatic speech recognition ASR system the front end processing or low level signal analysis involves the explicit extraction of speech specific information This speech specific information consists of the acoustic correlates of the linguistic features Chomsky and Halle 1968 which comprise a phonological description of the speech sounds The processing occurs in two steps The first step consists of the automatic detection of acoustic events also called landmarks that 
signal significant articulatory changes such as the transition from a more open to a more closed vocal tract configuration and vice versa i e changes in the manner of articulation a sudden release of air pressure and changes in the state of the larynx There is evidence that the auditory system responds in a distinctive way to such acoustic events e g Delgutte and Kiang 1984 The second step involves the use of these landmarks to extract other relevant acoustic information regarding place of articulation that helps in the classification of the sounds spoken Given the extensive variability in the speech signal a complete ASR system would integrate this front end processa Portions of this work have appeared in Segmentation of Continuous Speech Using Acoustic Phonetic Parameters and Statistical Learning International Conference on Neural Information Processing Singapore 2002 and Speech segmentation using probabilistic phonetic feature hierarchy and support vector machines International Joint Conference on Neural 
Networks Portland Oregon 2003 and Significance of invariant acoustic cues in a probabilistic framework for landmark based speech recognition in the proceedings of From Sound to Sense 50 Years of Discoveries in Speech Communication June ing with a lexical access system that handles pronunciation variability and takes into account prosody grammar syntax and other higher level information State of the art ASR systems are based on hidden Markov modeling HMM and the standard parametrization of the speech signal consists of Mel frequency cepstral coefficients MFCCs and their first and second derivatives Rabiner and Juang 1993 Young et al 2006 The HMM framework assumes independence of the speech frames so that each one is analyzed and all of the MFCCs are looked at in every frame In contrast a landmark based approach to speech recognition can target level of effort where it is needed This efficiency can be seen in several ways First while each speech frame may be analyzed for manner ofarticulation cues resulting in 
the landmarks analysis thereafter is carried out only at significant locations designated by the landmarks This process in effect takes into account the strong correlation among the speech frames Second analysis at different landmarks can be done with different resolutions For example the transient burst of a stop consonant may be only 5 ms long Thus a short temporal window is needed for analysis On the other hand vowels which are considerably longer 50 ms for a schwa to 300 ms for an ae need a longer analysis window Third the acoustic parameters APs used to extract relevant information will depend upon the type of landmark For example at a burst landmark appropriate APs will be those that characterize the spectral shape of the burst maybe relative to the vowel to take into account contextual influences to distinguish between labial alveolar and velar stops However at a vowel landmark appropriate APs will be those that look at the relative spacing of the first three formants to determine where 0001 4966 2008 
1232 1154 15 23 00 the vowel fits in terms of the phonetic features front back high and low Another prominent feature of a landmark ASR system is that it is a tool for uncovering and subsequently understanding variability Given the physical significance of the APs and a recognition framework that uses only the relevant APs error analysis often points to variability that has not been accounted for For example an early implementation of the landmark based ASR system Bitar 1997 used zero crossing rate as an important measure to capture the turbulent noise of strident fricatives The zero crossing rate will not be large however during a voiced strident fricative z when it contains strong periodicity In this case the high frequency random fluctuations are modulated by the low frequency periodicity This situation occurs when the z is produced with a weakened constriction so that the glottal source is comparatively stronger and therefore the supraglottal source is weaker than it is during a more canonically produced 
z Spectrographically a weakened z shows periodic formant structure at low frequencies like a sonorant consonant and some degree of turbulence at high frequencies like an obstruent This understanding led to the development of an aperiodicity periodicity pitch APP detector which along with fundamental frequency information provides a spectrotemporal profile of aperiodicity and periodicity Deshmukh et al 2005 The APP detector however was not used in this work due its computational requirements A significant amount of work has gone into understanding the acoustic correlates of the linguistic features Stevens 2002 Studies have shown that the acoustic correlates of the phonetic features can be reliably and automatically extracted from the speech signal Espy Wilson 1987 Bitar 1997 Ali 1999 Carbonell et al 1987 Glass 1984 HasegawaJohnson 1996 and that landmarks can be automatically detected Bitar 1997 Salomon 2000 Ali 1999 Liu 1996 Stevens 2002 has laid out a model for lexical access based on acoustic landmarks and 
phonetic features However to date no one has implemented a complete ASR system based on a landmark approach The previous landmark detection systems Bitar 1997 Salomon 2000 Ali 1999 Liu 1996 performed well but they lacked a probabilistic framework for handling pronunciation variability that would make the systems scalable to large vocabulary recognition tasks where higher level information has to be integrated For example it could not be demonstrated in these systems that a voiced obstruent realized as a sonorant consonant will ultimately be recognized correctly due to higher level constraints These systems were primarily rule based and it has been pointed out Rabiner and Juang 1993 that in rule based systems the difficulty in the proper decoding of phonetic units into words and sentences increases sharply with an increase in the rate of phoneme insertion deletion and substitution In this work a probabilistic framework is developed that selectively uses knowledge based APs for each decision and it can be 
constrained by a high level pronunciation model of words and probability densities of durations of phonetic units Since recognition can be constrained by higher level knowledge the system does not have to decode phonetic units into words in a separate step J Acoust Soc Am Vol 123 No 2 February 2008 Probabilistic frameworks exist for segment based Glass et al 1996 Zue et al 1989 Halberstadt 1998 and syllable based Chang 2002 ASR But these systems are not targeted at selectively using knowledge based acoustic correlates of phonetic features for detection of landmarks or for place of articulation detection Many HMM based approaches to speech recognition have used knowledge based APs Bitar and Espy Wilson 1996 Deshmukh et al 2002 Hosom 2000 or the concept of phonetic features Deng and Sun 1994 Kirchhoff 1999 Eide et al 1993 However these were not landmark based methods in that they did not involve an initial step of segmenting or detecting events in speech In this paper a probabilistic framework for a 
landmarkbased ASR system called event based system EBS Bitar 1997 is presented The focus of this paper is on the implementation and performance of the probabilistic landmark detection module of the framework The framework was discussed in brief earlier Juneja and Espy Wilson 2004 but it was not sufficiently detailed because of the limited space available Initial results during the development of the probabilistic landmark detection system were reported earlier Juneja and Espy Wilson 2002 2003 but these only involved statistical classifiers for phonetic features and did not involve the probabilistic scoring with duration constraints Because of the lack of probabilistic duration constraints these initial systems resulted in numerous insertions of segments of very small durations II METHOD A Overview Figure 1 shows a block diagram of EBS and it highlights the part of EBS that is the focus of this paper To start the landmark detection process the knowledge based APs Bitar 1997 shown in Table I for each of the 
phonetic features 1155 A Juneja and C Y Espy Wilson Probabilistic detection of speech landmarks FIG 2 Probabilistic Phonetic Feature Hierarchy FIG 1 Color online Overview of the landmark based speech recognition system sonorant syllabic continuant and silence are automatically extracted from each frame of the speech signal Then a support vector machine SVM Vapnik 1995 based binary classifier is applied at each node of the hierarchy shown in Fig 2 such that only the relevant APs for the feature at that node serve as input to the classifier Probabilistic decisions obtained from the outputs of SVMs are combined with class dependent duration probability densities to obtain one or more segmentations of the speech signal into the broad classes vowel V fricative Fr sonorant consonant SC including nasals semivowels stop burst ST and silence SILEN including stop closures A segmentation is then used along with the knowledge based measurements to deterministically find landmarks related to each of the broad class 
segments For a fixed vocabulary segmentation paths can be constrained using broad class pronunciation models The phonetic feature hierarchy shown in Fig 2 is the upper part of a complete hierarchy that has manner features at the top place features at the lower nodes and phonemes at the lowest level Several studies have provided evidence for a hierarchical organization of the phonetic features e g Clements 1985 Probabilistic hierarchies with phonemes at the terminal nodes have been used before in speech recognition Halberstadt 1998 Chun 1996 where the use of such hierarchies occurs after an initial segmentation step EBS uses the hierarchy as a uniform framework for obtaining manner based landmarks and place and voicing feature detection The complete hierarchy used by EBS is shown in Juneja 2004 Figure 3 shows the two kinds of landmarks that EBS is expected to extract the landmark locations in this figure are hand marked abrupt landmarks and nonabrupt landmarks In the previous implementation of EBS Bitar 1997 
the maxima and minima of the APs E640 Hz 2800 Hz and E2000 Hz 3000 Hz were used in a rule based system to obtain the nonabrupt landmarks that occur at syllabic peaks and syllabic dips Thresholds on energy onsets and energy offsets were used to obtain the abrupt stop burst landmarks Figure 3 shows that E640 Hz 2800 Hz has maxima in the vowel nuclei at the syllabic peak landmarks and minima in the sonorant consonant regions at the syllabic dip landmarks Spurious peaks or dips caused insertions and peaks or dips that are not fully realized caused deletions in this system There was no way to recover from such errors The presented framework can be viewed as a probabilistic version of the system in Bitar 1997 as it finds the landmarks using the following two steps 1 The system derives multiple probabilistic segmentations from statistical classifiers that use relevant APs as input taking into account the probability distributions of the durations of the broad class segments The probabilistic duration models 
penalize the insertions of very small broad class segments for example TABLE I APs used in broad class segmentation f s sampling rate F3 third formant average a b frequency band aHz bHz Ea b energy in the frequency band aHz bHz Phonetic Feature Silence APs 1 E0 F3 1000 2 EF3 f s 2 3 ratio of spectral peak in 0 400 Hz to the spectral peak in 400 f s 2 4 Energy onset Bitar 1997 5 Energy offset Bitar 1997 1 E0 F3 1000 2 EF3 f s 2 3 Ratio of E0 F3 1000 to EF3 1000 f s 2 4 E100 400 1 E640 2800 2 E2000 3000 3 Energy peak in 0 900 Hz 4 Location in Hz of peak in 0 900 Hz 1 Energy onset Bitar 1997 2 Energy offset Bitar 1997 3 E0 F3 1000 4 EF3 1000 f s 2 sonorant syllabic continuant 1156 J Acoust Soc Am Vol 123 No 2 February 2008 A Juneja and C Y Espy Wilson Probabilistic detection of speech landmarks FIG 3 Illustration of manner landmarks for the utterance diminish from the TIMIT database a Phoneme Labels b Spectrogram c Landmarks characterized by sudden change d Landmarks in stable regions of speech e Onset waveform 
an acoustic correlate of phonetic feature continuant f E640 2800 an acoustic correlate of syllabic feature The ellipses show how landmarks were obtained in Bitar 1997 using certain APs Ellipse 1 shows the location of stop burst landmark for the consonant d using the onset Ellipse 2 shows the location of syllabic dip for the nasal m using the minimum of E640 2800 Ellipse 3 shows that the maximum of the E640 2800 can be used to locate a syllabic peak landmark of the vowel ix 2 The landmarks are then derived using the boundaries of the broad classes as abrupt landmarks and the maxima and minima of the AP E640 Hz 2800 Hz inside individual sonorant segments to get the nonabrupt landmarks The global maximum inside the vowel segment is used to get the syllabic peak landmark and the global minima inside the sonorant consonant is used to get the syllabic dip landmark Therefore presence of multiple peaks or dips does not cause insertions at this step SVMs are used for the purpose of binary classification although 
other classifiers for example neural networks or Gaussian mixture models could be used as well of phonetic features because of their ability to generalize well to new test data after learning from a relatively small amount of training data Additionally SVMs have been shown to perform better than HMMs for phonetic feature detection in speech Niyogi 1998 Keshet et al 2001 and for phonetic classification from hand transcribed segments Clarkson and Moreno 1999 Shimodaira et al 2001 The success of SVMs can be attributed to their property of large margin classification Fig ure 4 shows two types of classifiers for linearly separable data 1 a linear classifier without maximum margin and 2 a linear classifier with maximum margin It can be seen from Fig 4 that the maximum margin classifier is more robust to noise because a larger amount of noise at least half of the margin for the samples shown is required to let a sample point cross the decision boundary It has been argued Vapnik 1995 that a maximization of the 
margin leads to the minimization of a bound on the test error Mathematically NSV SVMs select a set of NSV support vectors xSV i i 1 that is a l subset of l vectors in the training set xii 1 with class labels y iil 1 and find an optimal separating hyperplane f x in the sense of maximization of margin in a high dimensional space H NSV f x y iiKxSV i x b i 1 1 The space H is defined by a linear or nonlinear kernel function Kxi x j that satisfies the Mercer conditions Burges NSV 1998 The weights i the set of support vectors xSV i i 1 the FIG 4 a small margin classifiers b maximum margin classifiers J Acoust Soc Am Vol 123 No 2 February 2008 A Juneja and C Y Espy Wilson Probabilistic detection of speech landmarks 1157 TABLE II An illustrative example of the symbols B L and U z U u1 sonorant continuant strident voiced anterior Fr l1 Fon Foff I u2 sonorant syllabic back high lax V l2 VOP P r u3 sonorant syllabic nasal rhotic SC l3 Son D Soff o u4 sonorant syllabic back high low V l4 VOP P w u5 sonorant syllabic 
nasal labial SC l5 Son D Soff B L bias term b are found from the training data using quadratic optimization methods Two commonly used kernels are the radial basis function RBF kernel and the linear kernel For the RBF kernel Kxi x exp xi x2 where the parameter is usually chosen empirically by cross validation from the training data For the linear kernel Kxi x xi x 1 B Probabilistic framework The problem of speech recognition can be expressed as the maximization of the posterior probability of sets of phonetic features where each set represents a sound or a phoneme A set of phonetic features include 1 manner phonetic features represented by landmarks and 2 place or voicing phonetic features found using the landmarks Mathematically given an acoustic observation sequence O the problem can be expressed as L arg max PU LO arg max PLO PUO L U U L U L 2 N where L liiM 1 is a sequence of landmarks and U uii 1 is the sequence bundles of features corresponding to a phoneme sequence The meaning of these symbols is 
illustrated in Table II for the digit zero Computation of PL O is the process of probabilistic detection of acoustic landmarks given the acoustic observations and the computation of PU L O is the process of using the landmarks and acoustic observations to make probabilistic decisions on place and voicing phonetic features The goal of the rest of the paper is to show how PL O is computed for a given landmark sequence and how different landmark sequences and their probabilities can be found given an observation sequence There are several points to note with regards to the notation in Table II 2 Each set of landmarks li as shown in Table III is related to a broad class Bi of speech selected from the set vowel V fricative Fr sonorant consonant SC stop burst ST silence SILEN For example P and VOP are related to the broad class V Let B BiiM 1 denote the sequence of broad classes corresponding to the sequence of sets of landmarks L Note that in this paper that ST denotes the burst region of a stop consonant and the 
closure region is assigned the broad class SILEN 3 The number of broad classes M and the number of bundles of phonetic features N may not be the same in general This difference may occur because a sequence of sets of landmarks and the corresponding broad class sequence may correspond to one set of phonetic features or two sets of phonetic features For example SILEN ST could be the closure and release of one stop consonant or it could be that the closure corresponds to one stop consonant and the release corresponds to another stop consonant e g the cluster kt in the word vector Likewise one set of landmarks or the corresponding broad class may correspond to two sets of place features For example in the word omni with the broad class sequence V SC V the SC will have the features of the sound m calculated using the SC onset as well as the sound n calculated using SC offset The landmarks and the sequence of broad classes can be obtained deterministically from each other For example the sequence B SILEN Fr V SC V 
SC SILEN for zero in Table II will correspond to the sequence of sets of landmarks L shown Therefore TABLE III Landmarks and corresponding broad classes Broad class segment Vowel V Stop ST Sonorant consonant SC Landmark type Syllabic peak P Vowel onset point P Burst Syllabic dip D SC onset Son SC offset Soff Fricative onset Fon Fricative offset Foff 1 li denotes a set of related landmarks that occur during the same broad class For example the syllabic peak P and the vowel onset point VOP occur during a vowel The VOP should occur at the start of the vowel and P should occur during the vowel when the vocal tract is most open Also certain landmarks may be redundant in the sequence For example when a vowel follows a sonorant consonant the sonorant consonant offset and the vowel onset are identical 1158 J Acoust Soc Am Vol 123 No 2 February 2008 Fricative Fr A Juneja and C Y Espy Wilson Probabilistic detection of speech landmarks PLO PBLO 3 where BL is a sequence of broad classes for which the landmark sequence L 
is obtained Note that the symbols B U and L contain information about the order in which the broad classes or landmarks occur but they do not contain information about the exact start and end times of each of those units The equivalence of broad classes and landmarks is not intended as a general statement and it is assumed to hold only for the landmarks and broad classes shown in Table III correlates extracted from the analysis frame and the adjoining frames for a feature f be denoted by xtf Then Eq 8 can be rewritten as Pt sonorant PtVO Pt speechxspeech t speech xsonorant t Pt syllabic sonorant speech xsyllabic t 9 C Segmentation using manner phonetic features Given a sequence of T frames O o1 o2 oT where ot is the vector of APs at time t t is in the units of frame numbers the most probable sequence of broad classes B M BiiM 1 and their durations D Dii 1 have to be found The frame ot is considered as the set of all APs computed at frame t to develop the probabilistic framework although EBS does not use all 
of the APs in each frame The probability PB O can be expressed as PBO PB DO D The probability PB D O can now be expanded in terms of the underlying manner phonetic features of each broad class i the Denote the features for class Bi as the set f i1 f i2 f N broad class at time t as bt and the sequence b1 b2 bt as bt Note that B is the broad class sequence with no information about the duration of each broad class in the sequence On the other hand bt denotes a broad class at frame t Therefore the sequence bt includes the information of durations of each of the broad classes until time t Using this notation the posterior probability of a broad class sequence B and durations D can be expanded as i 1 M Di j 1D j Bi 4 The computation of PB D O for a particular B and all D is a very computationally intensive task in terms of storage and computation time Therefore an approximation is made that is similar to the approximation made by Viterbi decoding in HMM based recognition systems and the SUMMIT system Glass et al 
1996 PBO max PB DO D PB DO i 1 t 1 i 1 D j 1 j PtBiO bt 1 10 5 Because the probabilities PB O calculated this way for different B will not add up to one the more correct approximation is PBO maxD PB DO B maxDPB DO 6 Provided that a frame at time t lies in the region of one of the manner classes the posterior probability of the frame being part of a vowel at time t can be written as see Fig 2 PtVO Pt speech sonorant syllabicO Pt speechO Pt sonorant speech O 7 Pt syllabic sonorant speech O 8 The variable t in the above equation is the frame number the limits of which can be explained as follows ij 1 1D j is the sum of the durations of the i 1 broad classes before the broad class i and ij 1D j is the sum of durations of the first i broad classes Then ij 1D j ij 1 1D j is the duration of the ith broad class Therefore numbers 1 ij 1 1D j 2 i 1 ij 1 1D j Di j 1D j are the frame numbers of the frames that occupy the ith broad class When the lower and i 1 upper limits of t are specified as 1 ij 1 1D j and Di j 1D j 
respectively it means that the product is taken over all the frames of the ith broad class Making a stronger use of the definition of acoustic correlates by assuming that the acoustic correlates of a manner feature at time t are sufficient even if bt 1 is given i 1 M Di j 1D j NBi PB DO i 1 t 1 i 1 D j 1 j f Pt f ikxt k 1 ki i t 1 f i1 f k 1 b 11 Now expanding the conditional probability i 1 M Di j 1D j NBi where Pt is used to denote the posterior probability of a feature or a set of features at time t A similar expression can be written for each of the other manner classes Calculation of the posterior probability for each feature requires only the acoustic correlates of that feature Furthermore to calculate the posterior probability of a manner phonetic feature at time t only the acoustic correlates of the feature in a set of frames t s t s 1 t e using s previous frames and e following frames along with the current frame t are required to be used Let this set of acoustic J Acoust Soc Am Vol 123 No 2 
February 2008 PB DO i 1 t 1 i 1 D k 1 j 1 j i t 1 Pt f ik xtf k f i1 f k 1 b i t 1 Ptxtf k f i1 f k 1 b i i 12 Splitting the priors gives 1159 A Juneja and C Y Espy Wilson Probabilistic detection of speech landmarks FIG 5 Color online a Projection of 39 MFCCs into a one dimensional space with vowels and nasals as discriminating classes b similar projection for four APs used to distinguish sonorant sounds from sonorant sounds Because APs for the sonorant feature discriminate vowels and nasals worse than MFCCs they are more invariant PB DO i 1 M Di j 1D j NBi i 1 t 1 i 1 D k 1 j 1 j i Pt f ik f i1 f ki 1 bt 1 i PB DO PB PDB NB i i 1 M Di j 1D j i 1 t 1 i 1 D j 1 j Ptxtf k f i1 f ik bt 1 i t 1 Ptxtf k f i1 f k 1 b 13 Ptxtf k f i1 f ik i Ptxtf k f i1 f k 1 i i 15 k 1 The probability terms not involving the feature vector xtf k can now be combined to get the prior probabilities of the broad class sequence and the sequence dependent durations that is i 1 M Di j 1D j NBi i which can be rewritten as PB DO PB PDB 14 
i i 1 M Di j 1D j i 1 t 1 i 1 D j 1 j i NB i i t 1 Pt f ik f i1 f k 1 b PB D k 1 i 1 t 1 ij 1 1D j i Pt f ikxtf k f i1 f k 1 i Pt f ik f i1 f k 1 i 16 k 1 PB PDB i The posterior Pt f ik xtf k f i1 f k 1 is the probability of the fk i i i Now given the set f i1 f k 1 or the set f 1 f k xt is assumed to be independent of bt 1 This independence of the APs from the previous broad class frames is hard to establish but it can be shown to hold better for the knowledgebased APs than for the mel frequency cepstral coefficients MFCCs see Fig 5 under certain conditions as discussed in Sec III B In words this independence means that given a phonetic feature or the phonetic features above that feature in the hierarchy the APs for that phonetic feature are assumed to be invariant with the variation of the broad class labels of the preceding frames For example the APs for the feature sonorant in a sonorant frame are assumed to be invariant of whether the frame lies after vowel nasal or fricative frames This is further 
discussed in Sec III B Making this independence or invariance assumption and applying Eq 14 in Eq 13 binary feature f ik obtained using the APs xtf k and it is obtained in this work from an SVM based binary classifiers as dei scribed in Sec I below The term Pt f ik xi1 f k 1 normalizes the imbalance of the number of positive and negative samples in the training data The division on the right side of Eq 16 can be considered as the conversion of a posterior probability to a likelihood by division by a prior The prior is computed as the division of the number of training samples for the positive value of the feature to the number of training samples for the negative value of the feature 1 Training and application of binary classifiers i One SVM classifier was trained for each of the phonetic features shown in Fig 2 The input to the classifier is the set of APs shown in Table I for that feature The sounds used to get the training samples of class 1 and class 1 for each SVM are shown in Table IV For the feature 
continuant the TABLE IV Sounds used in training of each classifier Phonetic feature speech sonorant syllabic continuant Sounds with 1 label All speech sounds excluding stop closures Vowels nasals and semivowels Vowels Fricatives Sounds with 1 label Silence pauses and stop closures Fricatives affricates and stop bursts Nasals and semivowels Stop bursts 1160 J Acoust Soc Am Vol 123 No 2 February 2008 A Juneja and C Y Espy Wilson Probabilistic detection of speech landmarks FIG 6 A phonetic feature based pronunciation model for the word zero stop burst frame identified as the first frame of a stop consonant using TIMIT labeling was used to extract APs representative of the value 1 of that feature For the 1 class of the feature continuant the APs were extracted from all of the fricative frames For the other features all frames for each of the classes were extracted as training samples Flap dx syllabic sonorant consonants em el en er and eng and diphthongs i e ow a aw and uw were not used in the training of the 
feature syllabic and affricates jh and ch and glottal stops were not used in training of the feature continuant but these sounds were used for framebased testing The reason for not using these sounds for training is that they have different manifestations For example the affricates ch and jh may appear with or without a clear stop burst However such information is not marked in the TIMIT hand transcribed labels SVM Light Joachims 1998 an open source toolkit for SVM training and testing was used for building and testing the classifiers Two types of SVM kernels were used linear and radial basis function RBF to the build corresponding two types of classifiers The optimal number of adjoining frames s and e used in each classifier as well as the optimal SVM related parameters e g the bound on slack variables i and for RBF kernels were found using cross validation over a separate randomly chosen data set from the TIMIT training set A SVM outputs a real number for a teat sample To convert this real number into a 
probability the real space of the SVM outputs is divided into 30 bins of equal sizes between 3 and 3 This range was chosen empirically from observations of many SVM outputs After the SVMs are trained the proportion of samples of class 1 to the total numbers of training samples in each of the bins is noted and the proportions for all of the bins are stored in a table While testing the bin corresponding to the real number obtained for a particular test sample is noted and its probability is looked up from the stored table 2 Probabilistic segmentation Eq 16 unlike the algorithm developed by Lee 1998 where segment scores of observations in speech segments are used Another difference is that the transition points in the segmentation algorithm in the current work are obtained as those frames at which the ranking of the posterior probabilities of the broad classes changes In the work by Lee 1998 the transitions points were calculated from the points of significant change in the acoustic representation D 
Deterministic location of landmarks Once a broad class sequence with the start and end times of each of the broad classes is found the landmarks are located deterministically Fon and Foff are allotted the start and end times of the broad class Fr Son and Soff are assigned the start and end times of the broad class SC The stop burst B is found as the location of the maximum value of the temporal onset measure within a 60 ms window centered at the first frame of the segment ST VOP is assigned the first frame of the segment V and P is assigned the location of highest value of E640 2800 in the segment V The syllabic dip D for an intervocalic SC is assigned the location of the minimum in E640 2800 in the segment SC For prevocalic and postvocalic SC D is assigned the middle frame of the SC segment E Constrained landmark detection for word recognition A Viterbi like probabilistic segmentation algorithm Juneja 2004 takes as input the probabilities of the manner phonetic features sonorant syllabic continuant and 
silence from the SVM classifiers and outputs the probabilities PB O under the assumption of Eq 5 The algorithm is similar to the one used by Lee 1998 The algorithm operates on the ratio of posterior probabilities on the right side of J Acoust Soc Am Vol 123 No 2 February 2008 For isolated word or connected word recognition manner class segmentation paths are constrained by a pronunciation model in the form of a finite state automata FSA Jurafsky and Martin 2000 Figure 6 shows an FSA based pronunciation model for the digit zero and the canonical pronunciation z I r ow The broad manner class representation corresponding to the canonical representation is Fr VSC V SC the last SC is for the off glide of ow The possibility that the off glide of the final vowel ow may or may not be recognized as a sonorant consonant is represented by a possible direct transition from the V state to the SILEN state Starting with the start state S0 the posterior probability of a particular path through the FSA for zero can be 
calculated using the likelihood of a transition along a particular broad class Bi as k 1 NB i i Pt f ikxtf k f i1 f k 1 i Pt f ik f i1 f k 1 i A Juneja and C Y Espy Wilson Probabilistic detection of speech landmarks 1161 The likelihoods of all of the state transitions along a path are multiplied with the prior PB and the duration densities PD B using the durations along that path The probabilistic segmentation algorithm gives for an FSA and an observation sequence the best path and the posterior probability computed for that path Note that word posterior probabilities can be found by multiplying the posterior probability PL O of the landmark sequence with the probability PU OL of the place and voicing features computed at the landmarks Juneja 2004 about computing complete word probabilities is out of the scope of this paper III EXPERIMENTS AND RESULTS A Database The si and sx sentences from the training section of the TIMIT database were used for training and development For training the SVM classifiers 
randomly selected speech frames were used because SVM training with all of the available frames was impractical For training the HMMs all of the si and sx sentences from the training set were used All of the si and sx sentences from the testing section of the TIMIT database were used for testing how well the systems perform broad class recognition The 2240 isolated digit utterances from the TIDIGITS training corpus were used to obtain word level recognition results If spoken canonically the digits are uniquely specified by their broad class sequence Thus word level results are possible for this constrained database Note that the TIMIT database is still used for training since the TIDIGITS database is not transcribed Thus this experiment not only shows how well the systems perform word level recognition but it also allows for cross database testing B Sufficiency and invariance In this section an illustration of how the APs satisfy the assumptions of the probabilistic framework better than the MFCCs is 
presented Although it is not clear how sufficiency and invariance can be rigorously established for certain parameters some idea can be obtained from classification and scatter plot experiments For example sufficiency of the four APs used for the sonorant feature detection E0 F3 E100 Hz 400 Hz EF3 f s 2 ratio of the E0 F3 to the energy in F3 half of sampling rate1 can be viewed in relation to the 39 mel frequency cepstral coefficients MFCCs in terms of classification accuracy of the sonorant feature Two SVMs with linear kernels were trained one for the APs and one for the MFCCs using a set of 20 000 randomly selected sample frames of each of the sonorant and sonorant frames from dialect region 1 of the TIMIT training set The same number of samples were extracted from dialect region 8 of the TIMIT training set for testing A frame classification accuracy of 93 0 was obtained on data using the APs and SVMs which compares well to 94 2 accuracy obtained using the MFCCs and SVMs Note that for the two SVMs the same 
speech frames were used for training as well as testing and only the types of acoustic features were different 1162 J Acoust Soc Am Vol 123 No 2 February 2008 In Eq 16 the APs xtf k for a manner feature were assumed to be independent of the manner class labels of the i preceding frames bt 1 when either f i1 f ik or f i1 f k 1 i was given For example for the feature f k sonorant the i set f i1 f ik is speech sonorant and f i1 f k 1 is i i speech Consider the case where f 1 f k is given that is the value of the feature whose APs are being investigated is known A typical case where the assumption may be hard to satisfy is when the APs for the sonorant feature are assumed to be invariant of whether the analysis frame lies in the middle of a vowel region or the middle of a nasal region both vowels and nasals are sonorant That is bt 1 will be composed of nasal frames in one case and vowel frames in the other case Such independence can roughly be measured by the similarity in the distribution of the vowels and 
nasals based on the APs for the feature sonorant To test this independence sonorant APs were extracted from dialect region 8 of the TIMIT training set from each of the nasal and vowel segments Each set of APs was extracted from a single frame located at the center of the vowel or the nasal The APs were then used to discriminate vowels and nasals using Fischer linear discriminant analysis LDA Figure 5a shows the distribution of the projection of the 39 MFCCs extracted from the same 200 frames into a one dimensional space using LDA A similar projection is shown for the four sonorant APs in Fig 5b It can be seen from these figures that there is considerably more overlap in the distribution of the vowels and the nasals for the APs of the sonorant feature than for the MFCCs Thus the APs for the sonorant feature are more independent of the manner context than are the MFCCs The overlap does not show worse performance of the APs compared to MFCCs because the sonorant APs are not meant to separate vowels and nasals 
They separate vowels nasals and semivowels i e sonorants from fricatives stop consonants and affricates i e obstruents Thus the APs for the feature sonorant are invariant across different sonorant sounds but successfully discriminate sonorant sounds from sonorant sounds Further discussion of the sufficiency and the invariance properties of the APs can be found in Juneja 2004 C Frame based results i The SVMs for each feature utilized APs extracted from the analysis frame as well as s starting frames and e ending frames The values of the two variables e and s were obtained for each classifier by performing validation over a subset of the TIMIT training data Juneja 2004 Training was performed on randomly picked samples 20 000 samples for each class from the si and sx sentences of the TIMIT training set The binary classification results on the whole of the TIMIT test set at the optimal values of s and e are shown in Table V in two cases 1 when all the frames were used for testing and 2 when only the middle one 
third portion of each broad class was used for testing The difference in the results indicates the percentage of errors that are made due to boundary or coarticulation effects Note that in the presented landmark based system it is not important to classify each frame correctly The results on the middle one third segment A Juneja and C Y Espy Wilson Probabilistic detection of speech landmarks TABLE V Binary classification results for manner features in Accuracy on middle frames is not shown for the feature continuant because the feature distinguishes the stop releases from the beginning of fricatives Feature sonorant syllabic Speech silence continuant s 4 16 3 4 e 1 24 2 4 Accuracy on middle frames 96 59 85 00 94 60 Accuracy on all frames 94 39 80 06 93 50 95 58 are more representative of the performance of the system because if the frames in a stable region are correctly recognized for a particular manner feature this would mean that the corresponding landmarks may still be correctly obtained For example if 
the middle frames of an intervocalic sonorant consonant are correctly recognized as syllabic then the correct recognition of frames near the boundary is not significant because landmarks for the sonorant consonant will be obtained accurately For the feature continuant the classification error on middle frames is not relevant because the SVM is trained to extract the stop burst as opposed to a certain stable region of speech Also the transient effects at broad class boundaries are minimized by low probability density values of very small broad class durations Figures sounds About 30 of the frames of reduced vowels are also misrecognized as sonorant consonants This typically happened when a sonorant consonant followed a stressed vowel and preceded a reduced vowel such that the reduced vowel is confused as a continuation of the sonorant consonant A similar result was shown by Howitt 2000 where the vowel landmarks were missed for reduced vowels more than other vowels The performance of the feature continuant is 
95 6 which indicates the accuracy on classification of onset frames of all nonsonorant sounds That is an error was counted if a stop burst was wrongly classified as continuant or a fricative onset was wrongly classified as a stop burst The major source of error is the misclassification of 13 7 of fricative onsets as stop bursts D Sequence based results The SVM models obtained in the frame based analysis procedure were used to obtain broad class segmentation as well as the corresponding landmark sequences for all of the si and sx sentences of the TIMIT test set using the probabilistic segmentation algorithm Not all broad class sequences were allowed as the segmentation paths were constrained using a pronunciation graph such that 1 SCs only occur adjacent to vowels 2 ST is always preceded by SILEN for stop closure and 3 each segmentation path starts and ends with silence The same pronunciation graph was used for both the EBS system and the HMM system The duration probability for each broad class was modeled by 
a mixture of Rayleighs using a single Rayleigh density for the classes SC FIG 7 Color online Sounds with high error percentages for the feature sonorant voic stop represents voiced stop consonants J Acoust Soc Am Vol 123 No 2 February 2008 FIG 8 Color online Sounds with high error percentages for the feature continuant Fon represents fricative onsets 1163 A Juneja and C Y Espy Wilson Probabilistic detection of speech landmarks FIG 9 Color online Sounds with high error percentages for the feature syllabic Red vow represents reduced vowels FIG 10 Color online Sounds with high error percentages in speech silence distinction Weak fric represents weak fricatives and strong fric represents strong fricatives Voiced clos represents closures of voiced stop consonants V Fr and ST and a mixture of two Rayleigh densities for SILEN one density targets short silence regions like pauses and closures and the other density targets beginning and ending silence The parameter for each Rayleigh density was found using the 
empirical means of the durations of each of the classes from the TIMIT training data For the purpose of scoring the reference phoneme labels from the TIMIT database were mapped to manner class labels Some substitutions splits and merges as shown in Table VI were allowed in the scoring process Specifically note that two identical consecutive broad classes were allowed to be merged into one since the distinction between such sounds is left to the place classifiers Also note that affricates were allowed to be recognized as ST Fr as well as Fr and similarly diphthongs i e ow a aw and uw were allowed to be recognized as V SC as well as V because the off glides may or may not be present Scoring was done on the sequences of hypothesized symbols without using information of the start and end of the broad class segments which is similar to word level scoring in continuous speech recognition The same knowledge based APs were used to construct an 11 parameter front end for a HMM based broad class segmentation system 
The comparison with the HMM based system does not show that the presented system performs TABLE VI Allowed splits merges and substitutions Allowed hypothesis V Fr V ST Fr V SC SC Fr SILEN ST superior or inferior to the HMM based systems but it shows an acceptable level of performance The HMM based systems have been developed and refined over decades and the work presented in this paper is only the beginning of the development of a full speech recognition system based on phonetic features and acoustic landmarks All the HMMs were context independent three state excluding entry and exit states left to right HMMs with diagonal covariance matrices and eight component mixture observation densities for each state All the si and sx utterances from the TIMIT training set were used for training the HMM broad classifier A HMM was built for each of the five broad classes and a separate HMM was built for each of the special sounds affricate diphthong glottal stop syllabic sonorant consonant flap dx and voiced aspiration 
hv making a total of 11 HMMs Only the five broad class models were used in testing and the HMMs for the special sounds were ignored so that the training and testing sounds of HMM and EBS were identical The HMM models were first initialized and trained using all of the training segments for each model separately for example using semivowel and nasal segments for the sonorant consonant model and then improved using embedded training on the concatenated HMMs for Reference V V Fr Fr q V V q t p k g d em en er el hv dx Reference SC SC SILEN SILEN q v ch jh dx i ow e o aw uw ow Allowed hypothesis SC SILEN ST SC SC Fr ST Fr SC V SC 1164 J Acoust Soc Am Vol 123 No 2 February 2008 A Juneja and C Y Espy Wilson Probabilistic detection of speech landmarks TABLE VII Broad class segmentation results in percent Correctness Corr Accuracy Ace are shown when the system is scored on the basis of numbers of deletions insertions and substitutions of broad classes A in a cell means that the particular system was computationally 
too intensive to get a result from EBS RBF Corr Acc 86 2 79 5 EBS linear Corr Acc 84 0 77 1 86 1 78 2 HMM Corr Acc 80 9 73 7 86 8 80 0 11 APs 39 MFCCs each sentence Triphone models and other similarly improved HMM models may give better results than the ones presented in this paper but the focus here is to build a base line HMM system to which EBS s performance can be compared The results are shown in Table VII The results are also shown for EBS for two different front ends AP and MFCC including MFCCs their delta and acceleration coefficients which gives a 39 parameter front end The performance of all of the systems except when EBS is used with MFCCs is comparable although the HMM MFCC system gives the maximum accuracy The inferior performance of the MFCCs with EBS is perhaps because of the better agreement of APs with the invariance assumptions of the probabilistic framework Similarly better performance of MFCCs in the HMM framework may be because of better agreement with the diagonal covariance assumption 
of the HMM system applied here That is APs are not processed by a diagonalization step prior to application to the HMM systems while MFCCs go through such a process These are possible explanations of these results and they are open to further investigation An example of landmarks generated by EBS on a test sentence of TIMIT is shown in Fig 11 which also shows how errors in the system can be analyzed The pattern recognizer calls the dh in the as marked by the ellipse a sonorant constant SC instead of the correct broad class Fr The reason is that the parameter E0 F3 EF3 f s 2 does not dip adequately as it usually does in most sonorant sounds This indicates that improved APs for example from the APP detector Deshmukh et al 2005 that directly captures the aperiodicity are needed to correct errors like this one The confusion matrix of various landmarks for EBS using the AP front end is shown in Table VIII without including the sounds diphthongs syllabic sonorant consonants flaps v affricates and the glottal stop 
q For this latter set of sounds the confusion matrix is shown in Table IX There is a considerable number of insertion errors Insertions are common in any speech recognition system because typical speaking rates vary from training segments to test segments There are sudden onsets of vowels and fricatives that give rise to stop burst insertions 68 of stop burst insertions were at the begining of fricative segments and 46 were at the beginning of the sentences possibly because speakers are highly likely to start speaking with a sudden onset Highfrequency noise in the silence regions and aspiration at the end or beginning of vowels cause fricative insertions 44 of all fricative insertions occur with an adjoining silence region 43 of the rest of the fricative insertions have an adjoining vowel E Word level results and constrained segmentation results The SVM and HMM models obtained by training on the TIMIT database were then applied to the isolated digits of the TIDIGITS database in both the vocabulary 
constrained FIG 11 Color online Top spectrogram of the utterance time they re worn A Broad class labels B Landmark labels C phoneme labels bottom ratio of E0 F3 to EF3 f s 2 Broad class and phoneme labels are marked at the end of each sound and the landmark labels show the time instant of each landmark The ellipse shows an error made by the system on this utterance E0 F3 EF3 f s 2 does not dip in the dh region which makes the pattern recognizer call the fricative a sonorant sound J Acoust Soc Am Vol 123 No 2 February 2008 A Juneja and C Y Espy Wilson Probabilistic detection of speech landmarks 1165 TABLE VIII Confusion matrix for landmarks with exclusion of affricates syllabic sonorant consonants v glottal stop q diphthongs and flap dx Only nonredundant landmarks are shown For example VOP implies presence of a syllabic peak P and vice versa therefore only VOP is used in the confusion matrix Total Fon SIL VOP Son B Insertions 6369 10 232 12 467 5504 9152 3439 Fon 5607 15 50 155 448 682 SIL 10 9281 56 65 2 692 
VOP 1 12 11 146 1 24 206 Son 136 104 18 4565 104 1038 B 185 0 24 95 2755 821 Deletions 430 820 1173 1290 797 Correct 88 03 90 71 89 40 70 82 84 98 and the unconstrained modes In the unconstrained mode the models were tested in exactly the same way as on the TIMIT database To get the results on constrained segmentation the segmentation paths were constrained using the broad class pronunciation models for the digits 0 1 2 3 4 5 6 7 8 9 The segmentation was identically constrained for both the HMM system and EBS The results are shown in Table X for EBS with linear as well as RBF kernels and for the HMM systems trained on TIMIT and tested on TIDIGITS On moving from unconstrained to constrained segmentation a similar improvement in performance of the EBS RBF and HMM AP systems can be seen in this table This result shows that EBS can be constrained in a successful manner as for the HMM system The overall performance of EBS using RBFs is also very close to the HMM AP system HMM AP system shows better generalization 
than the HMM MFCC system over cross database testing which may be attributed to better speaker independence of the APs compared to the MFCCs Deshmukh et al 2002 Figure 12 shows an example of the output of the unconstrained probabilistic segmentation algorithm for the utterance two with canonical pronunciation t uw The two most probable landmark sequences obtained from the algorithm are shown in this figure The landmark sequence obtained with the second highest probability for this case is the correct sequence It is hoped that once probabilistic place and voicing decisions are made the second most probable sequence of landmarks will yield an overall higher posterior word probability for the word two Finally word level accuracies were obtained for all of the systems The state of the art word recognition accuracy using word HMM models on TIDIGITS is above 98 Hirrsh and Pearce 2000 Recognition rates of 99 88 have also been obtained when using word HMM models for rec ognition of the TI 46 isolated digit database 
Deshmukh et al 2002 These results were obtained using the same topology as in the present experiments i e three state HMMs with eight mixture components The difference is that instead of three state HMM word models we are now using three state HMM broad class models to make the comparison with EBS Note that a full word recognition system including place features is not presented here and only broad class models are presented Therefore a complete segmentation for a digit was scored as correct if it was an acceptable broad class segmentation for that digit The results are shown in Table XI A fully correct segmentation of 68 7 was obtained using the EBS AP system About 84 0 of the digits had a correct segmentation among the top two choices Note that the top two or three choices can be combined with place information to get final probabilities of words A significant increase in correct recognition in the top two choices over the top one choice shows that there is a good scope of recovery of errors when place 
information is added An accuracy of 67 6 was obtained by the HMM AP system and an accuracy of 63 8 was obtained by the HMM MFCC system These results further confirm the comparable performance of the EBS and the HMM AP systems Specifically this result shows that a system that selectively uses knowledge based APs for phonetic feature detection can be constrained as well as the HMM systems for limited vocabulary tasks and can also give a similar performance in terms of recognition accuracy IV DISCUSSION A landmark based ASR system has been described for generating multiple landmark sequences of a speech utterance along with a probability of each sequence The land TABLE IX Confusion matrix for affricates syllabic sonorant consonants SSCs v glottal stop q diphthongs and flap dx Empty cells indicate that those confusions were scored as correct but the exact number of those confusions were not available from the scoring program Total q Diph SSCs v dx ch jh hv 927 4390 1239 710 632 570 233 Fon 2 23 11 40 562 SIL VOP 
0 3991 1071 2 0 1 0 Son 5 25 27 B Deletions Correct 99 25 90 91 86 44 65 63 80 85 98 60 80 26 18 14 40 6 0 0 5 1 4 0 3 328 115 198 75 7 43 1166 J Acoust Soc Am Vol 123 No 2 February 2008 A Juneja and C Y Espy Wilson Probabilistic detection of speech landmarks TABLE X Broad class results on TIDIGITS Correct Accurate in percent EBS linear Constrained Unconstrained 91 7 82 8 89 5 64 0 EBSRBF 92 6 85 2 93 0 74 3 HMM MFCC 92 4 84 3 88 6 74 1 HMM AP 92 3 85 8 84 2 72 9 mark sequences can be constrained using broad class pronunciation models For unconstrained segmentation on TIMIT an accuracy of 79 5 is obtained assuming certain allowable splits merges and substitutions that may not affect the final lexical access On cross database constrained detection of landmarks a correct segmentation was obtained for about 68 7 of the words This compares well with a correct segmentation for about 67 6 of the words for the HMM system using APs and 63 8 for the HMM system using MFCCs The percentage accuracy of broad class 
recognition improved from 74 3 for unconstrained segmentation to 84 2 for constrained segmentation which is very similar to the improvement from 72 9 to 85 5 for the HMM system using APs These results show that EBS can be constrained by a higher level pronunciation model similar to the HMM systems The comparison with previous work on phonetic feature detection is very difficult because of the different test conditions definitions of features and levels of implementation used by different researchers At the frame level the 94 4 binary classification accuracy on the sonorant feature compares well with previous work by Bitar 1997 where an accuracy of 94 6 for sonorancy detection on the same database was obtained The continuant result of 95 6 is not directly comparable with previously obtained stop detection results Bitar 1997 Liu 1996 Niyogi 1998 In the work by Niyogi 1998 results were presented at a frame rate of 1 ms and in the work by Liu 1996 and Bitar 1997 results were not presented at the frame level A 
full probabilistic landmark detection system was not developed in the research cited above An 81 7 accuracy on the syllabic feature may seem low but note that there is usually no sharp boundary between vowels and semivowels Therefore a very high accuracy at the frame level for this feature is not only very difficult to achieve but also it is not very important as long as sonorant consonants are correctly detected The authors have not been able to find a previous result to which this number can be suitably compared At the sequence level the overall accuracy of 79 5 is comparable to 77 8 accuracy obtained in a nonprobabilistic version of EBS Bitar 1997 Note that the most significant improvement over the system by Bitar 1997 is that the current system can be constrained for limited vocabulary and it can be used for obtaining multiple landmark sequences instead of one There are various other systems to which segmentation results can be compared Salomon et al 2004 Ali 1999 but the comparison is omitted in this 
work because the purpose of this paper is to present how the ideas from such systems can be applied to a practical speech recognizer The complete system for word recognition is currently being developed There has been some success in small vocabulary isolated word recognition Juneja 2004 and in landmark detection for large vocabulary continuous speech recognition Hasegawa Johnson et al 2005 EBS benefits directly from research in discriminative APs for phonetic features therefore the system will improve as more powerful APs are designed for various phonetic features By the use of APs specific to each phonetic feature EBS provides a platform for the evaluation of new knowledge gained on discrimination of different speech sounds EBS provides easier evaluation of newly designed APs than HMM based systems If certain APs give better performance in binary classification of phonetic features and are more context independent than currently used APs then they will give overall better recognition rates Therefore 
complete speech recognition experiments are not required in the process of designing the APs In the future apart from the research that will be carried out on the automatic extraction of APs for all the phonetic features further research will be done on better glide detection and incorporation of previous research Howitt 2000 on detection of vowel landmarks APs to separate nasals from semivowels Pruthi and Epsy Wilson 2003 and to detection nasalization in vowels Pruthi 2007 will be integrated along with an improved formant tracker Xia and Espy Wilson 2000 Studies of pronunciation variability derived from previous work Zhang 1998 as well as continuing research will be integrated into EBS TABLE XI Percent of TIDIGITS isolated digits with fully accurate broad class sequence FIG 12 A sample output of the probabilistic landmark detection for the digit two The spectrogram is shown in a Two most probable landmark sequences b and c are obtained by the probabilistic segmentation algorithm The first most probable 
sequence b has a missed stop consonant but the second most probable sequence gets it J Acoust Soc Am Vol 123 No 2 February 2008 EBS RBF 68 7 HMM AP 67 6 HMM MFCC 63 8 A Juneja and C Y Espy Wilson Probabilistic detection of speech landmarks 1167 ACKNOWLEDGMENTS This work was supported by Honda Initiation Grant No 2003 and NSF Grant No BCS 0236707 The authors would like to thank Om Deshmukh at the University of Maryland for help with the HMM experiments F3 was computed as an average over the third formant values obtained in voiced regions using on the ESPS formant tracker Entropic 1997 The same average value of F3 was used in each speech frame for computation of manner APs Ali A M A 1999 Auditory based acoustic phonetic signal processing for robust continuous speech recognition Ph D thesis University of Pennsylvania Bitar N 1997 Acoustic analysis and modeling of speech based on phonetic features Ph D thesis Boston University Bitar N and Espy Wilson C 1996 A knowledge based signal representation for speech 
recognition International Conference on Acoustics Speech and Signal Processing Atlanta GA 1 Hirrsh H G and Pearce D 2000 The aurora experimental framework for the performance evaluation of speech recognition systems under noisy conditions Proc ISCA ITRW ASR2000 Paris France 1168 J Acoust Soc Am Vol 123 No 2 February 2008 A Juneja and C Y Espy Wilson Probabilistic detection of speech landmarks 
25	a	Introduction to Arabic Speech Recognition Using CMUSphinx System H Satori 1 2 M Harti 1 2 and N Chenfour 2 1 UFR Informatique et Nouvelles Technologies d Information et de Communication B P 1796 Dhar Mehraz E mail hsnsatori yahoo fr Although Arabic is currently one of the most widely spoken language in the world there has been relatively little speech recognition research on Arabic compared to the other languages 9 11 The first works on Arabic ASR has concentrated on developing recognizers for modern standard Arabic MSA The most difficult problems in developing highly accurate ASRs for Arabic are the predominance of non diacritized text material the enormous dialectal variety and the morphological complexity D Vergyri et al investigate the use of morphology based language model at different stages in a speech recognition system for conversational Arabic 9 K Kirchhoff et al 10 investigate the recognition of dialectal Arabic and study the discrepancies between dialectal and formal Arabic in the speech 
recognition point of view D Vergyri et al 11 investigate the automatic diacritizing Arabic text for use in acoustic model training for ASR CMU Carnegie Mellon University Sphinx speech recognition system is freely available and currently is one of the most robust speech recognizers in English This system enables research groups with modest budgets to quickly begin conducting research and developing applications In this work we attempt to build an ASR based on CMU Sphinx for the Arabic language We describe our experience for the extension of this system into Arabic language Abstract In this paper Arabic was investigated from the speech recognition problem point of view We propose a novel approach to build an Arabic Automated Speech Recognition System ASR This system is based on the open source CMU Sphinx 4 from the Carnegie Mellon University CMU Sphinx is a large vocabulary speaker independent continuous speech recognition system based on discrete Hidden Markov Models HMMs We build a model using utilities from 
the OpenSource CMU Sphinx We will demonstrate the possible adaptability of this system to Arabic voice recognition Keywords Speech recognition Arabic language HMMs CMUSphinx 4 Artificial intelligence I INTRODUCTION Automatic Speech Recognition ASR is a technology that allows a computer to identify the words that a person speaks into a microphone or telephone It has a wide area of applications Command recognition Voice user interface with the computer Dictation Interactive Voice Response it can be used to learn a foreign language ASR can help also handicapped people to interact with society It is a technology which makes life easier and very promising 1 View the importance of ASR too many systems are developed the most popular are Dragon Naturally Speaking IBM Via voice Microsoft SAPI Open source speech recognition systems are available too such as HTK 2 ISIP 3 AVCSR 4 and CMU Sphinx 4 5 7 We are interested to this last which is based on Hidden Markov Models HMMs 8 A Hidden Markov Model HMM is a statistical 
model where the system being modeled is assumed to be a Markov process with unknown parameters and the challenge is to determine the hidden parameters from the observable parameters based on this assumption The extracted model parameters can then be used to perform further analysis for example for pattern recognition applications Its extension into foreign languages II PHINX English is the standard represent a real research challenge area Fig 1 CMUSphinx logo A Presentation 1 Sphinx 4 Sphinx 4 speech recognition system has been jointly developed by the Carnegie Mellon University Sun Microsystems Laboratories and Mitsubishi Electric Research Laboratories M E R L It has been built entirely in Java TM programming language Since his starting the Sphinx Group is dedicated to release a set of reasonably mature speech components Those provide a basic technology level to anyone interested in creating speech recognition systems Since 2000 first with CMU Sphinx I CMU Sphinx II CMU SphinxTrain then CMU Sphinx III and 
CMUSphinx 4 a large part of CMU Sphinx project has been made available as open source packages 12 14 2 SphinxTrain SphinxTrain is the acoustic training environment for CMU Sphinx for sphinx2 sphinx3 and sphinx4 which was first publicly released on June 7th 2001 It is a suite of programs script and documentation for building acoustic models from data for the Sphinx suite of recognition engines With this contribution people should be able to build models for any language and condition for which there is enough acoustic data It is not possible to proceed to the recognition without having an acoustic model which is necessary to compare the data coming from FrontEnd see Fig 2 This model should be prepared using Sphinx Train tool B Architecture The Sphinx 4 architecture has been designed with a high degree of flexibility and modularity Each labelled element in Figure 2 represents a module that can be easily replaced allowing researchers to experiment with different module implementations without needing to modify 
other portions of the system The main blocks in Sphinx 4 architecture are frontend decoder and Linguist Fig 2 Sphinx 4 Architecture the main blocks are the FrontEnd the Decoder and the Linguist Front End it parameterizes an impute signal e g audio into a sequence of output features It performs Digital Signal Processing DSP on the incoming data Feature The outputs of the front end are features used for decoding in the rest of the system Linguist Or knowledge base it provides the information the decoder needs to do its job It is made up of three modules which are Acoustic Model Contains a representation often statistical of a sound created by training using many acoustic data Dictionary It is responsible for determining how a word is pronounced Language Model It contains a representation often statistical of the probability of occurrence of words Search Graph The graph structure produced by the linguist according to certain criteria e g the grammar using knowledge from the dictionary the acoustic model and the 
language model Decoder It is the main bloc of the Sphinx 4 system which performs the bulk of the work It reads features from the front end couples this with data from the knowledge base and feedback from the application and performs a search to determine the most likely sequences of words that could be represented by a series of features C Installation 1 Sphinx 4 Sphinx 4 can be downloaded either in binary format or in source codes 15 It was compiled and tested on several versions of Linux and on Windows operating systems Running building and testing sphinx 4 requires additional software and the D Implementation The Sphinx 4 implementation consists of 1 A phoneme is the smallest element of speech that indicates a difference in meaning word or sentence In this paper we describe our experience to create and develop an Arabic Version of CMU Sphinx 4 speech recognition system In what follows we present a Hello_Arabic_Digit application for the recognition of the ten Arabic digits Table 1 An automatic Speech 
recognizer system like Sphinx 4 uses three types of language dependent models An acoustic model which represents statistically a range of possible audio representations for the phonemes A pronunciation dictionary specifying how each word is pronounced in terms of the phonemes in the acoustic model A language model or grammar model which models patterns of word usage This is normally customized for the application Every word in the language model must be in the pronunciation dictionary In Hello_Arabic_Digits we proceed to the modification of those three elements to fit to our application 1 Corpus preparation An in house corpus was created from all 10 Arabic digits A number of 6 Moroccan speakers 6 males were asked to utter all digits 5 times Hence the corpus consists of 5 repetitions of every digit produced by each speaker Depending on this the corpus consists of 300 tokens During the recording session each utterance was played back to ensure that the entire digit was included in the recorded signal All the 
300 10 Table 1 Ten Arabic digits how to pronounce them type of syllable IPA representation 24 and number of syllables in every spoken digit 25 Table 2 Results of the test of Hello_Arabic_Digit application 11 D Vergyri K Kirchhoff Auto matic diacritization of Arabic for acoustic modelling in speech recognition In Ali Farghaly andKarine Megerdoomian editors COLING 2004 Computational Approaches to Arabic Script based Languages pp Fig 3 Execution of the Hello_Arabic_Digit application Results are very satisfactory taken into account the very small size of the corpus of training personal corpus which was used if compared with corpora used for English IV CONCLUSION To conclude a spoken Arabic recognition system was designed to investigate the process of automatic speech recognition This system was based on CMUSphinx 4 from Carnegie Mellon University An application Hello_Arabic_Digit was presented to demonstrate the possible adaptability of this system to Arabic speech We project to extend to application for wide 
Arabic language recognition especially for the Moroccan dialect language REFERENCES A Yousfi Introduction de la vitesse 
27	a	BAYESIAN STATISTICS 8 pp Generative or Discriminative Getting the Best of Both Worlds Christopher M Bishop Microsoft Research UK cmbishop microsoft com Cambridge University UK jal62 cam ac uk Julia Lasserre Summary For many applications of machine learning the goal is to predict the value of a vector c given the value of a vector x of input features In a classification problem c represents a discrete class label whereas in a regression problem it corresponds to one or more continuous variables From a probabilistic perspective the goal is to find the conditional distribution p c x The most common approach to this problem is to represent the conditional distribution using a parametric model and then to determine the parameters using a training set consisting of pairs xn cn of input vectors along with their corresponding target output vectors The resulting conditional distribution can be used to make predictions of c for new values of x This is known as a discriminative approach since the conditional 
distribution discriminates directly between the different values of c An alternative approach is to find the joint distribution p x c expressed for instance as a parametric model and then subsequently uses this joint distribution to evaluate the conditional p c x in order to make predictions of c for new values of x This is known as a generative approach since by sampling from the joint distribution it is possible to generate synthetic examples of the feature vector x In practice the generalization performance of generative models is often found to be poorer than than of discriminative models due to differences between the model and the true distribution of the data When labelled training data is plentiful discriminative techniques are widely used since they give excellent generalization performance However although collection of data is often easy the process of labelling it can be expensive Consequently there is increasing interest in generative methods since these can exploit unlabelled data in addition 
to labelled data Although the generalization performance of generative models can often be improved by training them discriminatively they can then no longer make use of unlabelled data In an attempt to gain the benefit of both generative and discriminative approaches heuristic procedure have been proposed which interpolate between these two extremes by taking a convex combination of the generative and discriminative objective functions Julia Lasserre is funded by the Microsoft Research European PhD Scholarship programme 4 Christopher M Bishop and Julia Lasserre Here we discuss a new perspective which says that there is only one correct way to train a given model and that a discriminatively trained generative model is fundamentally a new model Minka 2006 From this viewpoint generative and discriminative models correspond to specific choices for the prior over parameters As well as giving a principled interpretation of discriminative training this approach opens the door to very general ways of interpolating 
between generative and discriminative extremes through alternative choices of prior We illustrate this framework using both synthetic data and a practical example in the domain of multi class object recognition Our results show that when the supply of labelled training data is limited the optimum performance corresponds to a balance between the purely generative and the purely discriminative We conclude by discussing how to use a Bayesian approach to find automatically the appropriate trade off between the generative and discriminative extremes Keywords and Phrases Generative discriminative Bayesian inference semi supervized unlabelled data machine learning 1 INTRODUCTION In many applications of machine learning the goal is to take a vector x of input features and to assign it to one of a number of alternative classes labelled by a vector c for instance if we have C classes then c might be a C dimensional binary vector in which all elements are zero except the one corresponding to the class In the simplest 
scenario we are given a training data set X comprising N input vectors X x1 xN together with a set of corresponding labels C c1 cN in which we assume that the input vectors and their labels are drawn b independently from the same fixed distribution Our goal is to predict the class c b and so we require the conditional distribution for a new input vector x b x b X C p c 1 To determine this distribution we introduce a parametric model governed by a set of parameters In a discriminative approach we define the conditional distribution p c x where are the parameters of the model The likelihood function is then given by N Y p cn xn 2 L p C X n 1 The likelihood function can be combined with a prior p to give a joint distribution p C X p L from which we can obtain the posterior distribution by normalizing p X C where p C X p L p C X 4 3 Z p L d 5 Generative or Discriminative 5 Predictions for new inputs are then made by marginalizing the predictive distribution with respect to weighted by the posterior distribution 
Z b x b X C p c b x b p X C d p c 6 In practice this marginalization as well as the normalization in 5 are rarely tractable and so approximation schemes such as variational inference must be used If training data is plentiful a point estimate for can be made by maximizing the posterior distribution to give MAP and the predictive distribution then estimated using b x b X C p c b x b MAP p c 7 Note that maximizing the posterior distribution 4 is equivalent to maximizing the joint distribution 3 since these differ only by a multiplicative constant In practice we typically take the logarithm before maximizing as this gives rise to both analytical and numerical simplifications If we consider a prior distribution p which is constant over the region in which the likelihood function is large then maximizing the posterior distribution is equivalent to maximizing the likelihood More generally we make predictions by marginalizing over all values of using either Monte Carlo methods or an appropriate deterministic 
approximation framework Bishop 2006 In all cases however the key quantity for model training is the likelihood function L Discriminative methods give good predictive performance and have been widely used in many applications In recent years there has been growing interest in a complementary approach based on generative models which define a joint distribution p x c over both input vectors and class labels Jebara 2004 One of the motivations is that in complex problems such as object recognition where there is huge variability in the range of possible input vectors it may be difficult or impossible to provide enough labelled training examples and so there is increasing use of semi supervised learning in which the labelled training examples are augmented with a much larger quantity of unlabelled examples A discriminative model cannot make use of the unlabelled data as we shall see and so in this case we need to consider a generative approach The complementary properties of generative and discriminative models 
have led a number of authors to seek methods which combine their strengths In particular there has been much interest in discriminative training of generative models Bouchard and Triggs 2004 Holub and Perona 2005 Yakhnenko Silvescu and Honavar 2005 with a view to improving classification accuracy This approach has been widely used in speech recognition with great success Kapadia 1998 where generative hidden Markov models are trained by optimizing the predictive conditional distribution As we shall see later this form of training can lead to improved performance by compensating for model mis specification that is differences between the true distribution of the process which generates the data and the distribution specified by the model However as we have noted discriminative training cannot take advantage of unlabelled data In particular it has been shown Ng and Jordan 2002 that logistic regression the discriminative counterpart of a Naive Bayes generative model works better than its generative counterpart 
but only for a large number of training data points large depending on the complexity of the problem which confirms the need for using unlabelled data Recently several authors Bouchard and Triggs 2004 Holub and Perona 2005 Raina Shen Ng and McCallum 2003 have proposed hybrids of the generative 6 Christopher M Bishop and Julia Lasserre and discriminative approaches in which a model is trained by optimizing a convex combination of the generative and discriminative log likelihood functions Although the motivation for this procedure was heuristic it was sometimes found that the best predictive performance was obtained for intermediate regimes in between the discriminative and generative limits In this paper we develop a novel viewpoint Minka 2005 Bishop 2006 which says that for a given model there is a unique likelihood function and hence there is only one correct way to train it The discriminative training of a generative model is instead interpreted in terms of standard training of a different model 
corresponding to a different choice of distribution This removes the apparently adhoc choice for the training criterion so that all models are trained according to the principles of statistical inference Furthermore by introducing a constraint between the parameters of this model through the choice of prior the original generative model can be recovered As well as giving a novel interpretation for discriminative training of generative models this viewpoint opens the door to principled blending of generative and discriminative approaches by introducing priors having a soft constraint amongst the parameters The strength of this constraint therefore governs the balance between generative and discriminative In Section 2 we give a detailed discussion of the new interpretation of discriminative training for generative models and in Section 3 we illustrate the advantages of blending between generative and discriminative viewpoints using a synthetic example in which the role of unlabelled data and of model mis 
specification becomes clear In Section 4 we show that this approach can be applied to a large scale problem in computer vision concerned with object recognition in images and finally we draw some conclusions in Section 5 2 A NEW VIEW OF DISCRIMINATIVE TRAINING A generative model can be defined by specifying the joint distribution p x c of the input vector x and the class label c conditioned on a set of parameters Typically this is done by defining a prior probability for the classes p c along with a class conditional density for each class p x c so that p x c p c p x c 8 where Since the data points are assumed to be independent the joint distribution is given by LG p X C p N Y n 1 p xn cn 9 This can be maximized to determine the most probable MAP value of Again since p X C p X C p X C this is equivalent to maximizing the posterior distribution p X C In order to improve the predictive performance of generative models it has been proposed to use discriminative training Yakhnenko et al 2005 which involves 
maximizing N Y LD p C X p p cn xn 10 n 1 Generative or Discriminative 7 in which we are conditioning on the input vectors instead of modelling their distribution Here we have used p c x P p x c p x c 11 c Note that 10 is not the joint distribution for the original model defined by 9 and so does not correspond to MAP for this model The terminology of discriminative training is therefore misleading since for a given model there is only one correct way to train it It is not the training method which has changed but the model itself This concept of discriminative training has been taken a stage further Yakhnenko Silvescu and Honavar 2005 by maximizing a function given by a convex combination of 9 and 10 of the form ln LD 1 ln LG 12 where 0 1 so as to interpolate between generative 0 and discriminative 1 approaches Unfortunately this criterion was not derived by maximizing the distribution of a well defined model Following Minka 2005 we therefore propose an alternative view of discriminative training which will 
lead to an elegant framework for blending generative and discriminative approaches Consider a model which contains an additional indepene e in addition to the parameters in e dent set of parameters which the likelihood function is given by e p c x p x e q x c where e p x X c 13 e p x c 14 e has independent parameters e Here p c x is defined by 11 while p x c e The model is completed by defining a prior p over the model parameters giving a joint distribution of the form e p e q X C N Y n 1 e p cn xn p xn 15 Now suppose we consider a special case in which the prior factorizes so that e p p e p 16 e in the usual way by We then determine optimal values for the parameters and maximizing 15 which now takes the form p N Y n 1 p cn xn e p N Y n 1 e p xn 17 8 Christopher M Bishop and Julia Lasserre We see that the resulting value of will be identical to that found by maximizing 11 since it is the same function which is being maximized Since it is and e which determines the predictive distribution p c x we see that 
this model not is equivalent in its predictions to the discriminatively trained generative model This gives a consistent view of training in which we always maximize the joint distribution and the distinction between generative and discriminative training lies in the choice of model The relationship between the generative model and the discriminative model is illustrated using directed graphs in Fig 1 c p c q x N images x N images Figure 1 Probabilistic directed graphs showing on the left the original generative model and on the right the corresponding discriminative model Now suppose instead that we consider a prior which enforces equality between the two sets of parameters e p e p 18 e in 13 from which we recover the original generative model Then we can set p x c Thus we have a single class of distributions in which the discriminative model corresponds to independence in the prior and the generative model corresponds to an equality constraint in the prior 2 1 Blending Generative and Discriminative Clearly 
we can now blend between the generative and discriminative extremes by e and Why should we considering priors which impose a soft constraint between wish to do this First of all we note that the reason why discriminative training might give better results than direct use of the generative model is that 15 is more flexible e Of course if the generative than 9 since it relaxes the implicit constraint model were a perfect representation of reality in other words the data really came from the model then increasing the flexibility of the model would lead to poorer Generative or Discriminative 9 results Any improvement from the discriminative approach must therefore be the result of a mis match between the model and the true distribution of the process which generates the data In other words the benefit of discriminative training is dependent on model mis specification Conversely the benefit of the generative approach is that it can make use of unlabelled data to augment the labelled training set Suppose we have a 
data set comprising a set of inputs XL for which we have corresponding labels CL together with a set of inputs XU for which we have no labels For the correctly trained generative model the function which is maximized is given by Y Y p p xn c n p xm 19 nL m U where p x is defined by p x X c p x c 20 We see that the unlabelled data influences the choice of and hence affects the predictions of the model By contrast for the discriminatively trained generative model the function which is now optimized is again the product of the prior and the likelihood function and so takes the form Y p p xc xn 21 nL and we see that the unlabelled data plays no role Thus in order to make use of unlabelled data we cannot use a discriminative approach Now let us consider how a combination of labelled and unlabelled data can be exploited from the perspective of our new approach defined by 15 for which the joint distribution becomes e p e q XL CL XU Y Y e e p cn xn p xn p xm nL mU 22 We see that the unlabelled data as well as the 
labelled data influences the parame which in turn influence via the soft constraint imposed by the prior eters In general if the model is not a perfect representation of reality and if we have unlabelled data available then we would expect the optimal balance to lie neither at the purely generative extreme nor at the purely discriminative extreme As a simple example of a prior which interpolates smoothly between the generative and discriminative limits consider the class of priors of the form ff e p p e 1 exp 1 e 2 p 23 2 2 If desired we can relate to an like parameter by defining a map from 0 1 to 0 for example using 10 Christopher M Bishop and Julia Lasserre For 0 we have 0 and we obtain a hard constraint of the form 18 which corresponds to the generative model Conversely for 1 we have and we obtain an independence prior of the form 16 which corresponds to the discriminative model 3 ILLUSTRATION We now illustrate the new framework for blending between generative and discriminative approaches using an 
example based on synthetic data This is chosen to be as simple as possible and so involves data vectors xn which live in a two dimensional Euclidean space for easy visualization and which belong to one of two classes Data from each class is generated from a Gaussian distribution as illustrated in Fig 2 Data distribution 7 6 5 4 3 2 1 0 1 2 0 2 4 6 8 Figure 2 Synthetic training data shown as crosses and dots together with contours of probability density for each of the two classes Two points from each class are labelled indicated by circles around the data points Here the scales on the axes are equal and so we see that the class conditional densities are elongated in the horizontal direction We now consider a continuum of models which interpolate between purely generative and purely discriminative To define this model we consider the generative limit and represent each class conditional density using an isotropic Gaussian distribution Since this does not capture the horizontally elongated nature of the true 
class distributions this represents a form of model mis specification The parameters of the model are the means and variances of the Gaussians for each class along with the class prior probabilities Generative or Discriminative 11 We consider a prior of the form 23 in which is defined by 24 Here e p N e where p is the usual conjugate prior a we choose p Gaussian gamma prior for the means and variances and a Dirichlet prior for the class label The generative model consists of a spherical Gaussian per class with mean e k where N Influence of the alpha parameter on the classification performance 100 Classificaton performance 90 80 70 60 50 40 0 0 2 0 4 0 6 Values of alpha 0 8 1 Figure 3 Plot of the percentage of correctly classified points on the test set versus for the synthetic data problem 12 Christopher M Bishop and Julia Lasserre We see that the best generalization occurs for values of intermediate between the generative and discriminative extremes To gain insight into this behaviour we can plot the 
contours of density for each class corresponding to different values of as shown in Fig 4 Figure 4 Results of fitting an isotropic Gaussian model to the synthetic data for various values of The top left shows 0 generative case while the bottom right shows 1 discriminative case The gray area corresponds to points that are assigned to the red class We see that a purely generative model is strongly influenced by modelling the density of the data and so gives a decision boundary which is orthogonal to the correct one Conversely a purely discriminative model attends only to the labelled data points and so misses useful information about the horizontal elongation of the true class conditional densities which is present in the unlabelled data 4 OBJECT RECOGNITION We now apply our approach to a realistic application involving object recognition in static images This is a widely studied problem which has been tackled using a Generative or Discriminative 13 range of different discriminative and generative models The 
long term goal of such research is to achieve near human levels of recognition accuracy across thousands of object classes in the presence of wide variations in location scale orientation and lighting as well as changes due to intra class variability and occlusion 4 1 The Data We used eight different classes airplanes bikes cows faces horses leaves motorbikes sheep Bishop 2006 Together these images exhibit a wide variety of poses colours and illumination as illustrated by the sample images shown in Fig 5 The goal is to assign images from the test set to one of the eight classes Figure 5 Sample images from the training set 14 Christopher M Bishop and Julia Lasserre 4 2 The Features Our features are taken from Winn Criminisi and Minka 2005 in which the original RGB images are first converted into the CIE L a b colour space Kasson and Plouffe 1992 All images were re scaled to C Y k 1 kk c 26 Generative or Discriminative 15 y cn p tnj l znj J N Figure 6 The generative model for object recognition expressed as a 
directed acyclic graph for unlabelled images in which the boxes denote plates i e independent replicated copies Only the patch feature vectors xnj are observed corresponding to the shaded node The image class labels cn and patch class labels nj are latent variables Given the overall class for the image each patch is then drawn from either one of the foreground classes or the background k C 1 class The probability of generating a patch from a particular class is governed by a set of parameters k one for each class such that k 0 constrained by the subset of classes actually present in the image Thus C 1 X l 1 p j c 1 C 1 Y cl l ck k jk k 1 27 Note that there is an overall undetermined scale to these parameters which may be removed by fixing one of them e g C 1 1 For each class the distribution of the patch feature vector x is governed by a separate mixture of Gaussians which we denote by C 1 Y k 1 p x j k xj k jk 28 where k denotes the set of parameters means covariances and mixing coefficients associated with 
this mixture model If we assume N independent images and for image n we have J patches drawn 16 Christopher M Bishop and Julia Lasserre independently then the joint distribution of all random variables is N J Y Y p cn p xnj nj p nj cn n 1 j 1 29 Here we are assuming that each image has the same number J of patches though this restriction is easily relaxed if required The graph shown in Fig 6 corresponds to unlabelled images in which only the feature vectors xnj are observed with both the image category and the classes of each of the patches being latent variables It is also possible to consider images which are weakly labelled that is each image is labelled according to the category of object present in the image This corresponds to the graphical model of Fig 7 in which the node cn is shaded y cn p tnj l znj J N Figure 7 Graphical model corresponding to Fig 6 for weakly labelled images Of course for a given size of data set better performance is expected if all of the images are strongly labelled that is 
segmented images in which the region occupied by the object or objects is known so that the patch labels nj become observed variables The graphical model for a set of strongly labelled images is shown in Fig 8 Strong labelling requires hand segmentation of images and so is a time consuming and expensive process as compared with collection of the images themselves For a given level of effort it will always be possible to collect many unlabelled or weakly labelled images for the same cost as a single strongly labelled image Since the variability of natural images and objects is so vast we will always be operating in a regime in which the size of our data sets is statistically small though they will often be computationally large Generative or Discriminative 17 y cn p tnj l znj J N Figure 8 Graphical models corresponding to Fig 6 for strongly labelled images For this reason there is great interest in augmenting expensive strongly labelled images with lots of cheap weakly labelled or unlabelled images in order 
to better characterize the different forms of variability Although the two stage hierarchical model shown in Fig 6 appears to be more complicated than in the simple example shown in Fig 1 it does in fact fall within the same framework In particular for labelled images the observed data is xn cn nj while for unlabelled images only xn are observed The experiments described here could readily be extended to consider arbitrary combinations of strongly labelled weakly labelled and unlabelled images if desired If we let k k k denote the full set of parameters in the model then we can consider a model of the form 22 in which the prior is given by 23 with e taken to be constant defined by 24 and the terms p and p We use conjugate gradients to optimize the parameters Due to lack of space we do not write down all the derivatives of the log likelihood function required by the conjugate gradient algorithm However the correctness of the mathematical derivation of these gradients as well as their numerical implementation 
can easily be verified by comparison against numerical differentiation Bishop 1995 The conjugate gradients is the most used technique when it comes to blending generative and discriminative models thanks to its flexibility Indeed because of the discriminative component p cn xn which contains a normalizing factor an algorithm such as EM would require much more work as nothing is directly tractable anymore However a comparison of the two methods is currently being investigated 4 4 Results We use 50 training images per class giving 400 training images in total of which five images per class a total of 40 were fully labelled i e both the image and the 18 Christopher M Bishop and Julia Lasserre individual patches have class labels All the other images are left totally unlabelled i e not even the category they belong to is given Note that this kind of training data is 1 very cheap to get and 2 very unusual for a discriminative model The test set consists of 100 images per class giving a total of 800 images the 
task is to label each image Experiments are run five times with differing random initializations and the results used to compute a mean and variance over the test set classification which are shown by error bars in Fig 9 Figure 9 Influence of the term on the test set classification performance Note that since there are eight balanced classes random guessing would give 12 5 correct on average Again we see that the best performance is obtained with a blend between generative and discriminative extremes 5 CONCLUSIONS In this paper we have shown that discriminative training for generative models can be re cast in terms of standard training methods applied to a modified model This new viewpoint opens the door to a wide range of new models which interpolate smoothly between generative and discriminative approaches and which can benefit from the advantages of both The main drawback of this framework is that the number of parameters in the model is doubled leading to greater computational cost Although we have 
focussed on classification problems the framework is equally applicable to regression problems in which c corresponds to a set of continuous variables Generative or Discriminative 19 A principled approach to combining generative and discriminative approaches not only gives a more satisfying foundation for the development of new models but it also brings practical benefits In particular the parameter which governs the trade off between generative and discriminative is now a hyper parameter within a well defined probabilistic model which is trained using the unique correct likelihood function In a Bayesian setting the value of this hyper parameter can therefore be optimized by maximizing the marginal likelihood in which the model parameters have been integrated out thereby allowing the optimal trade off between generative and discriminative limits to be determined entirely from the training data without recourse to cross validation Bishop 2006 This extension of the work described here is currently being 
investigated REFERENCES Bishop C M 1995 Neural Networks for Pattern Recognition Oxford University Press Bishop C M 2006 Pattern Recognition and Machine Learning Berlin Springer Verlag Bouchard G and Triggs B 2004 The trade off between generative and discriminative classifiers IASC 16th International Symposium on Computational Statistics Prague Czech Republic 20 Christopher M Bishop and Julia Lasserre DISCUSSION HERBERT K H LEE University of California Santa Cruz USA Let me start by congratulating the authors for this paper In terms of Getting the best of both worlds this paper can also be seen as crossing between machine learning and statistics combining useful elements from both I can only hope that our two communities continue to interact deepening our connections The perspectives are often complementary which is an underlying theme of this discussion One of the obstacles to working across disciplines is that a very different vocabulary may be used to describe the same concepts Statisticians reading this 
paper may find it helpful to think of supervised learning as classification and unsupervised learning as clustering Discriminative learning is thus classification based on a probability model which it typically is in statistics while the generative approach is clustering based on a model such as a mixture model approach While machine learning and statistics are really quite similar there is a key difference in perspective In machine learning the main goal is typically to achieve good predictions and while a probability model may be used it is not explicitly required In contrast most statistical analyses see the probability model as the core of the analysis with the idea that optimal predictions will arise from accurate selection and fitting of the model In particular Bayesian analyses rely crucially on a fully specified probability model Thus one of the core points of this paper that of a unique likelihood function should seem natural to a Bayesian Yet it is an important insight in the context of the machine 
learning literature Bringing these machine learning algorithms into a coherent probabilistic framework is a big step forward and one that is not always fully valued This is an important contribution by these authors and their collaborators Uncertainty about the likelihood function can be dealt with by embedding the functions under consideration into a larger family with one or more additional parameters and this is exactly what has been done here This follows a strong tradition in statistics such as Box Cox transformations and model averaging but represents a relatively untapped potential in machine learning In contrast it is more common in machine learning to use implicit expansions of the model class or of the fitting algorithm when the model may not be explicitly stated Examples include bagging Breiman 1996 where individual predictions from over fit models are averaged over bootstrap samples to reduce over fitting and boosting Freund and Schapire 1997 where overly simple models are combined to create an 
improved ensemble prediction Such implicit expansion can work well in practice but it can be difficult to understand or describe the expanded class of models and hence difficult to leverage related knowledge from the literature On a related note the authors argue that a key benefit of using discriminative training for generative models is that it improves performance when the model is mis specified as vividly demonstrated by the example in Section 3 In practice this is quite useful as our parametric models are typically only approximations to reality and the approximations can be quite poor But this does leave open the possibility of explicit model expansion A larger parametric family may encompass a model which is close enough to reality Or taking things even further one could move to a fully nonparametric approach Then it becomes less clear what the trade offs are Many highly innovative and creative ideas have arisen in machine learning and the field of statistics has gained by importing some of these 
ideas Statistics in turn can offer a substantial literature that can be applied once a machine learning Generative or Discriminative 21 algorithm can be mapped to a probability model From the model one can draw from the literature to better understand when the algorithm will work best when it might perform poorly what diagnostics may be applicable and possibly how to further improve the algorithm The key is connecting the algorithm to a probability model either finding the model which implicitly underlies the algorithm or showing that the algorithm approximates a particular probability model These sorts of connections benefit both fields Thus thinking more about Bayesian probability models some possible further directions for this current work come to mind It would seem natural to put a prior on and to treat it as another parameter At least from the experiments shown so far it appears that there may be some information about likely best ranges of allowing the use of an informative prior possibly in 
comparison to a flat prior In addition to the possibility of marginalizing over one could also estimate to obtain a best fit Another natural possible direction would be to look at the full posterior rather than just getting a point estimate such as the maximum a posteriori class estimate Knowing about the uncertainty of the classification can often be useful It may also be useful to move to a more hierarchical model particularly for the image classes It would seem that images of horses and cows would be more similar to each other and images of bicycles and motorbikes would be similar to each other but that these two sets would be rather different from each other and further different from faces or leaves Working within a hierarchical structure should be straightforward in a fully Bayesian paradigm In terms of connections between machine learning and statistics it seems unfortunate that the machine learning literature takes little notice of related work in the statistics literature In particular there has 
been extensive work on model based clustering for example Fraley and Raftery 2002 and even a poster presented at this conference 22 Christopher M Bishop and Julia Lasserre with interpretations that are part of a generative process often of a dynamic form Inevitably our models are oversimplifications and we learn both from the activity of parameter estimation and model choice and from careful examination and interpretation of the posterior distribution of the parameters as well as from the form of various predictive distributions e g see Blei et al 2003a 2003b Erosheva 2003 and Erosheva et al 2004 When I teach statistical learning to graduate students in the Machine Learning Department I emphasize that the world of machine learning would be enriched by taking on at least part of this broader perspective My second observation is closely related While I very much appreciated the BL s goal and the boldness of their effort to formulate a different likelihood function to achieve an integration of the two 
perspectives I think the effort would be enhanced by consideration of some of the deeper implications of the subjective Bayesian paradigm As Bishop noted in his oral response to the discussion machine learning has been moving full force to adopt formal statistical ideas and so it is now common to see the incorporation of MAP and model averaging methodologies directly into the classification setting But as some of the other presentations at this conference make clear model assessment is a much more complex and subtle activity which we can situate within the subjective Bayesian framework cf Draper 1999 In particular I commend a careful reading of the Lauritzen 2006 discussion of the paper by Chakrabarti and Ghosh 2006 in which he emphasized that we can only come to grips with the model choice problem by considering model comparisons and specifications with respect to our own subjective Bayesian prior distribution thus taking advantage of the attendant coherence in the sense of de Finetti 1937 until such time 
as we need to step outside the formal framework and reformulate our model class and likelihoods Thus BL s new replicate distribution for possibly should be replaced by a real subjective prior distribution perhaps of a similar form and then they could explore the formulation of the generative model without introducing a likelihood that departs from the one that attempts to describe the underlying phenomenon of interest This I suspect would lead to an even clearer formulation that is more fully rooted in both the machine learning and statistical learning worlds REPLY TO THE DISCUSSION We would like thank the discussants for their helpful remarks and useful insights We would also like to take the opportunity to make some comments on an important issue raised by both discussants namely the relationship between the fields of machine learning and statistics Historically these fields evolved largely independently for many years Much of the motivation for early machine learning algorithms such as the perceptron in 
the 1960s and multi layered neural networks in the 1980s came from loose analogies with neurobiology Later the close connections to statistics became more widely recognized and these have strongly shaped the subsequent evolution of the field during the 1990s and beyond Bishop 1995 and Ripley 1996 Today there are many similarities in the techniques employed by the two fields although as the discussants note both the vocabulary and the underlying motivations can differ The discussants express frustration at the lack of appreciation of the statistics literature by some machine learning researchers Naturally the converse also holds and indeed this conference has provided examples of algorithms proposed as novel which are in fact well known and widely cited in the machine learning world To some extent this lack of appreciation is understandable The literature in either Generative or Discriminative 23 field alone is vast and the issue of vocabulary mentioned above is a further obstacle to cross fertilization It 
could be even be argued that the relative independence of the two fields has brought some benefits For example the use of large highly parameterized blackbox models trained on large data sets of the kind which characterized much of the applied work in neural networks in the 1990s did not fit well with the culture of the statistics community at the time and met with scepticism from some quarters Yet these efforts have led to numerous large scale applications of substantial commercial importance Nevertheless it seems clear that greater cross fertilization between the two communities would be desirable Conference such as AI Statistics Bishop 2003 explicitly seek to achieve this and several text books also span the divide Hastie 2001 and Bishop 2006 Increasingly the focus in the machine learning community is not just on welldefined probabilistic models but on fully Bayesian treatments in which distributions over unknown variables are maintained and updated However almost any model which has sufficient complexity 
to be of practical interest will not have a closedform analytical solution and hence approximation techniques are essential For many years the only general purpose approximation framework available was that of Monte Carlo sampling which is computationally demanding and which does not scale to large problems A crucial advance therefore has been the development of a wide range of powerful deterministic inference techniques These include variational inference expectation propagation loopy belief propagation and others Like Markov chain Monte Carlo these techniques have their origins in physics However they have primarily been developed and applied within the machine learning community They complement naturally the recent advances in probabilistic graphical models such as the development of factor graphs Also they are computationally much more efficient than Monte Carlo methods and have permitted for instance a fully Bayesian treatment of the player ranking problem in on line computer games through the 
TrueSkillTM system in which millions of players are compared and ranked with ranking updates and team matching being done in real time Herbrich 1996 Here the Bayesian treatment which maintains a distribution over player skill levels leads to substantially improved ranking accuracy compared to the traditional ELO method which used a point estimate and which can be viewed as a maximum likelihood technique This type of application would be inconceivable without the use of deterministic approximation schemes In our view these methods represent the single most important advance in practical Bayesian statistics in the last 10 years ADDITIONAL REFERENCES IN THE DISCUSSION Breiman L 1996 Bagging predictors Machine Learning 26 24 Christopher M Bishop and Julia Lasserre de Finetti B 1937 La 
28	a	INTERSPEECH 2007 Dynamic Language Model Adaptation Using Presentation Slides for Lecture Speech Recognition Hiroki Yamazaki Koji Iwano Koichi Shinoda Sadaoki Furui and Haruo Yokota Department of Computer Science Tokyo Institute of Technology Japan yamazaki ks cs titech ac jp iwano shinoda furui yokota cs titech ac jp Abstract We propose a dynamic language model adaptation method that uses the temporal information from lecture slides for lecture speech recognition The proposed method consists of two steps First the language model is adapted with the text information extracted from all the slides of a given lecture Next the text information of a given slide is extracted based on temporal information and used for local adaptation Hence the language model used to recognize speech associated with the given slide changes dynamically from one slide to the next We evaluated the proposed method with the speech data from four Japanese lecture courses Our experiments show the effectiveness of our proposed method 
especially for keyword detection The Fmeasure error rate for lecture keywords was reduced by 2 4 Index Terms language model adaptation speech recognition classroom lecture speech 1 Introduction Recent advancements in computer and storage technology enable archiving large multimedia databases The databases of classroom lectures in universities and colleges are particularly useful knowledge resources and they are expected to be used in education systems Recently much effort has been made to construct educational systems that use the multimedia content of classroom lectures to support distant learning 1 2 3 4 5 Among the various kinds of content related to lectures the transcription of speech data is expected to be the most important for indexing and searching lecture contents 2 6 Therefore highlevel speech recognition engine for lectures is required Lecture speech recognition has been studied extensively Many research projects for lecture transcriptions such as the European project CHIL Computers in the Human 
Interaction Loop 8 and the American iCampus Spoken Lecture Processing project 9 have been conducted Trancoso et al 7 investigated the automatic transcription of classroom lectures in Portuguese Large databases of conference presentations such as the Corpus of Spontaneous Japanese CSJ 10 11 and the TED corpus 12 have been collected to improve speech recognition accuracy With the use of these databases a state ofthe art speech recognition systems for conference presentations achieves accuracy of 70 80 Hence the recognition results provided by these systems are good enough to be used for speech summarization and speech indexing 13 The speaking style of classroom lectures is however much different from that of lectures in meetings or conferences Classroom lectures are not always practiced in advance and the same phrases are repeated many times for emphasis The lecture speaking style is closer to that in dialogue because lecturers are always ready to be interrupted by questions from students The spontaneity of 
this kind of speech is much higher than other kinds of presentations the lectures are characterized by strong coarticulation effects non grammatical constructions hesitations repetitions and filled pauses For these reasons speech recognition for classroom lecture speech is generally more difficult than that of speeches in conferences or meetings its recognition accuracy is around 40 60 Furthermore no large database of classroom lecture speech is available for training acoustic and language models In classrooms lecturers often use various materials e g textbooks or slides to help their students understand Since those materials include many keywords that also appear in lecture speech they are expected to be useful for language modeling in speech recognition Several adaptation methods for language models using such content have already been proposed for lecture speech recognition For example Togashi et al 14 proposed a method of using the text information in presentation slides If lecture speech is accompanied 
by slides a strong correlation can be observed between slides and speech In particular the speech corresponding to a given slide contains most of the text information presented in the slide We expect this relation between speech and text information of the slide can improve the model adaptation for lecture speech recognition We propose a dynamic adaptation method for language modeling that applies text information from slides In this method a slide dependent language model is constructed for each slide and this model is used afterwards to recognize the speech associated with the given slide The language model is changed dynamically as the lecture progresses This paper is organized as follows In Section 2 the base system applied in our studies is introduced In Section 3 the proposed language model adaptation method is explained and in Section 4 the effectiveness of the proposed method is discussed 2 UPRISE Unified Presentation Contents Retrieval by Impression Search Engine UPRISE Unified Presentation Contents 
Retrieval by Impression Search Engine 1 15 is a lecture presentation system to support distant learning It stores many types of multimedia materials such as texts pictures graphs images sounds voices and videos and provides a unified presentation view Figure 1 as a lecture video retrieval system The retrieval system returns appropriate lecture video scenes to match given keywords Since the speech information in lectures is used to narrow down the search candidates 6 a high level of speech recognition accuracy is strongly required 2349 August 27 31 Antwerp Belgium Table 1 Details of the collected lecture database LEC1 A 136 11 766 754 98 4 265 52 1 9 3 LEC2 A 56 10 831 803 96 7 282 60 2 5 5 LEC3 B 53 7 545 352 64 7 136 52 5 8 3 LEC4 C 135 11 952 948 99 6 270 13 0 3 4 Lecturer Size of keyword list Num of classes Lecture length min Lecture length using slide min Num of slides Num of Words per slide Num of Keywords per slide Figure 1 Unified presentation view in UPRISE for each slide is constructed by using the 
frequency FL Ni for all the n grams The content in lectures stored in UPRISE are synchronized as the class progresses that is each event in a lecture is marked with temporal information This synchronized content motivated us to investigate a dynamic adaptation method along the time axis by using the content strongly related to speech information We propose a dynamic language model adaptation method using one of the kinds of content slide information 4 Recognition Experiments 4 1 Experimental Conditions We collected audio and video of four Japanese lecture courses LEC1 LEC2 LEC3 and LEC4 for evaluation Each lecture course consisted of 12 classes where each class duration was around 80 minutes The audio data was recorded using a closetalking microphone The lecture courses LEC1 and LEC2 were given by the same lecturer The data from nine classes were excluded because their recording quality was poor We collected PowerPoint slides used in those lectures along with the temporal information The speech data were 
segmented using the temporal information such that each speech segment had the same boundary as its corresponding slide When an utterance occurred at the exact boundary of two slides the speech data was cut just after the utterance We manually transcribed all the speech data for evaluation except the utterances of speakers other than the lecturer Keywords which were mostly technical terms characterizing the lectures were selected subjectively by several researchers The details of the collected lecture database are listed in Table 1 The best way to improve classroom speech recognition accuracy is to use a fairly large amount of speech data with transcriptions as training sets for the acoustic and language models However as already discussed in Introduction no such large corpus for classroom lectures has been available Therefore to construct our models we used the CSJ database which consists of presentation speech data from academic conferences The CSJ data are expected to share similar properties with 
classroom speech to some extent as they are speech data of monologues with specified themes As the initial language model we constructed a trigram model by using 967 academic presentation transcriptions 3M morphemes from CSJ We call this model the baseline model We used ChaSen 17 a Japanese morphological analyzer for preprocessing the text data The recognition vocabulary consisted of 25 000 words which appeared most frequently in the training set We used the Witten Bell method for back off smoothing 18 The initial acoustic model was also constructed from the CSJ data set 953 academic presentations and 1 543 extemporaneous presentations that include both male and female speak 3 Dynamic Language Model Adaptation In UPRISE the temporal information of when each slide is shown in the class is detected automatically and recorded 16 This function enable us to investigate adaptation methods by using temporal information from a class Here we propose a dynamic adaptation method for language modeling that uses slides 
with this temporal information By using slide information the proposed method changes the language model parameters so that they are fitted to the technical terms that characterize the corresponding part of the lecture To update the language model n gram counts of the adaptation data extracted from the slides are added to those of the baseline training data with a weighting coefficient The vocabulary of the adapted language model consists of words from the original training data and the adaptation data The detailed algorithm of the proposed adaptation method is as follows First Global Adaptation GA is conducted in which the text data of all slides used in a course are used as adaptation data The frequency FG Ni of each n gram Ni is calculated as follows FG Ni F Ni w1 G Ni 1 where F Ni is the frequency of n gram Ni that appear in the baseline training data G Ni is the frequency of n gram Ni that appear in the adaptation data and w1 is a weight coefficient that should be optimized experimentally Next in Local 
Adaptation LA the language model from GA is further adapted locally to each slide The frequency FL Ni of each n gram Ni corresponding to each slide is calculated as follows FL Ni FG Ni w2 L Ni 2 where L Ni is the number of appearances of n gram Ni in each slide and w2 is a weighting factor A new language model 2350 Table 2 Results of speech recognition and keyword detection achieved using the baseline language model Word accuracy 64 Word Accuracy 49 0 62 F measure 48 5 F measure LEC1 LEC2 LEC3 LEC4 Avg Word acc 39 2 37 3 57 7 60 1 47 4 Recall 37 2 36 1 56 4 49 6 45 0 Precision 59 1 66 2 82 4 71 9 69 1 F measure 45 7 46 7 67 0 58 7 54 5 60 48 0 58 56 47 5 Table 3 Results of speech recognition and keyword detection by Global Adaptation GA LEC1 LEC2 LEC3 LEC4 Avg Word acc 41 1 38 7 59 8 61 4 49 0 Recall 50 6 49 8 65 8 57 1 55 5 Precision 65 9 71 6 83 1 74 4 72 8 F measure 57 2 58 7 73 4 64 6 63 0 54 0 10 20 w1 30 40 50 0 10 20 w1 30 40 50 Figure 2 Relationship between the weight w1 and recognition performance 
Table 4 The effect of the expansion of the vocabulary by Global Adaptation GA LM Baseline Baseline GA Dictionary Baseline Extended Extended Acc 47 4 47 8 49 0 FM 54 5 55 2 63 0 PP 167 5 167 5 161 7 OOV 4 3 3 4 3 4 ers We used 25 dimensional acoustic features 12 dimension MFCC 12 MFCC and power Cepstral Mean Subtraction CMS was used to filter each utterance We used leftto right 3 state triphone HMMs which have 3000 states and 16 mixtures per state and used HTK 19 as the training tool We also conducted unsupervised adaptation by using the MLLR method 20 As adaptation data we used the opening 10 minutes speech of collected classes The number of regression classes was 64 MLLR adaptation is expected to change the acoustic model parameters to fit the speaker or noises in the test set We used Julius an open source real time large vocabulary speech recognition engine as the decoder 21 We evaluated the recognition results in terms of word accuracy We evaluated the keyword detection in terms of recall and precision 
rates which are important for evaluating retrieval performance In the evaluation of keyword detection we compared the beginning time t1 of a keyword in the recognition result with that in the reference t2 When the difference between t1 and t2 was less than 500 ms we assumed that the keyword was correctly recognized 4 2 Results First we investigated the initial language model and the initial acoustic model performance The average recognition accuracy was 41 9 the keyword recall was 36 8 and the keyword precision was 62 6 The recognition results achieved by using the acoustic model adapted by unsupervised MLLR are given in Table 2 The average recognition accuracy was 47 4 the keyword recall was 45 0 and precision was 69 1 In all the following experiments we used the acoustic models adapted by MLLR Next we investigated the efficiency of Global Adaptation GA The relationship between the weight w1 and the recognition rate is shown in Figure 2 We found that the recognition rate was not much influenced by the 
change of w1 value According to these results we set the value of w1 to 20 The results obtained using GA are presented in Table 3 The average word error rate was reduced by 3 0 The average error rates for recall and precision of keyword detection were also reduced by 19 1 and 12 0 respectively These results indicate that GA was effective for speech recognition and keyword detection In GA the words included in the slides were added to the recognition dictionary In order to investigate the effect of vocabulary expansion we performed an experiment in which we used the baseline language model and the extended dictionary The recognition results the perplexities and the OOV rates are shown in Table 4 The obtained results indicate that the effectiveness of GA was the result of both the expansion of the vocabulary and the change of the statistical parameters of the language models Finally we evaluated the effectiveness of Local Adaptation LA The relationship between the weight w2 and recognition performance is 
presented in Figure 3 The recognition accuracy was not much influenced by w2 so we set it to 9 000 The recognition results are listed in Table 5 While the recognition accuracy was not much changed from the results obtained by GA the keyword detection rate was significantly improved The keyword F measure error rate was reduced by 2 4 on average Thus we confirmed the effectiveness of the proposed dynamic language model adaptation method for keyword detection The results shown in Table 5 indicate that the effectiveness of LA was not the same for all lectures While in LEC1 and LEC2 the error rate of keyword F measure was reduced by 3 8 that was only reduced by 1 2 in LEC3 and LEC4 In the case of LEC3 this difference might be due to the fact that the times of slide presentations was particularly short compared to those in the other lectures Table 1 When slides were not presented the GA model was used to recognize speech This means that only a small amount of speech in LEC3 was recognized with the LA model and 
thus LA did not have much impact as it did in the case of LEC1 and LEC2 In LEC4 the slides contain a relatively small amount of words or keywords Table 1 This might be the reason that the effectiveness of LA for LEC4 is less than for the other lectures 2351 Table 5 Results of speech recognition and keyword detection by Local Adaptation LA LEC1 LEC2 LEC3 LEC4 Avg Word acc 41 1 38 6 59 4 61 2 48 8 Recall 52 5 51 9 66 3 57 7 56 7 Precision 66 3 73 1 83 3 74 3 73 1 F measure 58 6 60 7 73 8 65 0 63 9 versity The Informedia Project http www informedia cs cmu edu 5 G D Abowd Classroom 2000 an experiment with the instrumentation of a living educational environment IBM Systems Journal vol 38 no 4 pp 508 530 1999 6 H Okamoto W Nakano T Kobayashi S Naoi H Yokota K Iwano and S Furui Unified presentation contents retrieval using voice information Proc DEWS2006 6c o1 2006 in Japanese 7 I Trancoso R Nunes and L Neves Recognition of classroom lectures in European Portuguese Proc INTERSPEECH 2006 ICSLP pp 281 284 2006 8 L 
Lamel G Adda E Bilinski and J L Gauvain Transcribing lectures and seminars Proc INTERSPEECH 2005 pp 1675 1660 2005 9 J Glass T Hazen I Hetherington and C Wang Analysis and processing of lecture audio data Preliminary investigations Proc Human Language Technology NAACL Speech Indexing Workshop Boston 2004 Word accuracy 49 0 64 0 F measure Word accuracy F measure 62 0 48 5 60 0 48 0 58 0 47 5 56 0 0 2000 4000 6000 w2 8000 10000 0 2000 4000 6000 w2 8000 10000 10 K Maekawa H Koiso S Furui and H Isahara Spontaneous speech corpus of Japanese Proc LREC2000 Athens Greece vol 2 pp 947 952 2000 11 The Corpus of Spontaneous Japanese National Institute for Japanese Language http www2 kokken go jp csj public 12 L Lamel F Schiel A Fourcin J Mariani and H Tillmann The Translanguage English Database TED Proc ICSLP vol 4 pp 1795 1798 2004 13 S Furui Recent progress in corpus based spontaneous speech recognition IEICE Transactions on Information and Systems vol E88 D no 3 pp 366 375 2005 14 S Togashi N Kitaoka and S Nakagawa 
Speech recognition of lecture documents with LM adapted by lecture slides Proc Acoustical Society of Japan Spring Meeting 1 P 24 pp 191 192 2006 in Japanese 15 H Yokota T Kobayashi H Okamoto and W Nakano Unified contents retrieval from an academic repository Proc International Symposium on Large scale Knowledge Resources LKR2006 Tokyo Japan pp 41 46 2006 16 N Ozawa H Takebe Y Katsuyama S Naoi and H Yokota Slide identification for lecture movies by matching characters and images Proc SPIE vol 5296 10 Document Recognition and Retrieval XI pp 74 81 2004 17 ChaSen ver 2 2 3 and ipadic ver 2 4 4 http chasen naist jp hiki ChaSen 18 I H Witten and T C Bell The zero frequency problem Estimating the probabilities of novel events in adaptive text compression IEEE Transactions on Information Theory vol 37 no 4 pp 1085 1094 1991 19 HMM Tool Kit HTK ver 3 2 http htk eng cam ac uk 20 C J Leggetter and P C Woodland Maximum likelihood linear regression for speaker adaptation of continuous density hidden Markov models 
Computer Speech and Language vol 9 no 2 pp 171 185 1995 21 Julius ver 3 5 http julius sourceforge jp en julius html Figure 3 Relationship between the weight w2 and recognition performance w1 20 5 Conclusions and Future Work We have proposed a dynamic adaptation method of language modeling that exploits slide information from lecture speech with temporal information We evaluated the proposed method with the speech data of four lecture courses in Japanese The results showed the effectiveness of our method especially for keyword detection In future we need to collect more lecture data because the size of our present database is still small for fair evaluation In addition we must investigate more efficient use of slides for adaptation We also plan to extend our framework to other content such as lecture speech in meetings or TV broadcasts 6 Acknowledgements This study was supported by the 21st COE program Frame work for Systemization and Application of Large scale Knowledge Resources 7 References 1 H Yokota T 
Kobayashi T Muraki and S Naoi UPRISE Unified Presentation Slide Retrieval by Impression Search Engine IEICE Transactions on Information and Systems vol E87 D no 2 pp 307 406 2004 2 A Fujii K Itou and T Ishikawa LODEM A system for on demand video lectures Speech Communication 48 pp 516 531 2006 3 R 2352 
29	a	Style Topic Language Model Adaptation Using HMM LDA Bo June Paul Hsu James Glass MIT Computer Science and Artificial Intelligence Laboratory 32 Vassar Street Cambridge MA 02139 USA bohsu glass mit edu Abstract Adapting language models across styles and topics such as for lecture transcription involves combining generic style models with topic specific content relevant to the target document In this work we investigate the use of the Hidden Markov Model with Latent Dirichlet Allocation HMM LDA to obtain syntactic state and semantic topic assignments to word instances in the training corpus From these context dependent labels we construct style and topic models that better model the target document and extend the traditional bag of words topic models to n grams Experiments with static model interpolation yielded a perplexity and relative word error rate WER reduction of 7 1 and 2 1 respectively over an adapted trigram baseline Adaptive interpolation of mixture components further reduced perplexity by 9 5 and 
WER by a modest 0 3 1 Introduction With the rapid growth of audio visual materials available over the web effective language modeling of the diverse content both in style and topic becomes essential for efficient access and management of this information As a prime example successful language modeling for academic lectures not only enables the initial transcription via automatic speech recognition but also assists educators and students in the creation and navigation of these materials through annotation retrieval summarization and even translation of the embedded content Compared with other types of audio content lecture speech often exhibits a high degree of spontaneity and focuses on narrow topics with specific terminology Furui 2003 Glass et al 2004 Unfortunately training corpora available for language modeling rarely match the target lecture in both style and topic While transcripts from other lectures better match the style of the target lecture than written text it is often difficult to find 
transcripts on the target topic On the other hand although topic specific vocabulary can be gleaned from related text materials such as the textbook and lecture slides written language is a poor predictor of how words are actually spoken Furthermore given that the precise topic of a target lecture is often unknown a priori and may even shift over time it is generally difficult to identify topically related documents Thus an effective language model LM need to not only account for the casual speaking style of lectures but also accommodate the topic specific vocabulary of the subject matter Moreover the ability of the language model to dynamically adapt over the course of the lecture could prove extremely useful for both increasing transcription accuracy as well as providing evidence for lecture segmentation and information retrieval In this paper we investigate the application of the syntactic state and semantic topic assignments from the Hidden Markov Model with Latent Dirichlet Allocation model to the 
problem of language modeling We explore the use of these context dependent labels to identify style and learn topics from both a large number of spoken lectures as well as written text By dynamically interpolating lecture style models with topicspecific models we obtain language models that better describe the subtopic structure within a lecture Initial experiments demonstrate a 16 1 perplexity reduction and a 2 4 WER reduction over an adapted trigram baseline To be presented at EMNLP 2006 Sydney Australia July In the following sections we first summarize related research on adaptive and topic mixture language models and describe previous work on the HMM LDA model We then examine the ability of the model to learn syntactic classes as well as topics from textbook materials and lecture transcripts Next we describe a variety of language model experiments we performed to combine style and topic models constructed from the state and topic labels with conventional trigram models trained from both spoken and 
written materials We also demonstrate the use of the combined model in an on line adaptive mode Finally we summarize the results of this research and suggest future opportunities for related modeling techniques in spoken lecture and other content processing research 2 Adaptive and Topic Mixture LMs The concept of adaptive and topic mixture language models has been previously explored by many researchers Adaptive language modeling exploits the property that words appearing earlier in a document are likely to appear again Cache language models Kuhn and De Mori 1990 Clarkson and Robinson 1997 leverage this observation and increase the probability of previously observed words in a document when predicting the next word By interpolating with a conditional trigram cache model Goodman 2001 demonstrated up to 34 decrease in perplexity over a trigram baseline for small training sets The cache intuition has been extended by attempting to increase the probability of unobserved but topically related words Specifically 
given a mixture model with topic specific components we can increase the mixture weights of the topics corresponding to previously observed words to better predict the next word Some of the early work in this area used a maximum entropy language model framework to trigger increases in likelihood of related words Lau et al 1993 Rosenfeld 1996 A variety of methods has been used to explore topic mixture models To model a mixture of topics within a document the sentence mixture model Iyer and Ostendorf 1999 builds multiple topic models from clusters of training sentences and defines the probability of a target sentence as a weighted combination of its probability under each topic model Latent Semantic Analysis LSA has been used to cluster topically related words and has demonstrated significant reduc tion in perplexity and word error rate Bellegarda 2000 Probabilistic LSA PLSA has been used to decompose documents into component word distributions and create unigram topic models from these distributions Gildea 
and Hofmann 1999 demonstrated noticeable perplexity reduction via dynamic combination of these unigram topic models with a generic trigram model To identify topics from an unlabeled corpus Blei et al 2003 extends PLSA with the Latent Dirichlet Allocation LDA model that describes each document in a corpus as generated from a mixture of topics each characterized by a word unigram distribution Hidden Markov Model with LDA HMM LDA Griffiths et al 2004 further extends this topic mixture model to separate syntactic words from content words whose distributions depend primarily on local context and document topic respectively In the specific area of lecture processing previous work in language model adaptation has primarily focused on customizing a fixed n gram language model for each lecture by combining ngram statistics from general conversational speech other lectures textbooks and other resources related to the target lecture Nanjo and Kawahara 2002 2004 Leeuwis et al 2003 Park et al 2005 Most of the previous 
work on topic mixture models focuses on in domain adaptation using large amounts of matched training data However most if not all of the data available to train a lecture language model are either cross domain or cross style Furthermore although adaptive models have been shown to yield significant perplexity reduction on clean transcripts the improvements tend to diminish when working with speech recognizer hypotheses with high WER In this work we apply the concept of dynamic topic adaptation to the lecture transcription task Unlike previous work we first construct a style model and a topic domain model using the classification of word instances into syntactic states and topics provided by HMM LDA Furthermore we leverage the context dependent labels to extend topic models from unigrams to ngrams allowing for better prediction of transitions involving topic words Note that although this work focuses on the use of HMM LDA to generate the state and topic labels any method that yields such labels suffices for 
the purpose of the language modeling experiments The following section describes the HMM LDA framework in more detail 3 3 1 HMM LDA Latent Dirichlet Allocation 3 2 Hidden Markov Model with LDA Discrete Principal Component Analysis describes a family of models that decompose a set of feature vectors into its principal components Buntine and Jakulin 2005 Describing feature vectors via their components reduces the number of parameters required to model the data hence improving the quality of the estimated parameters when given limited training data LSA PLSA and LDA are all examples from this family Given a predefined number of desired components LSA models feature vectors by finding a set of orthonormal components that maximize the variance using singular value decomposition Deerwester et al 1990 Unfortunately the component vectors may contain non interpretable negative values when working with word occurrence counts as feature vectors PLSA eliminates this problem by using non negative matrix factorization to 
model each document as a weighted combination of a set of non negative feature vectors Hofmann 1999 However because the number of parameters grows linearly with the number of documents the model is prone to overfitting Furthermore because each training document has its own set of topic weight parameters PLSA does not provide a generative framework for describing the probability of an unseen document Blei et al 2003 To address the shortcomings of PLSA Blei et al 2003 introduced the LDA model which further imposes a Dirichlet distribution on the topic mixture weights corresponding to the documents in the corpus With the number of model parameters dependent only on the number of topic mixtures and vocabulary size LDA is less prone to overfitting and is capable of estimating the probability of unobserved test documents Empirically LDA has been shown to outperform PLSA in corpus perplexity collaborative filtering and text classification experiments Blei et al 2003 Various extensions to the basic LDA model have 
since been proposed The Author Topic model adds an additional dependency on the author s to the topic mixture weights of each document Rosen Zvi et al 2005 The Hierarchical Dirichlet Process is a nonparametric model that generalizes distribution parameter modeling to multiple levels Without having to estimate the number of mixture components this model has been shown to match the best result from LDA on a document modeling task Teh et al 2004 HMM LDA model proposed by Griffiths et al 2004 combines the HMM and LDA models to separate syntactic words with local dependencies from topic dependent content words without requiring any labeled data Similar to HMM based part of speech taggers HMM LDA maps each word in the document to a hidden syntactic state Each state generates words according to a unigram distribution except the special topic state where words are modeled by document specific mixtures of topic distributions as in LDA Figure 1 describes this generative process in more detail For each document d in 
the corpus 1 Draw topic weights d from Dirichlet 2 For each word wi in document d a Draw topic zi from Multinomia l d b Draw state si from Multinomial si 1 c Draw word wi from Multinomia l zi si s topic Multinomia l si z1 otherwise s1 s2 w1 w2 d z2 zn wn sn D Figure 1 Generative framework and graphical model representation of HMM LDA The number of states and topics are pre specified The topic mixture for each document is modeled with a Dirichlet distribution Each word wi in the nword document is generated from its hidden state si or hidden topic zi if si is the special topic state Unlike vocabulary selection techniques that separate domain independent words from topicspecific keywords using word collocation statistics HMM LDA classifies each word instance according to its context Thus an instance of the word return may be assigned to a syntactic state in to return a but classified as a topic keyword in expected return for By labeling each word in the training set with its syntactic state and mixture topic 
HMM LDA not only separates stylistic words from content words in a context dependent manner but also decomposes the corpus into a set of topic word distributions This form of soft context dependent classifica tion has many potential uses for language modeling topic segmentation and indexing 3 3 Training To train an HMM LDA model we employ the MATLAB Topic Modeling Toolbox 1 3 Griffiths and Steyvers 2004 Griffiths et al 2004 This particular implementation performs Gibbs sampling a form of Markov chain Monte Carlo MCMC to estimate the optimal model parameters fitted to the training data Specifically the algorithm creates a Markov chain whose stationary distribution matches the expected distribution of the state and topic labels for each word in the training corpus Starting from random labels Gibbs sampling sequentially samples the label for each hidden variable conditioned on the current value of all other variables After a sufficient number of iterations the Markov chain converges to the stationary 
distribution We can easily compute the posterior word distribution for each state and topic from a single sample by averaging over the label counts and prior parameters With a sufficiently large training set we will have enough words assigned to each state and topic to yield a reasonable approximation to the underlying distribution In the following sections we examine the application of models derived from the HMM LDA labels to the task of spoken lecture transcription and explore techniques on adaptive topic modeling to construct a better lecture language model and will be referred to as the Lectures dataset To supplement the out of domain lecture transcripts with topic specific textual resources we added the CS course textbook Textbook as additional training data for learning the target topics To create topic cohesive documents the textbook is divided at every section heading to form 271 documents Next the text is heuristically segmented at sentence like boundaries and normalized into the words 
corresponding to the spoken form of the text Table 1 summarizes the data used in this evaluation Dataset Lectures Textbook CS Dev CS Test Documents 150 271 10 10 Sentences 58 626 6 762 4 102 3 595 Vocabulary 25 654 4 686 3 285 3 357 Words 1 390 039 131 280 93 348 87 518 Table 1 Summary of evaluation datasets In the following analysis we ran the Gibbs sampler against the Lectures dataset for a total of 2800 iterations computing a model every 10 iterations and took the model with the lowest perplexity as the final model We built the model with 20 states and 100 topics based on preliminary experiments We also trained an HMMLDA model on the Textbook dataset using the same model parameters We ran the sampler for a total of 2000 iterations computing the perplexity every 100 iterations Again we selected the lowest perplexity model as the final model 4 1 Semantic Topics 4 HMM LDA Analysis Our language modeling experiments have been conducted on high fidelity transcripts of approximately 168 hours of lectures from 
three undergraduate subjects in math physics and computer science CS as well as 79 seminars covering a wide range of topics Glass et al 2004 For evaluation we withheld the set of 20 CS lectures and used the first 10 lectures as a development set and the last 10 lectures for the test set The remainder of these data was used for training 1 center world and ideas new technology innovation community place building 2 work research right people computing network system information software computers 3 rights human U S government international countries president world support 4 system things robot systems work example person robots learning machine 5 laugh her children book Cambridge books street city library brother HMM LDA extracts words whose distributions vary across documents and clusters them into a set of components In Figure 2 we list the top 10 words from a random selection of 10 topics computed from the Lectures dataset As shown the words assigned to the LDA topic state are representative of content 
words and are grouped into broad semantic topics For example topic 4 8 and 9 correspond to machine learning linear algebra and magnetism respectively Since the Lectures dataset consists of speech transcripts with disfluencies it is interesting to 6 partial memory ah brain animal okay eye synaptic receptors mouse 7 class people tax wealth social American power world unintelligible society 8 basis v eh vector matrix transformation linear eight output t 9 magnetic current field loop surface direction e law flux m 10 light red water colors white angle blue here rainbow sun Figure 2 The top 10 words from 10 randomly selected topics computed from the Lectures dataset observe that laugh is the top word in a topic corresponding to childhood memories Cursory examination of the data suggests that the speakers talking about children tend to laugh more during the lecture Although it may not be desirable to capture speaker idiosyncrasies in the topic mixtures HMM LDA has clearly demonstrated its ability to capture 
distinctive semantic topics in a corpus By leveraging all documents in the corpus the model yields smoother topic word distributions that are less vulnerable to overfitting Since HMM LDA labels the state and topic of each word in the training corpus we can also visualize the results by color coding the words by their topic assignments Figure 3 shows a color coded excerpt from a topically coherent paragraph in the Textbook dataset Notice how most of the content words uppercase are assigned to the same topic color Furthermore of the 7 instances of the words and and or underlined 6 are correctly classified as syntactic or topic words demonstrating the contextdependent labeling capabilities of the HMMLDA model Moreover from these labels we can identify multi word topic key phrases e g output signals input signal and gate in addition to standalone keywords an observation we will leverage later on with n gram topic models We draw an INVERTER SYMBOLICALLY as in Figure 3 24 An AND GATE also shown in Figure 3 24 is a 
PRIMITIVE FUNCTION box with two INPUTS and ONE OUTPUT It drives its OUTPUT SIGNAL to a value that is the LOGICAL AND of the INPUTS That is if both of its INPUT SIGNALS BECOME 1 Then ONE and GATE DELAY time later the AND GATE will force its OUTPUT SIGNAL TO be 1 otherwise the OUTPUT will be 0 An OR GATE is a SIMILAR two INPUT PRIMITIVE FUNCTION box that drives its OUTPUT SIGNAL to a value that is the LOGICAL OR of the INPUTS That is the OUTPUT will BECOME 1 if at least ONE of the INPUT SIGNALS is 1 otherwise the OUTPUT will BECOME 0 4 2 Syntactic States Since the syntactic states are shared across all documents we expect words associated with the syntactic states when applying HMM LDA to the Lectures dataset to reflect the lecture style vocabulary In Figure 4 we list the top 10 words from each of the 19 syntactic states state 20 is the topic state Note that each state plays a clear syntactic role For example state 2 contains prepositions while state 7 contains verbs Since the model is trained on 
transcriptions of spontaneous speech hesitation disfluencies uh um partial are all grouped in state 3 along with other words so if okay that frequently indicate hesitation While many of these hesitation words are conjunctions the words in state 6 show that most conjunctions are actually assigned to a different state representing different syntactic behavior from hesitations As demonstrated with spontaneous speech HMM LDA yields syntactic states that have a good correspondence to part ofspeech labels without requiring any labeled training data 4 3 Discussions Figure 3 Color coded excerpt from the Textbook dataset showing the context dependent topic labels Syntactic words appear black in lowercase Topic words are shown in uppercase with their respective topic colors All instances of the words and and or are underlined Although MCMC techniques converge to the global stationary distribution we cannot guarantee convergence from observation of the perplexity alone Unlike EM algorithms random sampling may actually 
temporarily decrease the model likelihood Thus in the above analysis the number of iterations was chosen to be at least double the point at which the perplexity first appeared to converge In addition to the number of iterations the choice of the number of states and topics as well as the values of the hyper parameters on the Dirichlet prior also impact the quality and effectiveness of the resulting model Ideally we run the algorithm with different combinations of the parameter values and perform model selection to choose the model with the best complexitypenalized likelihood However given finite computing resources this approach is often im 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 so and it s very have the know I is a way it two going that can to it of uh but not more see you an this one what just be this are time doing will you in if or that s out little a do we was some thing that three one how would longer want for um think they because has I m much had that one lot there hundred looking where don 
t doesn t up on just them good as m sort get these with partial go let were no question which when could never different now that kind t done like my get let s goes there s that in he if do go at than then where point five able uh me our say he had two here why just physically got from that ll okay thank comes we re about case course d coming which need your make I ll any me important by anybody s try well look people which means also here idea who long years talking those about this as should as I d says you re all another problem they with but take is four trying because may as take their Figure 4 The top 10 words from the 19 syntactic states computed from the Lectures dataset practical As an alternative for future work we would like to perform Gibbs sampling on the hyper parameters Griffiths et al 2004 and apply the Dirichlet process to estimate the number of states and topics Teh et al 2004 Despite the suboptimal choice of parameters and potential lack of convergence the labels derived from HMM LDA are 
still effective for language modeling applications as described next the Lectures dataset L the combined model L S achieves a 3 6 perplexity reduction and 1 0 WER reduction over L as shown in Table 2 Without introducing topic specific training data we can already improve the generic lecture LM performance using the HMM LDA labels s for the SPATIAL MEMORY s unigrams for the spatial memory s bigrams s for for the the spatial spatial memory memory s trigrams s s for s for the for the spatial the spatial memory spatial memory s 5 Language Modeling Experiments To evaluate the effectiveness of models derived from the separation of syntax from content we performed experiments that compare the perplexities and WERs of various model combinations For a baseline we used an adapted model L T that linearly interpolates trigram models trained on the Lectures L and Textbook T datasets In all models all interpolation weights and additional parameters are tuned on a development set consisting of the first half of the CS 
lectures and tested on the second half Unless otherwise noted modified Kneser Ney discounting Chen and Goodman 1998 is applied with the respective training set vocabulary using the SRILM Toolkit Stolcke 2002 To compute the word error rates associated with a specific language model we used a speaker independent speech recognizer Glass 2003 The lectures were pre segmented into utterances by forced alignment of the reference transcription 5 1 Lecture Style Figure 5 Style model n grams Topic words in the utterance are in uppercase 5 2 Topic Domain In general an n gram model trained on a limited set of topic specific documents tends to overemphasize words from the observed topics instead of evenly distributing weights over all potential topics Specifically given the list of words following an n gram context we would like to deemphasize the observed occurrences of topic words and ideally redistribute these counts to all potential topic words As an approximation we can build such a topic deemphasized style trigram 
model S by using counts of only n gram sequences that do not end on a topic word smoothed over the Lectures vocabulary Figure 5 shows the n grams corresponding to an utterance used to build the style trigram model Note that the counts of topic to style word transitions are not altered as these probabilities are mostly independent of the observed topic distribution By interpolating the style model S from above with the smoothed trigram model based on Unlike Lectures the Textbook dataset contains content words relevant to the target lectures but in a mismatched style Commonly the Textbook trigram model is interpolated with the generic model to improve the probability estimates of the transitions involving topic words The interpolation weight is chosen to best fit the probabilities of these n gram sequences while minimizing the mismatch in style However with only one parameter all n gram contexts must share the same mixture weight Because transitions from contexts containing topic words are rarely observed in 
the off topic Lectures the Textbook model T should ideally have higher weight in these contexts than contexts that are more equally observed in both datasets One heuristic approach for adjusting the weight in these contexts is to build a topicdomain trigram model D from the Textbook ngram counts with Witten Bell smoothing Chen and Goodman 1998 where we emphasize the sequences containing a topic word in the context by doubling their counts In effect this reduces the smoothing on words following topic contexts with respect to lower order models without significantly affecting the transitions from non topic words Figure 6 shows the adjusted counts for an utterance used to build the domain trigram model s HUFFMAN CODE can be represented as a BINARY TREE unigrams huffman code can be represented as binary tree bigrams s huffman huffman code Figure 6 Domain model n grams Topic words in the utterance are in uppercase Empirically interpolating the lectures textbook and style models with the domain model L T S D 
further decreases the perplexity by 1 4 and WER by 0 3 over L T S validating our intuition Overall the addition of the style and domain models reduces perplexity and WER by a noticeable 7 1 and 2 1 respectively as shown in Table 2 Model L Lectures Trigram T Textbook Trigram S Style Trigram D Domain Trigram L S L T Baseline L T S L T S D L T S D Huffman tree Monte Carlo time segment assoc key relative frequency rand update the agenda the table relative frequencies random numbers segment time local table the tree trials remaining current time a table one hundred trials passed first agenda of records Figure 7 Sample of n grams from select topics 5 4 Topic Mixtures Word Error Rate Development Test 49 5 0 0 50 2 0 0 49 2 Table 2 Perplexity top and WER bottom performance of various model combinations Relative reduction is shown in parentheses 5 3 Textbook Topics In addition to identifying content words HMMLDA also assigns words to a topic based on their distribution across documents Thus we can apply HMM LDA with 
100 topics to the Textbook dataset to identify representative words and their associated contexts for each topic From these labels we can build unsmoothed trigram language models Topic100 for each topic from the counts of observed n gram sequences that end in a word assigned to the respective topic Figure 7 shows a sample of the word n grams identified via this approach for a few topics Note that some of the n grams are key phrases for the topic while others contain a mixture of syntactic and topic words Unlike bag of words models that only identify the unigram distribution for each topic the use of context dependent labels enables the construction of n gram topic models that not only characterize the frequencies of topic words but also describe the transition contexts leading up to these words Since each target lecture generally only covers a subset of the available topics it will be ideal to identify the specific topics corresponding to a target lecture and assign those topic models more weight in a 
linearly interpolated mixture model As an ideal case we performed a cheating experiment to measure the best performance of a statically interpolated topic mixture model L T S D Topic100 where we tuned the mixture weights of all mixture components including the lectures textbook style domain and the 100 individual topic trigram models on individual target lectures Table 2 shows that by weighting the component models appropriately we can reduce the perplexity and WER by an additional 7 9 and 0 7 respectively over the L T S D model even with simple linear interpolation for model combination To gain further insight into the topic mixture model we examine the breakdown of the normalized topic weights for a specific lecture As shown in Figure 8 of the 100 topic models 15 of them account for over 90 of the total weight Thus lectures tend to show a significant topic skew which topic adaptation approaches can model effectively 0 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 1 Figure 8 Topic mixture weight breakdown 5 5 Topic 
Adaptation Unfortunately since different lectures cover different topics we generally cannot tune the topic mixture weights ahead of time One approach without any a priori knowledge of the target lecture is to adaptively estimate the optimal mixture weights as we process the lecture Gildea and Hofmann 1999 However since the topic distribution shifts over a long lecture modeling a lecture as an interpolation of components with fixed weights may not be the most optimal Instead we employ an exponential decay strategy where we update the current mixture distribution by linearly interpolating it with the posterior topic distribution given the current word Specifically applying Bayes rule the probability of topic t generating the current word w is given by P t w P w t P t t P w t P t To achieve the exponential decay we update the topic distribution after each word according to Pi 1 t Figure 9 Adaptation of topic model weights on manual and ASR transcription of a single lecture T12 stream s streams integers series 
prime filter delayed interleave infinite T35 pairs i j k pair s integers sum queens t T98 sequence enumerate accumulate map interval filter sequences operations odd nil T99 of see and in for vs register data as make Figure 10 Top 10 words from select Textbook topics appearing in Figure 9 6 Summary and Conclusions In this paper we have shown how to leverage context dependent state and topic labels such as the ones generated by the HMM LDA model to construct better language models for lecture transcription and extend topic models beyond traditional unigrams Although the WER of the top recognizer hypotheses exceeds 45 by dynamically updating the mixture weights to model the topic substructure within individual lectures we are able to reduce the test set perplexity and WER by over 16 and 2 4 respectively relative to the combined Lectures and Textbook L T baseline Although we primarily focused on lecture transcription in this work the techniques extend to language modeling scenarios where exactly matched training 
data are often limited or nonexistent Instead we have to rely on appropriate combination of models derived from partially matched data HMM LDA and related techniques show great promise for finding structure in unlabeled data from which we can build more sophisticated models The experiments in this paper combine models primarily through simple linear interpolation As motivated in section 5 2 allowing for contextdependent interpolation weights based on topic labels may yield significant improvement for both perplexity and WER Thus in future work we would like to study algorithms for automatically learning appropriate context dependent interpolation weights Furthermore we hope to improve the convergence properties of the dynamic adaptation scheme at the start of lectures and across topic transitions Lastly we would like to extend the LDA framework to support speaker specific adaptation and apply the resulting topic distributions to lecture segmentation ment based Speech Recognition Computer Speech and Language 
17 137 152 J Glass T J Hazen L Hetherington and C Wang 2004 Analysis and Processing of Lecture Audio Data Preliminary Investigations In Proc HLTNAACL Workshop on Interdisciplinary Approaches to Speech Indexing and Retrieval 9 12 J Goodman 2001 A Bit of Progress in Language Modeling Extended Version Technical Report Microsoft Research T Griffiths and M Steyvers 2004 Finding Scientific Topics In Proc National Academy of Science 101 Suppl 1 5228 5235 T Griffiths M Steyvers D Blei and J Tenenbaum 2004 Integrating Topics and Syntax Adv in Neural Information Processing Systems 17 537 544 R Iyer and M Ostendorf 1999 Modeling Long Distance Dependence in Language Topic Mixtures Versus Dynamic Cache In IEEE Transactions on Speech and Audio Processing 7 30 39 R Kuhn and R De Mori 1990 A Cache Based Natural Language Model for Speech Recognition In IEEE Transactions on Pattern Analysis and Machine Intelligence 12 570 583 R Lau R Rosenfeld S Roukos 1993 TriggerBased Language Models a Maximum Entropy Approach In Proc 
ICASSP E Leeuwis M Federico and M Cettolo 2003 Language Modeling and Transcription of the TED Corpus Lectures In Proc ICASSP H Nanjo and T Kawahara 2002 Unsupervised Language Model Adaptation for Lecture Speech Recognition In Proc ICSLP H Nanjo and T Kawahara 2004 Language Model and Speaking Rate Adaptation for Spontaneous Presentation Speech Recognition In IEEE Trans SAP 12 4 391 400 A Park T Hazen and J Glass 2005 Automatic Processing of Audio Lectures for Information Retrieval Vocabulary Selection and Language Modeling In Proc ICASSP M Rosen Zvi T Griffiths M Steyvers and P Smyth 2004 The Author Topic Model for Authors and Documents 20th Conference on Uncertainty in Artificial Intelligence R Rosenfeld 1996 A Maximum Entropy Approach to Adaptive Statistical Language Modeling Computer Speech and Language 10 187 228 A Stolcke 2002 Acknowledgements We would like to thank the anonymous reviewers for their useful comments and feedback Support for this research was provided in part by the National Science 
Foundation under grant IIS 0415865 Any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF Reference Y Akita and T Kawahara 2004 Language Model Adaptation Based on PLSA of Topics and Speakers In Proc ICSLP J Bellegarda 2000 Exploiting Latent Semantic Information in Statistical Language Modeling In Proc IEEE 88 8 1279 1296 D Blei A Ng and M Jordan 1993 Latent Dirichlet Allocation Journal of Machine Learning Research 3 993 1022 W Buntine and A Jakulin 2005 Discrete Principal Component Analysis Technical Report Helsinki Institute for Information Technology S Chen and J Goodman 1996 An Empirical Study of Smoothing Techniques for Language Modeling In Proc ACL 310 318 P Clarkson and A Robinson 1997 Language Model Adaptation Using Mixtures and an Exponentially Decaying Cache In Proc ICASSP S Deerwester S Dumais G Furnas T Landauer R Harshman 1990 Indexing by Latent Semantic Analysis Journal of the American 
Society for Information Science 41 6 391 407 S Furui 2003 Recent Advances in Spontaneous Speech Recognition and Understanding In Proc IEEE Workshop on Spontaneous Speech Proc and Rec 1 6 D Gildea and T Hofmann 1999 Topic Based Language Models Using EM In Proc Eurospeech J Glass 2003 A Probabilistic Framework for Seg 
30	a	J Am Acad Audiol 17 The Effects of Speech and Speechlike Maskers on Unaided and Aided Speech Recognition in Persons with Hearing Loss Benjamin W Y Hornsby Todd A Ricketts Earl E Johnson Abstract Speech understanding in noise is affected by both the energetic and informational masking components of the background noise In addition when the background noise is everyday speech the relative contributions of the energetic and informational masking components to the overall difficulties in understanding speech are unclear This study estimated informational masking effects in conversational speech settings on the speech understanding of persons with and without hearing loss The benefits and limitations of amplification in settings containing both informational and energetic masking components were also explored Speech recognition was assessed in the presence of two types of maskers speech and noise that varied in the amount of informational masking they were expected to produce Persons with hearing loss were tested 
both unaided and aided Study results suggest that background noise consisting of individual talkers results in both informational and energetic masking In addition the benefits of amplification are limited when the background noise contains both informational and energetic masking components Key Words Hearing aids hearing loss informational masking perceptual masking speech intelligibility Abbreviations HI hearing impairment HINT Hearing in Noise Test NH normal hearing NM speech modulated noise masker SM speech masker SNHL sensorineural hearing loss Sumario La Department of Hearing and Speech Science Vanderbilt Bill Wilkerson Center Vanderbilt University Benjamin Hornsby Vanderbilt University Medical Center Dan Maddox Hearing Aid Research Laboratory Department of Hearing and Speech Sciences Room 8310 Medical Center East South Tower 1215 21st Ave South Nashville TN 372328242 Phone 615 936 5132 Fax 615 936 5013 E mail ben hornsby vanderbilt edu Portions of this research were presented at the International 
Hearing Aid Conference Lake Tahoe California in August 2004 This research was supported in part by the Dan Maddox Hearing Aid Research Endowment 432 Hearing Loss Hearing Aids and Informational Masking Hornsby et al trastornos auditivos Palabras Clave Auxiliares auditivos T he negative effects of background noise on the speech understanding of persons with hearing impairment are well documented e g Van Tasell 1993 Humes 2002 Although improving audibility through amplification can result in improvements in speech understanding in noise the benefits are often less than that observed for persons without hearing loss listening under conditions of comparable audibility Plomp 1986 Rankovic 1991 Ching et al 1998 Turner and Henry 2002 Hornsby and Ricketts 2003 This difficulty understanding in noise is due largely to the masking effects of the background competition When listening to speech in a background of noise consisting of other talkers at least two types of masking may occur The most welldescribed masking 
effects originate in the auditory periphery i e cochlea or proximal portions of the auditory nerve and occur when the excitation or neural response in a given frequency range due to the target is less than that produced by the background noise This type of masking has been referred to as energetic masking and its effects on threshold and speech understanding are on average quite predictable at least for persons with normal hearing e g French and Steinberg 1947 American National Standards Institute 1997 When the background noise contains speech however perceptual or informational masking may occur in addition to energetic masking Carhart et al 1969 Carhart et al 1975 Hawley et al 1999 2004 Brungart 2001a Brungart et al 2001 Arbogast et al 2002 2005 Carhart and colleagues 1969 used the term perceptual masking to refer to the additional masking observed when the masker consisted of actual talkers compared to modulated noise More recently the term informational masking which was originally used to describe 
results from certain types of masking experiments with nonspeech stimuli has been used to describe this effect e g Hawley et al 1999 Brungart 2001a Arbogast et al 2005 A precise definition of informational masking is currently a matter of debate In general however informational masking may be described as masking that is not energetic in nature thus implying more central masking effects e g Durlach et al 2003 Informational masking may occur with nonspeech signals when there are high levels of uncertainty regarding the target stimulus or masker Watson et al 1976 Lutfi 1989 Informational masking may also occur with speech signals particularly when the background competition is also speech In cases where informational masking occurs both the target speech and competition are audible to the listener yet the listener has difficulty separating the target and competition due to similarities in their temporal and or semantic structure Brungart 2001a Brungart et al 2001 Although energetic masking may exist in 433 
Journal of the American Academy of Audiology Volume 17 Number 6 2006 these types of situations it does not appear to be the only factor limiting speech understanding given that by definition both the target and masking speech are intelligible Under some experimental conditions informational masking effects in speech tasks have been shown to be quite large Brungart and colleagues Brungart 2001a Brungart et al 2001 estimated informational masking effects in persons with normal hearing using the Coordinate Response Measure test CRM Bolia et al 2000 Brungart 2001b The CRM is a speech recognition test in which the target and masker sentences have the same semantic and syntactic structure making them highly confusable and therefore useful in exploring informational masking effects In one experiment Brungart et al 2001 compared speech understanding using CRM sentences as maskers high informational masking condition to speech understanding when listening to a primarily energetic modulated noise masker with the same 
longterm spectral shape and the long term temporal envelope of the CRM speech maskers When four maskers were used the authors reported an approximate 80 decrease in monaural speech understanding Specifically the performance of persons with normal hearing was evaluated in the presence of a four talker speech masker and a noise modulated with the envelope of a four talker speech masker Brungart et al 2001 In this example however the decrease in speech recognition was due to a combination of energetic and informational masking as is the case when any broadband stimulus is used as both the target and masker In addition because potential differences in the energetic masking properties of the speech and noise maskers exist it is difficult to parse out the magnitude of the informational and energetic masking effects on these results In a series of well controlled studies Arbogast et al 2002 2005 attempted to isolate the effects of informational and energetic maskers by creating sinewave speech targets and maskers 
Sinewave speech is created by modulating pure tones of a given frequency with the amplitude envelope of actual speech filtered at that center frequency In one condition the masker consisted of sinewave speech with frequency components that were in disparate frequency bands than those for the sinewave target speech thus minimizing energetic masking effects In this condition an additional 22 dB of masking was observed in participants with normal hearing compared to that seen with a primarily energetic masker Although informational masking effects can be quite large in some laboratory conditions the magnitude of informational masking that occurs in everyday speech environments is unclear Several factors that are common in everyday speech environments are known to reduce informational masking such as knowledge of the target speaker reduced uncertainty spatial separation of the target and masking speech and differences in vocal characteristics between the target and masking speech e g Hawley et al 1999 2004 
Brungart 2001a Noble and Perrett 2002 Arbogast et al 2002 2005 In addition there is substantial research suggesting that it is the relationship between the amplitude spectrum of the speech and masker i e the signal to noise ratio or SNR that dictates speech understanding in many noise backgrounds not the similarity in semantic and temporal structure of conversational speech and background talkers French and Stienberg 1947 Miller 1947 Dirks and Bower 1969 American National Standards Institute 1997 Thus it is possible that informational masking in everyday environments plays only a small role in the speech understanding difficulties of persons with hearing loss There has been relatively little work exploring the interaction between hearing loss and informational masking Kidd et al 2002 using nonspeech stimuli suggested that hearing loss may limit the ability to perceptually segregate components of complex sounds This is a potentially important factor in our ability to understand speech in a background noise 
containing speech and suggests that the presence of hearing loss may increase susceptibility to informational masking In contrast some recent work suggests that informational masking effects are either relatively unaffected or actually reduced by the presence of hearing loss Micheyl et al 2000 Alexander and Lutfi 2004 Arbogast et al 2005 Alexander and Lutfi 2004 using nonspeech stimuli reported less informational masking in persons with hearing loss They measured thresholds for a 2000 Hz pure tone in the 434 Hearing Loss Hearing Aids and Informational Masking Hornsby et al presence of a masker designed to produce varying amounts of informational masking while limiting energetic masking When tested at equal sound pressure levels SPLs the authors found the participants with hearing impairment HI showed significantly less informational masking than the participants with normal hearing NH In contrast when SPLs were reduced for the NH group and they were retested at sensation levels SLs comparable to the HI group 
informational masking effects were quite similar The authors suggest that masker variance and hence uncertainty is reduced at the lower SL experienced by the HI participants resulting in reduced informational masking Arbogast and colleagues 2005 also reported less informational masking in persons with hearing loss compared to an age matched control group with normal hearing These differences were reduced when differences in SL between the normal hearing and hearing impaired groups were taken into account The influence of SL on informational masking has implications in terms of amplification for persons with hearing loss Recent results suggesting that in some conditions informational masking actually increases as SL increases Alexander and Lutfi 2004 Arbogast et al 2005 implies that the use of hearing aids could actually increase informational masking effects It is not clear whether the benefits of increased audibility through the use of hearing aids would offset any potential increase in informational 
masking Current rehabilitation methods often focus on methods to reduce energetic masking effects i e FM systems directional microphones Understanding the benefits and limitations of hearing aids in the presence of informational maskers however may be necessary to optimize rehabilitation strategies and speech understanding If traditional amplification strategies are not effective alternative schemes may need to be developed or minimally counseling strategies will need to incorporate this limitation The primary purpose of the current study was to estimate the effects of informational masking present in everyday conversational settings on the speech understanding of persons with and without hearing loss We were also interested in the benefits and limitations of amplification in settings containing both informational and energetic masker components PROCEDURES Participants Two groups of adults participated in this study One group consisted of a control group of 15 participants 12 female 3 male with hearing 
thresholds 25dB at Figure 1 Average audiograms of study participants with hearing loss Open and filled symbols are for the left and right ears respectively The range of hearing losses included in this study is shown by dashed lines 435 Journal of the American Academy of Audiology Volume 17 Number 6 2006 Participants with hearing loss ranged in age from Test Setting A 3 2 meter square 2 meters high sound treated room modified with reflective panels served as the test environment Frequency specific reverberation times Rt60 time required for 60 dB decay after signal offset were measured at the position of the listener s head without the listener present using frequency modulated tones Measured Rt60 values at octave frequencies were 485 msec 250 Hz 440 msec 500 Hz 400 msec 1000 Hz 310 msec 2000 Hz and 220 msec 4000 Hz RP prescriptive method were verified using the composite noise test signal on the Frye Systems Fonix 6500 real ear analyzer For a 65 dB SPL input measured insertion gain values were on average 
within 7 to 1 dB of prescribed gain values at test frequencies of 250 500 1000 2000 3000 4000 and 6000 Hz see Table 1 Nominal compression thresholds read from the software fitting screen varied with frequency and ranged from Speech and Masking Stimuli Speech understanding in noise was assessed using a modified version of the Hearing in Noise Test HINT Nilsson et al 1994 The modifications to the HINT procedure were all related to the presentation and type of competing noise used as described below The HINT is an adaptive procedure used to determine the SNR necessary to achieve 50 correct sentence recognition Using this test the sentence level is adaptively varied in the presence of a constant level background noise to determine threshold Each experimental condition was evaluated using two ten sentence lists Experimental condition and list order were randomly assigned to each participant using a latin square design In addition for the HI participants the aided unaided presentation order was counterbalanced 
Speech materials were presented through a single loudspeaker Tannoy System 600 at a Hearing Aids The BTE version of the Phonak ClaroTM was used as the test instrument in the aided conditions This is a digital 20 channel lowthreshold fast acting compression aid The devices were fit bilaterally in omnidirectional mode using the manufacturer s fast acting digital perception processing fast acting DPPTM compression algorithm Digital noise reduction known as fine scale noise cancellation was disabled for all testing Work by Moore and colleagues 1999 suggests that multichannel fast acting compression may provide optimal benefit for the types of maskers used in this study e g provide maximum gain in the temporal dips of the maskers Participants with hearing loss were fit bilaterally using custom made full shell earmolds with venting appropriate for the degree and configuration of hearing loss Hearing aid fittings based on the NAL Table 1 Average Measured and Target Real Ear Insertion Gain in dB for the Right and 
Left Ear Frequency Hz 250 500 1000 2000 3000 4000 6000 Target dB 4 0 13 6 24 1 23 5 24 2 25 7 27 3 Average Right REIG Mean Difference dB and SD dB 4 2 10 8 22 6 24 9 18 6 18 3 21 9 0 2 3 0 2 8 2 8 1 5 2 7 1 4 2 5 5 6 4 6 7 5 6 0 5 4 8 0 Target dB 4 2 14 1 24 2 24 1 24 8 26 3 28 4 Average Left REIG dB 4 3 11 3 22 9 24 7 19 5 19 8 25 4 Mean Difference and SD dB 0 1 2 3 2 8 2 7 1 3 2 6 0 6 2 2 5 3 5 0 6 5 2 7 3 0 11 0 436 Hearing Loss Hearing Aids and Informational Masking Hornsby et al Previous research has shown that the magnitude of informational masking produced by speech maskers varies with the number of maskers with the largest effects seen in the presence of a minimal number of maskers e g Carhart et al 1975 Brungart et al 2001 Therefore speech understanding was measured in three masker configurations two four and seven maskers to provide an estimate of the magnitude of informational masking across a range of everyday situations In each masker configuration speech understanding was assessed using two 
types of maskers that varied in the amount of informational masking they were expected to produce for a total of six speech and masker conditions The maskers consisted of male talkers reading on a specific topic more informational masking and speech shaped modulated noises less informational masking Given that a primary goal of the study was to provide an estimate of the magnitude of informational masking in everyday speech environments each masker was presented from a separate loudspeaker that was spatially separated from the target speech which was located at a was equated prior to adjusting the overall level of the combined maskers to 65 dBA Calibration prior to each test session was performed to assure an overall level of 65 dBA Figure 2 Panels A B and C show the masker configurations for the two four and seven masker conditions used in this study Speech and noise loudspeaker locations are shown by the open square and filled triangles respectively 437 Journal of the American Academy of Audiology Volume 
17 Number 6 2006 Derivation of Masker Materials The maskers used in this study included individual talkers and speech shaped noises modulated by the envelope of the individual talkers Each loudspeaker presented a different talker or talker matched noise from a separate spatial location Masker configurations using individual talkers as maskers consisted of two four or seven loudspeakers each outputting a single male talker reading test passages from the Connected Speech Test CST Cox et al 1987 1988 Each CST passage consists of approximately ten sentences describing a specific topic e g lemons The passages were derived from a children s educational reading source see Cox et al 1987 for details The CST passages were chosen as masking materials because they provide a contextually rich dialogue rather than single unrelated sentences that may result in informational masking more consistent with that present in everyday communication settings Male talkers were used in an attempt to create masking conditions that 
were both realistic yet highlighted informational masking effects Specifically the HINT test materials are also spoken by a male talker and informational masking is higher when the target speaker and maskers are of the same sex Brungart 2001a Brungart et al 2001 At the same time realistic environments exist in which the target and masking speech are predominately of a single sex Recordings of the individual talkers reading the CST passages were made in an anechoic chamber and stored on a computer hard disk All talkers were native speakers of American English Offline digital filtering using the FIR2 function and a 1000th order FIR filter implemented in MatlabTM was used to match the long term rms spectra of each CST passage recorded by each talker to that of the long term rms spectra of the HINT materials The modulated noise maskers used in this study were derived from uncorrelated segments of Gaussian noise that were first spectrally shaped using the FIR2 function and a 1000th order FIR filter implemented in 
MatlabTM to match the long term rms spectrum of the HINT materials The shaped noises were then modulated using the envelopes of the same single talkers reading the CST passages The envelopes of the single talker maskers were derived in MatlabTM by implementing half wave rectification of a given single talker passage followed by low pass filtering of the halfwave rectified signal using a sixth order butterworth filter with a 30 Hz low pass cutoff frequency The envelope was then applied to the shaped noise providing a primarily energetic masker that retained the long term spectral and temporal patterns of the single talker maskers The long term spectrum of all individual talker and modulated noise maskers closely approximated the long term spectrum of the HINT sentences The mean error across 1 3octave bands from 160 to 8000 Hz between individual talkers modulated noises and the HINT spectrum was 0 19 and 0 22 dB respectively with standard deviations less than 1 dB The maximum error in a given 1 3octave band 
for any individual talker or modulated noise was 3 dB RESULTS Effects of Masker Type NormalHearing Participants The current study examined the effects of type and number of maskers on speech understanding of persons with and without hearing loss The performance of the NH control group was examined first using a two factor repeated measures ANOVA The within subjects independent variables were masker type speech or noise and number of maskers two four or seven the dependent variable was the HINT score in each masker condition A 0 05 level of significance was used in all analyses described here Summary results of the primary analyses are shown in Table 2 The results from the two factor ANOVA revealed a significant main effect of number of maskers two four and seven but not type of masker SM and NM In addition a significant interaction between type and number of maskers was observed Followup analyses were conducted using a series of single factor ANOVAs that compared HINT thresholds in speech and noise maskers 
separately in the two four and sevenmasker configurations Results in the sevenmasker configuration revealed significantly 438 Hearing Loss Hearing Aids and Informational Masking Hornsby et al Table 2 Results from a Two Factor Repeated Measures ANOVA on Participants with Normal Hearing Effect Type Num_masker Type x Num_masker NH df 1 14 2 28 2 28 F 0 87 65 57 14 57 p level 0 367 0 001 0 001 Note Type speech or noise masker Num_masker number of maskers i e two four or seven poorer HINT thresholds when using the speech maskers In contrast HINT thresholds were significantly better in the two masker configuration in the speech compared to noise maskers and no significant difference between performance in the speech and noise maskers was observed in the four masker configuration These results suggest that spatially separated individual talkers similar to everyday settings do cause some informational masking However the relative effect of the informational masking components of our masking speech varied based on 
the number of maskers being most apparent in the sevenmasker configuration Figure 3 shows HINT thresholds for the NH participants in the two four and seven masker speech SM and noise NM configurations Effects of Hearing Loss on Speech Recognition in the Presence of Speech and Noise Maskers Following this initial analysis differences in the effects of masker type speech versus speech modulated noise in the NH and HI participants were compared We were interested in whether the pattern of performance in the various masker conditions differed from that of the NH group For comparative purposes HINT thresholds for the NH unaided HI and aided HI participants are shown together in Figure 4 Using a series of three factor mixedmodel ANOVAs NH performance was compared to HI performance separately in the unaided and aided conditions The between Figure 3 Thresholds for 50 sentence recognition HINT thresholds for participants with normal hearing NH obtained in the presence of single talker black bars and modulated speech 
shaped noises gray bars as a function of number of maskers two four or seven Stars show conditions where performance in the modulated noise maskers NM and single talker speech SM maskers were significantly different 439 Journal of the American Academy of Audiology Volume 17 Number 6 2006 subjects independent variable was group NH and HI unaided or HI aided and the withinsubjects independent variables were masker type and number of maskers The dependent variable was the HINT score in each masker condition An analysis of NH and unaided HI performance revealed significant main effects of group HI and NH type of masker SM and NM and number of maskers two four or seven As expected and as seen in Figure 4 performance was poorer for the unaided HI group across all test conditions In addition averaged across groups performance was poorer as the number of maskers increased and performance was significantly poorer although the difference was small when speech was used as a masker compared to the speech modulated noise 
maskers In addition significant two way interactions were observed Consistent with past research a significant interaction between number of maskers and group was observed Averaged across masker types performance for the NH group systematically improved as the number of maskers decreased while no such release from masking was observed for the HI participants Additionally a significant interaction between type and number of maskers was observed with patterns consistent with those described in the NH analysis That is in the sevenmasker configuration performance averaged across groups was poorer when the maskers were speech than when they were speechmodulated noises In contrast in the twomasker configuration performance was better when the maskers were speech Finally no significant two way interaction between group and type of masker or three way interaction was observed The lack of a significant interaction between group and masker type suggests that the masking effects of our speech and speech modulated noise 
were similar for the NH and unaided HI participants in this study Summary results of the ANOVA analyses on the NH and unaided HI participants are shown in Table 3 Figure 4 Thresholds for 50 sentence recognition HINT thresholds obtained in the presence of singletalker speech maskers SM filled symbols and modulated speech shaped noise maskers NM open symbols as a function of number of maskers two four or seven Results for NH squares and unaided circles and aided triangles HI participants are shown separately 440 Hearing Loss Hearing Aids and Informational Masking Hornsby et al Aided Speech Recognition Performance in the Presence of Speech and Noise Maskers Another focus of this study was on the benefits and limitations of hearing aids in the presence of informational maskers One way to examine this question is to compare the performance of aided HI participants to the gold standard provided by the NH control group Specifically we can ask whether hearing aids restore speech understanding in noise for HI persons 
to levels comparable to NH individuals when the background noise contains an informational masking component To compare NH and aided HI performance a three factor mixed model ANOVA was performed on the NH and aided HI participants HINT thresholds Summary results of the ANOVA analysis are shown in Table 4 Average HINT thresholds from these conditions are shown in Figure 4 and clearly show that performance of the aided HI participants remains substantially poorer than the NH control group The results of the statistical analysis as seen by the significant effect of group confirmed that overall aided HI performance remained significantly poorer than that of the NH control group Consistent with the comparison of NH and unaided HI performance a significant interaction between number of maskers and group was observed i e HI performance does not improve as the number of maskers is decreased In contrast to the unaided comparison however a significant two way interaction between type of masker and group and a 
significant three way interaction between type number of maskers and group was also observed Follow up analyses showed that in contrast to NH performance aided HI performance was consistently poorer in the presence of the speech maskers compared to the noise maskers regardless of the number of maskers presented Recall that for the NH participants performance in the two masker condition was significantly better in the presence of the speech masker Effect of Hearing Aids on Informational Masking Aided Benefit Although the results from the previous analysis revealed that hearing aids did not restore speech understanding to normal it is of interest to examine the benefit that hearing aids did provide to our study participants To Table 3 Results from Mixed Model NH vs unaided HI ANOVA Effect Type Num_masker Group Type x group Num_masker x group Type x num_masker Type x num_masker x group NH vs Unaided HI Df 1 28 2 56 1 28 1 28 2 56 2 56 2 56 F 6 21 18 15 79 02 1 17 18 92 11 84 1 74 p level 0 001 0 001 0 001 0 289 
0 001 0 001 0 185 Note Type speech or noise masker Num_masker number of maskers two four or seven Group with and without hearing loss Table 4 Results from Mixed Model NH vs aided HI ANOVA Effect Type Num_masker Group Type x group Num_masker x group Type x num_masker Type x num_masker x group NH vs Aided HI Df 1 28 2 56 1 28 1 28 2 56 2 56 2 56 F 18 42 33 76 128 77 9 94 17 89 6 97 4 35 p level 0 001 0 001 0 001 0 01 0 001 0 01 0 05 Note Type speech or noise masker Num_masker number of maskers two four or seven Group with and without hearing loss 441 Journal of the American Academy of Audiology Volume 17 Number 6 2006 investigate aided benefit a three factor repeated measures ANOVA was performed on the HI unaided and aided HINT scores As expected a significant main effect of aid condition was observed with performance improving in the aided condition Also consistent with the previous analyses a significant main effect of masker type was observed with performance being poorer in the speech masker conditions Of 
primary interest however was the significant interaction between aid condition and type of masker Averaged across number of maskers persons with hearing loss received less benefit from hearing aids in the presence of the more informational masker the speech maskers than the speech modulated noise primarily energetic maskers Summary results of the ANOVA analysis are shown in Table 5 To determine the source of this significant interaction a series of two factor ANOVAs examining unaided and aided performance in the speech and noise maskers were performed These analyses showed a significant main effect of aid condition only in the presence of the speech modulated noise with performance being significantly better in the aided condition Follow up testing comparing unaided and aided performance in the speech modulated noise in the two four and seven masker configurations revealed significantly better aided performance in the four and two masker conditions Aided performance in the sevenmasker condition followed a 
similar pattern but the benefit was not statistically significant A similar but not statistically significant pattern was observed in the speech masker conditions In other words on average hearing aids did not significantly improve speech understanding compared to unaided performance when listening in the presence of speech maskers Figure 5 shows aided benefit i e the difference between unaided and aided HINT thresholds in dB for both noise types as a function of number of maskers Quantifying Informational Masking Effects A primary focus of this study was to estimate the effects of informational masking present in everyday conversational settings on persons with and without hearing loss Informational masking was functionally defined as the additional masking observed in the presence of speech maskers which contain both energetic and informational masking components compared to that observed for a speech modulated noise which is primarily an energetic masker We can use the analyses described above to identify 
the situations in which significant informational masking occurred Informational masking effects the difference in HINT thresholds measured in speech and speech modulated noise maskers for the NH unaided HI and aided HI groups as a function of number of maskers are shown in Figure 6 The stars identify the specific conditions where significant informational masking occurred When plotted in this fashion informational masking effects appear present and similar in magnitude across groups in the seven masker configuration In addition the pattern of informational masking effects appears similar for the NH and unaided HI in that informational masking appears to increase as the number of maskers increases In contrast the magnitude of informational masking remains relatively constant as the number of maskers increases for the aided HI group Table 5 Results from the Three Factor Repeated Measures HI unaided vs aided ANOVAs Unaided vs Aided HI Aid condition Type Num_masker Aid condition x Type Aid condition x Num_
masker Type x Num_masker Aid x Type x Num_masker 1 14 1 14 2 28 1 14 2 28 2 28 2 28 4 9 20 42 0 55 10 38 0 36 1 47 1 21 0 05 0 001 0 582 0 01 0 699 0 246 0 314 Note Type speech or noise masker Num_masker number of maskers two four or seven Aid condition unaided or aided 442 Hearing Loss Hearing Aids and Informational Masking Hornsby et al Figure 5 Aided benefit defined as the difference between unaided and aided HINT thresholds in both the speech SM circles and noise NM triangles maskers as a function of number of maskers A positive value is representative of aided benefit Stars over a given symbol identify masker conditions that showed significant differences between the aided and unaided conditions Figure 6 Informational masking effects defined as the difference between the HINT thresholds in the speech masker SM minus the HINT thresholds in the noise masker NM as a function of number of maskers Results for NH circles unaided HI triangles and aided HI squares are shown as separate symbols Error bars 
represent one standard deviation around the mean Stars over a given symbol identify masker conditions that showed significant differences between the SM and NM masker conditions 443 Journal of the American Academy of Audiology Volume 17 Number 6 2006 DISCUSSION T he current study explored the magnitude of informational masking effects present in everyday environments when the background noise consists of conversational speech maskers We were interested specifically in how these effects would vary with the presence of hearing loss and whether hearing aids would be useful in limiting the negative effects of informational masking in everyday conversational speech settings However several factors discussed below may influence our estimate of informational masking effects and its interactions with hearing loss and hearing aids Energetic Masking Effects One issue in estimating the magnitude of informational masking when using broadband signals and maskers as was done in this study is that energetic masking is also 
occurring When using speech as both the target and masker disentangling the contribution of the informational and energetic masking components is not a trivial task In the current study our estimates of informational masking are confounded to some degree by differences in energetic masking between our two masker types i e speech and speech modulated noise Although the noise maskers matched the long term spectral and temporal properties of the speech maskers the short term spectral and temporal properties of the speech and noise were not matched Real speech contains rapid amplitude fluctuations that vary in a frequency dependent fashion These rapid spectro temporal fluctuations are not present in the speech modulated noise only the longterm changes in the envelope are matched It is likely that any given speech masker contained more spectro temporal dips than its matched speech modulated noise masker and thus provided less energetic masking at some instances than the modulated noise This potential difference 
in the energetic masking properties of the speech and noise maskers is likely the cause of the better performance seen in the NH participants in the two talker speech masker configuration compared to the two speech modulated noise maskers see Figures 4 and 6 In the twotalker configuration relatively large differences in the short term spectro temporal properties of the speech and noise maskers may exist and the NH participants are able to take advantage of these to listen in the dips As additional talkers are added differences in the temporal properties of the individual speakers fill in the dips making the long and short term spectrum of the speech and noise maskers more comparable and more steady state compared to the situation where only two talkers are used Despite differences in the energetic masking properties of the speech and noise maskers these study results support the idea that everyday speech can cause both informational and energetic masking This is supported by the fact that as additional 
maskers were added essentially reducing the differences in spectro temporal fluctuations between masker types the speech maskers became more effective than the noise maskers see Figure 6 If differences in performance when using the speech and noise maskers were due solely to differences in energetic masking we would expect the noise maskers to continue to remain more effective than the speech maskers even as the numbers of maskers increased from two to seven Figures 4 and 6 show that at least for the NH and unaided HI groups the speech maskers become more effective than the noise maskers as the number of maskers increases suggesting factors other than energetic masking are impacting the results Figure 6 shows that the relative effect of informational masking resulting from conversational speech can be large in some conditions A maximum informational masking effect 2 5 dB change in SNR required to achieve 50 correct sentence recognition was observed in the aided HI group when using four maskers This change in 
SNR corresponds to a change in percent correct score that will vary based on the specific test material Assuming a transfer function slope of 12 dB a change in SNR of 444 Hearing Loss Hearing Aids and Informational Masking Hornsby et al test materials designed to highlight informational masking effects such as the CRM Effects of Age and Hearing Loss and Sensation Level on Informational Masking Our study results showed that informational masking effects were similar between persons without hearing loss and unaided persons with hearing loss at least when differences in the energetic masking properties of the masking stimuli are small e g in the seven masker configuration see Figure 6 This is in contrast to some recent work suggesting that informational masking effects may be smaller in persons with hearing loss at least when tested at comparable SPLs Alexander and Lutfi 2004 Arbogast et al 2005 Several factors however make it difficult to draw conclusions based on the current study about the effects of hearing 
loss per se on informational masking As mentioned previously the use of more realistic test settings and speech stimuli while providing face validity results in settings where informational masking effects are reduced Thus the fact that informational masking effects were similar between groups in the current study may be due to the fact that in real world settings informational masking effects are small compared to energetic masking effects which were larger in the HI and likely dominate performance In addition previous work has shown that masker SL affects the amount of informational masking in a given test situation Alexander and Lutfi 2004 Arbogast et al 2005 Given that in the current study masker levels were fixed at 65 dB SPL for both the NH and HI groups resulting in lower masker SLs for the HI participants we might expect less informational masking for the HI participants in this study In addition to the experimental configuration limiting informational masking e g using spatially separated maskers 
age differences between our NH and HI groups may have also played a role in our findings Unfortunately in the current study the age range of our HI participants is quite narrow and substantially different than that of the NH group This coupled with the fact that substantial differences in hearing thresholds exist between our groups makes disentangling the effects of age from hearing loss quite difficult We can however speculate on the potential impact Although recent work by Li et al 2004 found no differences in the negative effects of speech distracters on young adults with normal hearing and older adults the older adults had only mild high frequency hearing loss the ages of their older adults ranged from 63 to 75 years In contrast nine of the 15 HI participants in the current study were over the age of 75 with five between the ages of Benefits and Limitations of Hearing Aids on Informational Masking A primary result from this study regarding hearing aids and informational masking is that omnidirectional 
hearing aids were relatively ineffective in improving 445 Journal of the American Academy of Audiology Volume 17 Number 6 2006 speech understanding when the masking noises were spatially separated from the speech and contained both energetic and informational masking components i e speech masker conditions The poorer thannormal performance of the aided HI group regardless of masker condition is consistent with decades of past research showing a similar result e g Plomp 1986 Even when aided substantial research has shown that HI performance in noise particularly modulated background noises is consistently poorer than performance of a NH control group e g Bronkhorst and Plomp 1992 These study results show that the decrement experienced by our HI participants was largest when the noise contained informational as well as energetic masking components at least in the four and sevenmasker conditions see Figure 6 In addition the use of hearing aids in the higher informational masking conditions speech maskers did 
not significantly improve speech understanding over the unaided condition for the HI participants Hearing aids provided only limited benefit 1 1 dB averaged across the three masker configurations in the presence of the speech maskers see Figure 5 As discussed earlier this may be related in part to the age of our participants and the higher masker SLs experienced in the aided conditions Regardless hearing aids are designed primarily to restore audibility and are thus not expected to heavily impact the central factors e g stimulus masker uncertainty similarity that appear to be largely responsible for informational masking effects that occur in speech settings Thus the finding of limited benefit of hearing aids in the presence of informational maskers is not surprising This does however highlight the limitations of omnidirectional hearing aids in real world noisy environments particularly those that contain speech signals as the primary masker Given these limitations there is a clear need for additional 
research investigating the utility of additional technological e g directional microphones FM systems and counseling strategies such as reducing background noises improving lighting and access to visual cues etc that may help reduce these deficits SUMMARY T he results from the current study suggest the following 1 Speech maskers presented from discrete spatial locations in most configurations result in additional masking compared to a primarily energetic masker speech modulated noise This finding supports the idea that background noise consisting of multiple spatially separated talkers as occurs in everyday environments results in both informational and energetic masking 2 The relative effect of informational masking resulting from spatially separated talkers varies based on the number of talkers When the number of talkers is small e g two the informational masking component of everyday conversational speech may be obscured by energetic masking effects As the number of talkers increases from two to seven 
both informational and energetic masking effects can be observed 3 Informational masking effects due to spatially separated talkers appear to be similar for older adults with hearing loss in the unaided condition and younger adults without hearing loss 4 When the masking noise contains both energetic and informational masking components increasing audibility via the use of omnidirectional microphone hearing aids results in only limited improvements in speech understanding REFERENCES Alexander JM Lutfi RA 2004 Informational masking in hearing impaired and normal hearing listeners sensation level and decision weights J Acoust Soc Am 116 4 446 Hearing Loss Hearing Aids and Informational Masking Hornsby et al Brungart DS 2001a Informational and energetic masking effects in the perception of two simultaneous talkers J Acoust Soc Am 109 3 Lutfi RA 1989 Informational processing of complex sound I Intensity discrimination J Acoust Soc Am 86 3 447 Reproduced with permission of the copyright owner Further reproduction 
prohibited without permission 
31	a	INFORMATICA 2006 Vol 17 No 1 111 Cache based Statistical Language Models of English and Highly Inflected Lithuanian UNAS Department of Applied Informatics Vytautas Magnus University Vileikos 8 LT 44404 Kaunas Lithuania e mail airenas freemail lt g raskinis if vdu lt Received August 2005 Abstract This paper investigates a variety of statistical cache based language models built upon three corpora English Lithuanian and Lithuanian base forms The impact of the cache size type of the decay function including custom corpus derived functions and interpolation technique static vs dynamic on the perplexity of a language model is studied The best results are achieved by models consisting of 3 components standard 3 gram decaying cache 1 gram and decaying cache 2 gram that are joined together by means of linear interpolation using the technique of dynamic weight update Such a model led up to 36 and 43 perplexity improvement with respect to the 3 gram baseline for Lithuanian words and Lithuanian word base forms 
respectively The best language model of English led up to a 16 perplexity improvement This suggests that cache based modeling is of greater utility for the free word order highly inflected languages Key words language models n grams cache models dynamic interpolation perplexity reduction inflected language free word order language Lithuanian 1 Introduction Statistical language models LM have become key components for large vocabulary continuous speech recognition LVCSR systems These models provide prior probabilities that are used to rate hypothesized sentences and to disambiguate their acoustical similarities During the last few decades much experimental work has been done in the field of statistical language modeling covering widespread world languages such as English French and German The most popular modeling techniques developed for those languages are known as n grams Although n grams have shown a good performance they are far from optimal because of false word independency assumption Lithuanian 
language modeling has started since 2002 Lithuanian has free word order and is highly inflected i e new words are easily formed by inflectional affixation These properties of a language result in difficulties of statistical modeling known as huge vocabulary size model sparseness high perplexity and a high out of vocabulary OOV word rate The attempts to overcome the abovementioned difficulties of Lithuanian included word parsing into stems and endings Vai 112 A Vai 2004 In this paper we investigate an alternative cache based modeling To our knowledge the cache based modeling of highly inflected free word order languages has not been attempted The cache based models presented in this paper are interesting in two respects They are able to adapt dynamically to the text under investigation and they have the potential of catching dependencies spanning longer word sequences than n grams do The impact of the model architecture cache size type of the decay function including custom corpus derived functions and 
interpolation technique static vs dynamic on the perplexity of a language model is studied Cache language models of Lithuanian are compared to the corresponding English ones 2 Related Work Cache based n gram model for linguistic applications was first introduced by Kuhn and De Mori 1988 1990 It can be thought of as a usual n gram Markov model trained on a relatively short history of recent words of some particular word wi Let wi be the i th word of a text and let h wi K wi 1 denote the cache or history of wi where K is the size of the cache Let C h K be the number of words within h belonging to the chosen vocabulary V Let C wi h be the number of occurrences of a word wi within h Let C wi 1 wi h be the number of word pairs wi 1 wi within h Finally let I condition denote the indicator function taking the value 1 if condition is true and 0 otherwise Then the conditional probabilities of 1 gram and 2 gram cache language models can be estimated by formulas 1 and 2 respectively C wi h PH wi h C h PH 2 wi wi 1 h i 
1 j i K I wi wj i 1 j i K I wj V i 2 C wi 1 wi h j i K I wi 1 wj wi i 2 C wi 1 h j i K I wi 1 wj 1 wj 1 2 Conditional probabilities of a 3 gram cache LM can be estimated in a similar way Jelinek et al 1991 showed that 2 gram and 3 gram cache outperformed 1 gram cache in terms of LM perplexity1 Rosenfeld 1996 and Goodman 2001 reported just minor improvements of 3 gram cache over the 2 gram cache Nevertheless 1 gram cache language models are often used because of the problem of LM sparseness arising due to the limited cache size K Clarkson and Robinson 1997 suggested an improvement to 1 and 2 based on the experimental evidence that the probability of a word reoccurrence in a text exponentially 1 Perplexity refers to how many different equally probable words a statistical LM expects to appear in average for a particular type of a context It is estimated on the test subset of the corpora Cache based Statistical Language Models of English and Highly Inflected Lithuanian 113 decays as the distance to that word 
increases Otherwise stated recent words wj have greater influence on probability distribution of the current word wi The influence diminishes as the distance i j increases The decay cache is used to model this phenomenon i 1 j i K I wi wj Pd H wi h 3 wj 1 4 where d x is the decay function that tends to zero as the distance x increases Two exponentially decaying functions are often used d x e bx and d x ae bx c The decay speed b as well as parameters a and c are chosen experimentally or estimated by approximating the function of word reoccurrence i e the actual histogram of distances between the two consecutive repetitions of the same word Because of a very limited cache size standalone cache models Here s are interpolation weights optimized on the validation corpus Sometimes conditional interpolation formula is used Goodman 2001 PW 3 H H 2 wi wi 2 wi 1 PW 3 H H 2 wi wi 2 wi 1 PW 3 H wi wi 2 wi 1 if wi 1 h 7 otherwise Besides the standard word 3 gram models cache models can be interpolated with class based 
skip sentence mixture models Goodman 2001 topic mixture models Kneser and Steinbiss 1993 Iyer and Ostendorf 1999 and trigger pair models Tillmann and Ney 1996 Models 2 The percentage of words wi of the test corpus such that C wi h 0 114 A Vai Dynamic weights may be adapted on a word by word basis by optimizing perplexity on the recent word history hD wi D wi 1 PW 3 H H 2 wi wi 2 wi 1 W 3 i hD PW 3 wi wi 2 wi 1 H i hD PH wi h H 2 i hD PH 2 wi wi 1 h 8 where W 3 i hD H i hD H 2 i hD 1 for all i and D represents the length of an empirically chosen word history The M i hD can be estimated by an expectation maximization algorithm see Kneser and Steinbiss 1993 Martin et al 1997 or Gotoh and Renals 1997 before estimating the combined probability estimate 8 Dynamic interpolation was previously introduced in topic mixture models of highly inflected Slovenian Maucec and Kacic 2001 and Finish Siivola et al 2001 Some other attempts to avoid using static interpolation weights include the definition of interpolation 
weights as the function of a cache size K Goodman 2001 and the use of distinct weights i for classes of topic specific and general purpose words Federico and Bertoldi 2001 Gotoh and Renals 1997 Martin et al 1997 Seymore et al 1998 There is no consensus about the efficiency of the cache based LM embedded in a speech recognition system Jelinek et al 1991 Rosenfeld 1996 Tillmann and Ney 1996 reported WER3 reduction while Clarkson 1999 and Goodman 2001 reported WER degradation due to the use of a cache based LM In all those cases the perplexity of the cache based LM was significantly better than the perplexity of a word 3 gram LM 3 Resources and Tools Our investigations were based on three corpora The main corpus was the Lithuanian text corpus compiled by the Center of Computational Linguistics at Vytautas Magnus University Marcinkevi cien e 2000 containing 84 202 576 word tokens henceforth LT corpus This corpus represented a great variety of genres and topics of the present day written Lithuanian It was used 
for the investigation of cache based language modeling phenomena of inflected Lithuanian Two auxiliary corpora were the corpus of Lithuanian base forms LTBF and The Sunday Times English corpus of the year 1995 EN The LTBF corpus was derived from the LT corpus by replacing each word with its base form4 Auxiliary corpora served for inflected non inflected and Lithuanian English comparison purposes All corpora were divided into training validation and test subsets constituting 98 1 and 1 of the original corpora respectively The same proportions of text genres were kept within all subsets We used some text clearing punctuation was removed numbers and out of vocabulary OOV words i e words found in the test subset but absent from the vocabulary V where replaced by tags num and oov respectively Error Rate is the standard measure of accuracy of a speech recognition system forms the infinitive for verbs the singular nominative case for nouns etc were obtained with the morphological lemmatizer of Lithuanian Zinkevi 
cius 2000 In case of morphological ambiguity the first base form out of the list of possible base forms was selected 4 Base 3 Word Cache based Statistical Language Models of English and Highly Inflected Lithuanian Table 1 Summary of corpora characteristics Word types vocabulary 1158k 84 202 k LTBF EN 371k 235k 40 525 k 409 k 400 k 91167 445 853 k 713 k 1996 42185 Word tokens Articles training validation testing Average words per article 115 Corpus LT Majority of our investigations were carried out using locally developed cache based language modeling tools Simple n grams were built using CMU Cambridge Statistical Language Modeling Toolkit Clarkson and Rosenfeld 1997 that was extended to handle vocabularies of more than 65k words 4 Experimental Results We have investigated cache based models in order of increasing complexity First the simple 1 gram cache 1 gram decaying cache and 1 gram decaying cache using dynamic weight adaptation were investigated Thereafter the best performing 1 gram cache models were 
complemented with the components of 2 gram cache 2 gram decaying cache and 2 gram decaying cache using dynamic weight adaptation Throughout all experiments cache based LMs were compared on the basis of perplexity and perplexity improvement with respect to the baseline The results are briefly summarized in the Table 2 More detailed description of our investigations is presented in the subsections that follow Table 2 Summary of cache based language modeling experiments Language model LT 1157k LTBF 371k Perplexity PW 3 3 gram baseline Kneser Ney 1027 21 451 27 259 46 EN 235k Perplexity improvement PW 3 H 1 gram cache PW 3 d H 1 gram decaying cache PW 3 d H dynamic weight adaptation PW 3 d H H 2 2 gram cache static weights PW 3 d H d H 2 2 gram decaying cache PW 3 d H d H 2 dynamic weight adaptation 24 72 28 20 29 62 33 51 33 32 36 21 30 64 34 24 35 94 39 55 40 13 43 03 12 24 13 49 13 71 15 69 16 02 16 20 116 A Vai Table 3 Perplexities and OOV rates of 3 gram language models obtained with Kneser Ney and Good 
Turing smoothing techniques Vocabulary size Perplexity of PW 3 Kneser Ney smoothing 1027 21 451 27 259 46 Good Turing smoothing 1117 42 478 68 276 76 1 73 1 15 0 31 Corpus OOV LT LTBF EN 1157k 371k 235k All experiments were carried out without cache flushing5 as we wanted to investigate the ability of LMs to adapt to the changes in text topics OOV handling was realized in the following way terms of type P oov h and P oov wi h were skipped but P wi oov h were included into perplexity calculations 4 1 Choice of the Baseline Language Model We have chosen the conventional word based 3 gram PW 3 wi wi 2 wi 1 including all singleton 3 grams and smoothed using Kneser Ney Kneser and Ney 1995 smoothing technique as our baseline model Kneser Ney smoothing systematically outperformed Katz backoff technique Jelinek 2001 coupled with Good Turing smoothing see Table 3 4 2 1 gram Cache based Models We have constructed a series of 1 gram cache based models PW 3 H for cache sizes K ranging from 50 to 1000 Perplexity 
improvement and cache hit for each K was measured The obtained results are summarized by Fig 1 and Fig 2 1 gram cache based model significantly improved perplexity with respect to baseline PW 3 by 12 EN 25 LT and 31 LTBF Perplexity improvement showed similar cache size dependency curves for both languages The best improvements were achieved at K 300 EN and LTBF and K 500 LT Cache hit estimates confirmed our intuition that Lithuanian words were less used by the cache because of a bigger inflected vocabulary Cache hit curve for LTBF was similar to that of EN but the perplexity improvement for EN was much lower 4 3 1 gram Decaying Cache based Models We have investigated 1 gram cache based models PW 3 d H with four types of decay functions 5 The term cache flushing means that all words are removed from the cache at the end of an article Cache based Statistical Language Models of English and Highly Inflected Lithuanian 117 Fig 1 Impact of the 1 gram cache size on the perplexity improvement Fig 2 Impact of the 1 
gram cache size on the cache hit Exponential decay function Linear decay function6 Gamma like decay function7 Corpus derived decay function exp db x e bx linear da x max a x 0 9 10 11 dgamma x xa 1 e bx a b x dcorpus o N i x 1 I wi wi x Occ i x o i 1 12 Here N is the size of the corpus and Occ i x j i x 1 I wi wj The expression wi wi x Occ i x o is true if and only if wi wi x and there is exactly o occurrences of the same word in between wi and wi x Thus the functions dcorpus x and dcorpus x represent respectively the histograms of distances between 0 1 two consecutive and two next to consecutive repetitions of the same word8 linear decay function was included for comparison purposes only popular exponential decay function has maximum at the position one But this contradicts empirical data as identical words rarely follow one another Empirical evidence suggests that the probability of the word to reoccur grows from the start and then starts decaying after some position 8 In all decaying cache experiments we 
used a discrete array implementation for storing function values The maximum cache size K was truncated at K 1000 for speed up purposes The values of d K are relative small for K 1000 7 The 6 The 118 A Vai Optimum parameters for the decay functions 9 11 were found experimentally by optimizing perplexity on the validation data set Corpus derived decay functions were individually estimated on training subsets of LT LTBF and EN corpora Adding decay to 1 gram cache resulted in an improvement of about 3 5 for Lithuanian and 1 3 for English models The optimum cache size and decay speed were inversely related Thus slower decaying functions were used for the LT task as it had longer caches It is interesting to note that dexp 0 01 x was one of the best decay functions for EN corpus and actually had a decay speed different from the decay speed of the distribution of word reoccurrences dEN 0 x Fig 3a 3d Nevertheless taking into account the second reoccurrence of the word could be of some help for both languages Table 4 
last line It is also interesting to note that Fig 3 Sample decay functions normalized by dividing by the maximum Table 4 Perplexity improvements obtained with various decay function types of 1 gram cache Perplexity of PW 3 d H LT 1157k None best cache size dlinear x 500 dexp x 0 015 dexp 0 01 x dexp 0 005 x dexp 0 0025 x dgamma 1 05 0 005 x dgamma 1 10 0 01 x dcorpus x 0 dcorpus x dcorpus x 0 1 773 31 500 757 52 756 83 746 70 742 13 750 90 743 22 746 23 737 56 737 49 LTBF 371k 313 00 300 305 20 302 84 299 23 299 15 305 06 299 88 299 25 296 07 296 14 EN 235k 227 71 300 225 90 225 79 224 60 224 99 227 40 225 19 224 47 225 17 224 46 Decay function d x Cache based Statistical Language Models of English and Highly Inflected Lithuanian 119 LT the distribution of word reoccurrences dEN 0 x and d0 x of English and Lithuanian appeared to be very similar This distribution seems to be a language independent parameter 4 4 2 gram Cache based Models We have constructed a series of 2 gram cache based models PW 3 d H H 2 7 
for cache sizes K ranging from 50 to 50000 2 gram cache based model PW 3 d H H 2 signifi x dcorpus x cantly outperformed 1 gram model PW 3 d H having d x dcorpus 0 1 by 5 LT LTBF and 2 EN The optimum K value was about 30000 2000 and 500 words for LT LTBF and EN corpora respectively Important differences in optimum K values could be explained by the fact that the average article size is more than 40k words in LT and only 445 words in EN corpus Adding decay to the cache 2 gram improved LTBF and EN models but not the LT model Table 5 This can be explained by the fact that decay functions used truncated cache size of K 1000 much less than the optimum cache size for LT models Corpus derived decay functions analogous to 12 seem to be best suited for Lithuanian corpora and exponential decay works best for the English corpus An interesting fact is that 2 gram cache hit on LTBF and even on LT was larger than on EN texts see Fig 4 This can probably explain why 2 gram cache improves English LMs not as much as 
Lithuanian LMs 4 5 Dynamic Adaptation of Component Weights We have constructed a series of 1 gram and 2 gram cache based models of type PW 3 d H and PW 3 d H d H 2 8 for D ranging from 20 to 500 As it was expected dynamic weight adaptation outperformed static weight optimization The model Table 5 Perplexity improvements obtained with various decay function types of 2 gram cache Perplexity of PW 3 d H d2 H 2 LT 1157k None best cache size dlinear 1000 x dexp 0 015 x dexp 0 01 x dexp 0 005 x dexp 0 0025 x dgamma 1 05 0 005 x dgamma 1 10 0 01 x dcorpus x 0 dcorpus x dcorpus x 0 1 683 04 30000 687 06 688 71 687 01 685 48 686 10 685 55 686 85 685 11 684 97 LTBF 371k 272 81 2000 271 32 272 30 271 29 270 51 270 79 270 54 271 29 270 28 270 19 EN 235k 218 76 500 218 22 218 23 217 88 217 67 217 99 217 69 217 83 217 76 217 89 Decay function d x 120 A Vai Fig 4 Impact of the 2 gram cache size on the cache hit Fig 5 Impact of the size of interpolation optimization history D on the perplexity of PW 3 d H PW 3 d H d H 2 
added about 3 LT LTBF and 0 2 EN of improvement Tiny improvement of LMs built over EN corpus was probably due to the shortness of EN articles The 2 gram cache component of EN models had its utility as well as its average weight reduced Thus weight adaptation procedure could bring little gain over static weights In contrary 2 gram cache component was extremely useful for some articles of LT and LTBF corpora In this case the dynamic weight adaptation boosted LM performance Though short interpolation optimization histories hD had the potential of better adaptation to the changes in article or text topic there were no perplexity improvements for short histories D 50 words because of small reliability of such short histories The optimum history size was found to be D 200 for both 1 gram and 2 gram models for all three corpora The perplexity grew slowly for D 200 see Fig 5 It is interesting to note that LMs using dynamic weight adaptation had different average component weights for texts belonging to different 
stylistic categories Table 6 For instance legal documents had average H 2 i hD 0 2 indicating repeated usage of word pair collocations Cache based Statistical Language Models of English and Highly Inflected Lithuanian Table 6 Average weights of PW 3 d H d H 2 components per text category of LT corpus Average weights of PW 3 d H d H 2 8 components W 3 i hD National newspapers Translated philosophy Legal documents 0 88 0 70 0 75 H i hD 0 11 0 23 0 05 H 2 i hD 0 01 0 07 0 20 121 Text category 4 6 Other Approaches Related Cache based Modeling Rosenfeld 1996 found that the reduction of the perplexity could be achieved by using the cache for rare words only Such cache usage appeared not to be useful to Lithuanian However we found that some perplexity reduction could be gained by omitting certain unpromising words of the validation corpus from the cache boosting the probabilities of remaining words The unpromising words were defined as those having average probability estimate given by PW 3 H lower than PW 3 i e 
words w having perf w less than some negative constant where NV al perf w i 1 log2 PW 3 H wi wi 2 wi 1 log2 PW 3 wi wi 2 wi 1 and the sum is over validation corpus This approach resulted in some though negligible improvement 5 Conclusions In this paper we described a number of experiments with the cache based LMs of Lithuanian and English Our work confirmed that significant reduction of perplexity 43 03 36 21 and 16 20 for LTBF LT and EN corpora respectively could be achieved by the use of the cache based modeling Improvements over the baseline are higher than twice for Lithuanian with respect to English English 3 gram baseline model performs relatively well and is hard to improve as English has a strict word order Simplistic claim that worse models can be better improved cannot explain this difference Actually we repeated the whole set of experiments by replacing Kneser Ney smoothed 3 grams with worse GoodTuring smoothed 3 grams Perplexity improvement obtained with those worse models was the same as with 
the better ones through the whole set of experiments This suggests that cache based modeling brings more benefits to the free word order languages by being capable of capturing some dependencies that lie besides the strict word order 122 A Vai Cache improved LMs of Lithuanian word base forms LTBF better than LMs of Lithuanian words LT This suggests that additional efficiency can be brought into language modeling of Lithuanian by methods that are able to cope with the highly inflected nature of Lithuanian The impact of different modeling techniques had similar tendencies in case of both languages Adding 1 gram cache component to the 3 gram model brought the greatest part of improvement Additional improvement was gained by adding a 2 gram cache component by selecting an appropriate decay function and by replacing static component interpolation weights with the procedure of dynamic weight update It was found that optimal decaying function differs from the distribution of the distances of the word reoccurrence 
in general However it is possible to construct better decay functions by analyzing longer relations for example distribution of the distance to the second reoccurrence It appeared that the best 1 gram cache size is independent of language i e it is the same for EN and LTBF tasks Experiments confirmed that longer cache size should be used in the 2 gram cache case These last findings should be regarded with care because of the differences in article size in Lithuanian and English corpora This research confirms that cache based modeling significantly improves LM perplexity Our next task is to integrate them into a speech recognition system ant to investigate their impact on a speech recognition accuracy References Clarkson P 1999 Adaptation of Statistical Language Models for Automatic Speech Recognition PhD thesis Cambridge University Engineering Department Cambridge Clarkson P and R Rosenfeld 1997 Statistical language modeling using the CMU Cambridge Toolkit In Proceedings of 5th European Conference on Speech 
Communication and Technology pp Cache based Statistical Language Models of English and Highly Inflected Lithuanian 123 Kuhn R 1988 Speech recognition and the frequency of recently used words a modified Markov model for natural language In Proceedings of 12th International Conference on Computational Linguistics pp 124 A Vai Statistiniai kalbos modeliai naudojantys trumpalaike atminti anglu ir lietuviu kalboms UNAS Siame straipsnyje aprasomi statistiniu kalbos modeliu naudojan ciu trumpalaike atminti tyrimai Modeliai ivertinami naudojant tris skirtingus tekstynus angliska lietuviska ir lietuviska pae nuo atminties grindiniu formu tekstyna Darbe pateikiama siu modeliu maisaties priklausomyb cios funkcijos tipo bei modeliu interpoliavimo 
32	a	IEICE TRANS INF SYST VOL 446 PAPER Special Section on Corpus Based Speech Technologies Dialogue Speech Recognition by Combining Hierarchical Topic Classification and Language Model Switching Ian R LANE a Student Member Tatsuya KAWAHARA Tomoko MATSUI and Satoshi NAKAMURA Members SUMMARY An efficient scalable speech recognition architecture combining topic detection and topic dependent language modeling is proposed for multi domain spoken language systems In the proposed approach the inferred topic is automatically detected from the user s utterance and speech recognition is then performed by applying an appropriate topic dependent language model This approach enables users to freely switch between domains while maintaining high recognition accuracy As topic detection is performed on a single utterance detection errors may occur and propagate through the system To improve robustness a hierarchical back off mechanism is introduced where detailed topic models are applied when topic detection is confident and 
wider models that cover multiple topics are applied in cases of uncertainty The performance of the proposed architecture is evaluated when combined with two topic detection methods unigram likelihood and SVMs Support Vector Machines On the ATR Basic Travel Expression Corpus both methods provide a significant reduction in WER 9 7 and 10 3 respectively compared to a single language model system Furthermore recognition accuracy is comparable to performing decoding with all topic dependent models in parallel while the required computational cost is much reduced key words speech recognition topic detection topic dependent language modeling support vector machines multi domain spoken dialogue 1 Introduction Speech is a natural communication medium and is thus an efficient interface for human machine communications In recent years there has been significant growth in the development and commercial deployment of interactive spoken language systems Applications include spoken dialogue systems for guidance and 
transactions Manuscript received June 29 2004 Manuscript revised September 21 2004 The authors are with the School of Informatics Kyoto University Kyoto shi to weather restaurant and urban navigation information respectively Limiting operation to a single task domain however forces users to make use of several independent systems when they require information from multiple domains for example transit information to a particular city as well as the weather forecast for that city For improved usability systems should operate across multiple domains allowing users to gain the required information quickly and efficiently within a single interaction Commercial Voice Portal systems provide the simplest spoken language interface for information retrieval over multiple domains In these systems a pre defined set of command keywords are used to traverse an information hierarchy to obtain the required information This approach only requires the recognition of a limited set of keywords and thus realizes reasonable 
performance However the usability of such systems is limited as users require knowledge of both the system s keywords and information structure before they can effectively use the system Expert users are not favored either as they are forced to traverse the information hierarchy even when they have full knowledge of the system An alternative approach to multi domain spoken dialogue includes systems developed for the DARPA Communicator project 10 11 These systems cover the travel domain allowing users to retrieve up to date flight schedules flight pricing hotel information and rental car availability They improve over the Voice Portal approach by applying a conversational front end rather than just keyword recognition These systems typically adopt a single recognition front end that provides coverage over all of the subdomains contained within the system A similar approach was also applied in 12 for an office assistant spoken dialogue system that operates over a large number of subdomains When performing 
speech recognition over multiple domains topic or sub task dependent language modeling increases both the accuracy and efficiency of the system However current dialogue systems that use multiple topicdependent language models typically adopt a system initiative approach 13 14 where the appropriate LM is applied based on the system s prompt determined by the dialogue flow of the system Increased usability can be achieved by allowing users to switch between domains as in 15 but in this case users must explicitly state the domain they require Copyright c 2005 The Institute of Electronics Information and Communication Engineers LANE et al DIALOGUE SPEECH RECOGNITION 447 before making a query A multi domain spoken dialogue system based on this approach would behave as follows S stands for system and U for User S Welcome to the Kyoto City Information Portal U S U S U S U S What system do you require tourist restaurant or bus Tourist information please You are now in the Kyoto Tourist Information System What time 
is the Golden Pavilion open until The Golden Pavilion is open from 8 30am until 5 00pm every day of the week Restaurant information You are now in the Kyoto Restaurant Information System Japanese style restaurants near the Golden Pavilion There are 2 Japanese restaurants in that vicinity Fig 1 Topic dependent recognition based on topic detection In this approach the number of dialogue turns is unreasonably large due to the overhead required to switch between domains Rather than explicitly stating which domain is required the system should automatically detect it from the users utterance For this purpose approaches used in call routing can be applied 16 17 In this study we propose a recognition architecture combining topic detection and topic dependent language modeling The inferred domain is automatically detected from the user s utterance and speech recognition is then performed with an appropriate topic dependent language model This allows the user to seamlessly switch between domains while maintaining 
high recognition accuracy Based on this approach the above spoken dialogue system would behave as follows S Welcome to the Kyoto City Information Portal Please query the system on tourist restaurant or bus information What time is the Golden Pavilion open until The Golden Pavilion is open from 8 30am until 5 00pm every day of the week Are there any good Japanese restaurants near there There are 2 Japanese restaurants in that vicinity Therefore a mechanism that provides robustness against topic detection errors is required For this purpose we introduce a hierarchical back off mechanism that applies detailed topic models when topic detection is confident and wider models that cover multiple topics in cases of uncertainty Previous studies have typically investigated topicdependent recognition on long speech materials such as transcription of news articles 18 19 and the Switchboard corpus 20 In these studies a large number of utterances were used to perform topic detection thus detection errors were not 
considered Also a rescoring framework was typically used which provided only a limited gain in recognition accuracy while requiring the generation of a large N best list which is computationally expensive In the proposed approach we re perform decoding applying an appropriate topic dependent language model from the topic detection result in the initial recognition pass The topic detection result can also be used for other applications such as improving the back end translation system performance for speech tospeech translation 21 2 Language Model Switching Based on Topic Detection An overview of the proposed system is shown in Fig 1 Speech recognition is performed in two stages In the first recognition stage a G LM generalized language model built from the entire training set is applied and topic detection is performed on the recognition result of this pass Based on the topic detection result and its confidence an appropriate granularity of TD LM topic dependent language model is selected This TD LM is then 
used to re decode the utterance As a final fallback the result of the topic dependent recognition and that of the initial topic independent recognition are compared and the hypothesis with maximum ASR score is selected The ASR score is a weighted product of the acoustic and language model probabilities This allows the system to back off completely to the topicindependent G LM in cases where the TD LM hypothesis is unlikely System turn around time can be reduced by running the current topic dependent and generalized recognition in parallel and performing re decoding only when a topic change occurs The recognition performance of the proposed architecture is dependent on the TD LMs applied and the accuracy of topic detection When TD LMs cover narrow or individ U S U S This approach significantly reduces the number of dialogue turns required As the domain of each user utterance is automatically detected the overhead required to explicitly switch between domains is eliminated Recognition accuracy is also 
maintained as topic dependent recognition is applied An alternative approach for topic dependent recognition is to perform decoding with multiple topic dependent language models in parallel however this approach requires large computational overhead and also hampers scalability as the addition of each new topic domain requires an additional recognition process One problem in implementing the proposed architecture is that topic detection errors can occur as it is performed on the recognition hypothesis of a single utterance These errors may propagate through the system causing an incorrect topic dependent language model to be selected and thus resulting in highly erroneous recognition hypotheses IEICE TRANS INF SYST VOL 448 ual topics a large increase in recognition accuracy can be gained however topic detection errors will also increase Training LMs for very narrow topics also generally suffer from data sparseness On the other hand LMs that provide coverage over wide topics will reduce the gain in 
recognition accuracy In this paper a multi layer framework is introduced where a hierarchy of LMs is generated that cover an increasing number of topics Individual topic LMs are applied when topic detection is confident and in cases of uncertainty the system backs off to wider models that provide coverage over multiple or all topic classes 3 Topic Detection Topic detection is performed in a manner similar to that used for call routing 16 17 Each sentence in the training set was initially manually labeled with a single topic from a set of pre defined topic classes T t1 t M The features used for topic detection consist of word base form tokens word tokens with no tense information Appropriate cutoffs are applied to remove those features with low occurrence in the training set The set of word features for an input sentence is defined by the occurrence counts of each word feature wi that is W w1 w2 wV where V is the vocabulary size In this study we investigate two topic detection methods unigram likelihood and 
SVM Support Vector Machines Initially classification models are trained for each topic class Topic detection is then performed by applying each classification model to the input utterance and selecting the topic with maximum classification score 3 1 Unigram Likelihood Based Topic Detection In this approach topic dependent unigram language models are trained for each topic class The unigram probabilities p wi t j are estimated based on the occurrence counts of each word feature wi in the training sentences of that topic t j The topic classification score scoreUN I X t j is calculated as log likelihood of topic t j s unigram model for the input sentence X consisting of N words x1 xN At recognition X is the 1 best hypothesis from the initial recognition pass The topic detection result is the topic with maximum score N an individual point within this space where vector components relate to word token occurrence counts Based on this vector space model an SVM hyperplane H j is trained for each topic class t j 
Sentences labeled with that topic t j are used as positive examples and the remainder of the training set is used as negative training examples Due to the high dimensionality of this space up to 10 000 features a linear SVM kernel is adequate for classification Topic detection is performed by comparing the vector representation of the input sentence X to each SVM hyperplane The vector representation W w1 wV is derived by counting word occurrences in the input sentence X x1 xN The topic classification score scoreS V M X t j is calculated as the perpendicular distance between W and the hyperplane H j of topic t j This value should be positive if W is in class and negative otherwise The detection result is the topic with maximum classification score scoreS V M X t j dist W H j 4 Topic Dependent Language Modeling The corpus used in this work contains manually assigned topic tags for each sentence Although these tags can be used directly to train TD LMs the resulting models may not be optimal in terms of either 
perplexity or topic detection accuracy due to subjective definition of labels and inconsistencies between labelers Thus each sentence in the training set is re labeled with the result from topic detection In the case of unigram re labeling initial unigram models are created based on the original hand labeled topic tags and each sentence in the training set is re labeled as the topic with maximum classification score This process of model creation and data re labeling is repeated until convergence For SVM re labeling this process is done only once Topic detection models are created directly from the hand labeled tags and the training set is then re labeled using these models The re labeling process improves topic detection accuracy and reduces LM perplexity by clustering similar sentences together TD LMs are trained based on the new labels To reduce the effect of data sparseness each TD LM is then linearly interpolated with the domain independent G LM which is trained on the entire training set Interpolation 
weights are selected to minimize the perplexity of a development set which has been re labeled in the same manner 5 Language Modeling Based on Hierarchical Topic Classification To increase the system s flexibility and robustness a hierarchical topic back off mechanism is introduced In this approach rather than applying only topic dependent language models that provide coverage over individual topics a hierarchy of language models is constructed that provides coverage over an increasing number of topics 2 scoreUNI X t j i 1 log p xi t j 1 3 2 SVM Based Topic Detection SVM support vector machines 22 is a popular classification technique based on margin maximization SVM has been shown to be appropriate for text classification tasks which typically consist of sparse high dimensional vector space models In this approach a sentence is represented as LANE et al DIALOGUE SPEECH RECOGNITION 449 Table 1 Topic hierarchy construction T t1 t2 tM k T while k 2 do determine ti t j such that dist ti t j is minimized i merge 
ti and t j to create parent node ti j include ti j to topic set T remove ti and t j from T k k 1 end while j Fig 2 SVM based language model hierarchy The topic hierarchy is automatically constructed by clustering together those topics likely to be confused during topic detection The resulting hierarchy for the SVM case is shown in Fig 2 The top node layer 1 corresponds to a topic independent G LM that provides coverage over all topics The lowest layer layer 3 corresponds to the most detailed models that provide coverage for individual topics Intermediate nodes in layer 2 correspond to models that cover multiple topics Intermediate nodes lower in the hierarchy provide coverage for topics more likely to be confused during topic detection Ascending the hierarchy the language models become less topic dependent and typically cover an increasing number of topics When the topic detection result is confident increased recognition accuracy can be gained by applying a language model lower in the hierarchy which is 
more topicdependent In cases of uncertainty however the system should back off to an intermediate model covering multiple plausible topics or to the topic independent G LM rather than selecting a possibly incorrect individual topic This approach reduces the cases where a topic dependent language model is selected that does not match the current utterance In the following sub sections we describe the topic hierarchy clustering algorithm the inter topic distance measures used during clustering and the hierarchy back off mechanism used to select an appropriate TD LM at runtime 5 1 Topic Hierarchy Construction The topic hierarchy is automatically constructed applying agglomerative hierarchical clustering as described in Table 1 Clustering involves iteratively determining the closest topic pairs and merging them until only two clusters remain The resulting hierarchy is then pruned of outlying models Those models that realize less than a 10 reduction in perplexity compared to the G LM are removed from the 
hierarchy The resulting hierarchy for the SVM case after pruning is shown in Fig 2 During clustering an inter topic distance measure related to topic detection confusability is required This dis Fig 3 Inter topic distance for SVM based topic detection tance measure is dependent on the topic detection method used 5 2 Unigram Based Inter Topic Distance For unigram based topic detection the distance between two topics ti and t j distUNI ti t j is calculated as the normalized log likelihood based on the training data and unigram models for each topic class Eq 3 Normalization is applied to compensate for varied topic class sizes and perplexities scoreUN I X t j scoreUNI X ti scoreUNI X ti scoreUN I X t j 3 distUNI ti t j X S i X S i X S j X S j S i set of training sentences of topic class ti 5 3 SVM Based Inter Topic Distance For SVM based topic detection the inter topic distance measure is defined as the average distance between topic ti s training set S i and topic t j s SVM hyperplane H j and vice versa Eq 4 
The distance perpendicular to the SVM hyper plane is used as it directly relates to the topic detection score used IEICE TRANS INF SYST VOL 450 Table 3 Topic Class accommodation airplane airport basic communication contact eat drink exchange shopping sightseeing transit trouble No of Training Sentences 16473 7117 10101 21880 11613 7514 18746 2151 19278 14581 17027 22337 10 4 6 13 7 4 11 1 11 9 10 13 Description of topic classes Example sentences I asked for a room with a shower What s the price excluding meals Which channel is the film on What kind of drinks do you have Please show me your passport and immigration form Where can I change travelers cheques Sorry Hi What does that mean Are you from New Zealand May I use your bathroom Extension two thirty four please Call to Japan please Sorry wrong number Could we have an ashtray please I d like some bread Cash this please How would you like it Traveler s checks okay How will you pay for this Can I buy it in Japanese yen Do you know where the baseball stadium 
is Could you take a photo of us please Is this the right platform for the train to Minneapolis Can you send a mechanic please There s something wrong with the clutch distS V M ti t j average scoreS V M X t j average scoreS V M X t j X S i X S j Table 2 Description of basic travel expression corpus average scoreS V M X ti average scoreS V M X ti X S j X S i Training set 12 topics 168 818 sentences Lexicon size 18 k Development set 10 346 sentences Test set 1 990 utterances 0 67 OOV rate 4 S i set of training sentences of topic class ti this study 12 sub domain topic classes as described in Table 3 are used The original corpus consists of a larger set of 15 topic tags however very small topic classes 2 000 training sentences were merged to obtain the above set of topics For example the topic classes restaurant snack food and drinks were merged to form the single topic class eat drink The training set of 168 818 sentences was used for training all language and topic detection models and for constructing the 
topic hierarchy The development set of 10 346 sentences was used to determine the linear interpolation weights applied during TD LM smoothing The test set of 1 990 utterances was used for evaluation 6 2 Experimental Setup Recognition was performed with our Julius recognition engine 24 For acoustic analysis 12 dimensional MFCC energy and first and second order derivatives were computed The acoustic model applied during recognition was a triphone HMM with 1 841 shared states and 23 Gaussian mixture components set up for 26 phones For the baseline ASR system a topic independent generalized LM G LM trained on the entire training set was used On the test set this baseline model had perplexities of 44 8 2 gram and 23 8 3 gram and a WER Word Error Rate of 8 08 6 3 Effect of Topic Dependent Language Modeling 6 1 Evaluation Corpus The proposed recognition architecture was evaluated on the ATR Basic Travel Expression Corpus BTEC 23 This corpus consists of Japanese sentences that Japanese travelers are likely to use or 
encounter while traveling overseas An overview of the corpus is shown in Table 2 In First the effect of topic dependent language modeling TDLM on test set perplexity was investigated We compared three topic labeling schemes the original hand labels relabeling using unigram topic detection and SVM based relabeling Based on these labeling schemes TD LMs were then trained and the test set perplexity was calculated Ta A visual representation of this distance measure is given in Fig 3 where dist S i H j average scoreS V M X t j X S i 5 4 Hierarchical Language Model Back Off In the proposed recognition architecture topic detection involves selecting an appropriate LM from the topic hierarchy This model is then applied during the topic dependent recognition pass Model selection is based on a hierarchical back off mechanism which is dependent on the topic detection method used For unigram based topic detection unigram models are trained for each node in the hierarchy Topic detection involves selecting the node in 
all layers that gives the maximum unigram likelihood In the SVM case an individual topic model is used when the SVM classification score for only one topic is positive Otherwise we determine the two topics with the highest topic classification scores and select their lowest parent node in the hierarchy 6 Experimental Evaluation LANE et al DIALOGUE SPEECH RECOGNITION 451 Table 4 Perplexities by topic dependent language modeling Perplexity Reduction compared to G LM 2 gram 3 gram 44 78 23 77 33 51 25 2 18 94 20 2 28 00 37 5 16 85 29 1 29 60 34 0 17 34 27 1 Topic Labeling Method G LM Hand Unigram SVM ble 4 For the baseline G LM system perplexities by bigram and tri gram models were 44 8 and 23 8 respectively TDLMs created based on the original hand labeled topic tags provide a 20 2 reduction in perplexity over the single GLM This reduction verifies the effectiveness of topic dependent modeling Compared to the hand labeled case both unigram and SVM based re labeling provide a further reduction in perplexity 29 1 
and 27 1 respectively This shows the effectiveness of automatic re labeling Unigram re labeling provides lower perplexity than the SVM case because there is consistency between the log likelihood score used for topic detection and the perplexity measure The unigram method also tends to divide the training set more evenly over the 12 topic classes while SVM re labeling results in class sizes similar to the original topics In the unigram case the smallest topic class contains 5 of the training data while in the SVM case this is only 1 6 4 Performance of Topic Detection Next the performance of the unigram and SVM based topic detection methods were compared The re labeling process was applied to the correct transcriptions of the test set and the automatically assigned tag is regarded as the correct topic of that sentence Detection accuracy was evaluated by comparing the result when topic detection is applied to the ASR hypothesis to these topic tags The topic detection error rate for the two methods when only 
individual topics layer 1 and when hierarchical back off layer 1 or layer2 was applied are shown in Fig 4 For the hierarchical back off case the topic detection result is correct if either the correct individual model layer 3 or its parent model from layer 2 is selected Both unigram and SVM based methods achieve high performance with detection error rates of 9 8 and 7 7 respectively SVM significantly outperforms the unigrambased method for both the individual topic case relative reduction of 21 4 and for the hierarchical back off case relative reduction of 14 6 This indicates that SVM realizes improved robustness against recognition errors For both methods the hierarchical back off mechanism reduces topic detection errors by around 14 This shows the effectiveness of the back off mechanism Fig 4 Performance of topic detection methods Table 5 Classification Method Topic dependent recognition performance WER Relative reduction Layer 1 Layer 3 Layers All G LM 1 3 Layers baseline Topic detection applied to 
correct transcription Oracle Unigram 8 08 7 36 6 93 6 91 Oracle 8 9 21 7 21 8 SVM 8 08 7 64 7 10 7 04 Oracle 5 2 12 0 12 0 Topic Detection applied to ASR result Unigram 8 08 8 12 7 36 7 30 0 5 8 9 9 7 SVM 8 08 8 24 7 42 7 25 1 2 8 2 10 3 6 5 Performance of Topic Dependent ASR The proposed recognition framework was implemented by combining topic detection and TD LMs The recognition performance WER for the unigram and SVM based systems when various layers of the topic hierarchy are used are shown in Table 5 The baseline system applies only the GLM layer 1 during recognition For reference the system performance when oracle topic detection is applied is also shown In this scheme the correct transcription of the input utterance is used for topic detection instead of the ASR result Applying this approach when only the layer 3 models are applied relative reductions in WER of 8 9 and 5 2 are gained over the baseline system for the unigram and SVM systems respectively This shows that improved recognition accuracy can 
be gained by more constrictive topic dependent language modeling Compared to the SVM system the unigram system had a slightly lower WER Including the comparison with the layer 1 model G LM further improves recognition accuracy For around 5 of the utterances the topic independent G LM model gave a better recognition hypothesis than the appropriate topic model As the G LM is trained over the entire training set it is less affected by data sparseness than the individual topic models The inclusion of the layer 2 models using hierarchical back off provided little improvement in recognition accuracy in the oracle case IEICE TRANS INF SYST VOL 452 Fig 5 Comparison with parallel systems Fig 6 Extension to multiple TD LMs Next we investigate the system performance when TDLMs are selected based on the topic detection result from the ASR hypothesis in the initial recognition pass When only the layer 3 TD LMs are applied the recognition performance drops below that of the baseline system This shows that applying an 
incorrect TD LM model significantly degrades recognition performance for that utterance In the SVM case even a small number of mis classified utterances less than 8 degrades the overall system performance from the baseline Introducing the comparison with the layer 1 mitigates the effect of topic detection errors This comparison is vital for effective performance The inclusion of the layer 2 models selected by hierarchical back off further improves recognition accuracy Compared to the baseline system a relative improvement of around 10 for both systems is gained This shows that the proposed hierarchy back off mechanism realizes robustness against topic detection errors caused in the initial ASR pass While the unigram method provided a large reduction in TD LM perplexity and WER for the oracle case SVM based topic detection improved topic detection robustness Both approaches realize comparable recognition performance when combined with the proposed architecture 6 6 Comparison with Parallel Decoding Scheme 6 7 
Extension to Multiple Topics In the proposed recognition framework a single TD LM is selected based on the topic detection result This framework can be extended to perform recognition with multiple TD LMs In this case the top m topics with highest topic classification scores are determined by topic detection and then speech recognition re decoding is performed in parallel with the selected TD LMs The recognition results from the G LM and TD LMs are finally compared and the hypothesis with maximum ASR score is output This approach will reduce the number of topic detection errors and improve recognition performance but incurs increased computational cost compared to the original framework The performance of the extended system is shown in Fig 6 for various values of m As the number of TD LMs applied was increased recognition performance was also improved At m 3 the system performance was similar to that using the proposed framework with hierarchical backoff Proposed However at m 6 the system performance begins 
to degrade Since it is not easy to determine the best value of m a priori the proposed framework applying a single TD LM offers a reasonable solution and requires only two recognition passes 7 Conclusion Next the performance of the proposed framework is compared with a parallel recognition scheme which performs recognition in parallel with the layer 1 G LM and all layer 3 TD LMs The recognition result with maximum ASR score is output The recognition performance for unigram and SVM based TD LMs combined with the proposed framework and parallel decoding is shown in Fig 5 Both decoding schemes provide a significant reduction in WER compared to the baseline system Although similar performance is gained with the two approaches the proposed framework had a much lower computational cost requiring only 2 recognition processes compared to 13 for the parallel system We have presented an efficient speech recognition architecture for multi domain spoken language systems combining topic detection and topic dependent 
language modeling In the proposed approach the inferred domain of the user s utterance is automatically detected and speech recognition is then performed with an appropriate topic dependent language model To improve robustness against topic detection errors a hierarchical back off mechanism was introduced that applies detailed topic models when topic detection is confident and applies wider models that cover multiple topics in cases of uncertainty The performance of the proposed architecture was evaluated when combined with unigram and SVM based topic detection On the ATR Basic Travel Expression Corpus both methods provided improved recognition perfor LANE et al DIALOGUE SPEECH RECOGNITION 453 mance compared to a single language model system relative reductions in WER of 9 8 and 10 3 respectively The unigram based approach provided lower TD LM perplexity and improved recognition accuracy when the topic was given however SVM provided improved topic detection robustness to ASR errors The overall system 
performance of both methods was comparable Finally in comparison with a parallel approach the proposed architecture achieves similar recognition accuracy while requiring much less computational cost Thus the proposed system realizes more accurate speech recognition in an efficient manner Acknowledgment The research reported here was supported in part by a contract with the Telecommunications Advancement Organization of Japan entitled A study of speech dialog translation technology based on a large corpus References 1 V Zue S Seneff J Glass J Polifroni C Pao T Hazen and L Hetherington JUPITER A telephone based conversational interface for weather information IEEE Trans Speech Audio Process vol 8 no 1 pp 12 D B Moran A J Cheyer L E Julia D L Martin and S Park Multimodal user interfaces in the open agent architecture Proc International Conference on Intelligent User Interfaces vol Ian R Lane received the B Tech degree in information engineering from Massey University New Zealand in 2000 He was awarded a 
Monbukagakusho scholarship to study in the School of Informatics Kyoto University where he received the M E degree in 2003 and is currently undertaking research towards a Ph D degree He is also an intern researcher at ATR Spoken Language Translation Research Laboratories Mr Lane is a member of the Acoustical Society of Japan ASJ and IEEE IEICE TRANS INF SYST VOL 454 Tatsuya Kawahara received the B E degree in 1987 the M E degree in 1989 and the Ph D degree in 1995 all in information science from Kyoto University Kyoto Japan In 1990 he became a Research Associate with Department of Information Science Kyoto University From 1995 to 1996 he was a Visiting Researcher at Bell Laboratories Murray Hill NJ USA Currently he is a Professor with Academic Center for Computing and Media Studies Kyoto University He is also an Invited Researcher at ATR Spoken Language Translation Research Laboratories He has published more than 100 technical papers covering speech recognition confidence measures and spoken dialogue systems 
He has been managing several speech related projects in Japan including a free large vocabulary continuous speech recognition software project http julius sourceforge jp Dr Kawahara received the 1997 Awaya Memorial Award from the Acoustical Society of Japan and the 2000 Sakai Memorial Award from the Information Processing Society of Japan Since 2003 he has been a member of the IEEE SPS Speech Technical Committee Tomoko Matsui received the Ph D degree in the Computer Science Department Tokyo Institute of Technology Tokyo in 1997 From 1988 to 2002 she was with NTT where she worked on speaker and speech recognition From 1998 to 2002 she belonged to the Spoken Language Translation Research Laboratory ATR Kyoto as a senior researcher and worked on speech recognition From January to June in 2001 she was an invited researcher in the Acoustic and Speech Research Department Bell Laboratories Murray Hill NJ USA working on finding effective confidence measures for verifying speech recognition results She is presently 
an associate professor in the Institute of Statistical Mathematics Tokyo working on statistical modeling for speech and speaker recognition applications She received the paper award of the Institute of Electronics Information and Communication Engineers of Japan IEICE in 1993 Satoshi Nakamura was born in Japan on August 4 1958 He received B S degree in electronics engineering from Kyoto Institute of Technology in 1981 and the Ph D degree in information science from Kyoto University in 1992 Between 
33	a	SMLI TR2004 0811 c 2004 SUN MICROSYSTEMS INC 1 Sphinx 4 A Flexible Open Source Framework for Speech Recognition Willie Walker Paul Lamere Philip Kwok Bhiksha Raj Rita Singh Evandro Gouvea Peter Wolf Joe Woelfel Abstract Sphinx 4 is a flexible modular and pluggable framework to help foster new innovations in the core research of hidden Markov model HMM recognition systems The design of Sphinx 4 is based on patterns that have emerged from the design of past systems as well as new requirements based on areas that researchers currently want to explore To exercise this framework and to provide researchers with a research ready system Sphinx 4 also includes several implementations of both simple and state of the art techniques The framework and the implementations are all freely available via open source I I NTRODUCTION W HEN researchers approach the problem of core speech recognition research they are often faced with the problem of needing to develop an entire system from scratch even if they only want to 
explore one facet of the field Open source speech recognition systems are available such as HTK 1 ISIP 2 AVCSR 3 and earlier versions of the Sphinx systems W Walker P Lamere and P Kwok are with Sun Microsystems E Gouvea and R Singh are with Carnegie Mellon University B Raj P Wolf and J Woelfel are with Mitsubishi Electric Research Labs SMLI TR2004 0811 c 2004 SUN MICROSYSTEMS INC 2 Fig 1 Sphinx 4 Decoder Framework The main blocks are the FrontEnd the Decoder and the Linguist Supporting blocks include the ConfigurationManager and the Tools blocks The communication between the blocks as well as communication with an application is depicted III S PHINX 4 F RAMEWORK The Sphinx 4 framework has been designed with a high degree of flexibility and modularity Figure 1 shows the overall architecture of the system Each labeled element in Figure 1 represents a module that can be easily replaced allowing researchers to experiment with different module implementations without needing to modify other portions of the system 
There are three primary modules in the Sphinx 4 framework the FrontEnd the Decoder and the Linguist The FrontEnd takes one or more input signals and parameterizes them into a sequence of Features The Linguist translates any type of standard language model along with pronunciation information from the Dictionary and structural information from one or more sets of AcousticModels into a SearchGraph The SearchManager in the Decoder uses the Features from the FrontEnd and the SearchGraph from the Linguist to perform the actual decoding generating Results At any time prior to or during the recognition process the application can issue Controls to each of the modules effectively becoming a partner in the recognition process The Sphinx 4 system is like most speech recognition systems in that it has a large number of configurable parameters such as search beam size for tuning the system performance The Sphinx 4 ConfigurationManager is used to configure such parameters Unlike other systems however the 
ConfigurationManager also gives Sphinx 4 the ability to dynamically load and configure modules at run time yielding a flexible and pluggable system For example Sphinx 4 is typically configured with a FrontEnd see Section IV that produces Mel Frequency Cepstral Coefficients MFCCs 15 Using the ConfigurationManager however it is possible to reconfigure Sphinx 4 to construct a different FrontEnd that produces Perceptual Linear Prediction coefficients PLP 16 without needing to modify any source code or to recompile the system To give applications and developers the ability to track decoder statistics such as word error rate 17 runtime speed and memory usage Sphinx 4 provides a number of Tools As with the rest of the system the Tools are highly configurable allowing users to perform a wide range of system analysis Furthermore the Tools also provides an interactive runtime environment that allows users to modify the parameters of the system while the system is running allowing for rapid experimentation with various 
parameters settings Sphinx 4 also provides support for Utilities that support application level processing of recognition results For example these utilities include support for obtaining result lattices confidence scores and natural language understanding IV F RONT E ND The purpose of the FrontEnd is to parameterize an Input signal e g audio into a sequence of output Features As illustrated in Figure 2 the FrontEnd comprises one or more parallel chains of replaceable communicating signal processing modules called DataProcessors Supporting multiple chains permits simultaneous computation of different types of parameters from the same or different input signals This enables the creation of systems that can simultaneously decode using different parameter types such as MFCC and PLP and even parameter types derived from non speech signals such as video 3 SMLI TR2004 0811 c 2004 SUN MICROSYSTEMS INC 3 Fig 2 Sphinx 4 FrontEnd The FrontEnd comprises one or more parallel chains of communicating DataProcessors Like 
the ISIP 2 system each DataProcessor in the FrontEnd provides an input and an output that can be connected to another DataProcessor permitting arbitrarily long sequences of chains The inputs and outputs of each DataProcessor are generic Data objects that encapsulate processed input data as well as markers that indicate data classification events such as end point detection The last DataProcessor in each chain is responsible for producing a Data object composed of parameterized signals called Features to be used by the Decoder Like the AVCSR system 3 Sphinx 4 permits the ability to produce parallel sequences of features Sphinx 4 is unique however in that it allows for an arbitrary number of parallel streams The communication between blocks follows a pull design With a pull design a DataProcessor requests input from an earlier module only when needed as opposed to the more conventional push design where a module propagates its output to the succeeding module as soon as it is generated This pull design enables 
the processors to perform buffering allowing consumers to look forwards or backwards in time The ability to look forwards or backwards in time not only permits the Decoder to perform frame synchronous Viterbi searches 18 but also allows the decoder to perform other types of searches such as depth first and A 19 Within the generic FrontEnd framework the Sphinx 4 provides a suite of DataProcessors that implement common signal processing techniques These implementations include support for the following reading from a variety of input formats for batch mode operation reading from the system audio input device for live mode operation preemphasis windowing with a raised cosine transform e g Hamming and Hanning windows discrete fourier transform via FFT mel frequency filtering bark frequency warping discrete cosine transform DCT linear predictive encoding LPC end pointing cepstral mean normalization CMN mel cepstra frequency coefficient extraction MFCC and perceptual linear prediction coefficient extraction PLP 
Using the ConfigurationManager described in Section III users can chain the Sphinx 4 DataProcessors together in any manner as well as incorporate DataProcessor implementations of their own design As such the modular and pluggable nature of Sphinx 4 not only applies to the higher level structure of Sphinx 4 but also applies to the higher level modules themselves i e the FrontEnd is a pluggable module yet also consists of pluggable modules itself V L INGUIST The Linguist generates the SearchGraph that is used by the decoder during the search while at the same time hiding the complexities involved in generating this graph As is the case throughout Sphinx 4 the Linguist is a pluggable module allowing people to dynamically configure the system with different Linguist implementations A typical Linguist implementation constructs the SearchGraph using the language structure as represented by a given LanguageModel and the topological structure of the AcousticModel HMMs for the basic sound units used by the system The 
Linguist may also use a Dictionary typically a pronunciation lexicon to map words from the LanguageModel into sequences of AcousticModel elements When generating the SearchGraph the Linguist may also incorporate sub word units with contexts of arbitrary length if provided By allowing different implementations of the Linguist to be plugged in at runtime Sphinx 4 permits individuals to provide different configurations for different system and recognition requirements For instance a simple numerical digits recognition application might use a simple Linguist that keeps the search space entirely in memory On the other hand a dictation application with a 100K word vocabulary might use a sophisticated Linguist that keeps only a small portion of the potential search space in memory at a time The Linguist itself consists of three pluggable components the LanguageModel the Dictionary and the AcousticModel which are described in the following sections A LanguageModel The LanguageModel module of the Linguist provides 
word level language structure which can be represented by any number of pluggable implementations These implementations typically fall into one of two categories graph driven grammars and stochastic N Gram models The graph driven grammar represents a directed word graph where each node represents a single word and each arc represents the probability of a word transition taking place The stochastic N Gram models provide probabilities for words given the observation of the previous n 1 words SMLI TR2004 0811 c 2004 SUN MICROSYSTEMS INC 4 The Sphinx 4 LanguageModel implementations support a variety of formats including the following SimpleWordListGrammar defines a grammar based upon a list of words An optional parameter defines whether the grammar loops or not If the grammar does not loop then the grammar will be used for isolated word recognition If the grammar loops then it will be used to support trivial connected word recognition that is the equivalent of a unigram grammar with equal probabilities B 
Dictionary The Dictionary provides pronunications for words found in the LanguageModel The pronunciations break words into sequences of sub word units found in the AcousticModel The Dictionary interface also supports the classification of words and allows for a single word to be in multiple classes Sphinx 4 currently provides implementions of the Dictionary interface to support the CMU Pronouncing Dictionary 23 The various implementations optimize for usage patterns based on the size of the active vocabulary For example one implementation will load the entire vocabulary at system initialization time whereas another implementation will only obtain pronunciations on demand C AcousticModel The AcousticModel module provides a mapping between a unit of speech and an HMM that can be scored against incoming features provided by the FrontEnd As with other systems the mapping may also take contextual and word position information into account For example in the case of triphones the context represents the single 
phonemes to the left and right of the given phoneme and the word position represents whether the triphone is at the beginning middle or end of a word or is a word itself The contextual definition is not fixed by Sphinx 4 allowing for the definition of AcousticModels that contain allophones as well as AcousticModels whose contexts do not need to be adjacent to the unit Typically the Linguist breaks each word in the active vocabulary into a sequence of context dependent sub word units The Linguist then passes the units and their contexts to the AcousticModel retrieving the HMM graphs associated with those units It then uses these HMM graphs in conjunction with the LanguageModel to construct the SearchGraph Unlike most speech recognition systems which represent the HMM graphs as a fixed structure in memory the Sphinx 4 HMM is merely a directed graph of objects In this graph each node corresponds to an HMM state and each arc represents the probability of transitioning from one state to another in the HMM By 
representing the HMM as a directed graph of objects instead of a fixed structure an implementation of the AcousticModel can easily supply HMMs with different topologies For example the AcousticModel interfaces do not restrict the HMMs in terms of the number of states the number or transitions out of any state or the direction of a transition forward or backward Furthermore Sphinx 4 allows the number of states in an HMM to vary from one unit to another in the same AcousticModel Each HMM state is capable of producing a score from an observed feature The actual code for computing the score is done by the HMM state itself thus hiding its implementation from the rest of the system even permitting differing probability density functions to be used per HMM state The AcousticModel also allows sharing of various components at all levels That is the components that make up a particular HMM state such as Gaussian mixtures transition matrices and mixture weights can be shared by any of the HMM states to a very fine 
degree As with the rest of Sphinx 4 individuals can configure Sphinx 4 with different implementations of the AcousticModel based upon their needs Sphinx 4 currently provides a single AcousticModel implementation that is capable of loading and using acoustic models generated by the Sphinx 3 trainer D SearchGraph Even though Linguists may be implemented in very different ways and the topologies of the search spaces generated by these Linguists can vary greatly the search spaces are all represented as a SearchGraph Illustrated by example in Figure 3 the SearchGraph is the primary data structure used during the decoding process SMLI TR2004 0811 c 2004 SUN MICROSYSTEMS INC 5 Fig 3 Example SearchGraph The SearchGraph is a directed graph composed of optionally emitting SearchStates and SearchStateArcs with transition probabilities Each state in the graph can represent components from the LanguageModel words in rectangles Dictionary sub word units in dark circles or AcousticModel HMMs The graph is a directed graph 
in which each node called a SearchState represents either an emitting or a non emitting state Emitting states can be scored against incoming acoustic features while non emitting states are generally used to represent higher level linguistic constructs such as words and phonemes that are not directly scored against the incoming features The arcs between states represent the possible state transitions each of which has a probability representing the likelihood of transitioning along the arc The SearchGraph interface is purposely generic to allow for a wide range of implementation choices relieving the assumptions and hard wired constraints found in previous recognition systems In particular the Linguist places no inherent restrictions on the following SMLI TR2004 0811 c 2004 SUN MICROSYSTEMS INC 6 VI D ECODER The primary role of the Sphinx 4 Decoder block is to use Features from the FrontEnd in conjunction with the SearchGraph from the Linguist to generate Result hypotheses The Decoder block comprises a 
pluggable SearchManager and other supporting code that simplifies the decoding process for an application As such the most interesting component of the Decoder block is the SearchManager The Decoder merely tells the SearchManager to recognize a set of Feature frames At each step of the process the SearchManager creates a Result object that contains all the paths that have reached a final non emitting state To process the result Sphinx 4 also provides utilities capable of producing a lattice and confidence scores from the Result Unlike other systems however applications can modify the search space and the Result object in between steps permitting the application to become a partner in the recognition process Like the Linguist the SearchManager is not restricted to any particular implementation For example implementations of the SearchManager may perform search algorithms such as frame synchronous Viterbi A bi directional and so on Each SearchManager implementation uses a token passing algorithm as described 
by Young 24 A Sphinx 4 token is an object that is associated with a SearchState and contains the overall acoustic and language scores of the path at a given point a reference to the SearchState a reference to an input Feature frame and other relevant information The SearchState reference allows the SearchManager to relate a token to its state output distribution context dependent phonetic unit pronunciation word and grammar state Every partial hypothesis terminates in an active token As illustrated in Figure 1 implementations of a SearchManager may construct a set of active tokens in the form of an ActiveList at each time step though the use of an ActiveList is not required As it is a common technique however Sphinx 4 provides a sub framework to support SearchManagers composed of an ActiveList a Pruner and a Scorer The SearchManager sub framework generates ActiveLists from currently active tokens in the search trellis by pruning using a pluggable Pruner Applications can configure the Sphinx 4 implementations 
of the Pruner to perform both relative and absolute beam pruning The implementation of the Pruner is greatly simplifed by the garbage collector of the Java platform With garbage collection the Pruner can prune a complete path by merely removing the terminal token of the path from the ActiveList The act of removing the terminal token identifies the token and any unshared tokens for that path as unused allowing the garbage collector to reclaim the associated memory The SearchManager sub framework also communicates with the Scorer a pluggable state probability estimation module that provides state output density values on demand When the SearchManager requests a score for a given state at a given time the Scorer accesses the feature vector for that time and performs the mathematical operations to compute the score In the case of parallel decoding using parallel acoustic models the Scorer matches the acoustic model set to be used against the feature type The Scorer retains all information pertaining to the state 
output densities Thus the SearchManager need not know whether the scoring is done with continuous semi continuous or discrete HMMs Furthermore the probability density function of each HMM state is isolated in the same fashion Any heuristic algorithms incorporated into the scoring procedure for speeding it up can also be performed locally within the scorer In addition the scorer can take advantage of multiple CPUs if they are available The current Sphinx 4 implementation provides pluggable implementations of SearchManagers that support frame synchronous Viterbi 18 Bushderby 25 and parallel decoding 26 SMLI TR2004 0811 c 2004 SUN MICROSYSTEMS INC 7 WER RT Sphinx 3 3 Sphinx 4 Sphinx 3 3 Sphinx 4 1 CPU Sphinx 4 2 CPU TI46 11 words 1 217 0 168 0 14 0 03 0 02 TIDIGITS 11 words 0 661 0 549 0 16 0 07 0 05 AN4 79 words 1 300 1 192 0 38 0 25 0 20 RM1 1000 words 2 746 2 739 0 50 0 50 0 40 WSJ5K 5000 words 7 323 7 174 1 36 1 22 0 96 HUB 4 64000 words 18 845 18 878 3 06 4 40 3 80 TABLE I S PHINX 4 PERFORMANCE W ORD E 
RROR R ATE WER IS GIVEN IN PERCENT R EAL T IME RT SPEED IS THE RATIO OF UTTERANCE DURATION TO THE TIME TO DECODE THE UTTERANCE Test F OR BOTH A LOWER VALUE INDICATES BETTER PERFORMANCE D ATA GATHERED ON A D UAL CPU 1015M HZ U LTRA SPARC R III WITH 2G RAM Furthermore the modularity of Sphinx 4 also allows it to support a wide variety of tasks For example the various SearchManager implementations allow Sphinx 4 to efficiently support tasks that range from small vocabulary tasks such as TI461 28 and TIDIGITS2 29 to large vocabulary tasks such as HUB 4 30 As another example the various Linguist implementations allow Sphinx 4 to support different tasks such as traditional CFG based command and control applications in addition to applications that use stochastic language models The modular nature of Sphinx 4 was enabled primarily by the use of the Java programming language In particular the ability of the Java platform to load code at run time permits simple support for the pluggable framework and the Java 
programming language construct of interfaces permits separation of the framework design from the implementation The Java platform also provides Sphinx 4 with a number of other advantages 1 TI46 2 TIDIGITS refers to the NIST CD ROM Version of the Texas Instruments developed 46 Word Speaker Dependent Isolated Word Speech Database refers to the NIST CD ROM Version of the Texas Instruments developed Studio Quality Speaker Independent Connected Digit Corpus SMLI TR2004 0811 c 2004 SUN MICROSYSTEMS INC 8 the Sphinx 4 acoustic model design allows for very fine parameter tying We predict that taking advantage of these capabilities will greatly increase both the speed and accuracy of the decoder We have created a design for a Sphinx 4 acoustic model trainer that can produce acoustic models with these desirable characteristics 31 As with the Sphinx 4 framework the Sphinx 4 acoustic model trainer has been designed to be a modular pluggable system Such an undertaking however represents a significant effort As an interim 
step another area for experimentation is to create FrontEnd and AcousticModel implementations that support the models generated by the HTK system 1 We have also considered the architectural changes that would be needed to support segment based recognition frameworks such as the MIT SUMMIT speech recognizer 32 A cursory analysis indicates the modifications to the Sphinx 4 architecture would be minimal and would provide a platform to do meaningful comparisons between segemental and fixed frame size systems Finally the SearchManager provides fertile ground for implementing a variety of search approaches including A fast match bi directional and multiple pass algorithms IX C ONCLUSION After careful development of the Sphinx 4 framework we created a number of differing implementations for each module in the framework For example the FrontEnd implementations support MFCC PLP and LPC feature extraction the Linguist implementations support a variety of language models including CFGs FSTs and N Grams and the Decoder 
supports a variety of SearchManager implementations including traditional Viterbi Bushderby and parallel searches Using the ConfigurationManager the various implementations of the modules can be combined in various ways supporting our claim that we have developed a flexible pluggable framework Furthermore the framework is performing well both in speed and accuracy when compared to its predecessors The Sphinx 4 framework is already proving itself as being research ready easily supporting various work such as the parallel and Bushderby SearchManagers as well as a specialized Linguist that can apply unigram smear probabilities to lex trees We view this as only the very beginning however and expect Sphinx 4 to support future areas of core speech recognition research Finally the source code to Sphinx 4 is freely available under a BSD style license The license permits others to do academic and commercial research and to develop products without requiring any licensing fees More information is available at http 
cmusphinx sourceforge net sphinx4 ACKNOWLEDGMENTS The authors would like to thank Prof Richard Stern at CMU Robert Sproull at Sun Microsystems Laboratories and Joe Marks at MERL for making this team possible We also thank Sun Microsystems Laboratories and the current management for their continued support and collaborative research funds Rita Singh was sponsored by the Space and Naval Warfare Systems Center San Diego under Grant No N66001 99 1 8905 The content of this paper does not necessarily reflect the position or the policy of the U S Government and no official endorsement should be inferred R EFERENCES 1 S Young The HTK hidden Markov model toolkit Design and philosophy Cambridge University Engineering Department UK Tech Rep CUED FINFENG TR152 Sept 1994 2 N Deshmukh A Ganapathiraju J Hamaker J Picone and M Ordowski A public domain speech to text system in Proceedings of the 6th European Conference on Speech Communication and Technology vol 5 Budapest Hungary Sept 1999 pp SMLI TR2004 0811 c 2004 SUN 
MICROSYSTEMS INC 9 14 X Huang A Acero F Alleva M Hwang L Jiang and M Mahajan From SPHINX II to Whisper Making speech recognition usable in Automatic Speech and Speaker Recognition Advanced Topics C Lee F Soong and K Paliwal Eds Norwell MA Kluwer Academic Publishers 1996 15 S B Davis and P Mermelstein Comparison of parametric representations for monosyllable word recognition in continuously spoken sentences in IEEE Transactions on Acoustic Speech and Signal Processing vol 28 no 4 Aug 1980 16 H Hermansky Perceptual linear predictive PLP analysis of speech Journal of the Acoustical Society of America vol 87 no 4 pp 
34	a	Conditional Random Fields An Introduction Hanna M Wallach February 24 2004 1 Labeling Sequential Data The task of assigning label sequences to a set of observation sequences arises in many fields including bioinformatics computational linguistics and speech recognition 6 9 12 For example consider the natural language processing task of labeling the words in a sentence with their corresponding part of speech POS tags In this task each word is labeled with a tag indicating its appropriate part of speech resulting in annotated text such as 1 PRP He VBZ reckons DT the JJ current NN account NN deficit MD will VB narrow TO to RB only CD 1 8 CD billion IN in NNP September Labeling sentences in this way is a useful preprocessing step for higher natural language processing tasks POS tags augment the information contained within words alone by explicitly indicating some of the structure inherent in language One of the most common methods for performing such labeling and segmentation tasks is that of employing hidden 
Markov models 13 HMMs or probabilistic finite state automata to identify the most likely sequence of labels for the words in any given sentence HMMs are a form of generative model that defines a joint probability distribution p X Y where X and Y are random variables respectively ranging over observation sequences and their corresponding label sequences In order to define a joint distribution of this nature generative models must enumerate all possible observation University of Pennsylvania CIS Technical Report MS CIS 04 21 1 depend on the state or label at that time This is an appropriate assumption for a few simple data sets however most real world observation sequences are best represented in terms of multiple interacting features and long range dependencies between observation elements This representation issue is one of the most fundamental problems when labeling sequential data Clearly a model that supports tractable inference is necessary however a model that represents the data without making 
unwarranted independence assumptions is also desirable One way of satisfying both these criteria is to use a model that defines a conditional probability p Y x over label sequences given a particular observation sequence x rather than a joint distribution over both label and observation sequences Conditional models are used to label a novel observation sequence x by selecting the label sequence y that maximizes the conditional probability p y x The conditional nature of such models means that no effort is wasted on modeling the observations and one is free from having to make unwarranted independence assumptions about these sequences arbitrary attributes of the observation data may be captured by the model without the modeler having to worry about how these attributes are related Conditional random fields 8 CRFs are a probabilistic framework for labeling and segmenting sequential data based on the conditional approach described in the previous paragraph A CRF is a form of undirected graphical model that 
defines a single log linear distribution over label sequences given a particular observation sequence The primary advantage of CRFs over hidden Markov models is their conditional nature resulting in the relaxation of the independence assumptions required by HMMs in order to ensure tractable inference Additionally CRFs avoid the label bias problem 8 a weakness exhibited by maximum entropy Markov models 9 MEMMs and other conditional Markov models based on directed graphical models CRFs outperform both MEMMs and HMMs on a number of real world sequence labeling tasks 8 11 15 2 Undirected Graphical Models A conditional random field may be viewed as an undirected graphical model or Markov random field 3 globally conditioned on X the random variable representing observation sequences Formally we define G V E to be an undirected graph such that there is a node v V corresponding to each of the random variables representing an element Yv of Y If each random variable Yv obeys the Markov property with respect to G then 
Y X is a conditional random field In theory the structure of graph G may be arbitrary provided it represents the conditional independencies in the label sequences being modeled However when modeling sequences the simplest and most common graph structure encountered is that in which the nodes corresponding to elements of 2 Y form a simple first order chain as illustrated in Figure 1 Y1 Y2 Y3 Yn 1 Yn X X1 Xn 1 Xn Figure 1 Graphical structure of a chain structured CRFs for sequences The variables corresponding to unshaded nodes are not generated by the model 2 1 Potential Functions The graphical structure of a conditional random field may be used to factorize the joint distribution over elements Yv of Y into a normalized product of strictly positive real valued potential functions derived from the notion of conditional independence 1 Each potential function operates on a subset of the random variables represented by vertices in G According to the definition of conditional independence for undirected graphical 
models the absence of an edge between two vertices in G implies that the random variables represented by these vertices are conditionally independent given all other random variables in the model The potential functions must therefore ensure that it is possible to factorize the joint probability such that conditionally independent random variables do not appear in the same potential function The easiest way to fulfill this requirement is to require each potential function to operate on a set of random variables whose corresponding vertices form a maximal clique within G This ensures that no potential function refers to any pair of random variables whose vertices are not directly connected and if two vertices appear together in a clique this relationship is made explicit In the case of a chain structured CRF such as that depicted in Figure 1 each potential function will operate on pairs of adjacent label variables Yi and Yi 1 It is worth noting that an isolated potential function does not have a direct 
probabilistic interpretation but instead represents constraints on the configurations of the random variables on which the function is defined This in turn affects the probability of global 1 The product of a set of strictly positive real valued functions is not guaranteed to satisfy the axioms of probability A normalization factor is therefore introduced to ensure that the product of potential functions is a valid probability distribution over the random variables represented by vertices in G 3 3 Conditional Random Fields Lafferty et al 8 define the the probability of a particular label sequence y given observation sequence x to be a normalized product of potential functions each of the form exp j j tj yi 1 yi x i k 2 where tj yi 1 yi x i is a transition feature function of the entire observation sequence and the labels at positions i and i 1 in the label sequence sk yi x i is a state feature function of the label at position i and the observation sequence and j Each feature function takes on the value of 
one of these real valued observation features b x i if the current state in the case of a state function or previous and current states in the case of a transition function take on particular values All feature functions are therefore real valued For example consider the following transition function tj yi 1 yi x i b x i if yi 1 IN and yi NNP 0 otherwise In the remainder of this report notation is simplified by writing s yi x i s yi 1 yi x i and Fj y x i 1 n fj yi 1 yi x i where each fj yi 1 yi x i is either a state function s yi 1 yi x i or a transition function t yi 1 yi x i This allows the probability of a label sequence y given an observation sequence x to be written as p y x Z x is a normalization factor 4 1 exp Z x j Fj y x j 3 4 Maximum Entropy The form of a CRF as given in 3 is heavily motivated by the principle of maximum 5 Maximum Likelihood Parameter Inference This function is concave guaranteeing convergence to the global maximum Differentiating the log likelihood with respect to parameter j 
gives L Ep Y X Fj Y X j Ep Y x k Fj Y x k k Assuming the training data x k y k are independently and identically distributed the product of 3 over all training sequences as a function of the parameters is known as the likelihood denoted by p y k x k Maximum likelihood training chooses parameter values such that the logarithm of the likelihood known as the log likelihood is maximized For a CRF the loglikelihood is given by 1 log j Fj y k x k L Z x k j k where p Y X is the empirical distribution of training data and Ep zero yields the maximum entropy model constraint The expectation of each feature with respect to the model distribution is equal to the expected value under the empirical distribution of the training data It is not possible to analytically determine the parameter values that maximize the log 6 CRF Probability as Matrix Computations For a chain structured CRF in which each label sequence is augmented by start and end states y0 and yn 1 with labels start and end respectively the probability p y x 
of label sequence y given an observation sequence x may be efficiently computed using matrices Letting Y be the alphabet from which labels are drawn and y and y be labels drawn from this alphabet we define a set of n 1 matrices Mi x i 1 n 1 where each Mi x is a j j fj y y x i The unnormalized probability of label sequence y given observation sequence x may be written as the product of the appropriate elements of the n 1 matrices for that pair of sequences p y x 1 Z x n 1 Mi yi 1 yi x i 1 Similarly the normalization factor Z x for observation sequence x may be computed from the set of Mi x matrices using closed semirings an algebraic structure that provides a general framework for solving path problems in graphs Omitting details Z x is given by the start end entry of the product of all n 1 Mi x matrices n 1 Z x i 1 Mi x start end 4 6 7 Dynamic Programming In order to identify the maximum likelihood parameter y p Y y x k Fj y x k 5 Performing such calculations in a n p Yi 1 y Yi y x k fj y y x k i 1 y y 6 
eliminating the need to sum over n Y sequences Furthermore a dynamic programming method similar to the forward backward algorithm for hidden Markov models may be used to calculate p Yi 1 y Yi y x k Defining forward and backward Z x is given by the start stop entry of the product of all n 1 Mi x matrices as in 4 Substituting this expression into 6 yields an efficient dynamic programming method for computing feature expectations 7 References 1 A L Berger The improved iterative scaling algorithm A gentle introduction 1997 2 A L Berger S A Della Pietra and V J Della Pietra A maximum entropy approach to natural language processing Computational Linguistics 22 1 15 F Sha and F Pereira Shallow parsing with conditional random fields Proceedings of Human Language Technology NAACL 2003 2003 16 C E Shannon A mathematical theory of communication Bell System Tech Journal 27 9 
35	a	Multi Pass ASR using Vocabulary Expansion Katsutoshi Ohtsuki Nobuaki Hiroshima Shoichi Matsunaga and Yoshihiko Hayashi NTT Cyber Space Laboratories NTT Corporation 1 1 Hikari no oka Yokosuka shi Kanagawa 239 0847 Japan ohtsuki katsutoshi lab ntt co jp vocabulary using morphological knowledge was also studied in 9 for Serbo Croatian and German broadcast news speech Our approach directly estimates relevant words to input speech based on the conceptual base that models word cooccurrence patterns 10 The conceptual base enables one to measure the distance between words in word co occurrence pattern space and direct estimation of relevant words can reduce OOV words more effectively than estimation of relevant words via relevant documents or sub word sequences An expanded vocabulary is built by adding the relevant words to a reference vocabulary and is used in the second recognition process Since the vocabulary expansion process just adds relevant words to a reference vocabulary the second recognition process runs 
just after the first recognition We refer to this approach of multiple recognition processes as MASSIVE Multi pass Automatic Speech recognition uSIng Vocabulary Expansion The rest of the paper is organized as follows Section 2 presents an overview of multi pass speech recognition using vocabulary expansion Section 3 describes vocabulary expansion based on the conceptual base Section 4 presents the evaluation of MASSIVE on broadcast news speech Section 5 concludes the paper Abstract Current automatic speech recognition ASR systems have to limit their vocabulary size depending on available memory size expected processing time and available text data for building a vocabulary and a language model Although the vocabularies of ASR systems are designed to achieve high coverage for the expected input data it cannot be avoided that input data includes out of vocabulary OOV words This is called the OOV problem We propose dynamic vocabulary expansion using a conceptual base and multi pass speech recognition using an 
expanded vocabulary Relevant words to content of input speech are extracted based on a speech recognition result obtained using a reference vocabulary An expanded vocabulary that includes fewer OOV words is built by adding the extracted words to the reference vocabulary The second recognition process is performed using the new vocabulary The experimental results for broadcast news speech show our method achieves a 30 reduction in OOV rate and improves speech recognition accuracy 1 Introduction Out of vocabulary OOV words that are not included in a recognition vocabulary not only cannot be recognized when they appear in input speech but also affect their surrounding words and cause them to be misrecognized Although the vocabularies of automatic speech recognition ASR systems are generally designed to cover as many expected words in input speech as possible vocabulary sizes are limited depending on available memory size expected latency of speech recognition processes and the quantity and variety of available 
training text data Therefore OOV problems cannot be avoided by current ASR technologies and more or fewer OOV words can be included in input speech In some kinds of speech recognition applications such as broadcast news indexing 1 2 and meeting speech transcription 3 4 newly appearing words and infrequent words specific to a certain topic which tend to be OOV are critical and therefore need to be recognized accurately Modeling OOV words using sub word units has been shown to have an effect on OOV detection and estimating subword sequences 5 6 For retrieving contents by keyword queries in indexing applications notations of OOV words especially names of persons places and products need to be obtained instead of sub word sequences Information retrieval IR techniques were applied to the OOV problem in 7 and 8 They dynamically adapted a vocabulary and a language model to the topic of input speech using relevant articles obtained from a database or the Web Dynamic adaptation of 2 Multi Pass Speech Recognition 
using Vocabulary Expansion The speech recognition process using vocabulary expansion is executed according to the following procedure 1 First run recognize input speech using a reference vocabulary 2 Extract relevant words estimate relevant words to input speech using the recognition result of the first run 3 Rebuild vocabulary build an expanded vocabulary by adding relevant words to the reference vocabulary 4 Second run recognize input speech using the expanded vocabulary Although this kind of procedure needs to run recognition processes for input speech at least twice and cannot be executed in real time some applications such as transcription or indexing of archived speech data does not have to obtain recognition results in real time In view of this multi pass recognition approaches combined with unsupervised adaptation techniques for improving recognition accuracy befit such applications It takes much fewer processes to add words to a vocabulary than to rebuild a language model In addition the second run 
can be executed much faster than the first run if the acoustic model does not change and acoustic likelihood calculation of Currently at Department of Computer and Information Sciences Nagasaki University Currently at Graduate School of Language and Culture Osaka University the first run can be used for the second run 3 Vocabulary Expansion using Conceptual Base Relevant words for vocabulary expansion are extracted using word conceptual vectors that model word co occurrence patterns Words that appear in a recognition result of the first run are clustered based on word conceptual vectors and words with similar vectors to the centroid vectors of the obtained clusters are extracted from a vocabulary database as relevant words 3 1 Conceptual Base A conceptual base is a database that consists of concept words and corresponding conceptual vectors To build a conceptual base first a word co occurrence matrix is created by collecting word content word co occurrence frequencies within one sentence in a training text 
corpus Each row on the matrix is a word co occurrence vector for a particular word Since the matrix is quite sparse even with a huge corpus it is transformed into a lower order matrix by singular value decomposition SVD The reduced matrix is composed of word conceptual vectors The word conceptual vector is a vector representation of a word co occurrence pattern and if a pair of words has similar conceptual vectors they tend to appear in the same sentence and are relevant to each other 3 2 Vocabulary Database As mentioned word conceptual vectors are derived based on statistical word co occurrence patterns so that they cannot be derived properly for infrequent words which are to be extracted in vocabulary expansion for reducing OOV words To assign conceptual vectors to all words regardless of their frequency we calculated smoothed conceptual vectors as follows 1 Obtain sentence conceptual vectors for each sentence that includes a target word by calculating centroid vectors of all concept words appearing in the 
sentence A sentence conceptual vector v s for a sentence s which includes N s concept words is calculated as where s j is an auxiliary function that returns 1 if a target word j is included in a sentence s and returns 0 otherwise The denominator of the right side of equation 2 is the number of sentences that includes word j The smoothed conceptual vector is based on word conceptual vectors of co occurrence words with a target word and can be derived even for infrequent words if they co occur with concept words in a sentence Regardless of the frequency of words the distance between words depends on the distance between word conceptual vectors of co occurred concept words for each word The vocabulary database consists of all the words appearing in training text data and their corresponding smoothed conceptual vectors and is used for vocabulary expansion 3 3 Relevant Word Extraction Relevant words to input speech are extracted from the vocabulary database based on the distance between input speech and the words 
in the vocabulary database To measure the distance a result of the first run for input speech is represented as conceptual vectors through the clustering process 3 3 1 Clustering All the concept words appearing in the result of the first run are clustered based on their word conceptual vectors using the centroid method The clusters are considered to consist of a lot of concept words representing the content of input speech and the clusters with small number of words are irrelevant to the content or include recognition error words As centroid vectors of large clusters are considered to represent the content of the input speech the relevant words are extracted based on them By ignoring small clusters negative effects of speech recognition errors are reduced Also by using centroid vectors of multiple large clusters input that includes multiple topics can be handled 3 3 2 Relevance Score vs 1 NS c k 1 Ns wk 1 The distance between the input speech and words in the vocabulary database is measured by using 
relevance score r j D between a word j and a document D calculated as follows where cwk is a word conceptual vector of concept word wk 2 Obtain a smoothed conceptual vector of the target word by calculating a centroid vector for the sentence conceptual vectors of all sentences that include the target word A smoothed conceptual vector g j for a target word j is calculated as r j D max k g j dk g j dk 4 gj s j v s s s j s 2 where j 1 J is a word in the vocabulary database g j is a smoothed conceptual vector of word j k 1 K is a cluster of concept words appearing in the results of the first run and d k is a centroid vector of cluster k By calculating the relevance score to the input document or the recognition results of the first run for all the words in the vocabulary database the words with high relevance scores are extracted as relevant words to input speech 3 4 Rebuilding Vocabulary for Second Run s j 1 if j s 0 otherwise 3 The extracted relevant words are added to a reference vocabulary to build an 
expanded vocabulary Adaptation techniques for n gram language models can be applied to assign n gram probabilities to the new words In case of class n gram language models existing probabilities can be shared for the new words Since n gram probabilities can be used without any changes in the case the second run can be executed just after the vocabulary expansion gram probability of the added words in the OOV word class was 0 01 when adding 100 words and was 0 001 when adding 1000 words 4 4 Experimental Results The experimental results for the 25k vocabulary are shown in Table 1 For the reference vocabulary REF we compared our proposed method using the smoothed conceptual vector SCV with the results of a relevant document retrieval approach using the Okapi similarity measure OKAPI 7 The proposed method reduced many more OOV words oov number of OOV oov OOV rate red OOV reduction rate than the conventional method for both number 100 or 1000 of additional words for each news story add Though the word error rate 
wer of speech recognition increased when 100 words were added using the conventional method the word error rates were improved when the vocabulary was expanded by the proposed method Table 2 shows the results for the 50k vocabulary and a similar trend to the 25k It is notable that the 25k vocabulary using vocabulary expansion yielded better speech recognition performance than the 50k reference vocabulary Table 1 Experimental results 25k vocabulary add REF OKAPI conventional SCV proposed 100 1000 100 1000 oov 1471 1440 1159 1206 1002 oov 2 10 2 06 1 66 1 72 1 43 red 2 1 21 2 18 0 31 9 wer 27 50 27 71 27 38 27 17 27 00 4 Evaluations We evaluated OOV reduction in proposed dynamic vocabulary expansion and ASR performance of MASSIVE using broadcast news speech data 4 1 Evaluation Data We used 30 Japanese broadcast news programs which were aired on December 2002 as evaluation data The programs which vary from 5 to 30 minutes in length include 265 news stories in total The number of utterances is 2 898 and the 
number of words is 69 068 in the evaluation data An evaluation was carried out for each news story 4 2 Speech Recognition A speech recognition engine called VoiceRex which is being developed at NTT was used for the speech recognition experiments The acoustic models were 3 state 12 mixture state tied triphone HMMs male female and gender independent trained using approximately 300 hours of speech 150 hours each for male and female models There were approximately 5 000 states for each model The beginning of each utterance was evaluated with 96 mixture GMMs each one representing one of the three acoustic models and the model used for recognition was selected automatically 2 The reference vocabularies and the trigram language models were trained using 450 thousand sentences 15 million words of broadcast news transcription and newspaper text collected before December 2002 The vocabulary sizes were 25 thousands 25k and 50 thousands 50k and include words appearing ten or more times and words appearing twice or more 
respectively The 25k vocabulary covered 99 18 of the training text and 97 90 of the evaluation data 2 10 OOV The 50k covered 99 87 and 98 98 1 02 OOV 4 3 Vocabulary Expansion The conceptual base was trained using one year 2002 of newspaper text approximately 100 thousand articles The rows of the word co occurrence matrix or concept words had 47 thousand frequent words in the training data and the columns had 1 thousand frequent words except the 50 most frequent The columns were compressed to 100 by SVD that is to say the word conceptual vectors had a hundred dimensions The vocabulary database consisted of 160 thousand words that were all the content words appearing in the training data and smoothed conceptual vectors as described in Section 3 2 were assigned to all of them The clustering started from the status of each word constituting a single cluster and stopped when the number of clusters was less than one fifth from the start The centroid vector of the single largest cluster was used for calculating the 
relevance score The top 100 and 1000 words in terms of relevance score were extracted and added to the reference vocabulary for each news story Class n gram probability of the OOV word class is distributed equally to the added words That is to say the uni Table 2 Experimental results 50k vocabulary add REF OKAPI conventional SCV proposed 100 1000 100 1000 oov 712 710 580 586 506 oov 1 02 1 01 0 83 0 84 0 72 red 0 3 18 5 17 7 28 9 wer 27 32 27 39 27 22 27 08 26 99 Table 3 shows the number of OOV words oov and the number of word errors word error with their reduction red when the 25k reference vocabulary REF was expanded using the proposed method The efficiency eff which is the ratio of the reduction in word error to the reduction in OOV words was quite high and the additional words contributed greatly to word error reduction Table 4 shows the same numbers for the 50k vocabulary In this case the efficiency was more than 100 That is to say more words came to be recognized than the obtained OOV words Tables 5 
and 6 show the experimental results using the proposed method by breaking the news stories for evaluation down by their OOV rate with the 25k and 50k reference vocabularies For both OOV rates oov were reduced for the news stories in every range of OOV rate oov range The word error rates were also reduced for every range except the news stories with less than 1 OOV rate for the 25k vocabulary in Table 5 Table 3 Reduction of OOV and word errors 25k vocabulary oov REF 100 1000 1471 1206 1002 red 265 469 word error 18995 18767 18646 red 228 349 eff 86 0 74 4 ond run can be executed shortly after the first run The experimental results show that the proposed method can reduce the number of OOV words effectively and the obtained words contribute to reducing word error rate efficiently 6 Acknowledgements The authors would like to thank Hisashi Ohara Manager of the Speech Acoustics and Language Laboratory and Masahiko Hase Director of Cyber Space Laboratories for their support and encouragement of this work 7 
References Table 4 Reduction of OOV and word errors 50k vocabulary oov REF 100 1000 712 586 506 red 126 206 word error 18867 18703 18643 red 164 224 eff 130 2 108 7 1 J Makhoul F Kubala T Leek D Liu L Nguyen R Schwartz and A Srivastava Speech and Language Technologies for Audio Indexing and Retrieval Proc of the IEEE Vol 88 No 8 pp 1338 1353 2000 2 K Ohtsuki K Bessho Y Matsuo S Matsunaga and Y Hayashi Automatic Indexing of Multimedia Content by Integration of Audio Spoken Language and Visual Information Proc of ASRU pp 601 606 2003 3 A Waibel T Schultz M Bett M Denecke R Malkin I Rogina R Stiefelhagen and J Yang SMaRT The Smart Meeting Room Task at ISL Proc of ICASSP pp 752 755 2003 4 N Morgan D Baron J Edwards D Ellis D Gelbart A Janin T Pfau E Shriberg and A Stolcke The Meeting Project at ICSI Proc of HLT pp 246 252 2001 5 I Bazzi and J Glass A Multi Class Approach for Modeling Out of Vocabulary Words Proc of ICSLP pp 1613 1616 2002 6 K Tanigaki H Yamamoto and Y Sagisaka A Hierarchical Language Model 
Incorporating Class Dependent Word Models for OOV Words Recognition Proc of ICSLP Vol III pp 123 126 2000 7 T Kemp and A Waibel Reducing the OOV Rate in Broadcast News Speech Recognition Proc of ICSLP pp 1839 1842 1998 8 H Yu T Tomokiyo Z Wang and A Waibel New Developments in Automatic Meeting Transcription Proc of ICSLP Vol IV pp 310 313 2000 9 P Geutner M Finke and P Scheytt Adaptive Vocabularies for Transcribing Multilingual Broadcast News Proc of ICASSP pp 925 928 1998 10 T Kato S Shimada M Kumamoto and K Matsuzawa Idea Deriving Information Retrieval System Proc of 1st NTCIR Workshop pp 187 193 1999 Table 5 Classification by OOV rate 25k vocabulary oov range story 1 1 2 2 3 3 4 4 5 5 89 57 46 31 15 27 oov REF 0 53 1 39 2 48 3 49 4 56 8 55 100 0 44 1 21 1 95 2 99 2 80 7 44 1000 REF 0 35 1 00 1 57 2 39 2 45 6 55 20 0 28 7 31 7 28 4 29 7 46 1 wer 100 20 2 28 6 31 1 27 6 27 4 45 6 1000 20 2 28 5 30 9 26 9 27 1 44 4 Table 6 Classification by OOV rate 50k vocabulary oov range story 1 1 2 2 3 3 4 4 5 5 142 53 
30 20 5 15 oov REF 0 48 1 41 2 42 3 42 4 59 7 54 100 0 29 0 76 1 31 2 07 3 53 5 59 1000 REF 0 26 0 64 0 97 1 65 3 18 5 32 23 0 33 0 28 3 24 5 43 5 53 4 wer 100 22 9 32 9 27 5 23 7 43 3 53 2 1000 22 9 32 9 27 2 23 3 42 3 52 9 5 Conclusions This paper described a multi pass approach of speech recognition using dynamic vocabulary expansion based on the conceptual base and its experimental results for broadcast news speech Relevant words to input speech were extracted from the vocabulary database based on the relevance score that was calculated by using word conceptual vectors The extracted words were added to the reference vocabulary to build an expanded vocabulary that is used for the second recognition process The vocabulary expansion process is simple and ngram probabilities do not have to be changed Thus the sec 
36	a	Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 CHAPTER 10 Audio Visual Automatic Speech Recognition An Overview Gerasimos Potamianos Chalapathy Neti Human Language Technologies Department IBM Thomas J Watson Research Center Yorktown Heights NY 10598 USA e mail fgpotam cnetig us ibm com Juergen Luettin Robert Bosch GmbH Automotive Electronics D 7152 Leonberg Germany e mail Juergen Luettin de bosch com Iain Matthews Robotics Institute Carnegie Mellon University Pittsburgh PA 15213 USA e mail iainm cs cmu edu INTRODUCTION We have made significant progress in automatic speech recognition ASR for well defined applications like dictation and medium vocabulary transaction processing tasks in relatively controlled environments However ASR performance has yet to reach the level required for speech to become a truly pervasive user interface Indeed even in clean acoustic environments and for a variety of tasks state of the art ASR 
system performance lags human speech perception by up to an order of magnitude Lippmann 1997 In addition current systems are quite sensitive to channel environment and style of speech variations A number of techniques for improving ASR robustness have met limited success in severely degraded environments mismatched to system training Ghitza 1986 Nadas et al 1989 Juang 1991 Liu et al 1993 Hermansky and Morgan 1994 Neti 1994 Gales 1997 Jiang et al 2001 Clearly novel non traditional approaches that use orthogonal sources of information to the acoustic input are needed to achieve ASR performance closer to the human speech perception level and robust enough to be deployable in field applications Visual speech is the most promising source of additional speech information and it is obviously not affected by the acoustic environment and noise Human speech perception is bimodal in nature Humans combine audio and visual information in deciding what has been spoken especially in noisy environments The visual modality 
benefit to speech intelligibility in noise has been quantified as far back as in Sumby and Pollack 1954 Furthermore bimodal fusion of audio and visual stimuli in perceiving speech has been demonstrated by the McGurk effect McGurk and MacDonald 1976 For example when the spoken sound ga is superimposed on the video of a person uttering ba most people perceive the speaker as uttering the sound da In addition visual speech is of particular importance to the hearing impaired Mouth movement is known to play an important role in both sign language and simultaneous communication between the deaf Marschark et al 1998 The hearing impaired speechread well and possibly better than the general population Bernstein et al 1998 There are three key reasons why vision benefits human speech perception Summerfield 1987 It helps speaker audio source localization it contains speech segmental information that supplements the audio and it provides complimentary information about the place of articulation The latter is due to the 
partial or full Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 AUDIO AUDIO FEATURE EXTRACTION AUDIO ONLY ASR t VIDEO t VISUAL FRONT END FACE DETECTION MOUTH LOCALIZATION LIP TRACKING VISUAL FEATURE EXTRACTION AUDIO VISUAL FUSION AUDIO VISUAL ASR VISUAL ONLY ASR AUTOMATIC SPEECHREADING Figure 1 The main processing blocks of an audio visual automatic speech recognizer The visual front end design and the audio visual fusion modules introduce additional challenging tasks to automatic recognition of speech as compared to traditional audio only ASR They are discussed in detail in this chapter visibility of articulators such as the tongue teeth and lips Place of articulation information can help disambiguate for example the unvoiced consonants p a bilabial and k a velar the voiced consonant pair b and d a bilabial and alveolar respectively and the nasal m a bilabial from the nasal alveolar n Massaro and Stork 1998 All three 
pairs are highly confusable on basis of acoustics alone In addition jaw and lower face muscle movement is correlated to the produced acoustics Yehia et al 1998 Barker and Berthommier 1999 and its visibility has been demonstrated to enhance human speech perception Summerfield et al 1989 Smeele 1996 The above facts have motivated significant interest in automatic recognition of visual speech formally known as automatic lipreading or speechreading Stork and Hennecke 1996 Work in this field aims at improving ASR by exploiting the visual modality of the speaker s mouth region in addition to the traditional audio modality leading to audio visual automatic speech recognition systems Not surprisingly including the visual modality has been shown to outperform audio only ASR over a wide range of conditions Such performance gains are particularly impressive in noisy environments where traditional acoustic only ASR performs poorly Improvements have also been demonstrated when speech is degraded due to speech impairment 
Potamianos and Neti 2001a and Lombard effects Huang and Chen 2001 Coupled with the diminishing cost of quality video capturing systems these facts make automatic speechreading tractable for achieving robust ASR in certain scenarios Hennecke et al 1996 Automatic recognition of audio visual speech introduces new and challenging tasks compared to traditional audio only ASR The block diagram of Figure 1 highlights these In addition to the usual audio front end feature extraction stage visual features that are informative about speech must be extracted from video of the speaker s face This requires robust face detection as well as location estimation and tracking of the speaker s mouth or lips followed by extraction of suitable visual features In contrast to audio only recognizers there are now two streams of features available for recognition one for each modality The combination of the audio and visual streams should ensure that the resulting system performance is better than the best of the two single modality 
recognizers and hopefully significantly outperform it Both issues namely the visual front end design and audio visual fusion constitute difficult problems and they have generated significant research work by the scientific community Indeed since the mid eighties over a hundred articles have concentrated on audio visual ASR with the vast majority appearing during the last decade The first automatic speechreading system was reported by Petajan 1984 Given the video of the speaker s face and by using simple image thresholding he was able to extract binary black and white mouth images and subsequently mouth height width perimeter and area as visual speech features He then developed a visual only recognizer based on dynamic time warping Rabiner and Juang 1993 to rescore the best two choices of the output of the baseline audio only system His method improved ASR for a single speaker isolated word recognition task on a 100 word vocabulary that included digits and letters Petajan s work generated significant 
excitement and soon various sites established research in audio visual ASR Among the pioneer sites was the group headed by Christian Beno it at the Institute Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 de la Communication VISUAL FRONT ENDS FOR AUTOMATIC SPEECHREADING As it was briefly mentioned in the Introduction see also Figure 1 the first main difficulty in the area of audio visual ASR is the visual front end design The problem is two fold Face lips or mouth tracking is first required followed by visual speech representation in terms of a small number of informative features Clearly the two issues are closely related Employing a lip tracking algorithm allows one to use visual features such as mouth height or width Adjoudani and Beno it 1996 Chan et al 1998 Potamianos et al 1998 or parameters of a suitable lip model Chandramohan and Silsbee 1996 Dalton et al 1996 Luettin et al 1996 On the other hand only a crude 
detection of the mouth region is sufficient to obtain visual features using transformations of this region s pixel values that achieve sufficient dimensionality reduction Bregler Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 et al 1993 Duchnowski et al 1994 Matthews et al 1996 Potamianos et al 2001b Needless to say robust tracking of the lips or mouth region is of paramount importance for good performance of automatic speechreading systems Iyengar et al 2001 Face Detection Mouth and Lip Tracking The problem of face and facial part detection has attracted significant interest in the literature Graf et al 1997 Rowley et al 1998 Sung and Poggio 1998 Senior 1999 In addition to automatic speechreading it has applications to other areas such as visual text to speech Cohen and Massaro 1994 Chen et al 1995 Cosatto et al 2000 person identification and verification Jourlin et al 1997 Wark and Sridharan 1998 Face Detection and 
Mouth Region of Interest Extraction A typical algorithm for face detection and facial feature localization is described in Senior 1999 This technique is used in the visual front end design of Neti et al 2000 and Potamianos et al 2001b when processing the video of the IBM ViaVoiceTM audio visual database described later Given a video frame face detection is first performed by employing a combination of methods some of which are also used for subsequent face feature finding A face template size is first chosen an 11 11 pixel square here and an image pyramid over all permissible face locations and scales given the video frame and face template sizes is used to search for possible face candidates This search is constrained by the minimum and maximum allowed face candidate size with respect to the frame size the face size increment from one pyramid level to the next the spatial shift in searching for faces within each pyramid level and the fact that no candidate face can be of smaller size than the face template 
In Potamianos et al 2001b the face square side is restricted to lie within 10 and 75 of the frame width with a face size increase of 15 across consecutive pyramid levels Within each pyramid level a local horizontal and vertical shift of one pixel is used to search for candidate faces In case the video signal is in color skin tone segmentation can be used to quickly narrow the search to face candidates that contain a relatively high proportion of skin tone pixels The normalized red green blue values of each frame pixel are first transformed to the hue saturation color space where skin tone is known Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Figure 2 Region of interest extraction examples Upper rows Example video frames of eight subjects from the IBM ViaVoiceTM audio visual database described in a later section with superimposed facial features detected by the algorithm of Senior 1999 Lower row Corresponding mouth 
regions of interest extracted as in Potamianos et al 2001b to occupy a largely invariant to most humans and lighting conditions range of values Graf et al 1997 Senior 1999 In the particular implementation all face candidates that contain less than 25 of pixels with hue and saturation values that fall within the skin tone range are eliminated This substantially reduces the number of face candidates depending on the frame background speeding up computation and reducing spurious face detections Every remaining face candidate is subsequently size normalized to the 11 11 face template size and its greyscale pixel values are placed into a 121 dimensional face candidate vector Each such vector is given a score based on both a two class face versus non face Fisher linear discriminant and the candidate s distance from face space DFFS i e the face vector projection error onto a lower 40 dimensional space obtained by means of principal components analysis PCA see below All candidate regions exceeding a threshold score 
are considered as faces Among such faces at neighboring scales and locations the one achieving the maximum score is returned by the algorithm as a detected face Senior 1999 Once a face has been detected an ensemble of facial feature detectors are used to estimate the locations of 26 facial features including the lip corners and centers twelve such facial features are marked on the frames of Figure 2 Each feature location is determined by using a score combination of prior feature location statistics linear discriminant and distance from feature space similar to the DFFS discussed above based on the chosen feature template size such as 11 11 pixels Before incorporating the described algorithm into our speechreading system a training step is required to estimate the Fisher discriminant and eigenvectors PCA for face detection and facial feature estimation as well as the facial feature location statistics Such training requires a number of frames manually annotated with the faces and their visible features When 
training the Fisher discriminant both face and non face or facial feature and non feature vectors are used whereas in the case of PCA face and facial feature only vectors are considered Senior 1999 Given the output of the face detection and facial feature finding algorithm described above five located lip Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Figure 3 Examples of lip contour estimation by means of active shape models Luettin et al 1996 Depicted mouth regions are from the Tulips1 audio visual database Movellan and Chadderdon 1996 and they have been extracted preceding lip contour estimation contour points are used to estimate the mouth center and its size at every video frame four such points are marked on the frames of Figure 2 To improve ROI extraction robustness to face and mouth detection errors the mouth center estimates are smoothed over twenty neighboring frames using median filtering to obtain the ROI 
center whereas the mouth size estimates are averaged over each utterance A size normalized square ROI is then extracted see 1 below with sides M N 64 see also Figure 2 This can contain just the mouth region or also parts of the lower face Potamianos and Neti 2001b Lip Contour Tracking Once the mouth region is located a number of algorithms can be used to obtain lip contour estimates Some popular methods are snakes Kass et al 1988 templates Yuille et al 1992 Silsbee 1994 and active shape and appearance models Cootes et al 1995 1998 A snake is an elastic curve represented by a set of control points The control point coordinates are iteratively updated by converging towards the local minimum of an energy function defined on basis of curve smoothness constraints and a matching criterion to desired features of the image Kass et al 1988 Such an algorithm is used for lip contour estimation in the speechreading system of Chiou and Hwang 1997 Another widely used technique for lip tracking is by means of lip templates 
employed in the system of Chandramohan and Silsbee 1996 for example Templates constitute parametrized curves that are fitted to the desired shape by minimizing an energy function defined similarly to snakes B splines used by Dalton et al 1996 work similarly to the above techniques as well Active shape and appearance models construct a lip shape or ROI appearance statistical model as discussed in following subsections These models can be used for tracking lips by means of the algorithm proposed by Cootes et al 1998 This assumes that given small perturbations from the actual fit of the model to a target image a linear relationship exists between the difference in the model projection and image and the required updates to the model parameters An iterative algorithm is used to fit the model to the image data Matthews et al 1998 Alternatively the fitting can be performed by the downhill simplex method Nelder and Mead 1965 as in Luettin et al 1996 Examples of lip contour estimation by means of active shape models 
using the latter fitting technique are depicted in Figure 3 Visual Features Various sets of visual features for automatic speechreading have been proposed in the literature over the last 20 years In general they can be grouped into three categories a Video pixel or appearance based ones b Lip contour or shape based features and c Features that are a combination of both appearance and Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 shape Hennecke et al 1996 In the following we present each category in more detail Possible feature post extraction processing is discussed at the end of this section Appearance Based Features In this approach to visual feature extraction the image part typically containing the speaker s mouth region is considered as informative for lipreading i e the region of interest ROI Such region can be a rectangle containing the mouth and possibly include larger parts of the lower face such as the jaw and 
cheeks Potamianos and Neti 2001b or the entire face Matthews et al 2001 Often it can be a three dimensional rectangle containing adjacent frame rectangular ROIs in an effort to capture dynamic speech information at this early stage of processing Li et al 1995 Potamianos et al 1998 Alternatively the ROI can correspond to a number of image profiles vertical to the lip contour Dupont and Luettin 2000 or be just a disc around the mouth center Duchnowski et al 1994 By concatenating the ROI pixel greyscale Bregler et al 1993 Duchnowski et al 1994 Potamianos et al 1998 Dupont and Luettin 2000 or color values Chiou and Hwang 1997 a feature vector is obtained For example in the case of an M N pixel rectangular ROI which is centered at location mt nt of video frame Vt m n at time t the resulting feature vector of length d MN will be after a lexicographic ordering 1 xt f Vtm n mt bM 2c m mt dM 2e nt bN 2c n nt dN 2e g 1 This vector is expected to contain most visual speech information Notice that approaches that use 
optical flow as visual features Mase and Pentland 1991 Gray et al 1997 can fit within this framework by replacing in 1 the video frame ROI pixels with optical flow estimates Typically the dimensionality d of vector 1 is too large to allow successful statistical modeling Chatfield and Collins 1991 of speech classes by means of a hidden Markov model HMM for example Rabiner and Juang 1993 Therefore appropriate transformations of the ROI pixel values are used as visual features Movellan and Chadderdon 1996 for example use low pass filtering followed by image subsampling and video frame ROI differencing whereas Matthews et al 1996 propose a nonlinear image decomposition using image sieves for dimensionality reduction and feature extraction By far however the most popular appearance feature representations achieve such reduction by using traditional image transforms Gonzalez and Wintz 1977 These transforms are typically borrowed from the image compression literature and the hope is that they will preserve most 
relevant to speechreading information In general a D d dimensional linear transform matrix is sought such that the transformed data vector t t contains most speechreading d elements To obtain matrix L training examples are given denoted by l information in its D l 1 L A number of possible such matrices are described in the following P P y Px x Principal components analysis PCA This constitutes the most popular pixel based feature representation for automatic speechreading Bregler et al 1993 Bregler and Konig 1994 Duchnowski et al 1994 Li et al 1995 Brooke 1996 Tomlinson et al 1996 Chiou and Hwang 1997 Gray et al 1997 Luettin and Thacker 1997 Potamianos et al 1998 Dupont and Luettin 2000 The PCA data projection achieves optimal information compression in the sense of minimum square error between the original vector t and its reconstruction based on its projection t however appropriate data scaling constitutes a problem in the classification of the resulting vectors Chatfield and Collins 1991 In the PCA 
implementation of Potamianos et al 1998 the data are scaled according to their inverse variance and their correlation matrix is Chatfield and Collins 1991 Press et al 1995 computed Subsequently is diagonalized as where 1 d has as columns the eigenvectors of and is a diagonal matrix containing the eigenvalues of Assuming that the D largest such eigenvalues are located at the j1 jD diagonal positions the data projection matrix is PCA j1 jD Given a data vector t this is first element wise mean and variance normalized and subsequently its feature vector is extracted as t PCA t y x A a a R R R AA a R R P a x y P x Discrete cosine wavelet and other image transforms As an alternative to PCA a number of popular linear image transforms Gonzalez and Wintz 1977 have been used in place of for obtaining speechreading P Throughout this work boldface lowercase symbols denote column vectors and boldface capital symbols denote matrices In addition denotes vector or matrix transpose and diag det denote matrix diagonal and 
determinant respectively 1 Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 features For example the discrete cosine transform DCT has been adopted in several systems Duchnowski et al 1994 Potamianos et al 1998 Nakamura et al 2000 Neti et al 2000 Scanlon and Reilly 2001 Nefian et al 2002 the discrete wavelet transform DWT Daubechies 1992 in others Potamianos et al 1998 and the Hadamard and Haar transforms by Scanlon and Reilly 2001 Most researchers use separable transforms Gonzalez and Wintz 1977 which allow fast implementations Press et al 1995 when M and N are powers of 2 typically values M N 16 32 or 64 are considered Notice that in each case matrix can have as rows the image transform matrix rows that maximize the transformed data energy over the training set Potamianos et al 1998 or alternatively that correspond to a priori chosen locations Nefian et al 2002 P Linear discriminant analysis LDA The data vector 
transforms presented above are more suitable for ROI compression than ROI classification into the set of speech classes of interest For the latter task LDA Rao 1965 is more appropriate as it maps features to a new space for improved classification LDA was first proposed for automatic speechreading by Duchnowski et al 1994 There it was applied directly to vector 1 LDA has also been considered in a cascade following the PCA projection of a single frame ROI vector or on the concatenation of a number of adjacent PCA projected vectors Matthews et al 2001 LDA assumes that a set of classes C such as HMM states is a priori chosen and in addition that the training set data vectors l l 1 L are labeled as cl 2 C Then it seeks matrix LDA such that the projected training sample f LDA l l 1 L g is well separated into the set of classes C according to a function of the training sample within class scatter matrix W and its between class scatter matrix B Rao 1965 These matrices are given by x 2 C is the class empirical 
probability mass function where Lc respectively L and 1 if i j 0 otherwise in addition c and c denote the class sample mean and i j c l c l 1 covariance respectively and finally c2C Prc c is the total sample mean To estimate LDA the are generalized eigenvalues and right eigenvectors of the matrix pair B W that satisfy B W first computed Rao 1965 Golub and Van Loan 1983 Matrix 1 d has as columns the generalized eigenvectors Assuming that the D largest eigenvalues are located at the j1 jD diagonal positions of then LDA j1 jD It should be noted that due to 2 the rank of B is at most jCj 1 where jCj denotes the number of classes the cardinality of set C hence D jCj 1 should hold In addition the rank of the d d dimensional matrix W cannot exceed L jCj therefore having insufficient training data with respect to the input feature vector dimension d is a potential problem 2C In 2 Prc Lc L c c SW X Prc P x P c and SB m X Prc m m m m c c S S c 2C 2 m m S S F f f P S F S F P f f S S Maximum Likelihood Data Rotation 
MLLT In our speechreading system Potamianos et al 2001b LDA is followed by the application of a data maximum likelihood linear transform MLLT This transform seeks a square non singular data rotation matrix MLLT that maximizes the observation data likelihood in the original feature space under the assumption of diagonal data covariance in the transformed space Gopinath 1998 Such a rotation is beneficial since in most ASR systems diagonal covariances are typically assumed when modeling the observation class conditional probability distribution with Gaussian mixture models The desired rotation matrix is obtained as P PMLLT arg max P f det PL Y det diag P 2C c P 2 g Lc c Gopinath 1998 This can be solved numerically Press et al 1995 Notice that LDA and MLLT are data transforms aiming in improved classification performance and maximum likelihood data modeling Therefore their application can be viewed as a feature post processing stage and clearly should not be limited to appearance only visual data Shape Based 
Features In contrast to appearance based features shape based feature extraction assumes that most speechreading information is contained in the shape contours of the speaker s lips or more generally Matthews et al Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 1 1 ORIGINAL OUTER LIP CONTOUR FEATURES NORMALIZED SEQUENCE 81926 1 0 9 0 8 0 7 0 6 WIDTH w h w 1 2 HEIGHT h 0 5 AREA 20 RECONSTRUCTED CONTOURS FOR VARIOUS NUMBERS OF FOURIER COEFFICIENTS USED 3 0 4 0 3 20 40 60 80 FRAME t 100 120 140 Figure 4 Geometric feature approach Left Outer lip width and height Middle Reconstruction of an estimated outer lip contour upper part from 1 2 3 and 20 sets of its Fourier coefficients lower part clockwise Right Three geometric visual features displayed on a normalized scale tracked over the spoken utterance 81926 of the connected digits database of Potamianos et al 1998 Lip contours are estimated as in Graf et al 1997 2001 in the 
face contours e g jaw and cheek shape in addition to the lips Two types of features fall within this category Geometric type ones and shape model based features In both cases an algorithm that extracts the inner and or outer lip contours or in general the face shape is required A variety of such algorithms were discussed above Lip geometric features Given the lip contour a number of high level features meaningful to humans can be readily extracted such as the contour height width perimeter as well as the area contained within the contour As demonstrated in Figure 4 such features do contain significant speech information Not surprisingly a large number of speechreading systems makes use of all or a subset of them Petajan 1984 Adjoudani and Beno it 1996 Alissali et al 1996 Goldschen et al 1996 w h S in terms Given a set of vectors 3 PCA can be used to identify the optimal orthogonal linear transform PCA of the variance described along each dimension resulting in a statistical model of the lip or facial shape 
see xS x y x y xK yK 1 1 2 2 3 P Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Figure 5 Statistical shape model The top four modes are plotted left to right at 3 standard deviations around the mean These four modes describe 65 of the variance of the training set which consists of 4072 labeled images from the IBM ViaVoiceTM audio visual database Neti et al 2000 Matthews et al 2001 Figure 5 To identify axes of genuine shape variation each shape in the training set must be aligned This is achieved using a similarity transform translation rotation and scaling by means of an iterative procrustes analysis Cootes et al 1995 Dryden and Mardia 1998 Given a tracked lip contour the extracted visual S S Note that vectors 3 can be the output of a tracking algorithm based on features will be S PCA B splines for example as in Dalton et al 1996 y P x Joint Appearance and Shape Features Appearance and shape based visual features are 
quite different in nature In a sense they code low and highlevel information about the speaker s face and lip movements Not surprisingly combinations of features from both categories have been employed in a number of automatic speechreading systems In most cases features from each category are just concatenated For example Chan 2001 combines geometric lip features with the PCA projection of a subset of pixels contained within the mouth Luettin et al 1996 as well as Dupont and Luettin 2000 combine ASM features with PCA based ones extracted from a ROI that consists of short image profiles around the lip contour Chiou and Hwang 1997 on the other hand combine a number of snake lip contour radial vectors with PCA features of the color pixel values of a rectangle mouth ROI A different approach to combining the two classes of features is to create a single model of face shape and appearance An active appearance model AAM Cootes et al 1998 provides a framework to statistically combine them Building an AAM requires 
three applications of PCA S com a Shape eigenspace calculation that models shape deformations resulting in PCA matrix PCA puted as above see 3 A b Appearance eigenspace calculation to model appearance changes resulting in a PCA matrix PCA of the ROI appearance vectors If the color values of the M N pixel ROI are considered such vectors are A r g b r g b r g b 4 1 1 1 2 2 2 MN MN MN P P x similar to vectors 1 c Using these calculation of a combined shape and appearance eigenspace The latter is a PCA matrix A S PCA on training vectors P where is a suitable diagonal scaling matrix Matthews et al 2001 The aim of this final PCA is to remove the redundancy due to the shape and appearance correlation and to create a single model that compactly describes shape and the corresponding appearance deformation W A xS PS xA S xA W PPCA PCA yA W yS Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Figure 6 Combined shape and appearance 
statistical model Center row Mean shape and appearance Top row Mean shape and appearance 3 standard deviations Bottom row Mean shape and appearance 3 standard deviations The top four modes depicted left to right describe 46 of the combined shape and appearance variance of 4072 labeled images from the IBM ViaVoiceTM audio visual database Neti et al 2000 Matthews et al 2001 Such a model has been used for speechreading in Neti et al 2000 and Matthews et al 2001 An example of the resulting learned joined model is depicted in Figure 6 A block diagram of the method including the dimensionalities of the input shape and appearance vectors 3 and 4 respectively their PCA projections S A and the final feature vector A S A S A S is depicted in Figure 7 PCA y y y P x Visual Feature Post Extraction Processing In an audio visual speech recognition system in addition to the visual features audio features are also extracted from the acoustic waveform For example such features could be mel frequency cepstral coefficients 
MFCCs or linear prediction coefficients LPCs typically extracted at a 100 Hz rate Deller et al 1993 Rabiner and Juang 1993 Young et al 1999 In contrast visual features are generated at the video frame rate commonly 25 or 30 Hz or twice that in case of interlaced video Since feature stream synchrony is required in a number of algorithms for audio visual fusion as discussed in the next section the two feature streams must attain the same rate Typically this is accomplished whenever required either after feature extraction by simple element wise linear interpolation of the visual features to the audio frame rate as in Figure 7 or before feature extraction by frame duplication to achieve a 100 Hz video input rate to the visual Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 1 64 1 64 1 1 4096 1 1 DCT FEATURES 24 t INTERPOLATION TO 100 Hz DCT 24 4096 1 APPEARANCE INFO OR FEATURE MEAN NORMALIZATION yt xt J 15 E 1 6000 1 186 1 
186 1 PCA 6000 1 1 134 AAM FEATURES 197 1 86 1 1 1 41 41 SHAPE INFO PCA 187 197 86 LDA 1 41 MLLT 1 1 41 V 1 11 PLDA V PMLLT V PCA 134 41 ot Figure 7 DCT versus AAM based visual feature extraction for automatic speechreading followed by visual feature post extraction processing using linear interpolation feature mean normalization adjacent frame feature concatenation and the application of LDA and MLLT Vector dimensions as implemented in the system of Neti et al 2000 are depicted front end Occasionly the audio front end processing is performed at the lower video rate Another interesting issue in visual feature extraction has to do with feature normalization In a traditional audio front end cepstral mean subtraction is often employed to enhance robustness to speaker and environment variations Liu et al 1993 Young et al 1999 A simple visual feature mean normalization FMN by element wise subtraction of the vector mean over each sentence has been demonstrated to improve appearance feature based visual only 
recognition Potamianos et al 1998 2001b Alternatively linear intensity compensation has been investigated preceding the appearance feature extraction by Vanegas et al 1998 A very important issue in the visual feature design is capturing the dynamics of visual speech Temporal information often spanning multiple phone segments is known to help human perception of visual speech Rosenblum and Salda na 1998 Borrowing again from the ASR literature dynamic speech information can be captured by augmenting the visual feature vector by its first and second order temporal derivatives Rabiner and Juang 1993 Young et al 1999 Alternatively LDA can be used as a means of learning a transform that optimally captures the speech dynamics Such a transform is applied on the concatenation of consecutive feature vectors adjacent and including the current frame see also Figure 7 i e on x t yt bJ c yt yt 2 dJ 2e 1 5 with J 15 for example as in Neti et al 2000 and Potamianos et al 2001b Clearly and as we already mentioned LDA could 
be applied to any category of features discussed The same holds for MLLT a method that aims in improving maximum likelihood data modeling and in practice ASR performance For example a number of feature post processing steps discussed above including LDA and MLLT have been interchangeably applied to DCT appearance features as well as to AAM ones in our visual front end experiments during the Johns Hopkins workshop as depicted in Figure 7 Neti et al 2000 Matthews et al 2001 Alternate ways of combining feature post extraction processing steps can easily be envisioned For example LDA and MLLT can be applied to obtain within frame discriminant features Potamianos and Neti 2001b which can then be augmented by their first and second order derivatives or followed by LDA and MLLT across frames see also Figure 11 Finally an important problem in data classification is the issue of feature selection within a larger pool of candidate features Jain et al 2000 In the context of speechreading this matter has been directly 
addressed in the selection of geometric lip contour based features by Goldschen et al 1996 Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Summary of Visual Front End Algorithms We have presented a summary of the most common visual feature extraction algorithms proposed in the literature for automatic speechreading Such techniques differ both in their assumptions about where the speechreading information lies as well as in the requirements that they place on face detection facial part localization and tracking On the one extreme appearance based visual features consider a broadly defined ROI and then rely on traditional pattern recognition and image compression techniques to extract relevant speechreading information On the opposite side shape based visual features require adequate lip or facial shape tracking and assume that the visual speech information is captured by this shape s form and movement alone Bridging the 
two extremes various combinations of the two types of features have also been used ranging from simple concatenation to their joint modeling Comparisons between features within the same class are often reported in the literature Duchnowski et al 1994 Goldschen et al 1996 Gray et al 1997 Potamianos et al 1998 Matthews et al 2001 Scanlon and Reilly 2001 Unfortunately however comparisons across the various types of features are rather limited as they require widely different sets of algorithms for their implementation Nevertheless Matthews et al 1998 demonstrate AAMs to outperform ASMs and to result in similar visual only recognition to alternative appearance based features Chiou and Hwang 1997 report that their joint features outperform their shape and appearance feature components whereas Potamianos et al 1998 as well as Scanlon and Reilly 2001 report that DCT transform based visual features are superior to a set of lip contour geometric features However the above results are reported on single subject data 
and or small vocabulary tasks In a larger experiment Matthews et al 2001 compare a number of appearance based features with AAMs on a speaker independent LVCSR task All appearance features considered outperformed AAMs however it is suspected that the AAM used there was not sufficiently trained Although much progress has been made in visual feature extraction it seems that the question of what are the best visual features for automatic speechreading that are robust in a variety of visual environments remains to a large extent unresolved Of particular importance is that such features should exhibit sufficient speaker pose camera and environment independence However it is worth mentioning two arguments in favor of appearance based features First their use is well motivated by human perception studies of visual speech Indeed significant information about the place of articulation such as tongue and teeth visibility cannot be captured by the lip contours alone Human speech perception based on the mouth region is 
superior than perception on basis of the lips alone and it further improves when the entire lower face is visible Summerfield et al 1989 Second the extraction of certain highly performing appearance based features such as the DCT is computationally efficient Indeed it requires a crude mouth region detection algorithm which can be applied at a low frame rate whereas the subsequent pixel vector transform is amenable to fast implementation for suitable ROI sizes Press et al 1995 These facts enable the implementation of real time automatic speechreading systems AUDIO VISUAL INTEGRATION FOR SPEECH RECOGNITION Audio visual fusion is an instance of the general classifier combination problem Jain et al 2000 In our case two observation streams are available audio and visual modalities and provide information about speech classes such as context dependent sub phonetic units or at a higher level word sequences Each observation stream can be used alone to train single modality statistical classifiers to recognize such 
classes However one hopes that combining the two streams will give rise to a bimodal classifier with superior performance to both single modality ones Various information fusion algorithms have been considered in the literature for audio visual ASR for example Bregler et al 1993 Adjoudani and Beno it 1996 Hennecke et al 1996 Potamianos and Graf 1998 Rogozan 1999 Teissier et al 1999 Dupont and Luettin 2000 Neti et al 2000 Chen 2001 Chu and Huang 2002 The proposed techniques differ both in their basic design as well as in the adopted terminology The architecture of some of these methods Robert Ribes et al 1996 Teissier et al 1999 is motivated by models of human speech perception Massaro 1996 Massaro and Stork 1998 Berthommier 2001 In Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 FUSION TYPE Feature fusion One classifier is used Decision fusion Two classifiers are used AUDIO VISUAL FEATURES 1 Concatenated features 2 
Hierarchical discriminant features 3 Enhanced audio features Concatenated features CLASSIFICATION LEVEL Sub phonetic early 1 Sub phonetic early 2 Phone or word intermediate 3 Utterance late Table 1 Taxonomy of the audio visual integration methods considered in this section Three feature fusion techniques that differ in the features used for recognition and three decision fusion methods that differ in the combination stage of the audio and visual classifiers are described in more detail in this chapter most cases however research in audio visual ASR has followed a separate track from work on modeling the human perception of audio visual speech Audio visual integration techniques can be broadly grouped into feature fusion and decision fusion methods The first ones are based on training a single classifier i e of the same form as the audio and visual only classifiers on the concatenated vector of audio and visual features or on any appropriate transformation of it Adjoudani and Beno it 1996 Teissier et al 1999 
Potamianos et al 2001a In contrast decision fusion algorithms utilize the two single modality audio and visual only classifier outputs to recognize audio visual speech Typically this is achieved by linearly combining the class conditional observation log likelihoods of the two classifiers into a joint audio visual classification score using appropriate weights that capture the reliability of each single modality classifier or data stream Hennecke et al 1996 Rogozan et al 1997 Potamianos and Graf 1998 Dupont and Luettin 2000 Neti et al 2000 In this section we provide a detailed description of some popular fusion techniques from each category see also Table 1 In addition we briefly address two issues relevant to automatic recognition of audiovisual speech One is the problem of speech modeling for ASR which poses particular interest in automatic speechreading and helps establish some background and notation for the remainder of the section We also consider the subject of speaker adaptation an important element 
in practical ASR systems Audio Visual Speech Modeling for ASR Two central aspects in the design of ASR systems are the choice of speech classes that are assumed to generate the observed features and the statistical modeling of this generation process In the following we briefly discuss both issues since they are often embedded into the design of audio visual fusion algorithms Speech Classes for Audio Visual ASR The basic unit that describes how speech conveys linguistic information is the phoneme For American English there exist approximately 42 such units Deller et al 1993 generated by specific positions or movements of the vocal tract articulators Only some of the articulators are visible however therefore among these phonemes the number of visually distinguishable units is much smaller Such units are called visemes in the audio visual ASR and human perception literatures Stork and Hennecke 1996 Campbell et al 1998 Massaro and Stork 1998 In general phoneme to viseme mappings are derived by human 
speechreading studies Alternatively such mappings can be generated using statistical clustering techniques as proposed by Goldschen et al 1996 and Rogozan 1999 There is no universal agreement about the exact partitioning of phonemes into visemes but some visemes are well defined such as the bilabial viseme consisting of phoneme set f p b m g A typical clustering into 13 visemes is used by Neti et al 2000 to conduct visual speech modeling experiments and is depicted in Table 2 In traditional audio only ASR the set of classes c 2C that need to be estimated on basis of the observed feature Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Silence Lip rounding based vowels Alveolar semivowels sil sp ao ah aa er oy aw hh uw uh ow ae eh ey ay ih iy ax l el r y Alveolar fricatives Alveolar Palato alveolar Bilabial Dental Labio dental Velar s z t d n en sh zh ch jh p b m th dh f v ng k g w Table 2 The 44 phoneme to 13 viseme 
mapping considered by Neti et al 2000 using the HTK phone set Young et al 1999 sequence most often consist of sub phonetic units and occasionly of sub word units in small vocabulary recognition tasks For LVCSR a large number of context dependent sub phonetic units are used obtained by clustering the possible phonetic contexts tri phone ones for example by means of a decision tree Deller et al 1993 Rabiner and Juang 1993 Young et al 1999 In this chapter such units are exclusively used defined over tri or eleven phone contexts as described in the Experiments section For automatic speechreading it seems appropriate from the human visual speech perception point of view to use visemic sub phonetic classes and their decision tree clustering based on visemic context Such clustering experiments are reported by Neti et al 2000 In addition visual only recognition of visemes is occasionly considered in the literature Potamianos et al 2001b Visemic speech classes are also used for audio visual ASR at the second stage of 
a cascade decision fusion architecture proposed by Rogozan 1999 However the use of different classes for its audio and visual only components complicates audio visual fusion with unclear performance gains Therefore in the remainder of this section identical classes and decision trees are being used for both modalities HMM Based Speech Recognition The most widely used classifier for audio visual ASR is the hidden Markov model HMM a very popular method for traditional audio only speech recognition Deller et al 1993 Rabiner and Juang 1993 Young et al 1999 Additional methods also exist for automatic recognition of speech and have been employed in audio visual ASR systems such as dynamic time warping DTW used for example by Petajan 1984 artificial neural networks ANN as in Krone et al 1997 hybrid ANN DTW systems Bregler et al 1993 Duchnowski et al 1994 or hybrid ANN HMM ones Heckmann et al 2001 Various types of HMMs have also been used for audio visual ASR such as HMMs with discrete observations after vector 
quantization of the feature space Silsbee and Bovik 1996 or HMMs with non Gaussian continuous observation probabilities Su and Silsbee 1996 However the vast majority of audio visual ASR systems and to which we restrict the presentation in this chapter employ HMMs with a continuous observation probability density modeled as a mixture of Gaussian densities Typically in the literature single stream HMMs are used to model the generation of a sequence of audioonly or visual only speech informative features f ts g of dimensionality Ds where s A V denotes the audio or visual modality stream The HMM emission class conditional observation probabilities are modeled by Gaussian mixture densities given by o Pr ot j c s Xw Ks c k 1 sck ND ot ms c k ss c k s s 6 c 2 C whereas the HMM transition probabilities between the various classes are given by rs fPr c0 j c00 c0 c002C g The HMM parameter vector is therefore as rs bs where bs f ws k ms k ss k k 1 Ks c 2C g 7 In 6 and 7 c 2 C denote the HMM context dependent states 
whereas mixture weights ws k are positive for all classes c c c c c Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 AV Enh AUDIO AUDIO FEATURE EXTRACTION 1 1 o A t AV Concat 1 ENH 1 101 1 1 P ENH AV MSE 60 60 t 60 60 o AEnh t o AClean t VIDEO t 1 VISUAL FEATURE EXTRACTION 1 ot 41 V AV HiLDA 101 ot AV LDA 1 101 1 MLLT 1 1 60 1 P LDA AV 60 60 P MLLT AV 60 60 o HiLDA t Figure 8 Three types of feature fusion considered in this section Plain audio visual feature concatenation AV Concat hierarchical discriminant feature extraction AV HiLDA and audio visual speech enhancement AV Enh adding to one Ks c denotes the number of mixtures and ND is the D variate normal distribution and a diagonal covariance matrix its diagonal being denoted by with mean m oms s The expectation maximization EM algorithm Dempster et al 1977 is typically used to obtain maximum j likelihood estimates of 7 Given a current HMM parameter vector at EM 
algorithm iteration j s a re estimated parameter vector is obtained as a as 1 arg max a Q as a j O j j s s In 8 s denotes training data observations from L utterances l l 1 L and Q the EM algorithm auxiliary function defined as Rabiner and Juang 1993 8 O O j represents 9 Q a0 a00 j O s X X Pr O clj a0 L s l l 1 cl 00 log Pr O l clj a s In 9 l denotes any HMM state sequence for utterance l Replacing it with the best HMM path reduces EM to Viterbi training Deller et al 1993 As an alternative to maximum likelihood discriminative training methods can instead be used for HMM parameter estimation Bahl et al 1986 Chou et al 1994 c Feature Fusion Techniques for Audio Visual ASR As already mentioned feature fusion uses a single classifier to model the concatenated vector of timesynchronous audio and visual features or appropriate transformations of it Such methods include plain feature concatenation Adjoudani and Beno it 1996 feature weighting Teissier et al 1999 Chen 2001 both also known as direct identification 
fusion Teissier et al 1999 and hierarchical linear discriminant feature extraction Potamianos et al 2001a The dominant and motor recording fusion models discussed by Teissier et al 1999 also belong to this category as they seek a data to data mapping of either the visual features into the audio space or of both modality features to a new common space followed by linear combination of the resulting features Audio feature enhancement on basis of either visual input Girin et al 1995 Barker and Berthommier 1999 or concatenated audio visual features Girin et al 2001b Goecke et al 2002 falls also within this category of fusion under its general definition adopted above In this section we expand on three feature fusion techniques schematically depicted in Figure 8 Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Concatenative Feature Fusion V Given time synchronous audio and visual feature vectors A t and t with dimensionalities 
DA and D V respectively the joint concatenated audio visual feature vector at time t becomes V oAV oA t t ot o o 2 RD 10 where D DA D V As with all feature fusion methods i e also for vectors 11 and 12 below the generation process of a sequence of features 10 is modeled by a single stream HMM with emission probabilities see also 6 Pr oAV j c t Xw Kc k 1 ck ND oAV t mck sck for all classes c 2 C Adjoudani and Beno it 1996 Concatenative feature fusion constitutes a simple approach for audio visual ASR implementable in most existing ASR systems with minor changes However the dimensionality of 10 can be rather high causing inadequate modeling in 6 due to the curse of dimensionality Chatfield and Collins 1991 The following fusion technique aims to avoid this by seeking lower dimensional representations of 10 Hierarchical Discriminant Feature Fusion The visual features contain less speech classification power than audio features even in the case of extreme noise in the audio channel see Table 4 in the Experiments 
section One would therefore expect that an appropriate lower dimensional representation of 10 could lead to equal and possibly better HMM performance given the problem of accurate probabilistic modeling in high dimensional spaces Potamianos et al 2001a have considered LDA as a means of obtaining such a dimensionality reduction Indeed the goal being to obtain the best discrimination among the classes of interest LDA achieves this on basis of the data and their labels alone without a priori bias in favor of any of the two feature streams LDA is subsequently followed by an MLLT based data rotation see also Figure 8 in order to improve maximum likelihood data modeling using 6 In the audio visual ASR system of Potamianos et al 2001a the proposed method amounts to a two stage application of LDA and MLLT first intra modal on the original audio MFCC and visual DCT features and then inter modal on 10 as also depicted in Figure 11 It is therefore referred to as HiLDA hierarchical LDA The final audio visual feature 
vector is see also 10 AV PAV oAV oHiLDA PMLLT LDA t t 11 One can set the dimensionality of 11 to be equal to the audio feature vector size as implemented by Neti et al 2000 Audio Feature Enhancement Audio and visible speech are correlated since they are produced by the same oral facial cavity Not surprisingly a number of techniques have been proposed to obtain estimates of audio features utilizing the visual only modality Girin et al 1995 Yehia et al 1998 Barker and Berthommier 1999 or joint audiovisual speech data in the case where the audio signal is degraded Girin et al 2001b Goecke et al 2002 The latter scenario corresponds to the speech enhancement paradigm Under this approach the enhanced auAEnh can be simply obtained as a linear transformation of the concatenated audio visual dio feature vector t feature vector 10 namely as o AV oAV oAEnh PENH t t AV AV AV AV consists of D dimensional row vectors pAV where matrix PENH p p pDA i 1 DA and has dimension DA D see also Figure 8 1 2 12 for i Chapter to 
appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 AV is by considering the approximation AEnh AClean in the A simple way to estimate matrix ENH t t denotes clean audio features available in addition to visual Euclidean distance sense where vector AClean t and noisy audio vectors for a number of time instants t in a training set T Due to 12 this becomes equivalent to solving DA mean square error MSE estimations P o o o pAV arg min i p P AV Equations 13 result to D systems of Yulefor i 1 DA i e one per row of the matrix ENH A Walker equations that can be easily solved using Gauss Jordan elimination Press et al 1995 A more AV by using a Mahalanobis type distance instead of 13 is considered sophisticated way of estimating ENH by Goecke et al 2002 whereas non linear estimation schemes are proposed by Girin et al 2001b and Deligne et al 2002 t2T Xo P AClean t i p oAV t 2 13 Decision Fusion Techniques for Audio Visual ASR Although feature 
fusion techniques for example HiLDA have been documented to result in improved ASR over audio only performance Neti et al 2000 they cannot explicitly model the reliability of each modality Such modeling is extremely important as speech information content and discrimination power of the audio and visual streams can vary widely depending on the spoken utterance acoustic noise in the environment visual channel degradations face tracker inaccuracies and speaker characteristics In contrast to feature fusion methods the decision fusion framework provides a mechanism for capturing the reliability of each modality by borrowing from classifier combination literature Classifier combination based on their individual decisions about the classes of interest is an active area of research with many applications Xu et al 1992 Kittler et al 1998 Jain et al 2000 Combination strategies differ in various aspects such as the architecture used parallel cascade or hierarchical combination possible trainability static or adaptive 
and information level considered at integration abstract rank order or measurement level i e whether information is available about the best class only the top n classes or the ranking of all possible ones or the scores likelihoods of them In the audio visual ASR literature examples of most of these categories can be found For example Petajan 1984 rescores the two best outputs of the audio only classifier by means of the visual only classifier a case of cascade static rank order level decision fusion Combinations of more than one categories as well as cases where the one of the two classifiers of interest corresponds to a feature fusion technique are also possible For example Rogozan and Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 asynchrony between the two HMMs and c Intermediate integration typically implemented by means of the product HMM Varga and Moore 1990 or the coupled HMM Brand et al 1997 which force HMM 
synchrony at the phone or word boundaries Notice that such terminology is not universally agreed upon and our reference to early or late integration at the temporal level should not be confused with the feature vs decision fusion meaning of these terms in other work Adjoudani and Beno it 1996 Early Integration The State Synchronous Multi Stream HMM In its general form the class conditional observation likelihood of the multi stream HMM is the product of the observation likelihoods of its single stream components raised to appropriate stream exponents that capture the reliability of each modality or equivalently the confidence of each single stream classifier Such model has been considered in audio only ASR where for example separate streams are used for the energy audio features MFCC static features as well as their first and possibly second order derivatives as in Hernando et al 1995 and Young et al 1999 or for band limited audio features in the multi band ASR paradigm Hermansky et al 1996 as in Bourlard 
and Dupont 1996 Okawa et al 1999 and Glotin and Berthommier 2000 among others In the audio visual domain the model becomes a two stream HMM with one stream devoted to the audio and another to the visual modality As such it has been extensively used in small vocabulary audio visual ASR tasks Jourlin 1997 Potamianos and Graf 1998 Dupont and Luettin 2000 Miyajima et al 2000 Nakamura et al 2000 In the system reported by Neti et al 2000 and Luettin et al 2001 the method was applied for the first time to the LVCSR domain Given the bimodal audio visual observation vector AV t the state emission score it no longer represents a probability distribution of the multi stream HMM is see also 6 and 10 Pr oAV t jc Notice that 14 corresponds to a linear combination in the log likelihood domain In 14 s c t denote the stream exponents weights that are non negative and in general are a function of the modality s the HMM state c 2 C and locally the utterance frame time t Such state and time dependence can be used to model the 
speech class and local environment based reliability of each stream The exponents are often constrained to A c t V c t 1 or 2 In most systems they are set to global modality only dependent values i e s s c t for all classes c 2C and time instants t with the class dependence occasionly being preserved i e s c s c t for all t In the latter case the parameters of the multi stream HMM are see also 6 7 and 14 s2fA Vg k 1 Y Xw Ks c o s c k Ds t N o m s k s s k s c c sct 14 aAV aAV f A V c 2C g where aAV r bA bV c c 15 consists of the HMM transition probabilities single stream components r and the emission probability parameters bA and bV of its The parameters of AV can be estimated separately for each stream component using the EM algorithm namely 8 for s 2 fA Vg and subsequently by setting the joint HMM transition probability vector equal to the audio one i e A or alternatively to the product of the transition probabilities of the two HMMs i e diag A V see also 7 The latter scheme is referred to in the 
Experiments section as AV MSSep An obvious drawback of this approach is that the two single modality HMMs are trained asynchronously i e using different forced alignments whereas 14 assumes that the HMM stream components are state synchronous The alternative is to jointly estimate parameters AV in order to enforce state synchrony Due to the linear combination of stream log likelihoods in 14 the EM algorithm carries on in the multi stream HMM case with minor changes Rabiner and Juang 1993 Young et al 1999 As a result r a r r r r a 1 arg max Q a a j OAV aAV AV a j j 16 can be used a scheme referred to as AV MS Joint Notice that the two approaches basically differ in the E step of the EM algorithm Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 AUDIO HMM STATES VISUAL HMM STATES COMPOSITE HMM STATES Figure 9 Left Phone synchronous state asynchronous multi stream HMM with three states per phone and modality Right Its 
equivalent product composite HMM black circles denote states that are removed when limiting the degree of within phone allowed asynchrony to one state The single stream emission probabilities are tied for states along the same row column to the corresponding audio visual state ones In both separate and joint HMM training the remainder of parameter vector AV consisting of the stream exponents needs to be obtained Maximum likelihood estimation cannot be used for such parameters and discriminative training techniques have to be employed instead Jourlin 1997 Potamianos and Graf 1998 Nakamura 2001 Gravier et al 2002a The issue is discussed later Notice that HMM stream parameter and stream exponent training iterations can be alternated in 16 a Intermediate Integration The Product HMM It is well known that visual speech activity precedes the audio signal by as much as 120 ms Bregler and Konig 1994 Grant and Greenberg 2001 which is close to the average duration of a phoneme A generalization of the state synchronous 
multi stream HMM can be used to model such audio and visual stream asynchrony to some extent by allowing the single modality HMMs to be in asynchrony within a model but forcing their synchrony at model boundaries instead Single stream log likelihoods are linearly combined at such boundaries using weights similarly to 14 For LVCSR a reasonable choice for forcing synchrony constitute the phone boundaries The resulting phone synchronous audio visual HMM is depicted in Figure 9 for the typical case of three states used per phone and modality Recognition based on this intermediate integration method requires the computation of the best state sequences for both audio and visual streams To simplify decoding the model can be formulated as a product HMM Varga and Moore 1990 Such model consists of composite states 2 CC that have audio visual emission probabilities of a form similar to 14 namely Pr oAVj c t Y Xw Ks cs s2fA Vg k 1 c s cs k ND ot m s c k s s c k s s s s s cs t 17 where cA cV Notice that in 17 the audio 
and visual stream components correspond to the emission probabilities of certain audio and visual only HMM states as depicted in Figure 9 These single stream emission probabilities are tied for states along the same row or column depending on the modality therefore the original number of mixture weight mean and variance parameters is kept in the new model However this is usually not the case with the number of transition probability parameters fPr 0 j 00 0 002CC g as additional transitions between the composite states need to be modeled Such probabilities are often 00 Pr c0V j 00 in which case the resulting product HMM is typically factored as Pr 0 j 00 Pr c0 Aj referred to in the literature as the coupled HMM Brand et al 1997 Chu and Huang 2000 2002 Nefian et al 2002 A further simplification of this factorization is sometimes employed namely Pr 0 j 00 0 00 Pr c0A j c00 A Pr cV j cV as in Gravier et al 2002b for example which results in a product HMM with the same number of parameters as the state 
synchronous multi stream HMM c c c c c c c c c c c Given audio visual training data product HMM training can be performed similarly to separate or joint multi stream HMM parameter estimation discussed in the previous subsection In the first case the composite model is constructed based on individual single modality HMMs estimated by 8 and on transition Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 probabilities equal to the product of the audio and visual only ones In the second case referred to as AV MSPROD in the experiments reported later all transition probabilities and HMM stream component parameters are estimated at a single stage using 16 with appropriate parameter tying In both schemes stream exponents need to be estimated separately In the audio visual ASR literature product or coupled HMMs have been considered in some small vocabulary recognition tasks Tomlinson et al 1996 Dupont and Luettin 2000 Huang and 
Chen 2001 Nakamura 2001 Chu and Huang 2002 Nefian et al 2002 where synchronization is sometimes enforced at the word level and recently for LVCSR Neti et al 2000 Luettin et al 2001 Gravier et al 2002b It is worth mentioning that the product HMM allows the restriction of the degree of asynchrony between the two streams by excluding certain composite states in the model topology In the extreme case when only the states that lie in its diagonal are kept the model becomes equivalent to the state synchronous multi stream HMM see also Figure 9 Late Integration Discriminative Model Combination A popular stage of combining audio and visual only recognition log likelihoods is at the utterance end giving rise to late intergration In small vocabulary isolated word speech recognition this can be easily implemented by calculating the combined likelihood for each word model in the vocabulary given the acoustic and visual observations Adjoudani and Beno it 1996 Su and Silsbee 1996 Cox et al 1997 Gurbuz et al 2001 However 
for connected word recognition and even more so for LVCSR the number of possible hypotheses of word sequences becomes prohibitively large Instead one has to limit the log likelihood combination to the top n best only hypotheses Such hypotheses can be generated by the audio only HMM an alternative audio visual fusion technique or can be the union of audio only and visual only n best lists In this approach the list of n best hypotheses for a particular utterance f 1 2 n g are first forcedaligned to their corresponding phone sequences i fci 1 ci 2 ci Ni g by means of both audio and start end ti j s for s 2 fA Vg visual only HMMs Let the resulting phone c i j boundaries be denoted by ti j s j 1 Ni and i 1 n Then the audio visual likelihoods of the n best hypotheses are computed as h h h h Pr h i Pr LM h i LM where Pr LM i denotes the language model LM probability of hypothesis i The exponents in 18 can be estimated using discriminative training criteria as in the discriminative model combination method of 
Beyerlein 1998 and Vergyri 2000 The method is proposed for audio visual LVCSR in Neti et al 2000 and it is referred to as AV DMC in the Experiments section h s2fA Vg j 1 Y Y Pr o t 2 t Ni s t i j s ti j s start end j ci j s c i j 18 h Stream Exponent Estimation and Reliability Modeling We now address the issue of estimating stream exponents weights when combining likelihoods in the audio visual decision fusion techniques presented above see 14 17 and 18 As already discussed such exponents can be set to constant values computed for a particular audio visual environment and database In this case the audio visual weights depend on the modality and possibly on the speech class capturing the confidence of the individual classifiers for the particular database conditions and are estimated by seeking optimal system performance on matched data However in a practical audio visual ASR system the quality of captured audio and visual data and thus the speech information present in them can change dramatically over time 
To model this variability utterance level or even frame level dependence of the stream exponents is required This can be achieved by first obtaining an estimate of the local environment conditions and then using pre computed exponents for this condition or alternatively by seeking a direct functional mapping between environment estimates and stream exponents In the following we expand on these methodologies In the first approach constant exponents are estimated based on training data or more often on held out data Such stream exponents cannot be obtained by maximum likelihood estimation Potamianos and Graf 1998 Nakamura 2001 Instead discriminative training techniques have to be used Some of these methods Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 seek to minimize a smooth function of the minimum classification error MCE of the resulting audio visual model on the data and employ the generalized probabilistic descent 
GPD algorithm Chou et al 1994 for stream exponent estimation Potamianos and Graf 1998 Miyajima et al 2000 Nakamura et al 2000 Gravier et al 2002a Other techniques use maximum mutual information MMI training Bahl et al 1986 such as the system reported by Jourlin 1997 Alternatively one can seek to directly minimize the word error rate of the resulting audio visual ASR system on a held out data set In the case of global exponents across all speech classes constrained to add to a constant the problem reduces to one dimensional optimization of a non smooth function and can be solved using simple grid search Miyajima et al 2000 Luettin et al 2001 Gravier et al 2002a For class dependent weights the problem becomes of higher dimension and the downhill simplex method Nelder and Mead 1965 can be employed This technique is used by Neti et al 2000 to estimate exponents for late decision fusion using 18 A different approach is to minimize frame misclassification rate by using the maximum entropy criterion Gravier et al 
2002a In order to capture the effects of varying audio and visual environment conditions to the reliability of each stream utterance level and occasionly frame level dependence of the stream weights needs to be considered In most cases in the literature exponents are considered as a function of the audio channel signal to noise ratio SNR and each utterance is decoded based on the fusion model parameters at its SNR Adjoudani and Beno it 1996 Meier et al 1996 Cox et al 1997 Teissier et al 1999 Gurbuz et al 2001 This SNR value is either assumed known or estimated from the audio channel Cox et al 1997 A linear dependence between SNR and audio stream weight has been demonstrated by Meier et al 1996 An alternative technique sets the stream exponents to a linear function of the average conditional entropy of the recognizer output computed using the confusion matrix at a particular SNR for a small vocabulary isolated word ASR task Cox et al 1997 A different approach considers the audio stream exponent as a function 
of the degree of voicing present in the audio channel estimated as in Berthommier and Glotin 1999 The method was used at the Johns Hopkins summer 2000 workshop Neti et al 2000 Glotin et al 2001 and is referred to in the Experiments section as AV MS UTTER The above techniques do not allow modeling of possible variations in the visual stream reliability since they concentrate on the audio stream alone Modeling such variability in the visual signal domain is challenging and instead it can be achieved using confidence measures of the resulting visual only classifier For example Adjoudani and Beno it 1996 and Rogozan et al 1997 use the dispersion of both audio only and visualonly class posterior log likelihoods to model the single stream classifier confidences and then compute the utterance dependent stream exponents as a closed form function of these dispersions Similarly Potamianos and Neti 2000 consider various confidence measures such as entropy and dispersion to capture the reliability of audio and visual 
only classification at the frame level and estimate stream exponents on basis of held out data Such exponents are held constant within confidence value intervals Audio Visual Speaker Adaptation Speaker adaptation is traditionally used in practical audio only ASR systems to improve speaker independent system performance when little data from a speaker of interest are available Gauvain and Lee 1994 Leggetter and Woodland 1995 Neumeyer et al 1995 Anastasakos et al 1997 Gales 1999 Adaptation is also of interest across tasks or environments In the audio visual ASR domain adaptation is of great importance since audio visual corpora are scarce and their collection expensive Given few bimodal adaptation data from a particular speaker and a baseline speaker independent HMM one wishes to estimate adapted HMM parameters that better model the audio visual observations of the particular speaker Two popular algorithms for speaker adaptation are maximum likelihood linear regression MLLR Leggetter and Woodland 1995 and 
maximum a posteriori MAP adaptation Gauvain and Lee 1994 MLLR obtains a maximum likelihood estimate of a linear transformation of the HMM means while leaving covariance matrices mixture weights and transition probabilities unchanged and it provides successful adaptation with a small amount of adaptation data rapid adaptation On the other hand MAP follows the Bayesian paradigm for estimating the HMM parameters MAP estimates of HMM parameters slowly converge to their EM obtained estimates as the amount of adaptation data becomes large however such a convergence is slow and therefore MAP is not suitable for rapid adaptation In practice MAP is often used Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 in conjunction with MLLR Neumeyer et al 1995 Both techniques can be used in feature fusion Potamianos and Neti 2001a and decision fusion models discussed above Potamianos and Potamianos 1999 in a straightforward manner One can 
also consider feature level front end adaptation by adapting for example the audio only and visual only LDA and MLLT matrices and in case HiLDA fusion is used the joint audio visual LDA and MLLT matrices Potamianos and Neti 2001a Experiments using these techniques are reported in a later section Alternative adaptation algorithms also exist such as speaker adaptive training Anastasakos et al 1997 and front end MLLR Gales 1999 and can be used in audio visual ASR Vanegas et al 1998 Summary on Audio Visual Integration We have presented a summary of the most common fusion techniques for audio visual ASR We first discussed the choice of speech classes and statistical ASR models that influence the design of some fusion algorithms Subsequently we described a number of feature and decision integration techniques suitable for bimodal LVCSR and finally briefly touched upon the issue of audio visual speaker adaptation Among the fusion algorithms discussed decision fusion techniques explicitly model the reliability of 
each source of speech information by using stream weights to linearly combine audio and visual only classifier log likelihoods When properly estimated the use of weights results in improved ASR over feature fusion techniques as reported in the literature and demonstrated in the Experiments section Potamianos and Graf 1998 Neti et al 2000 Luettin et al 2001 In most systems reported such weights are set to a constant value over each modality possibly dependent on the audio only channel quality SNR However robust estimation of the weights at a finer level utterance or frame level on basis of both audio and visual channel characteristics has not been sufficiently addressed Furthermore the issue of whether speech class dependence of stream weights is desirable has also not been fully investigated Although such dependence seems to help in late integration schemes Neti et al 2000 or small vocabulary tasks Jourlin 1997 Miyajima et al 2000 the problem remains unresolved for early integration in LVCSR Gravier et al 
2002a There are additional open questions relevant to decision fusion The first concerns the stage of measurement level information integration i e the degree of allowed asynchrony between the audio and visual streams The second has to do with the functional form of stream log likelihood combination as integration by means of 14 is not necessarily optimal and it fails to yield an emission probability distribution Finally it is worth mentioning a theoretical shortcoming of the log likelihood linear combination model used in the decision fusion algorithms considered In contrast to feature fusion such combination assumes class conditional independence of the audio and visual stream observations This appears to be a non realistic assumption Yehia et al 1998 A number of models are being investigated to overcome this drawback Pavlovic 1998 Pan et al 1998 AUDIO VISUAL DATABASES A major contributor to the progress achieved in traditional audio only ASR has been the availability of a wide variety of large multi 
subject databases on a number of well defined recognition tasks of different complexities These corpora have often been collected using funding from U S government agencies for example the Defense Advanced Research Projects Agency and the National Science Foundation or through wellorganized European activities such as the Information System Technology program funded by the European Commission or the European Language Resources Association The resulting databases are available to the interested research groups by the Linguistic Data Consortium LDC or the European Language resources Distribution Agency ELDA for example Benchmarking research progress in audio only ASR has been possible on such common databases In contrast to the abundance of audio only corpora there exist only few databases suitable for audio visual ASR research This is because the field is relatively young but also due to the fact that audio visual databases pose additional challenges concerning database collection storage and distribution not 
found in the audio Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 only domain For example computer acquisition of visual data at full size frame rate and high image quality synchronous to the audio input requires expensive hardware whereas even highly compressed visual data storage consumes at least an order of magnitude more storage space than audio making widespread database distribution a non trivial task Although solutions have been steadily improving and becoming available at a lower cost these issues have seriously hindered availability of large audio visual corpora Additional difficulties stem from the proprietary nature of some collected corpora as well as privacy issues due to the inclusion of the visual modality Most existing audio visual databases are the result of efforts by few university groups or individual researchers with limited resources Therefore most of these corpora suffer from one or more 
shortcomings Chibelushi et al 1996 2002 Hennecke et al 1996 They contain a single or small number of subjects affecting the generalizability of developed methods to the wider population they typically have small duration often resulting in undertrained statistical models or non significant performance differences between various proposed algorithms and finally they mostly address simple recognition tasks such as small vocabulary ASR of isolated or connected words These limitations have caused a growing gap in the state of the art between audio only and audio visual ASR in terms of recognition task complexity To help bridge this gap we have recently completed the collection of a large corpus suitable for audio visual LVCSR which we used for experiments during the Johns Hopkins summer 2000 workshop Neti et al 2000 Some of these experiments are summarized in the following section In the remainder of this section we give an overview of the most commonly used audio visual databases in the literature Some of these 
sets have been used by multiple sites and researchers allowing some algorithm comparisons However benchmarking on common corpora is not widespread Subsequently we describe the IBM ViaVoiceTM audio visual database and additional corpora used in the experiments reported in the next section Overview of Small and Medium Vocabulary Audio Visual Corpora The first database used for automatic recognition of audio visual speech was collected by Petajan 1984 Data of a single subject uttering 2 10 repetitions of 100 isolated English words including letters and digits were collected under controlled lighting conditions Since then several research sites have pursued audiovisual data collection Some of the resulting corpora are discussed in the following A number of databases are designed to study audio visual recognition of consonants C vowels V or transitions between them For example Adjoudani and Beno it 1996 report a single speaker corpus of 54 V1CV2 CV1 non sense words three French vowels and six consonants are 
considered Su and Silsbee 1996 recorded a single speaker corpus of aCa non sense words for recognition of 22 English consonants Robert Ribes et al 1998 as well as Teissier et al 1999 report recognition of ten French oral vowels uttered by a single subject Czap 2000 considers a single subject corpus of V1CV1 and C1VC1 non sense words for recognition of Hungarian vowels and consonants The most popular task for audio visual ASR is isolated or connected digit recognition Various corpora allow digit recognition experiments For example the Tulips1 database Movellan and Chadderdon 1996 contains recordings of 12 subjects uttering digits one to four and has been used for isolated recognition of these four digits in a number of papers Luettin et al 1996 Movellan and Chadderdon 1996 Gray et al 1997 Vanegas et al 1998 Scanlon and Reilly 2001 The M2VTS database although tailored to speaker verification applications also contains digit 0 to 9 recordings of 37 subjects mostly in French Pigeon and Vandendorpe 1997 and it 
has been used for isolated digit recognition experiments Dupont and Luettin 2000 Miyajima et al 2000 XM2VTS an extended version of this database containing 295 subjects has recently been completed in the English language Messer et al 1999 Additional single subject digit databases include the NATO RSG10 digit triples set used by Tomlinson et al 1996 for isolated digit recognition and two connected digits databases reported by Potamianos et al 1998 and Heckmann et al 2001 Finally two very recent databases suitable for multi subject connected digit recognition have been collected at the University of Illinois at Urbana Champaign a 100 subject set with results reported in Chu and Huang 2000 and Zhang et al 2000 and at Clemson University the 36 subject CUAVE dataset Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Figure 10 Example video frames of ten subjects from the IBM ViaVoiceTM audio visual database The database contains 
approximately 50 hrs of continuous dictation style audio visual speech by 290 subjects collected with minor face pose lighting and background variation Neti et al 2000 as discussed in Patterson et al 2002 Isolated or connected letter recognition constitutes another popular audio visual ASR task German connected letter recognition on data of up to six subjects has been reported by Bregler et al 1993 Bregler and Konig 1994 Duchnowski et al 1994 and Meier et al 1996 whereas Krone et al 1997 work on singlespeaker isolated German letter recognition Single or two subject connected French letter recognition is considered in Alissali et al 1996 The IBM ViaVoiceTM Audio Visual Database To date the largest audio visual database collected and the only one suitable for speaker independent LVCSR is the IBM ViaVoiceTM audio visual database The corpus consists of full face frontal video and audio of 290 subjects see also Figure 10 uttering ViaVoiceTM training scripts i e continuous read speech with mostly verbalized 
punctuation dictation style The database video is of a 704480 pixel size interlaced captured in color at a rate of 30 Hz i e 60 fields per second are available at a resolution of 240 lines and it is MPEG2 encoded at the relatively high compression ratio of about 50 1 High quality wideband audio is synchronously collected with the video at a rate of 16 kHz and at a relatively clean audio environment quiet office with some background computer noise resulting in a 19 5 dB SNR The duration of the entire database is approximately 50 hours and it contains 24 325 transcribed utterances with a 10 403 word vocabulary from which 21 281 utterances are used in the experiments reported in the next section In addition to Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 AUDIO 25 msec MFCC EXTRA CTION 1 24 t FEATURE MEAN NORMALIZATION t 1 24 1 1 216 LDA JA 9 216 1 60 1 1 60 MLLT 1 60 1 t 10 msec shift 100 Hz t 1 64 1 64 4096 1 1 E P LDA A 
60 P MLLT A o A t 60 VIDEO 4096 t 1 100 1 1 101 DCT 1 P DCT V o AV t LDA 1 60 1 1 60 AV MLLT 1 60 1 100 P LDA 101 AV 60 P MLLT 60 ROI EXTRACTION PROCESSING AT 60 Hz o HiLDA t t INTERPOLATION FROM 60 TO 100 Hz 1 FEATURE MEAN NORMALIZATION 100 V DCT 1 100 1 100 LDA 1 30 1 1 30 30 V DCT t 1 1 30 1 1 690 MLLT P LDA P MLLT LDA 30 1 41 1 1 41 V 41 MLLT 1 41 E JV 23 P LDA 690 V 1 o V t 41 P MLLT Figure 11 The audio visual ASR system employed in some of the experiments reported in this chapter In addition to the baseline system used during the Johns Hopkins summer 2000 workshop a larger mouth ROI is extracted within frame discriminant features are used and a longer temporal window is considered in the visual front end compare to Figure 7 HiLDA feature fusion is employed LVCSR a 50 subject connected digit database has been collected at IBM in order to study the visual modality benefit to a popular small vocabulary ASR task This DIGITS corpus contains 6689 utterances of 7 and 10 digit strings both zero and oh are used 
with a total duration of approximately 10 hrs Furthermore to allow investigation of automatic speechreading performance for impaired speech Potamianos and Neti 2001a both LVCSR and DIGITS audio visual speech data of a single speech impaired male subject with profound hearing loss have been collected In Table 3 a summary of the above corpora is given together with their partitioning used in the experiments reported in the following section AUDIO VISUAL ASR EXPERIMENTS In this section we present experimental results on visual only and audio visual ASR using mainly the IBM ViaVoiceTM database discussed above Some of these results have been obtained during the Johns Hopkins summer 2000 workshop Neti et al 2000 Experiments conducted later on both these data as well as on the IBM connected digits task DIGITS are also reported Potamianos et al 2001a Goecke et al 2002 Gravier et al 2002a In addition the application of audio visual speaker adaptation methods on the hearing impaired dataset is also discussed 
Potamianos and Neti 2001a First however we briefly describe the basic audio visual ASR system as well as the experimental framework used The Audio Visual ASR System Our basic audio visual ASR system utilizes appearance based visual features that use a discrete cosine transform DCT of the mouth region of interest ROI as described in Potamianos et al 2001b Given the video of the speaker s face available at 60 Hz it first performs face detection and mouth center and size estimation employing the algorithm of Senior 1999 and on basis of these it extracts a size normalized 64 64 greyscale pixel mouth ROI as discussed in a previous section see also Figure 2 Subsequently a two dimensional separable fast DCT is applied on the ROI and its 24 highest energy coefficients over the training data are retained A number of post processing steps are applied on the resulting static feature Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 
Speech Recognition Training set Held out set Adaptation set Test set condition task Utter Dur Sub Utter Dur Sub Utter Dur Sub Utter Dur Sub Normal LVCSR 17111 34 55 239 2277 4 47 25 855 2 03 26 1038 2 29 26 DIGITS 5490 8 01 50 670 0 58 50 670 0 58 50 529 0 46 50 Impaired LVCSR N A N A 50 0 11 1 50 0 11 1 DIGITS N A N A 80 0 08 1 60 0 06 1 Table 3 The IBM audio visual databases discussed and used in the experiments reported in this chapter Their partitioning into training held out adaptation and test sets is depicted number of utterances duration in hours and number of subjects are shown for each set Both large vocabulary continuous speech LVCSR and connected digit DIGITS recognition are considered for normal as well as impaired speech The IBM ViaVoiceTM database corresponds to the LVCSR task in the normal speech condition For the normal speech DIGITS task the held out and adaptation sets are identical For impaired speech due to the lack of sufficient training data adaptation of HMMs trained in the normal 
speech condition is considered vector namely linear interpolation to the audio feature rate from 60 to 100 Hz feature mean normalization FMN for improved robustness to lighting and other variations concatenation of 15 adjacent features to capture dynamic speech information see also 5 and linear discriminant analysis LDA for optimal dimensionality reduction followed by a maximum likelihood data rotation MLLT for improved statistical data modeling The resulting feature vector V t has dimension 41 These steps are described in more detail in the Visual front end section of this chapter see also Figure 7 Improvements to this DCT based visual front end have been proposed in Potamianos and Neti 2001b including the use of a larger ROI a withinframe discriminant DCT feature selection and a longer temporal window see Figure 11 During the Johns Hopkins summer workshop and in addition to the DCT based features joint appearance and shape features by means of active appearance models AAMs have also been employed In 
particular 6000 dimensional appearance vectors containing the normalized face color pixel values and 134 dimensional shape vectors of the face shape coordinates are extracted at 30 Hz and are passed through two stages of principal components analysis PCA The resulting static AAM feature vector is 86 dimensional and it is post processed similarly to the DCT feature vector see Figure 7 resulting to 41 dimensional dynamic features o In parallel to the visual front end traditional audio features are extracted at a 100 Hz rate that consist of mel frequency cepstral coefficients MFCCs and its energy Rabiner and Juang 1993 Deller et al 1993 Young et al 1999 The obtained static feature vector is 24 dimensional and following FMN LDA on 9 adjacent frames and MLLT it gives rise to a 60 dimensional dynamic speech vector A t as depicted in Figure 11 The audio and visual front ends provide time synchronous audio and visual feature vectors that can be used in a number of fusion techniques discussed in a previous section 
The derived concatenated audio visual vector AV has dimension 101 whereas in the HiLDA feature fusion implementation the bimodal LDA generates t with a reduced dimensionality 60 see also Figure 11 features HiLDA t o o o In all cases where LDA and MLLT matrices are employed audio visual only and audio visual feature extraction by means of HiLDA fusion we consider jCj 3367 context dependent sub phonetic classes that coincide with the context dependent states of an available audio only HMM that has been previously developed at IBM for LVCSR trained on a number of audio corpora Polymenakos et al 1998 The forced alignment Rabiner and Juang 1993 of the training set audio based on this HMM and the data transcriptions produces labels cl 2 C for the training set audio visual and audio visual data vectors l l 1 L Such labeled vectors can then be used to estimate the required matrices LDA MLLT as described in the Visual front end section of this chapter P P x The Experimental Framework The audio visual databases 
discussed above have been partitioned into a number of sets in order to train and evaluate models for audio visual ASR as detailed in Table 3 For both LVCSR and DIGITS speech tasks in Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Modality Visual Remarks DCT DWT PCA AAM WER 58 1 58 8 59 4 64 0 Modality Acoustic None Remarks WER MFCC noisy 55 0 Oracle 31 2 Anti oracle 102 6 LM best path 62 0 Table 4 Comparisons of various visual features three appearance based features and one joint shape and appearance feature representation for speaker independent LVCSR Neti et al 2000 Matthews et al 2001 Word error rate WER is depicted on a subset of the IBM ViaVoiceTM database test set of Table 3 Visual performance is obtained after rescoring of lattices that have been previously generated based on noisy at 8 5 dB SNR audio only MFCC features For comparison characteristic lattice WERs are also depicted oracle anti oracle and best path 
based on language model scores alone Among the visual speech representations considered the DCT based features are superior and contain significant speech information the normal speech condition the corresponding training sets are used to obtain all LDA and MLLT matrices required the phonetic decision trees that cluster HMM states on basis of phonetic context as well as to train all HMMs reported The held out sets are used to tune parameters relevant to audio visual decision fusion and decoding such as the multi stream HMM and language model weights for example whereas the test sets are used for evaluating the performance of the trained HMMs Optionally the adaptation sets can be employed for tuning the front ends and or HMMs to the characteristics of the test set subjects In the LVCSR case the subject populations of the training held out and test sets are disjoint thus allowing for speaker independent recognition whereas in the DIGITS data partitioning all sets have data from the same 50 subjects thus 
allowing multi speaker experiments Due to this fact the adaptation and held out sets for DIGITS are identical For the impaired speech data the duration of the collected data is too short to allow HMM training Therefore LVCSR HMMs trained on the IBM ViaVoiceTM dataset are adapted on the impaired LVCSR and DIGITS adaptation sets see Table 3 To assess the benefit of the visual modality to ASR in noisy conditions in addition to the relatively clean audio condition of the database recordings we artificially corrupt the data audio with additive non stationary speech babble noise at various SNRs ASR results are then reported at a number of SNRs ranging within 1 5 19 5 dB for LVCSR and 3 5 19 5 dB for DIGITS with all corresponding front end matrices and HMMs trained in the matched condition In particular during the Johns Hopkins summer 2000 workshop only two audio conditions were considered for LVCSR The original 19 5 dB SNR audio and a degraded one at 8 5 dB SNR Notice that in contrast to the audio no noise is 
added to the video channel or features Many cases of visual noise could have been considered such as additive noise on video frames blurring frame rate decimation and extremely high compression factors among others Some preliminary studies on the effects of video degradations to visual recognition can be found in the literature Davoine et al 1997 Williams et al 1997 Potamianos et al 1998 These studies find automatic speechreading performance to be rather robust to video compression for example but to degrade rapidly for frame rates below 15 Hz The ASR experiments reported next follow two distinct paradigms The results on the IBM ViaVoiceTM data obtained during the Johns Hopkins summer 2000 workshop employ a lattice rescoring paradigm due to the limitations in large vocabulary decoding of the HTK software used there Young et al 1999 namely lattices were first generated prior to the workshop using the IBM Research decoder Hark with HMMs trained at IBM and subsequently rescored during the workshop by trained 
tri phone context dependent HMMs on various feature sets or fusion techniques using HTK Three sets of lattices were generated for these experiments and were based on clean audio only 19 5 dB noisy audio only and noisy audio visual at the 8 5 dB SNR condition HiLDA features In the second experimental paradigm full decoding results obtained by directly using the IBM Research recognizer are reported For the LVCSR experiments 11 phone context dependent HMMs with 2 808 context dependent states and 47 k Gaussian mixtures are used whereas for DIGITS recognition in normal speech the corresponding numbers are 159 and 3 2 k for single stream models Decoding using the closed set vocabulary 10 403 words and a trigram language model is employed for LVCSR this is the case also for the workshop results whereas the 11 digit zero to nine including Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Audio Condition Audio only AV Concat FF AV 
HiLDA FF AV DMC DF Clean 14 44 16 00 13 84 13 65 12 95 Noisy 48 10 40 00 36 99 Audio Condition AV MS Joint DF AV MS Sep DF AV MS PROD DF AV MS UTTER DF Clean 14 62 14 92 14 19 13 47 Noisy 36 61 38 38 35 21 35 27 Table 5 Test set speaker independent LVCSR audio only and audio visual WER for the clean 19 5 dB SNR and a noisy audio 8 5 dB condition Two feature fusion FF and five decision fusion DF based audio visual systems are evaluated using the lattice rescoring paradigm Neti et al 2000 Glotin et al 2001 Luettin et al 2001 oh word vocabulary is used for DIGITS with unknown digit string length Visual Only Recognition The suitability for LVCSR of a number of appearance based visual features and AAMs was studied during and after the Johns Hopkins summer workshop Neti et al 2000 Matthews et al 2001 For this purpose noisy audio only lattices were rescored by HMMs trained on the various visual features considered namely 86dimensional AAM features as well as 24 dimensional DCT PCA on 32 32 pixel mouth ROIs and DWT 
based features All features were post processed as previously discussed to yield 41 dimensional feature vectors see Figure 7 For the DWT features the Daubechies class wavelet filter of approximating order 3 is used Daubechies 1992 Press et al 1995 LVCSR recognition results are reported in Table 4 depicted in word error rate WER The DCT outperformed all other features considered Notice however that these results cannot be interpreted as visual only recognition since they correspond to cascade audio visual fusion of audio only ASR followed by visual only rescoring of a network of recognized hypotheses For reference a number of characteristic lattice WERs are also depicted in Table 4 including the audio only at 8 5 dB result All feature performances are bounded by the lattice oracle and anti oracle WERs It is interesting to note that all appearance based features considered attain lower WERs e g 58 1 for DCT features than the WER of the best path through the lattice based on the language model alone 62 0 
Therefore such visual features do convey significant speech information AAMs on the other hand did not perform well possibly due to severe undertraining of the models resulting in poor fitting to unseen facial data As expected visual only recognition based on full decoding instead of lattice rescoring is rather poor The LVCSR WER on the speaker independent test set of Table 3 based on per speaker MLLR adaptation is reported at 89 2 in Potamianos and Neti 2001b using the DCT features of the workshop Extraction of larger ROIs and the use of within frame DCT discriminant features and longer temporal windows as depicted in Figure 11 result in the improved WER of 82 3 In contrast to LVCSR DIGITS visual only recognition constitutes a much easier task Indeed on the multi speaker test set of Table 3 a 16 8 WER is achieved after per speaker MLLR adaptation Audio Visual ASR A number of audio visual integration algorithms presented in the fusion section of this chapter were compared during the Johns Hopkins summer 2000 
workshop As already mentioned two audio conditions were considered The original clean database audio 19 5 dB SNR and a noisy one at 8 5 dB SNR In the first case fusion algorithm results were obtained by rescoring pre generated clean audio only lattices at the second condition HiLDA noisy audio visual lattices were rescored The results of these experiments are summarized in Table 5 Notice that every fusion method considered outperformed audio only ASR in the noisy case reaching up to a 27 relative reduction in WER from 48 10 noisy audio only to 35 21 audio visual In the clean audio condition among the two feature fusion techniques considered HiLDA fusion Potamianos et al 2001a improved ASR from 14 44 audio only to a 13 84 audio visual WER however concatenative fusion degraded performance to 16 0 Among the decision fusion algorithms used the product HMM AV MS PROD with jointly trained audio visual components Luettin et al 2001 improved performance Chapter to appear in Issues in Visual and Audio Visual Speech 
Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 25 90 LVCSR AUDIO ONLY 20 DIGITS AUDIO ONLY AV Enhanced AV Concat 15 WORD ERROR RATE WER 80 70 AV Enhanced 60 50 40 30 20 10 0 0 5 7 5 dB GAIN AV Concat AV HiLDA AV MS Joint 7 dB GAIN AV HiLDA AV MS Joint 10 0 5 10 15 20 0 5 10 15 20 SIGNAL TO NOISE RATIO SNR dB SIGNAL TO NOISE RATIO SNR dB Figure 12 Comparison of audio only and audio visual ASR by means of three feature fusion AV Concat AV HiLDA and AV Enhanced algorithms and one decision fusion AV MS Joint technique using the full decoding experimental paradigm WERs vs audio channel SNR are reported on both the IBM ViaVoiceTM test set speaker independent LVCSR left as well as on the multi speaker DIGITS test set right of Table 3 HiLDA feature fusion outperforms alternative feature fusion methods whereas decision fusion outperforms all three feature fusion approaches resulting in an effective SNR gain of 7 dB for LVCSR and 7 5 dB for DIGITS at 10 dB SNR Potamianos et al 2001a Goecke 
et al 2002 Gravier et al 2002a Notice that the WER range in the two graphs differs to a 14 19 WER In addition utterance based stream exponents for a jointly trained multi stream HMM AV MS UTTER estimated using an average of the voicing present at each utterance further reduced WER to 13 47 Glotin et al 2001 achieving a 7 relative WER reduction over audio only performance Finally a late integration technique based on discriminative model combination AV DMC of audio and visual HMMs Beyerlein 1998 Vergyri 2000 Glotin et al 2001 produced a WER of 12 95 amounting to a 5 reduction from its clean audio only baseline of 13 65 this differs from the 14 44 audio only result due to the rescoring of n best lists instead of lattices Notice that for both clean and noisy audio conditions the best decision fusion method outperformed the best feature fusion technique considered In addition for both conditions joint multi stream HMM training outperformed separate training of the HMM stream components something not surprising 
since joint training forces state synchrony between the audio and visual streams To further demonstrate the differences between the various fusion algorithms and to quantify the visual modality benefit to ASR we review a number of full decoding experiments recently conducted for both the LVCSR and DIGITS tasks and at a large number of SNR conditions Potamianos et al 2001a Goecke et al 2002 Gravier et al 2002a All three feature fusion techniques discussed in the relevant section of this chapter are compared to decision fusion by means of a jointly trained multi stream HMM The results are depicted in Figure 12 Among the feature fusion methods considered HiLDA feature fusion is superior to both concatenative fusion and the enhancement approach In the clean audio case for example HiLDA fusion reduces the audio only LVCSR WER of 12 37 to 11 56 audio visual whereas feature concatenation degrades performance to 12 72 the enhancement method obviously provides the original audio only performance in this case Notice 
that these results are somewhat different to the ones reported in Table 5 due to the different experimental paradigm considered In the most extreme noisy case considered for LVCSR 1 5 dB SNR the audio only WER of 92 16 is reduced to 48 63 using HiLDA compared to 50 76 when feature concatenation is employed and 63 45 when audio feature enhancement is used Similar results hold for DIGITS recognition although the difference between HiLDA and concatenative feature fusion ASR is small possibly due to the fact that HMMs with significantly less Gaussian mixtures are used and the availability of sufficient data to train on high dimensional concatenated audio visual vectors The comparison Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Method j Task Modality Unadapted MLLR MAP MAP MLLR Mat MAP Mat MAP MLLR LVCSR AU VI AV 116 022 136 359 106 014 52 044 110 166 42 873 52 376 101 215 44 199 47 624 95 027 41 216 52 928 98 674 46 519 
50 055 93 812 41 657 AU 52 381 3 770 3 373 2 381 3 968 2 381 DIGITS VI 48 016 16 667 12 103 10 516 8 730 8 531 AV 24 801 0 992 1 190 0 992 1 190 0 992 Table 6 Adaptation results on the speech impaired data WER of the audio only AU visual only VI and audio visual AV modalities using HiLDA feature fusion is reported on both the LVCSR left table part and DIGITS test sets right table of the speech impaired data using unadapted HMMs trained in normal speech as well as a number of HMM adaptation methods All HMMs are adapted on the joint speech impaired LVCSR and DIGITS adaptation sets of Table 3 For the continuous speech results decoding using the test set vocabulary of 537 words is reported MAP followed by MLLR adaptation and possibly preceded by front end matrix adaptation Mat achieves the best results for all modalities and for both tasks considered Potamianos and Neti 2001a between multi stream decision fusion and HiLDA fusion reveals that the jointly trained multi stream HMM performs significantly better For 
example at 1 5 dB SNR LVCSR WER is reduced to 46 28 compared to 48 63 for HiLDA Similarly for DIGITS recognition at 3 5 dB the HiLDA WER is 7 51 whereas the multi stream HMM WER is significantly lower namely 6 64 This is less than one third of the audio only WER of 23 97 A useful indicator when comparing fusion techniques and establishing the visual modality benefit to ASR is the effective SNR gain measured here with reference to the audio only WER at 10 dB To compute this gain we need to consider the SNR value where the audio visual WER equals the reference audio only WER see Figure 12 For HiLDA fusion this gain equals approximately 6 dB for both LVCSR and DIGITS tasks Jointly trained multi stream HMMs improve these gains to 7 dB for LVCSR and 7 5 dB for DIGITS at 10 dB SNR Full decoding experiments employing additional decision fusion techniques are currently in progress In particular intermediate fusion results by means of the product HMM are reported in Gravier et al 2002b Audio Visual Adaptation We now 
describe recent experiments on audio visual adaptation in a case study of single subject audio visual ASR of impaired speech Potamianos and Neti 2001a As already indicated the small amount of speech impaired data collected see Table 3 is not sufficient for HMM training thus calling for speaker adaptation techniques instead A number of such methods described in a previous section are used for adapting audioonly visual only and audio visual HMMs suitable for LVCSR The results on both speech impaired LVCSR and DIGITS tasks are depicted in Table 6 Notice that due to poor accuracy on impaired speech decoding on the LVCSR task is performed using the 537 word test set vocabulary of the dataset Clearly the mismatch between the normal and impaired speech data is dramatic as the Unadapted table entries demonstrate Indeed the audio visual WER in the LVCSR task reaches 106 0 such large numbers occur due to word insertions whereas the audio visual WER in the DIGITS task is 24 8 in comparison the normal speech per subject 
adapted audio visual LVCSR WER is 10 2 and the audio visual DIGITS WER is only 0 55 computed on the test sets of Table 3 We first consider MLLR and MAP HMM adaptation using the joint speech impaired LVCSR and DIGITS adaptation tests Audio visual only and audio visual performances improve dramatically as demonstrated in Table 6 Due to the rather large adaptation set MAP performs similarly well to MLLR Applying MLLR after MAP improves results and it reduces the audio visual WER to 41 2 and 0 99 for the LVCSR and DIGITS tasks respectively amounting to a 61 and 96 relative WER reduction over the audio visual Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 unadapted results and to a 13 and 58 relative WER reduction over the audio only MAP MLLR adapted results Clearly therefore the visual modality dramatically benefits the automatic recognition of impaired speech We also apply front end adaptation possibly followed by MLLR 
adaptation with the results depicted in the Mat MAP MLLR entries of Table 6 Although visual only recognition improves the audio only recognition results fail to do so As a consequence audio visual ASR degrades possibly also due to the fact that in this experiment audio visual matrix adaptation is only applied to the second stage of LDA MLLT SUMMARY AND DISCUSSION In this chapter we provided an overview of the basic techniques for automatic recognition of audio visual speech proposed in the literature over the past twenty years The two main issues relevant to the design of audio visual ASR systems are First the visual front end that captures visual speech information and second the integration fusion of audio and visual features into the automatic speech recognizer used Both are challenging problems and significant research effort has been directed towards finding appropriate solutions We first discussed extracting visual features from the video of the speaker s face The process requires first the detection 
and tracking of the face mouth region and possibly the speaker s lip contours A number of mostly statistical techniques suitable for the task were reviewed Various visual features proposed in the literature were then presented Some are based on the mouth region appearance and employ image transforms or other dimensionality reduction techniques borrowed from the pattern recognition literature in order to extract relevant speech information Others capture the lip contour and possibly face shape characteristics by means of statistical or geometric models Combinations of features from these two categories are also possible Subsequently we concentrated on the problem of audio visual integration Possible solutions to it differ in various aspects including the classifier and classes used for automatic speech recognition the combination of single modality features vs single modality classification decisions and in the latter case the information level provided by each classifier the temporal level of the integration 
and the sequence of such decision combination We concentrated on HMM based recognition based on sub phonetic classes and assuming time synchronous audio and visual feature generation we reviewed a number of feature and decision fusion techniques Within the first category we discussed simple feature concatenation discriminant feature fusion and a linear audio feature enhancement approach For decision based integration we concentrated in linear log likelihood combination of parallel single modality classifiers at various levels of integration considering the state synchronous multi stream HMM for early fusion the product HMM for intermediate fusion and discriminative model combination for late integration and we discussed training the resulting models Developing and benchmarking feature extraction and fusion algorithms requires available audio visual data A limited number of corpora suitable for research in audio visual ASR have been collected and used in the literature A brief overview of them was also 
provided followed by a description of the IBM ViaVoiceTM database suitable for speaker independent audio visual ASR in the large vocabulary continuous speech domain Subsequently a number of experimental results were reported using this database as well as additional corpora recently collected at IBM Some of these experiments were conducted during the summer 2000 workshop at the Johns Hopkins University and compared both visual feature extraction and audio visual fusion methods for LVCSR More recent experiments as well as a case study of speaker adaptation techniques for audio visual recognition of impaired speech were also presented These experiments showed that a visual front end can be designed that successfully captures speaker independent large vocabulary continuous speech information Such a visual front end uses discrete cosine transform coefficients of the detected mouth region of interest suitably post processed Combining the resulting visual features with traditional acoustic ones results in 
significant improvements over audio only recognition in both clean and of course degraded acoustic conditions across small and large vocabulary tasks as well as for both normal and impaired speech A successful combination technique is the multi stream HMM based decision fusion approach or the simpler but inferior discriminant feature fusion HiLDA method This chapter clearly demonstrates that over the past twenty years much progress has been accomplished in capturing and integrating visual speech information into automatic speech recognition However the visual modality has yet to become utilized in mainstream ASR systems This is due to the fact that issues Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 of both practical and research nature remain challenging On the practical side of things the high quality of captured visual data which is necessary for extracting visual speech information capable of enhancing ASR 
performance introduces increased cost storage and computer processing requirements In addition the lack of common large audio visual corpora that address a wide variety of ASR tasks conditions and environments hinders development of audio visual systems suitable for use in particular applications On the research side the key issues in the design of audio visual ASR systems remain open and subject to more investigation In the visual front end design for example face detection facial feature localization and face shape tracking robust to speaker pose lighting and environment variation constitute challenging problems A comprehensive comparison between face appearance and shape based features for speakerdependent vs speaker independent automatic speechreading is also unavailable Joint shape and appearance three dimensional face modeling used for both tracking and visual feature extraction has not been considered in the literature although such an approach could possibly lead to the desired robustness and 
generality of the visual front end In addition when combining audio and visual information a number of issues relevant to decision fusion require further study such as the optimal level of integrating the audio and visual loglikelihoods the optimal function for this integration as well as the inclusion of suitable local estimates of the reliability of each modality into this function Further investigation of these issues is clearly warranted and it is expected to lead to improved robustness and performance of audio visual ASR Progress in addressing some or all of these questions can also benefit other areas where joint audio and visual speech processing is suitable Chen and Rao 1998 such as speaker identification and verification Jourlin et al 1997 Wark and Sridharan 1998 ACKNOWLEDGEMENTS We would like to acknowledge a number of people for particular contributions to this work Giridharan Iyengar and Andrew Senior IBM for their help with face and mouth region detection on the IBM ViaVoiceTM and other audio 
visual data discussed in this chapter Rich Wilkins and Eric Helmuth formerly with IBM for their efforts in data collection Guillaume Gravier currently at IRISA INRIA Rennes for the joint multistream HMM training and full decoding on the connected digits and LVCSR tasks Roland Goecke currently at the Australian National University for experiments on audio visual based enhancement of audio features during a summer internship at IBM REFERENCES Adjoudani A and Beno it C 1996 On the integration of auditory and visual parameters in an HMM based ASR In Stork D G and Hennecke M E Eds Speechreading by Humans and Machines Berlin Germany Springer pp Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Bahl L R Brown P F DeSouza P V and Mercer L R 1986 Maximum mutual information estimation of hidden Markov model parameters for speech recognition Proc International Conference on Acoustics Speech and Signal Processing Tokyo Japan pp Chapter 
to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Cootes T F Taylor C J Cooper D H and Graham J 1995 Active shape models their training and application Computer Vision and Image Understanding 61 1 Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Goldschen A J Garcia O N and Petajan E D 1996 Rationale for phoneme viseme mapping and feature selection in visual speech recognition In Stork D G and Hennecke M E Eds Speechreading by Humans and Machines Berlin Germany Springer pp Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Krone G Talle B Wichert A and Palm G 1997 Neural architectures for sensorfusion in speech recognition Proc European Tutorial Workshop on Audio Visual Speech Processing Rhodes Greece pp Chapter to appear in Issues in Visual and Audio Visual 
Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Neumeyer L Sankar A and Digalakis V 1995 A comparative study of speaker adaptation techniques Proc European Conference on Speech Communication and Technology Madrid Spain pp Chapter to appear in Issues in Visual and Audio Visual Speech Processing G Bailly E Vatikiotis Bateson and P Perrier Eds MIT Press 2004 Rowley H A Baluja S and Kanade T 1998 Neural network based face detection IEEE Transactions on Pattern Analysis and Machine Intelligence 20 1 
37	a	Journal of Machine Learning Research 3 2003 993 1022 Submitted 2 02 Published 1 03 Latent Dirichlet Allocation David M Blei Computer Science Division University of California Berkeley CA 94720 USA BLEI CS BERKELEY EDU Andrew Y Ng Computer Science Department Stanford University Stanford CA 94305 USA ANG CS STANFORD EDU Michael I Jordan Computer Science Division and Department of Statistics University of California Berkeley CA 94720 USA JORDAN CS BERKELEY EDU Editor John Lafferty Abstract We describe latent Dirichlet allocation LDA a generative probabilistic model for collections of discrete data such as text corpora LDA is a three level hierarchical Bayesian model in which each item of a collection is modeled as a finite mixture over an underlying set of topics Each topic is in turn modeled as an infinite mixture over an underlying set of topic probabilities In the context of text modeling the topic probabilities provide an explicit representation of a document We present efficient approximate inference 
techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation We report results in document modeling text classification and collaborative filtering comparing to a mixture of unigrams model and the probabilistic LSI model 1 Introduction In this paper we consider the problem of modeling text corpora and other collections of discrete data The goal is to find short descriptions of the members of a collection that enable efficient processing of large collections while preserving the essential statistical relationships that are useful for basic tasks such as classification novelty detection summarization and similarity and relevance judgments Significant progress has been made on this problem by researchers in the field of information retrieval IR Baeza Yates and Ribeiro Neto 1999 The basic methodology proposed by IR researchers for text corpora a methodology successfully deployed in modern Internet search engines reduces each document in the corpus to a vector of real 
numbers each of which represents ratios of counts In the popular tf idf scheme Salton and McGill 1983 a basic vocabulary of words or terms is chosen and for each document in the corpus a count is formed of the number of occurrences of each word After suitable normalization this term frequency count is compared to an inverse document frequency count which measures the number of occurrences of a c 2003 David M Blei Andrew Y Ng and Michael I Jordan B LEI N G AND J ORDAN word in the entire corpus generally on a log scale and again suitably normalized The end result is a term by document matrix X whose columns contain the tf idf values for each of the documents in the corpus Thus the tf idf scheme reduces documents of arbitrary length to fixed length lists of numbers While the tf idf reduction has some appealing features notably in its basic identification of sets of words that are discriminative for documents in the collection the approach also provides a relatively small amount of reduction in description 
length and reveals little in the way of inter or intradocument statistical structure To address these shortcomings IR researchers have proposed several other dimensionality reduction techniques most notably latent semantic indexing LSI Deerwester et al 1990 LSI uses a singular value decomposition of the X matrix to identify a linear subspace in the space of tf idf features that captures most of the variance in the collection This approach can achieve significant compression in large collections Furthermore Deerwester et al argue that the derived features of LSI which are linear combinations of the original tf idf features can capture some aspects of basic linguistic notions such as synonymy and polysemy To substantiate the claims regarding LSI and to study its relative strengths and weaknesses it is useful to develop a generative probabilistic model of text corpora and to study the ability of LSI to recover aspects of the generative model from data Papadimitriou et al 1998 Given a generative model of text 
however it is not clear why one should adopt the LSI methodology one can attempt to proceed more directly fitting the model to data using maximum likelihood or Bayesian methods A significant step forward in this regard was made by Hofmann 1999 who presented the probabilistic LSI pLSI model also known as the aspect model as an alternative to LSI The pLSI approach which we describe in detail in Section 4 3 models each word in a document as a sample from a mixture model where the mixture components are multinomial random variables that can be viewed as representations of topics Thus each word is generated from a single topic and different words in a document may be generated from different topics Each document is represented as a list of mixing proportions for these mixture components and thereby reduced to a probability distribution on a fixed set of topics This distribution is the reduced description associated with the document While Hofmann s work is a useful step toward probabilistic modeling of text it is 
incomplete in that it provides no probabilistic model at the level of documents In pLSI each document is represented as a list of numbers the mixing proportions for topics and there is no generative probabilistic model for these numbers This leads to several problems 1 the number of parameters in the model grows linearly with the size of the corpus which leads to serious problems with overfitting and 2 it is not clear how to assign probability to a document outside of the training set To see how to proceed beyond pLSI let us consider the fundamental probabilistic assumptions underlying the class of dimensionality reduction methods that includes LSI and pLSI All of these methods are based on the bag of words assumption that the order of words in a document can be neglected In the language of probability theory this is an assumption of exchangeability for the words in a document Aldous 1985 Moreover although less often stated formally these methods also assume that documents are exchangeable the specific 
ordering of the documents in a corpus can also be neglected A classic representation theorem due to de Finetti 1990 establishes that any collection of exchangeable random variables has a representation as a mixture distribution in general an infinite mixture Thus if we wish to consider exchangeable representations for documents and words we need to consider mixture models that capture the exchangeability of both words and documents 994 L ATENT D IRICHLET A LLOCATION This line of thinking leads to the latent Dirichlet allocation LDA model that we present in the current paper It is important to emphasize that an assumption of exchangeability is not equivalent to an assumption that the random variables are independent and identically distributed Rather exchangeability essentially can be interpreted as meaning conditionally independent and identically distributed where the conditioning is with respect to an underlying latent parameter of a probability distribution Conditionally the joint distribution of the 
random variables is simple and factored while marginally over the latent parameter the joint distribution can be quite complex Thus while an assumption of exchangeability is clearly a major simplifying assumption in the domain of text modeling and its principal justification is that it leads to methods that are computationally efficient the exchangeability assumptions do not necessarily lead to methods that are restricted to simple frequency counts or linear operations We aim to demonstrate in the current paper that by taking the de Finetti theorem seriously we can capture significant intra document statistical structure via the mixing distribution It is also worth noting that there are a large number of generalizations of the basic notion of exchangeability including various forms of partial exchangeability and that representation theorems are available for these cases as well Diaconis 1988 Thus while the work that we discuss in the current paper focuses on simple bag of words models which lead to mixture 
distributions for single words unigrams our methods are also applicable to richer models that involve mixtures for larger structural units such as n grams or paragraphs The paper is organized as follows In Section 2 we introduce basic notation and terminology The LDA model is presented in Section 3 and is compared to related latent variable models in Section 4 We discuss inference and parameter estimation for LDA in Section 5 An illustrative example of fitting LDA to data is provided in Section 6 Empirical results in text modeling text classification and collaborative filtering are presented in Section 7 Finally Section 8 presents our conclusions 2 Notation and terminology We use the language of text collections throughout the paper referring to entities such as words documents and corpora This is useful in that it helps to guide intuition particularly when we introduce latent variables which aim to capture abstract notions such as topics It is important to note however that the LDA model is not necessarily 
tied to text and has applications to other problems involving collections of data including data from domains such as collaborative filtering content based image retrieval and bioinformatics Indeed in Section 7 3 we present experimental results in the collaborative filtering domain Formally we define the following terms 995 B LEI N G AND J ORDAN We wish to find a probabilistic model of a corpus that not only assigns high probability to members of the corpus but also assigns high probability to other similar documents 3 Latent Dirichlet allocation Latent Dirichlet allocation LDA is a generative probabilistic model of a corpus The basic idea is that documents are represented as random mixtures over latent topics where each topic is characterized by a distribution over words 1 LDA assumes the following generative process for each document w in a corpus D 1 Choose N Poisson 2 Choose Dir 3 For each of the N words wn a Choose a topic zn Multinomial b Choose a word wn from p wn zn a multinomial probability 
conditioned on the topic zn Several simplifying assumptions are made in this basic model some of which we remove in subsequent sections First the dimensionality k of the Dirichlet distribution and thus the dimensionality of the topic variable z is assumed known and fixed Second the word probabilities are parameterized by a where the parameter is a k vector with components i 0 and where x is the Gamma function The Dirichlet is a convenient distribution on the simplex it is in the exponential family has finite dimensional sufficient statistics and is conjugate to the multinomial distribution In Section 5 these properties will facilitate the development of inference and parameter estimation algorithms for LDA Given the parameters and the joint distribution of a topic mixture a set of N topics z and a set of N words w is given by p z w p p zn p wn zn n 1 N 2 1 We refer to the latent multinomial variables in the LDA model as topics so as to exploit text oriented intuitions but we make no epistemological claims 
regarding these latent variables beyond their utility in representing probability distributions on sets of words 996 L ATENT D IRICHLET A LLOCATION z w N M Figure 1 Graphical model representation of LDA The boxes are plates representing replicates The outer plate represents documents while the inner plate represents the repeated choice of topics and words within a document where p zn is simply i for the unique i such that zi n 1 Integrating over and summing over z we obtain the marginal distribution of a document p w p n 1 zn p zn p wn zn N d 3 Finally taking the product of the marginal probabilities of single documents we obtain the probability of a corpus p D M d 1 p d n 1 zdn p zdn d p wdn zdn Nd d d The LDA model is represented as a probabilistic graphical model in Figure 1 As the figure makes clear there are three levels to the LDA representation The parameters and are corpuslevel parameters assumed to be sampled once in the process of generating a corpus The variables d are document level variables 
sampled once per document Finally the variables zdn and wdn are word level variables and are sampled once for each word in each document It is important to distinguish LDA from a simple Dirichlet multinomial clustering model A classical clustering model would involve a two level model in which a Dirichlet is sampled once for a corpus a multinomial clustering variable is selected once for each document in the corpus and a set of words are selected for the document conditional on the cluster variable As with many clustering models such a model restricts a document to being associated with a single topic LDA on the other hand involves three levels and notably the topic node is sampled repeatedly within the document Under this model documents can be associated with multiple topics Structures similar to that shown in Figure 1 are often studied in Bayesian statistical modeling where they are referred to as hierarchical models Gelman et al 1995 or more precisely as conditionally independent hierarchical models Kass 
and Steffey 1989 Such models are also often referred to as parametric empirical Bayes models a term that refers not only to a particular model structure but also to the methods used for estimating parameters in the model Morris 1983 Indeed as we discuss in Section 5 we adopt the empirical Bayes approach to estimating parameters such as and in simple implementations of LDA but we also consider fuller Bayesian approaches as well 997 B LEI N G AND J ORDAN 3 1 LDA and exchangeability A finite set of random variables z1 zN is said to be exchangeable if the joint distribution is invariant to permutation If is a permutation of the integers from 1 to N p z1 zN p z 1 z N An infinite sequence of random variables is infinitely exchangeable if every finite subsequence is exchangeable De Finetti s representation theorem states that the joint distribution of an infinitely exchangeable sequence of random variables is as if a random parameter were drawn from some distribution and then the random variables in question were 
independent and identically distributed conditioned on that parameter In LDA we assume that words are generated by topics by fixed conditional distributions and that those topics are infinitely exchangeable within a document By de Finetti s theorem the probability of a sequence of words and topics must therefore have the form p w z p n 1 p zn p wn zn N d where is the random parameter of a multinomial over topics We obtain the LDA distribution on documents in Eq 3 by marginalizing out the topic variables and endowing with a Dirichlet distribution 3 2 A continuous mixture of unigrams The LDA model shown in Figure 1 is somewhat more elaborate than the two level models often studied in the classical hierarchical Bayesian literature By marginalizing over the hidden topic variable z however we can understand LDA as a two level model In particular let us form the word distribution p w p w p w z p z z Note that this is a random quantity since it depends on We now define the following generative process for a 
document w 1 Choose Dir 2 For each of the N words wn a Choose a word wn from p wn This process defines the marginal distribution of a document as a continuous mixture distribution p w p n 1 p wn N d where p wn are the mixture components and p are the mixture weights Figure 2 illustrates this interpretation of LDA It depicts the distribution on p w which is induced from a particular instance of an LDA model Note that this distribution on the V 1 simplex is attained with only k kV parameters yet exhibits a very interesting multimodal structure 998 L ATENT D IRICHLET A LLOCATION Figure 2 An example density on unigram distributions p w under LDA for three words and four topics The triangle embedded in the x y plane is the 2 D simplex representing all possible multinomial distributions over three words Each of the vertices of the triangle corresponds to a deterministic distribution that assigns probability one to one of the words the midpoint of an edge gives probability 0 5 to two of the words and the centroid 
of the triangle is the uniform distribution over all three words The four points marked with an x are the locations of the multinomial distributions p w z for each of the four topics and the surface shown on top of the simplex is an example of a density over the V 1 simplex multinomial distributions of words given by LDA 4 Relationship with other latent variable models In this section we compare LDA to simpler latent variable models for text the unigram model a mixture of unigrams and the pLSI model Furthermore we present a unified geometric interpretation of these models which highlights their key differences and similarities 4 1 Unigram model Under the unigram model the words of every document are drawn independently from a single multinomial distribution p w p wn n 1 N This is illustrated in the graphical model in Figure 3a 999 B LEI N G AND J ORDAN w N M a unigram z w N M b mixture of unigrams d z w N M c pLSI aspect model Figure 3 Graphical model representation of different models of discrete data 4 2 
Mixture of unigrams If we augment the unigram model with a discrete random topic variable z Figure 3b we obtain a mixture of unigrams model Nigam et al 2000 Under this mixture model each document is generated by first choosing a topic z and then generating N words independently from the conditional multinomial p w z The probability of a document is p w p z p wn z z n 1 N When estimated from a corpus the word distributions can be viewed as representations of topics under the assumption that each document exhibits exactly one topic As the empirical results in Section 7 illustrate this assumption is often too limiting to effectively model a large collection of documents In contrast the LDA model allows documents to exhibit multiple topics to different degrees This is achieved at a cost of just one additional parameter there are k 1 parameters associated with p z in the mixture of unigrams versus the k parameters associated with p in LDA 4 3 Probabilistic latent semantic indexing Probabilistic latent semantic 
indexing pLSI is another widely used document model Hofmann 1999 The pLSI model illustrated in Figure 3c posits that a document label d and a word wn are 1000 L ATENT D IRICHLET A LLOCATION conditionally independent given an unobserved topic z p d wn p d p wn z p z d z The pLSI model attempts to relax the simplifying assumption made in the mixture of unigrams model that each document is generated from only one topic In a sense it does capture the possibility that a document may contain multiple topics since p z d serves as the mixture weights of the topics for a particular document d However it is important to note that d is a dummy index into the list of documents in the training set Thus d is a multinomial random variable with as many possible values as there are training documents and the model learns the topic mixtures p z d only for those documents on which it is trained For this reason pLSI is not a well defined generative model of documents there is no natural way to use it to assign probability to a 
previously unseen document A further difficulty with pLSI which also stems from the use of a distribution indexed by training documents is that the number of parameters which must be estimated grows linearly with the number of training documents The parameters for a k topic pLSI model are k multinomial distributions of size V and M mixtures over the k hidden topics This gives kV kM parameters and therefore linear growth in M The linear growth in parameters suggests that the model is prone to overfitting and empirically overfitting is indeed a serious problem see Section 7 1 In practice a tempering heuristic is used to smooth the parameters of the model for acceptable predictive performance It has been shown however that overfitting can occur even when tempering is used Popescul et al 2001 LDA overcomes both of these problems by treating the topic mixture weights as a k parameter hidden random variable rather than a large set of individual parameters which are explicitly linked to the training set As 
described in Section 3 LDA is a well defined generative model and generalizes easily to new documents Furthermore the k kV parameters in a k topic LDA model do not grow with the size of the training corpus We will see in Section 7 1 that LDA does not suffer from the same overfitting issues as pLSI 4 4 A geometric interpretation A good way of illustrating the differences between LDA and the other latent topic models is by considering the geometry of the latent space and seeing how a document is represented in that geometry under each model All four of the models described above unigram mixture of unigrams pLSI and LDA operate in the space of distributions over words Each such distribution can be viewed as a point on the V 1 simplex which we call the word simplex The unigram model finds a single point on the word simplex and posits that all words in the corpus come from the corresponding distribution The latent variable models consider k points on the word simplex and form a sub simplex based on those points 
which we call the topic simplex Note that any point on the topic simplex is also a point on the word simplex The different latent variable models use the topic simplex in different ways to generate a document 1001 B LEI N G AND J ORDAN 000000000000000000000 111111111111111111111 000000000000000000000 111111111111111111111 000000000000000000000 111111111111111111111 11 00 topic 1 000000000000000000000 111111111111111111111 00 11 000000000000000000000 111111111111111111111 00 11 000000000000000000000 111111111111111111111 000000000000000000000 111111111111111111111 000000000000000000000 111111111111111111111 000000000000000000000 111111111111111111111 000000000000000000000 111111111111111111111 000000000000000000000 111111111111111111111 topic simplex 000000000000000000000 111111111111111111111 000000000000000000000 x 111111111111111111111 000000000000000000000 111111111111111111111 000000000000000000000 111111111111111111111 000000000000000000000 111111111111111111111 000000000000000000000 
111111111111111111111 000000000000000000000 111111111111111111111 000000000000000000000 111111111111111111111 000000000000000000000 111111111111111111111 x x 000000000000000000000 111111111111111111111 x 000000000000000000000 111111111111111111111 000000000000000000000 111111111111111111111 000000000000000000000 111111111111111111111 000000000000000000000 111111111111111111111 x 000000000000000000000 111111111111111111111 word simplex x 000000000000000000000 111111111111111111111 x 000000000000000000000 111111111111111111111 x 000000000000000000000 111111111111111111111 000000000000000000000 111111111111111111111 x 000000000000000000000 111111111111111111111 x 111111111111111111111 000000000000000000000 000000000000000000000 111111111111111111111 x x 000000000000000000000 111111111111111111111 x 000000000000000000000 111111111111111111111 000000000000000000000 111111111111111111111 x 000000000000000000000 111111111111111111111 x 000000000000000000000 111111111111111111111 000000000000000000000 
111111111111111111111 x 000000000000000000000 x 111111111111111111111 x 000000000000000000000 111111111111111111111 x 000000000000000000000 111111111111111111111 x 000000000000000000000 111111111111111111111 000000000000000000000 111111111111111111111 x 000000000000000000000 111111111111111111111 000000000000000000000 111111111111111111111 x x 000000000000000000000 111111111111111111111 000000000000000000000 111111111111111111111 x x111111111111111111111 x 000000000000000000000 x 000000000000000000000 111111111111111111111 topic 2 000000000000000000000 111111111111111111111 000000000000000000000 111111111111111111111 000000000000000000000 111111111111111111111 00 11 0 1 00000000000000000000000000000 11111111111111111111111111111 00 11 0 1 00000000000000000000000000000 11111111111111111111111111111 00000000000000000000000000000 11111111111111111111111111111 00000000000000000000000000000 11111111111111111111111111111 topic 00000000000000000000000000000 3 11111111111111111111111111111 
00000000000000000000000000000 11111111111111111111111111111 00000000000000000000000000000 11111111111111111111111111111 00000000000000000000000000000 11111111111111111111111111111 00000000000000000000000000000 11111111111111111111111111111 00000000000000000000000000000 11111111111111111111111111111 00000000000000000000000000000 11111111111111111111111111111 Figure 4 The topic simplex for three topics embedded in the word simplex for three words The corners of the word simplex correspond to the three distributions where each word respectively has probability one The three points of the topic simplex correspond to three different distributions over words The mixture of unigrams places each document at one of the corners of the topic simplex The pLSI model induces an empirical distribution on the topic simplex denoted by x LDA places a smooth distribution on the topic simplex denoted by the contour lines 5 Inference and Parameter Estimation We have described the motivation behind LDA and illustrated its 
conceptual advantages over other latent topic models In this section we turn our attention to procedures for inference and parameter estimation under LDA 1002 L ATENT D IRICHLET A LLOCATION z w N M z N M Figure 5 Left Graphical model representation of LDA Right Graphical model representation of the variational distribution used to approximate the posterior in LDA 5 1 Inference The key inferential problem that we need to solve in order to use LDA is that of computing the posterior distribution of the hidden variables given a document p z w p z w p w Unfortunately this distribution is intractable to compute in general Indeed to normalize the distribution we marginalize over the hidden variables and write Eq 3 in terms of the model parameters p w i i i i i 1 i k i 1 n 1 i 1 j 1 i i j w N k V j n d a function which is intractable due to the coupling between and in the summation over latent topics Dickey 1983 Dickey shows that this function is an expectation under a particular extension to the Dirichlet 
distribution which can be represented with special hypergeometric functions It has been used in a Bayesian context for censored discrete data to represent the posterior on which in that setting is a random parameter Dickey et al 1987 Although the posterior distribution is intractable for exact inference a wide variety of approximate inference algorithms can be considered for LDA including Laplace approximation variational approximation and Markov chain Monte Carlo Jordan 1999 In this section we describe a simple convexity based variational algorithm for inference in LDA and discuss some of the alternatives in Section 8 5 2 Variational inference The basic idea of convexity based variational inference is to make use of Jensen s inequality to obtain an adjustable lower bound on the log likelihood Jordan et al 1999 Essentially one considers a family of lower bounds indexed by a set of variational parameters The variational parameters are chosen by an optimization procedure that attempts to find the tightest 
possible lower bound A simple way to obtain a tractable family of lower bounds is to consider simple modifications of the original graphical model in which some of the edges and nodes are removed Consider in particular the LDA model shown in Figure 5 left The problematic coupling between and 1003 B LEI N G AND J ORDAN arises due to the edges between z and w By dropping these edges and the w nodes and endowing the resulting simplified graphical model with free variational parameters we obtain a family of distributions on the latent variables This family is characterized by the following variational distribution q z q q zn n n 1 N 4 where the Dirichlet parameter and the multinomial parameters 1 N are the free variational parameters Having specified a simplified family of probability distributions the next step is to set up an optimization problem that determines the values of the variational parameters and As we show in Appendix A the desideratum of finding a tight lower bound on the log likelihood translates 
directly into the following optimization problem arg min D q z p z w 5 Thus the optimizing values of the variational parameters are found by minimizing the KullbackLeibler KL divergence between the variational distribution and the true posterior p z w This minimization can be achieved via an iterative fixed point method In particular we show in Appendix A 3 that by computing the derivatives of the KL divergence and setting them equal to zero we obtain the following pair of update equations ni iwn exp Eq log i i i N n 1 ni 6 7 As we show in Appendix A 1 the expectation in the multinomial update can be computed as follows Eq log i i k j 1 j 8 where is the first derivative of the log function which is computable via Taylor approximations Abramowitz and Stegun 1970 Eqs 6 and 7 have an appealing intuitive interpretation The Dirichlet update is a posterior Dirichlet given expected observations taken under the variational distribution E zn n The multinomial update is akin to using Bayes theorem p zn wn p wn zn p zn 
where p zn is approximated by the exponential of the expected value of its logarithm under the variational distribution It is important to note that the variational distribution is actually a conditional distribution varying as a function of w This occurs because the optimization problem in Eq 5 is conducted for fixed w and thus yields optimizing parameters that are a function of w We can write the resulting variational distribution as q z w w where we have made the dependence on w explicit Thus the variational distribution can be viewed as an approximation to the posterior distribution p z w In the language of text the optimizing parameters w w are document specific In particular we view the Dirichlet parameters w as providing a representation of a document in the topic simplex 1004 L ATENT D IRICHLET A LLOCATION 1 2 3 4 5 6 7 8 9 initialize 0 ni 1 k for all i and n initialize i i N k for all i repeat for n 1 to N for i 1 to k 1 iwn exp ti tni normalize tn 1 to sum to 1 t 1 t 1 N n 1 n until convergence 
Figure 6 A variational inference algorithm for LDA We summarize the variational inference procedure in Figure 6 with appropriate starting points for and n From the pseudocode it is clear that each iteration of variational inference for LDA requires O N 1 k operations Empirically we find that the number of iterations required for a single document is on the order of the number of words in the document This yields a total number of operations roughly on the order of N 2 k 5 3 Parameter estimation In this section we present an empirical Bayes method for parameter estimation in the LDA model see Section 5 4 for a fuller Bayesian approach In particular given a corpus of documents D w1 w2 wM we wish to find parameters and that maximize the marginal log likelihood of the data d 1 log p wd M As we have described above the quantity p w cannot be computed tractably However variational inference provides us with a tractable lower bound on the log likelihood a bound which we can maximize with respect to and We can thus 
find approximate empirical Bayes estimates for the LDA model via an alternating variational EM procedure that maximizes a lower bound with respect to the variational parameters and and then for fixed values of the variational parameters maximizes the lower bound with respect to the model parameters and We provide a detailed derivation of the variational EM algorithm for LDA in Appendix A 4 The derivation yields the following iterative algorithm 1 E step For each document find the optimizing values of the variational parameters d d d D This is done as described in the previous section 2 M step Maximize the resulting lower bound on the log likelihood with respect to the model parameters and This corresponds to finding maximum likelihood estimates with expected sufficient statistics for each document under the approximate posterior which is computed in the E step 1005 B LEI N G AND J ORDAN k z w N M Figure 7 Graphical model representation of the smoothed LDA model These two steps are repeated until the lower 
bound on the log likelihood converges In Appendix A 4 we show that the M step update for the conditional multinomial parameter can be written out analytically i j j dni wdn M Nd 9 d 1 n 1 We further show that the M step update for Dirichlet parameter can be implemented using an efficient Newton Raphson method in which the Hessian is inverted in linear time 5 4 Smoothing The large vocabulary size that is characteristic of many document corpora creates serious problems of sparsity A new document is very likely to contain words that did not appear in any of the documents in a training corpus Maximum likelihood estimates of the multinomial parameters assign zero probability to such words and thus zero probability to new documents The standard approach to coping with this problem is to smooth the multinomial parameters assigning positive probability to all vocabulary items whether or not they are observed in the training set Jelinek 1997 Laplace smoothing is commonly used this essentially yields the mean of the 
posterior distribution under a uniform Dirichlet prior on the multinomial parameters Unfortunately in the mixture model setting simple Laplace smoothing is no longer justified as a maximum a posteriori method although it is often implemented in practice cf Nigam et al 1999 In fact by placing a Dirichlet prior on the multinomial parameter we obtain an intractable posterior in the mixture model setting for much the same reason that one obtains an intractable posterior in the basic LDA model Our proposed solution to this problem is to simply apply variational inference methods to the extended model that includes Dirichlet smoothing on the multinomial parameter In the LDA setting we obtain the extended graphical model shown in Figure 7 We treat as a 2 An exchangeable Dirichlet is simply a Dirichlet distribution with a single scalar parameter The density is the same as a Dirichlet Eq 1 where i for each component 1006 L ATENT D IRICHLET A LLOCATION conditioned on the data Thus we move beyond the empirical Bayes 
procedure of Section 5 3 and consider a fuller Bayesian approach to LDA We consider a variational approach to Bayesian inference that places a separable distribution on the random variables and z Attias 2000 q 1 k z1 M 1 M Dir i i qd d zd d d i 1 d 1 k M where qd z is the variational distribution defined for LDA in Eq 4 As is easily verified the resulting variational inference procedure again yields Eqs 6 and 7 as the update equations for the variational parameters and respectively as well as an additional update for the new variational parameter i j M Nd j dni wdn d 1 n 1 Iterating these equations to convergence yields an approximate posterior distribution on and z We are now left with the hyperparameter on the exchangeable Dirichlet as well as the hyperparameter from before Our approach to setting these hyperparameters is again approximate empirical Bayes we use variational EM to find maximum likelihood estimates of these parameters based on the marginal likelihood These procedures are described in 
Appendix A 4 6 Example In this section we provide an illustrative example of the use of an LDA model on real data Our data are 16 000 documents from a subset of the TREC AP corpus Harman 1992 After removing a standard list of stop words we used the EM algorithm described in Section 5 3 to find the Dirichlet and conditional multinomial parameters for a 100 topic LDA model The top words from some of the resulting multinomial distributions p w z are illustrated in Figure 8 top As we have hoped these distributions seem to capture some of the underlying topics in the corpus and we have named them according to these topics As we emphasized in Section 4 one of the advantages of LDA over related latent variable models is that it provides well defined inference procedures for previously unseen documents Indeed we can illustrate how LDA works by performing inference on a held out document and examining the resulting variational posterior parameters Figure 8 bottom is a document from the TREC AP corpus which was not 
used for parameter estimation Using the algorithm in Section 5 1 we computed the variational posterior Dirichlet parameters for the article and variational posterior multinomial parameters n for each word in the article Recall that the ith posterior Dirichlet parameter i is approximately the ith prior Dirichlet parameter i plus the expected number of words which were generated by the ith topic see Eq 7 Therefore the prior Dirichlet parameters subtracted from the posterior Dirichlet parameters indicate the expected number of words which were allocated to each topic for a particular document For the example article in Figure 8 bottom most of the i are close to i Four topics however are significantly larger by this we mean i i 1 Looking at the corresponding distributions over words identifies the topics which mixed to form this document Figure 8 top 1007 B LEI N G AND J ORDAN Further insight comes from examining the n parameters These distributions approximate p zn w and tend to peak towards one of the k 
possible topic values In the article text in Figure 8 the words are color coded according to these values i e the ith color is used if qn zi n 1 0 9 With this illustration one can identify how the different topics mixed in the document text While demonstrating the power of LDA the posterior analysis also highlights some of its limitations In particular the bag of words assumption allows words that should be generated by the same topic e g William Randolph Hearst Foundation to be allocated to several different topics Overcoming this limitation would require some form of extension of the basic LDA model in particular we might relax the bag of words assumption by assuming partial exchangeability or Markovianity of word sequences 7 Applications and Empirical Results In this section we discuss our empirical evaluation of LDA in several problem domains document modeling document classification and collaborative filtering In all of the mixture models the expected complete log likelihood of the data has local maxima 
at the points where all or some of the mixture components are equal to each other To avoid these local maxima it is important to initialize the EM algorithm appropriately In our experiments we initialize EM by seeding each conditional multinomial distribution with five documents reducing their effective total length to two words and smoothing across the whole vocabulary This is essentially an approximation to the scheme described in Heckerman and Meila 2001 7 1 Document modeling We trained a number of latent variable models including LDA on two text corpora to compare the generalization performance of these models The documents in the corpora are treated as unlabeled thus our goal is density estimation we wish to achieve high likelihood on a held out test set In particular we computed the perplexity of a held out test set to evaluate the models The perplexity used by convention in language modeling is monotonically decreasing in the likelihood of the test data and is algebraicly equivalent to the inverse of 
the geometric mean per word likelihood A lower perplexity score indicates better generalization performance 3 More formally for a test set of M documents the perplexity is perplexity Dtest exp M d 1 log p wd M d 1 Nd In our experiments we used a corpus of scientific abstracts from the C Elegans community Avery 2002 containing 5 225 abstracts with 28 414 unique terms and a subset of the TREC AP corpus containing 16 333 newswire articles with 23 075 unique terms In both cases we held out 10 of the data for test purposes and trained the models on the remaining 90 In preprocessing the data 3 Note that we simply use perplexity as a figure of merit for comparing models The models that we compare are all unigram bag of words models which as we have discussed in the Introduction are of interest in the information retrieval context We are not attempting to do language modeling in this paper an enterprise that would require us to examine trigram or other higher order models We note in passing however that extensions 
of LDA could be considered that involve Dirichlet multinomial over trigrams instead of unigrams We leave the exploration of such extensions to language modeling to future work 1008 L ATENT D IRICHLET A LLOCATION The William Randolph Hearst Foundation will give 1 25 million to Lincoln Center Metropolitan Opera Co New York Philharmonic and Juilliard School Our board felt that we had a real opportunity to make a mark on the future of the performing arts with these grants an act every bit as important as our traditional areas of support in health medical research education and the social services Hearst Foundation President Randolph A Hearst said Monday in announcing the grants Lincoln Center s share will be 200 000 for its new building which will house young artists and provide new public facilities The Metropolitan Opera Co and New York Philharmonic will receive 400 000 each The Juilliard School where music and the performing arts are taught will get 250 000 The Hearst Foundation a leading supporter of the 
Lincoln Center Consolidated Corporate Fund will make its usual annual 100 000 donation too Figure 8 An example article from the AP corpus Each color codes a different factor from which the word is putatively generated 1009 B LEI N G AND J ORDAN 3400 3200 3000 2800 Smoothed Unigram Smoothed Mixt Unigrams LDA Fold in pLSI Perplexity 2600 2400 2200 2000 1800 1600 1400 0 10 20 30 40 50 60 70 80 90 100 Number of Topics 7000 6500 6000 5500 5000 4500 4000 3500 3000 2500 0 Smoothed Unigram Smoothed Mixt Unigrams LDA Fold in pLSI Perplexity 20 40 60 80 100 120 140 160 180 200 Number of Topics Figure 9 Perplexity results on the nematode Top and AP Bottom corpora for LDA the unigram model mixture of unigrams and pLSI 1010 L ATENT D IRICHLET A LLOCATION Num topics k 2 5 10 20 50 100 200 Perplexity Mult Mixt 22 266 2 Perplexity pLSI 7 052 17 588 63 800 2 Table 1 Overfitting in the mixture of unigrams and pLSI models for the AP corpus Similar behavior is observed in the nematode corpus not reported we removed a standard 
list of 50 stop words from each corpus From the AP data we further removed words that occurred only once We compared LDA with the unigram mixture of unigrams and pLSI models described in Section 4 We trained all the hidden variable models using EM with exactly the same stopping criteria that the average change in expected log likelihood is less than 0 001 Both the pLSI model and the mixture of unigrams suffer from serious overfitting issues though for different reasons This phenomenon is illustrated in Table 1 In the mixture of unigrams model overfitting is a result of peaked posteriors in the training set a phenomenon familiar in the supervised setting where this model is known as the naive Bayes model Rennie 2001 This leads to a nearly deterministic clustering of the training documents in the E step which is used to determine the word probabilities in each mixture component in the M step A previously unseen document may best fit one of the resulting mixture components but will probably contain at least one 
word which did not occur in the training documents that were assigned to that component Such words will have a very small probability which causes the perplexity of the new document to explode As k increases the documents of the training corpus are partitioned into finer collections and thus induce more words with small probabilities In the mixture of unigrams we can alleviate overfitting through the variational Bayesian smoothing scheme presented in Section 5 4 This ensures that all words will have some probability under every mixture component In the pLSI case the hard clustering problem is alleviated by the fact that each document is allowed to exhibit a different proportion of topics However pLSI only refers to the training documents and a different overfitting problem arises that is due to the dimensionality of the p z d parameter One reasonable approach to assigning probability to a previously unseen document is by marginalizing over d p w p wn z p z d p d d n 1 z N Essentially we are integrating over 
the empirical distribution on the topic simplex see Figure 4 This method of inference though theoretically sound causes the model to overfit The documentspecific topic distribution has some components which are close to zero for those topics that do not appear in the document Thus certain words will have very small probability in the estimates of 1011 B LEI N G AND J ORDAN each mixture component When determining the probability of a new document through marginalization only those training documents which exhibit a similar proportion of topics will contribute to the likelihood For a given training document s topic proportions any word which has small probability in all the constituent topics will cause the perplexity to explode As k gets larger the chance that a training document will exhibit topics that cover all the words in the new document decreases and thus the perplexity grows Note that pLSI does not overfit as quickly with respect to k as the mixture of unigrams This overfitting problem essentially 
stems from the restriction that each future document exhibit the same topic proportions as were seen in one or more of the training documents Given this constraint we are not free to choose the most likely proportions of topics for the new document An alternative approach is the folding in heuristic suggested by Hofmann 1999 where one ignores the p z d parameters and refits p z dnew Note that this gives the pLSI model an unfair advantage by allowing it to refit k 1 parameters to the test data LDA suffers from neither of these problems As in pLSI each document can exhibit a different proportion of underlying topics However LDA can easily assign probability to a new document no heuristics are needed for a new document to be endowed with a different set of topic proportions than were associated with documents in the training corpus Figure 9 presents the perplexity for each model on both corpora for different values of k The pLSI model and mixture of unigrams are suitably corrected for overfitting The latent 
variable models perform better than the simple unigram model LDA consistently performs better than the other models 7 2 Document classification In the text classification problem we wish to classify a document into two or more mutually exclusive classes As in any classification problem we may wish to consider generative approaches or discriminative approaches In particular by using one LDA module for each class we obtain a generative model for classification It is also of interest to use LDA in the discriminative framework and this is our focus in this section A challenging aspect of the document classification problem is the choice of features Treating individual words as features yields a rich but very large feature set Joachims 1999 One way to reduce this feature set is to use an LDA model for dimensionality reduction In particular LDA reduces any document to a fixed set of real valued features the posterior Dirichlet parameters w associated with the document It is of interest to see how much 
discriminatory information we lose in reducing the document description to these parameters We conducted two binary classification experiments using the Reuters 21578 dataset The dataset contains 8000 documents and 15 818 words In these experiments we estimated the parameters of an LDA model on all the documents without reference to their true class label We then trained a support vector machine SVM on the low dimensional representations provided by LDA and compared this SVM to an SVM trained on all the word features Using the SVMLight software package Joachims 1999 we compared an SVM trained on all the word features with those trained on features induced by a 50 topic LDA model Note that we reduce the feature space by 99 6 percent in this case 1012 L ATENT D IRICHLET A LLOCATION 95 98 97 Accuracy 90 Accuracy 96 95 LDA Features Word Features 94 LDA Features Word Features 85 0 0 05 0 1 0 15 0 2 Proportion of data used for training 0 25 93 0 0 05 0 1 0 15 0 2 Proportion of data used for training 0 25 a b 
Figure 10 Classification results on two binary classification problems from the Reuters 21578 dataset for different proportions of training data Graph a is EARN vs NOT EARN Graph b is GRAIN vs NOT GRAIN 600 550 Predictive Perplexity 500 450 400 350 300 250 200 0 10 LDA Fold in pLSI Smoothed Mixt Unigrams 20 30 Number of Topics 40 50 Figure 11 Results for collaborative filtering on the EachMovie data Figure 10 shows our results We see that there is little reduction in classification performance in using the LDA based features indeed in almost all cases the performance is improved with the LDA features Although these results need further substantiation they suggest that the topic based representation provided by LDA may be useful as a fast filtering algorithm for feature selection in text classification 1013 B LEI N G AND J ORDAN 7 3 Collaborative filtering Our final experiment uses the EachMovie collaborative filtering data In this data set a collection of users indicates their preferred movie choices A user 
and the movies chosen are analogous to a document and the words in the document respectively The collaborative filtering task is as follows We train a model on a fully observed set of users Then for each unobserved user we are shown all but one of the movies preferred by that user and are asked to predict what the held out movie is The different algorithms are evaluated according to the likelihood they assign to the held out movie More precisely define the predictive perplexity on M test users to be predictive perplexity Dtest exp M d 1 log p wd Nd wd 1 Nd 1 M We restricted the EachMovie dataset to users that positively rated at least 100 movies a positive rating is at least four out of five stars We divided this set of users into 3300 training users and 390 testing users Under the mixture of unigrams model the probability of a movie given a set of observed movies is obtained from the posterior distribution over topics p w wobs p w z p z wobs z In the pLSI model the probability of a held out movie is given 
by the same equation except that p z wobs is computed by folding in the previously seen movies Finally in the LDA model the probability of a held out movie is given by integrating over the posterior Dirichlet p w wobs p w z p z p wobs d z where p wobs is given by the variational inference method described in Section 5 2 Note that this quantity is efficient to compute We can interchange the sum and integral sign and compute a linear combination of k Dirichlet expectations With a vocabulary of 1600 movies we find the predictive perplexities illustrated in Figure 11 Again the mixture of unigrams model and pLSI are corrected for overfitting but the best predictive perplexities are obtained by the LDA model 8 Discussion We have described latent Dirichlet allocation a flexible generative probabilistic model for collections of discrete data LDA is based on a simple exchangeability assumption for the words and topics in a document it is therefore realized by a straightforward application of de Finetti s 
representation theorem We can view LDA as a dimensionality reduction technique in the spirit of LSI but with proper underlying generative probabilistic semantics that make sense for the type of data that it models Exact inference is intractable for LDA but any of a large suite of approximate inference algorithms can be used for inference and parameter estimation within the LDA framework We have presented a simple convexity based variational approach for inference showing that it yields a fast 1014 L ATENT D IRICHLET A LLOCATION algorithm resulting in reasonable comparative performance in terms of test set likelihood Other approaches that might be considered include Laplace approximation higher order variational techniques and Monte Carlo methods In particular Leisink and Kappen 2002 have presented a general methodology for converting low order variational lower bounds into higher order variational bounds It is also possible to achieve higher accuracy by dispensing with the requirement of maintaining a bound 
and indeed Minka and Lafferty 2002 have shown that improved inferential accuracy can be obtained for the LDA model via a higher order variational technique known as expectation propagation Finally Griffiths and Steyvers 2002 have presented a Markov chain Monte Carlo algorithm for LDA LDA is a simple model and although we view it as a competitor to methods such as LSI and pLSI in the setting of dimensionality reduction for document collections and other discrete corpora it is also intended to be illustrative of the way in which probabilistic models can be scaled up to provide useful inferential machinery in domains involving multiple levels of structure Indeed the principal advantages of generative models such as LDA include their modularity and their extensibility As a probabilistic module LDA can be readily embedded in a more complex model a property that is not possessed by LSI In recent work we have used pairs of LDA modules to model relationships between images and their corresponding descriptive 
captions Blei and Jordan 2002 Moreover there are numerous possible extensions of LDA For example LDA is readily extended to continuous data or other non multinomial data As is the case for other mixture models including finite mixture models and hidden Markov models the emission probability p wn zn contributes only a likelihood value to the inference procedures for LDA and other likelihoods are readily substituted in its place In particular it is straightforward to develop a continuous variant of LDA in which Gaussian observables are used in place of multinomials Another simple extension of LDA comes from allowing mixtures of Dirichlet distributions in the place of the single Dirichlet of LDA This allows a richer structure in the latent topic space and in particular allows a form of document clustering that is different from the clustering that is achieved via shared topics Finally a variety of extensions of LDA can be considered in which the distributions on the topic variables are elaborated For example we 
could arrange the topics in a time series essentially relaxing the full exchangeability assumption to one of partial exchangeability We could also consider partially exchangeable models in which we condition on exogenous variables thus for example the topic distribution could be conditioned on features such as paragraph or sentence providing a more powerful text model that makes use of information obtained from a parser Acknowledgements This work was supported by the National Science Foundation NSF grant IIS 9988642 and the Multidisciplinary Research Program of the Department of Defense MURI N00014 00 1 0637 Andrew Y Ng and David M Blei were additionally supported by fellowships from the Microsoft Corporation References M Abramowitz and I Stegun editors Handbook of Mathematical Functions Dover New York 1970 1015 B LEI N G AND J ORDAN R Baeza Yates and B Ribeiro Neto Modern Information Retrieval ACM Press New York 1999 D Blei and M Jordan Modeling annotated data Technical Report UCB CSD 02 1202 U C Berkeley 
Computer Science Division 2002 B de Finetti Theory of probability Vol 1 2 John Wiley Sons Ltd Chichester 1990 Reprint of the 1975 translation S Deerwester S Dumais T Landauer G Furnas and R Harshman Indexing by latent semantic analysis Journal of the American Society of Information Science 41 6 1016 L ATENT D IRICHLET A LLOCATION M Jordan Z Ghahramani T Jaakkola and L Saul Introduction to variational methods for graphical models Machine Learning 37 Appendix A Inference and parameter estimation In this appendix we derive the variational inference procedure Eqs 6 and 7 and the parameter maximization procedure for the conditional multinomial Eq 9 and for the Dirichlet We begin by deriving a useful property of the Dirichlet distribution 1017 B LEI N G AND J ORDAN A 1 Computing E log i The need to compute the expected value of the log of a single probability component under the Dirichlet arises repeatedly in deriving the inference and parameter estimation procedures for LDA This value can be easily computed from 
the natural parameterization of the exponential family representation of the Dirichlet distribution Recall that a distribution is in the exponential family if it can be written in the form p x h x exp T T x A where is the natural parameter T x is the sufficient statistic and A is the log of the normalization factor We can write the Dirichlet in this form by exponentiating the log of Eq 1 p exp k k k i 1 i 1 log i log i 1 i i 1 log i From this form we immediately see that the natural parameter of the Dirichlet is i i 1 and the sufficient statistic is T i log i Furthermore using the general fact that the derivative of the log normalization factor with respect to the natural parameter is equal to the expectation of the sufficient statistic we obtain E log i i k j 1 j where is the digamma function the first derivative of the log Gamma function A 2 Newton Raphson methods for a Hessian with special structure In this section we describe a linear algorithm for the usually cubic Newton Raphson optimization method 
This method is used for maximum likelihood estimation of the Dirichlet distribution Ronning 1989 Minka 2000 The Newton Raphson optimization technique finds a stationary point of a function by iterating new old H old 1 g old where H and g are the Hessian matrix and gradient respectively at the point In general this algorithm scales as O N 3 due to the matrix inversion If the Hessian matrix is of the form H diag h 1z1T 10 where diag h is defined to be a diagonal matrix with the elements of the vector h along the diagonal then we can apply the matrix inversion lemma and obtain H 1 diag h 1 diag h 1 11T diag h 1 1 z 1 k j 1 h j gi c hi Multiplying by the gradient we obtain the ith component H 1 g i 1018 L ATENT D IRICHLET A LLOCATION where c 1 z 1 k j 1 h j k j 1 g j h j Observe that this expression depends only on the 2k values hi and gi and thus yields a NewtonRaphson algorithm that has linear time complexity A 3 Variational inference In this section we derive the variational inference algorithm described in 
Section 5 1 Recall that this involves using the following variational distribution q z q q zn n n 1 N 11 as a surrogate for the posterior distribution p z w where the variational parameters and are set via an optimization procedure that we now describe Following Jordan et al 1999 we begin by bounding the log likelihood of a document using Jensen s inequality Omitting the parameters and for simplicity we have log p w log log p z w d z z z p z w q z d q z z q z log p z w d q z log q z d 12 Eq log p z w Eq log q z Thus we see that Jensen s inequality provides us with a lower bound on the log likelihood for an arbitrary variational distribution q z It can be easily verified that the difference between the left hand side and the right hand side of the Eq 12 is the KL divergence between the variational posterior probability and the true posterior probability That is letting L denote the right hand side of Eq 12 where we have restored the dependence on the variational parameters and in our notation we have log p w 
L D q z p z w 13 This shows that maximizing the lower bound L with respect to and is equivalent to minimizing the KL divergence between the variational posterior probability and the true posterior probability the optimization problem presented earlier in Eq 5 We now expand the lower bound by using the factorizations of p and q L Eq log p Eq log p z Eq log p w z Eq log q Eq log q z 1019 14 B LEI N G AND J ORDAN Finally we expand Eq 14 in terms of the model parameters and the variational parameters Each of the five lines below expands one of the five terms in the bound L log kj 1 j log i i 1 i kj 1 j i 1 i 1 k k N n 1 i 1 N k V ni k i k j 1 j 15 k n 1 i 1 j 1 ni wnj log i j k i 1 i 1 k log k j 1 j log i i 1 i j 1 j N n 1 i 1 ni log ni k where we have made use of Eq 8 In the following two sections we show how to maximize this lower bound with respect to the variational parameters and A 3 1 VARIATIONAL MULTINOMIAL We first maximize Eq 15 with respect to ni the probability that the nth word is generated by latent 
topic i Observe that this is a constrained maximization since k i 1 ni 1 We form the Lagrangian by isolating the terms which contain ni and adding the appropriate i Lagrange multipliers Let iv be p wv n 1 z 1 for the appropriate v Recall that each wn is a vector of size V with exactly one component equal to one we can select the unique v such that wv n 1 L ni ni i kj 1 j ni log iv ni log ni n kj 1 ni 1 where we have dropped the arguments of L for simplicity and where the subscript ni denotes that we have retained only those terms in L that are a function of ni Taking derivatives with respect to ni we obtain L i k j 1 j log iv log ni 1 ni Setting this derivative to zero yields the maximizing value of the variational parameter ni cf Eq 6 ni iv exp i k j 1 j 1020 16 L ATENT D IRICHLET A LLOCATION A 3 2 VARIATIONAL D IRICHLET Next we maximize Eq 15 with respect to i the ith component of the posterior Dirichlet parameter The terms containing i are L i 1 i kj 1 j ni i kj 1 j i 1 n 1 k log k j 1 j log i i 1 i j 1 j 
i 1 k k N This simplifies to L i kj 1 j i 1 k k i N n 1 ni i log j 1 j log i We take the derivative with respect to i L k i i N n 1 ni i j 1 j i Setting this equation to zero yields a maximum at i i N n 1 ni 17 j 1 k j N n 1 n j j Since Eq 17 depends on the variational multinomial full variational inference requires alternating between Eqs 16 and 17 until the bound converges A 4 Parameter estimation In this final section we consider the problem of obtaining empirical Bayes estimates of the model parameters and We solve this problem by using the variational lower bound as a surrogate for the intractable marginal log likelihood with the variational parameters and fixed to the values found by variational inference We then obtain approximate empirical Bayes estimates by maximizing this lower bound with respect to the model parameters We have thus far considered the log likelihood for a single document Given our assumption of exchangeability for the documents the overall log likelihood of a corpus D w1 w2 wM is 
the sum of the log likelihoods for individual documents moreover the overall variational lower bound is the sum of the individual variational bounds In the remainder of this section we abuse notation by using L for the total variational bound indexing the document specific terms in the individual bounds by d and summing over all the documents Recall from Section 5 3 that our overall approach to finding empirical Bayes estimates is based on a variational EM procedure In the variational E step discussed in Appendix A 3 we maximize the bound L with respect to the variational parameters and In the M step which we describe in this section we maximize the bound with respect to the model parameters and The overall procedure can thus be viewed as coordinate ascent in L 1021 B LEI N G AND J ORDAN A 4 1 C ONDITIONAL MULTINOMIALS To maximize with respect to we isolate terms and add Lagrange multipliers L d 1 n 1 i 1 j 1 M Nd dni wdn log i j i V j 1 i j 1 j i 1 k V k We take the derivative with respect to i j set it to 
zero and find i j A 4 2 D IRICHLET The terms which contain are j dni wdn M Nd d 1 n 1 L d 1 M k log k j 1 j log i i 1 di j 1 d j i 1 i 1 k k Taking the derivative with respect to i gives M L M k i di kj 1 d j j 1 j i d 1 This derivative depends on j where j i and we therefore must use an iterative method to find the maximal In particular the Hessian is in the form found in Eq 10 L i j M i k j 1 j i j and thus we can invoke the linear time Newton Raphson algorithm described in Appendix A 2 Finally note that we can use the same algorithm to find an empirical Bayes point estimate of the scalar parameter for the exchangeable Dirichlet in the smoothed LDA model in Section 5 4 1022 
38	a	MITSUBISHI ELECTRIC RESEARCH LABORATORIES http www merl com Design of the CMU Sphinx 4 Decoder Lamere P Kwok P Walker W Gouva E Singh R Raj B Wolf P TR2003 110 August 2003 Abstract The decoder of the sphinx 4 speech recognition system incorporates several new design strategies which have not been used earlier in conventional decoders of HMM based large vocabulary speech recognition systems Some new design aspects include graph construction for multilevel parallel decoding with independent simultaneous feature streams without the use of compound HMMs the incorporation of a generalized search algorithm that subsumes Viterbi and full forward decoding as special cases design of generalized language HMM graphs from grammars and language models of multiple standard formats that toggles trivially from flat search structure to tree search structure etc This paper describes some salient design aspects of the Sphinx 4 decoder and includes preliminary performance measures relating to speed and accuracy Eurospeech 2003 
This work may not be copied or reproduced in whole or in part for any commercial purpose Permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following a notice that such copying is by permission of Mitsubishi Electric Research Laboratories Inc an acknowledgment of the authors and individual contributions to the work and all applicable portions of the copyright notice Copying reproduction or republishing for any other purpose shall require a license with payment of fee to Mitsubishi Electric Research Laboratories Inc All rights reserved Copyright c Mitsubishi Electric Research Laboratories Inc 2003 201 Broadway Cambridge Massachusetts 02139 MERLCoverPageSide2 Publication History 1 First printing TR 2003 110 August 2003 DESIGN OF THE CMU SPHINX 4 DECODER Paul Lamere1 Philip Kwok1 William Walker1 Evandro 1 Sun Microsystems 2Carnegie Mellon University 3Mitsubishi Electric Research Labs 
USA ABSTRACT The decoder of the sphinx 4 speech recognition system incorporates several new design strategies which have not been used earlier in conventional decoders of HMM based large vocabulary speech recognition systems Some new design aspects include graph construction for multilevel parallel decoding with independent simultaneous feature streams without the use of compound HMMs the incorporation of a generalized search algorithm that subsumes Viterbi and full forward decoding as special cases design of generalized language HMM graphs from grammars and language models of multiple standard formats that toggles trivially from flat search structure to tree search structure etc This paper describes some salient design aspects of the Sphinx 4 decoder and includes preliminary performance measures relating to speed and accuracy module and the generalized Bushderby 2 classification algorithm used in it Section 5 describes the design of the frontend and acoustic scorer Section 6 presents some performance 
measures and lists the future remaining work on the decoder 2 OVERALL ARCHITECTURE The Sphinx 4 architecture has been designed with a high degree of modularity Figure 1 shows the overall architecture of the system Even within each module shown in Figure 1 the code is extremely modular with easily replaceable functions Speech Application Input control Search control Search results Frontend Decoder Knowledge base 1 INTRODUCTION The Sphinx 4 speech recognition system is a state of art HMM based speech recognition system being developed on open source cmusphinx sourceforge net in the JavaTM programming language It is the latest addition to Carnegie Mellon University s repository of Sphinx speech recognition systems The Sphinx 4 decoder has been designed jointly by researchers from CMU SUN Microsystems and Mitsubishi Electric Research Laboratories Over the last few years the demands placed on conventional recognition systems have increased significantly Several things are now additionally desired of a system such 
as the ability to perform multistream decoding in a theoretically correct manner with as much user control on the level of combination as possible that of at least some degree of basic easy control over the system s performance in the presence of varied and unexpected environmental noise levels and types portability across a growing number of computational platforms conformance to widely varying resource requirements easy restructuring of the architecture for distributed processing etc The importance of a good and flexible user interfacing is also clear as a myriad of devices attempt to use speech recognition for various purposes The design of the Sphinx 4 is driven by almost all of these current day considerations resulting in a system that is highly modular portable and easily extensible while at the same time incorporating several desirable features by extending conventional design strategies or inventing and incorporating new ones Its design is quite a bit more utilitarian and futuristic than most 
existing HMM based systems This paper describes selected set of important design innovations in the Sphinx 4 decoder The sections are arranged as follows section 2 describes the overall architecture of the decoder and some software aspects Section 3 describes the design of the graph construction module and design of the language HMM graph for parallel decoding Section 4 describes the design of the search Endpointer Search Dictionary Feature computation State probability computation Statistical parameters Graph construction Language model Structural information Acoustic model Figure 1 Architecture of the Sphinx 4 system The main blocks are Frontend Decoder and Knowledge base Except for the blocks within the KB all other blocks are independently replaceable software modules written in Java Stacked blocks indicate multiple types which can be used simultaneously There are three main blocks in the design which are controllable by any external application the frontend decoder and knowledge base KB The frontend 
module takes in speech and parametrizes it Within it the endpointer module can either endpoint the speech signal or the feature vectors computed from it The decoder block performs the actual recognition It is comprised of a graph construction module which translates any type of standard language model provided to KB by the application into an internal format and together with information from the dictionary and structural information from one set or a set of parallel acoustic models constructs the language HMM The latter is then used by the search module to decide the allowed path extensions in the trellis which is searched The trellis is not explicitly constructed Rather it is an implicit entity as in conventional decoders The application can tap the information in the tokens at each node to get search results at various levels such as state phone or word level lattices and segmentations The application can also control the level at which scores from parallel feature streams are combined during search and 
how each information stream is pruned The search module requires likelihood scores for any current feature vector to generate the active list Likelihoods are computed by the state probability computation module which is the only module that has access to the feature vectors Score computation is thus an ondemand task carried out whenever the search module communicates a state identity to the scoring module for which a score for the current feature of a specific type is desired In Sphinx 4 the graph construction module is also called the linguist and the score computation module is called the acoustic scorer The system permits the use of any level of context in the definition of the basic sound units One by product of the system s modular design is that it becomes easy to implement it in hardware Programming language The system is entirely developed on the JavaTM platform which is highly portable once compiled the bytecode can be used on any system that supports the Java platform This feature permits a high 
degree of decoupling between all modules Each module can be exchanged for another without requiring any modification of the other modules The particular modules to be used can be specified at run time through a command line argument with no need to recompile the code Also the garbage collection GC feature of Java simplifies memory management greatly memory management is no longer done through explicit code When a structure is no longer needed the program simply stops referring to it The GC frees all relevant memory blocks automatically Java also provides a standard manner of writing multithreaded applications to easily take advantage of multi processor machines Also the JavadocTM tool automatically extracts information from comments in the code and creates html files that provide documentation about the software interface YES NO Y EH S a N OW b c d Figure 2 a Language graph for a simple language with a two word vocabulary b Language phonetic graph derived from a c Language phonetic acoustic graph derived 
from b d Trellis formed as the crossproduct of c and a linear graph of observation data vectors separate HMMs for the individual feature streams Time synchronization of paths is ensured at the boundaries of combined units during search The internal grammar is then converted to a language HMM by another module which is independent of the grammar construction module Note that in the architecture diagram both modules are represented by a single graph construction module In the formation of the language HMM the word level network of the internal grammar is expanded using the dictionary and the structural information from the acoustic models In this graph subword units with contexts of arbitrary length can be incorporated if provided We assume however that silence terminates any context sub word units that are separated from each other by an intermediate silence context cannot affect each other The linguist converts the word graph to a Language HMM either dynamically or statically In dynamic construction the HMMs 
for grammar nodes that can follow a current node and are not already instantiated are constructed at run time using the appropriate context dependent phonemes at the word boundaries In static construction the entire language HMM is constructed statically HMMs are constructed for all words in the vocabulary Each word HMM is composed with several word beginning context dependent phones each corresponding to possible crossword left context Similarly each word HMM also has several word ending context dependent phonemes Each word is connected to every other word In linking any two words only the appropriate contextdependent phones are linked w w feature 1 er d er d feature 2 w w feature 1 er er feature 2 d d 3 GRAPH CONSTRUCTION MODULE In an HMM based decoder search is performed through a trellis a directed acyclic graph DAG which is the cross product of a language HMM and time The language HMM is a series parallel graph in which any path from source to sink represents the HMM for a valid word sequence in the 
given language This graph has loops permitting word sequences of arbitrary lengths Language probabilities are applied at transitions between words The language HMM graph is a composition of the language structure as represented by a given language model and the topological structure of the acoustic models HMMs for the basic sound units used by the system Figure 2 shows the relation between acoustic models language HMM and the trellis which is searched and also shows intermediate level graphs which are used implicitly in the composition of the language HMM The graph construction module in Sphinx 4 also called the linguist constructs the language HMM using the output of another module which interprets the language model provided by the application as a part of the KB and converts it into a single internal grammar format This permits the external grammar to be provided by the application in any format such as statistical N gram CFG FSA FST simple word lists etc The internal grammar is a literal translation of 
the external representation For word lists all words are linked parallely to a single source and sink node and a loopback is made from the sink to the source For N gram LMs an explicit bigram structure is formed where every word is represented by a node and there are explicit links from every node to every other node The language HMM accommodates parallel feature streams as shown in Figure 3 The design does not use compound HMMs 3 as in conventional systems but maintains Figure 3 Graph construction with two parallel features In the left panel scores for words are combined at word boundaries In the right panel they are combined at phoneme boundaries There are a number of strategies that can be used in constructing the language HMM that affect the search By altering the topology of the language HMM the memory footprint perplexity speed and recognition accuracy can be affected The modularized design of Sphinx 4 allows different language HMM compilation strategies to be used without changing other aspects of the 
search The choice between static and dynamic construction of language HMMs depends mainly on the vocabulary size language model complexity and desired memory footprint of the system and can be made by the application communicates with the state probability estimation module also called the acoustic scorer to obtain acoustic scores for the current data which are only seen by the acoustic scorer This module maintains an active list list of tokens Active lists are selected from currently active nodes in the trellis through pruning Sphinx 4 performs both relative and absolute pruning and also pruning for individual features when decoding with parallel feature streams The pruning thresholds are controllable by the application Search can be performed in either depth first or breadthfirst manner Depth first search is similar to conventional stack decoding where the most promising tokens are expanded in time sequentially Thus paths from the root of the token tree to currently active tokens can be of varying lengths 
In breadth first search all active tokens are expanded synchronously making the paths from the root of the tree to the currently active tokens of equal in length The Bushderby algorithm used during search currently applies to breadth first search Sphinx 4 provides a beam pruner that constrains the scores to a configurable minimum relative to the best score while also keeping the total number of active tokens to a configurable maximum New implementations can easily be created that provide alternative methods of storing and pruning the active list The garbage collector automatically reclaims unused tokens thus simplifying the implementation of the pruner by merely allowing a token to be removed from the set of active tokens When parallel streams of features are being decoded token stacks are used at the node sin the trellis The search module combines weighted scores from the features at appropriate nodes at the specific level in the graph as specified by the application Scores are combined from tokens which 
have identical word histories word entry and exit times This is done under the assumption that parallel features derived from the speech signal must at least traverse a word in the speech signal at concurrent times For decoding with parallel feature streams the pruning strategy has to be very carefully designed as it can lead to serious problems during search A schematic representation of such a problem for the case of two feature streams is shown in Figure 4 To avoid this problem the search module switches to a different pruning scheme in the case of parallel features In this scheme pruning is done at two levels One level consists of wide beam or loose pruning which is done separately for separate features This is possible since each token in the stack at each node carries scores for individual features as well as the combined score The second level of pruning is done at the specific user defined levels at which scores combine and is done only with respect to the combined scores This pruning uses much 
narrower beams Also features are combined in a weighted manner with weights that can be application controlled either directly or through any application enforced learning algorithm The result generated by the search module is in the form of a token tree which can be queried for the best recognition hypotheses or a set of hypotheses the tree also encodes the recognition lattice at all levels senones phoneme word etc 4 SEARCH MODULE Search in Sphinx 4 can be performed using conventional Viterbi algorithm or full forward algorithm so far as the latter can indeed be approximated in the compact language HMM graphs used by conventional HMM based decoders including sphinx 4 However the unique feature about search in Sphinx 4 is that these conventional algorithms are merely special cases of a more general algorithm called Bushderby 2 which performs classification based on free energy rather than the Bayesian rule Likelihoods are used in the computation of free energy but do not constitute the main objective 
function used for classification The theoretical motivations for this algorithm are described in 2 From an engineering perspective the algorithm can be viewed as the application of a single parameter function in the trellis which operates on path scores The parameter can take meaningful values in 0 and is user controllable Mathematically the function performs an norm over the score of a set of edges incident on a node U in the trellis that is being searched for the best hypothesis as U 1 When 1 this reduces to full forward or Bayesian decoding score U U When it reduces to viterbi decoding prob 2 score U max U prob 3 For values of which are not equal to either 1 or the expression in equation 1 has no bayesian interpretation However it can be related to free energy Classification over mismatched data can directly be controlled through this Bushderby parameter and has been shown to yield significant improvements in recognition performance The search module constructs a tree of hypotheses using a tokenpassing 
algorithm which is used in many conventional decoders 1 The token tree consists of a set of tokens that contain information about the nodes traversed in the trellis and provides a complete history of all active paths in the search Each token contains the overall acoustic and language scores of the path at a given point a Language HMM reference an input feature frame identification and a reference to the previous token thus allowing backtracing The Language HMM reference allows the search manager to relate a token to its senone context dependent phonetic unit pronunciation word and grammar state The search module also 5 FRONT END AND ACOUSTIC SCORER The sequence of operations performed by the Sphinx 4 front end module in its default configuration is very similar to what other speech recognition systems do creating mel cepstra from an audio file It is however parallelizable and can currently simultaneously feature 2 This path has contributions only from one stream feature 1 This path has contributions from 
both streams compared for pruning request from the search module to score a particular state it performs the mathematical operations required for score computation It matches the acoustic model set to be used against the feature type in case of parallel decoding with parallel acoustic models There are no restrictions on the allowed topology for the HMMs used in parallel scoring The scorer retains all information pertaining to the state output densities Thus the search module need not know the scoring is done with continuous semi continuous or discrete HMMs Any heuristic algorithms incorporated into the scoring procedure for speeding it up can be performed locally within the search module time Figure 4 The pruning problem encountered by the search module in decoding parallel feature streams If pruning is based on combined scores paths with different contributions from the multiple feature streams get compared for pruning 6 EXPERIMENTAL EVALUATION The performance of Sphinx 4 is compared with Sphinx 3 on the on 
the speaker independent portion of the Resource Management database RM1 4 in Table 1 Sphinx 3 builds the languageHMM dynamically while the code for dynamic construction in Sphinx 4 is not yet fully operational In any decoder static construction of a language HMM takes far longer than dynamic construction Hence graph construction times have been factored out of the real time numbers reported in Table 1 Acoustic models were 3 state 8 Gaussians state HMMs with 1000 tied states trained with the RM training data using the training module of Sphinx 3 Test results are reported using statistical N gram language models a flat unigram a unigram and a bigram The Lams were created from the LM training data provided with the RM database Sphinx 4 is currently also capable of working from an external FST language model We do not report those results here although they are consistently better than n gram results Table 1 shows both word error rate WER and decoding speed in times real time All experiments were run on a Sun 
BladeTM 1000 workstation with dual 750 MHz compute MFCC and PLP cepstra from speech signals with easy extensibility to other feature types The module is organized as a sequence of independent replaceable communicating blocks each with an input and output that can be tapped e g a pre emphasizer a spectrum analyser Thus outputs of intermediate blocks can be easily fed into the state computation module for example Features computed using independent sources by the application such as visual features can also be directly fed into the State probability computation module either in parallel with the features computed by the frontend for parallel decoding or independently Moreover one can easily add a processor between two existing processors making it very easy to add a noise cancellation or compensation module The communication between blocks follows a pull design In this design a block requests input from an earlier block when needed as opposed to the more conventional push design where a block gives data to the 
succeeding block as soon data are available to it At a global level in a pull design more speech is captured only when recognition is requested In a push design recognition is requested after speech is captured Each block operates in response to control signals which are interpreted from the data requested from the predecessor The control signal might indicate the beginning or end of speech might indicate data dropped or some other problem If the incoming data are speech they are processed and the output is buffered waiting for the successor block to request it Handling of control signals such as start or end of speech are essential for livemode operation This design allows the system to be used in live or batchmode without modification In addition to being responsive to a continuous stream of input speech the frontend is capable of three other modes of operation a fully endpointed in which explicit begining and end points of a speech signal are sensed b click to talk in which the user indicates the 
beginning of a speech segment but the system determines when it ends c push to talk in which the user indicates both the beginning and the end of a speech segment Currently endpoint detection is performed by a simple algorithm that compares the energy level to three threshold levels Two of these are used to determine start of speech and one for end of speech Acoustic scorer This is the State output probability computation module shown in the architecture diagram of Sphinx 4 Its operation is straight forward given a set of acoustic models and a Table 1 Performance comparison between Sphinx 3 S3 and Sphinx 4 S4 The speeds do not include loading time which are vastly different for s3 dynamic language HMM construction and s4 currently static language HMM construction The Sphinx 4 performance has not been optimized on the bigram and trigram tasks at the time of this submission As such the evaluation of medium vocabulary tasks is ongoing and large vocabulary tasks will be approached shortly The optimized trigram 
tasks and the completed medium and large vocabulary evaluations will be completed by the time the paper is presented They will also be reported on SourceForge as and when they are completed ACKNOWLEDGEMENTS We thank Prof Richard Stern at CMU Robert Sproull at Sun Microsystems and Joe Marks at MERL for making this team possible We also thank Sun Microsystems and the current management for their continued support and collaborative research funds Rita Singh was sponsored by the Space and Naval Warfare Systems Center San Diego under Grant No N66001 99 1 8905 The content of this paper does not necessarily reflect the position or the policy of the US Government and no official endorsement should be inferred REFERENCES 1 S J Young N H Russel and J H S Russel Token passing A simple conceptual model for connected speech recognition systems Tech Report Cambridge Univ Engg Dept 1989 2 R Singh M Warmuth B Raj and P Lamere Classification with free energy at raised temperatures in EUROSPEECH 2003 2003 3 Compound HMM 
reference 4 P Price W M Fisher J Bernstein and D S Pallett The DARPA 1000 word resource management database for continuous speech recognition in Proc ICASSP 1988 Vol 1 p 651 654 1988 
39	a	TOPIC BASED LANGUAGE MODELS USING EM Daniel Gildea and Thomas Hofmann University of California Berkeley and International Computer Science Institute 1947 Center Street Berkeley California gildea hofmann icsi berkeley edu history h for notational convenience all parameters are summarized in a vector A graphical model representation that emphasizes the bottleneck principle of the topic variable is depicted in Figure 1 ABSTRACT In this paper we propose a novel statistical language model to capture topic related long range dependencies Topics are modeled in a latent variable framework in which we also derive an EM algorithm to perform a topic factor decomposition based on a segmented training corpus The topic model is combined with a standard language model to be used for on line word prediction Perplexity results indicate an improvement over previously proposed topic models which unfortunately has not been translated into lower word error 1 INTRODUCTION The goal of statistical language models is to assign 
probabilities to sequences of words and their most prominent application is in speech recognition where language models provide prior probabilities that help in disambiguating acoustically similar utterances By virtue of the chain rule it is sufficient to estimate the probability P wi jhi of a word wi conditioned on the history of i 1 The main challenge in language preceding words hi w1 modeling is to deal with the combinatorial growth in the number of possible histories which implies a data sparseness problem and prevents a straightforward empirical estimation of the required conditional probabilities A simple but commonly applied strategy is to make a n th order Markov approximation i 1 P wi jhi P wi jwi n 1 which yields the class of n gram language models where typically n trigrams While trigram models and variants thereof have proven hard to improve upon they are unable to take advantage of longrange dependencies in natural language Several more recent approaches attempt to overcome this limitation 
Variable order models 15 adjust the length of the utilized contexts dynamically dependent on the available training data Cache models 13 3 increase the probability for words observed in the history e g by some factor which decays exponentially with distance Trigger models 16 are more general in that they allow to incorporate arbitrary word trigger pairs which are combined in an exponential model Grammar based techniques 12 2 exploit syntactical regularities to model long range dependencies Finally in topic mixture models 11 a number of language models e g n grams are trained on documents of various topics and are then combined at runtime Our approach is closely related to the latter class of topic mixtures in that the proposed model is based on a topic decomposition 9 History 11111 00000 00000 11111 00000 11111 h 00000 11111 00000 11111 Word 00000 11111 t Topic 11111 00000 00000 11111 w 00000 11111 00000 11111 Figure 1 Graphical model representation of the topic factor model The main difference to clustering 
approaches like the one proposed in 11 is that we do not assume that each document or history belongs to exactly one topic cluster Our approach is based on the less restrictive assumption of a low dimensional approximation in terms of a linear combination of a small number of topic factors A similar approach to language modeling based on a dimension reduction technique known as Latent Semantic Analysis LSA 7 has been proposed in 1 a detailed implementation is provided in 4 Yet compared to the LSA approach that makes use of Singular Value Decomposition techniques our method has the crucial advantage of a strict probabilistic interpretation cf 9 a fact that will be further discussed in Section 4 The model we describe here does not make use of syntax and ignores the order in which words appear In fact implicit in 1 is the simplifying assumption that the influence of different topics on the statistical properties of language is limited to the level of single words unigrams 1 Local regularities can be taken into 
account at a subsequent stage where we combine the topic model with a standard n gram language model We believe the model might also be profitable combined with a more sophisticated syntactic model such as those mentioned above 2 TOPIC DECOMPOSITION BY EM The topics used by our model are not taken from a predefined hand labeled hierarchy but rather emerge in a data driven manner from the statistics of a corpus of training documents d 2 D Based on the unigram assumption the data is reduced to simple word counts n w d of how often a word w was observed in a particular document d All word counts can be summarized in the term document matrix As a training criterion we utilize the log likelihood i e the log probability of the data under the model 1 3 P wjh X P wjtP tjh t N 1 l N X X nw dlog X P wjtP tjd w d t 2 Here t is a latent class variable that is supposed to refer to different topics P wjt are topic specific word probabilities or topic factors and P tjh are mixing proportions that depend on the 1 This is 
not a principled limitation of our model yet it offers significant advantages in terms of computational complexity In the training procedure the number of topics i e the number of values the latent variable t can take is predetermined and the parameters P wjt and P tjd are fitted by the Expectation Maximization EM algorithm 8 Starting from randomly initialized values for the parameters this involves the standard procedure of alternating two computational steps the E step to calculate the posterior probability of the latent variables for given parameters and the M step in which the parameters are re estimated The E step amounts to calculating the probability that a particular word w in a document d was generated by the topic factor t For the rth iteration Bayes rule yields history hi and the n gram context are independent conditioned on wi the following approximation formula cen be derived i 1 P wi jhi wi n 1 P r tjw d jtP r tjd PtPPr r ww jt0 P r t0 jd 1 1 0 1 1 3 Of course this assumption is not valid in 
general as the n word context of the n gram model is part of the history which implies that they are not even marginally independent In our experiments we have also evaluated two alternative interpolation methods of combining the n gram and the topic based model by averaging the respective probabilities i on the linear scale and ii on the log scale Both averaging schemes require an additional interpolation weight Notice that the approximation by 8 as well as log averaging require a re normalization step 4 EXPERIMENTS 4 1 Experimental Results on TDT 1 i 1 P wi jwi n 1 P wi jhi P wi 8 1 The M step adjusts the model parameters given the values for the latent variables calculated in the previous E step P r wjt Pd nw dP r tjw d Pw P nw0 dP r tjw0 d d Pw nw dP r tjw d P r tjd P P r 0 t w nw dP t jw d 0 4 5 0 In our experiments we used a modified annealed E step cf 9 to prevent overfitting This amounts to introducing an exponent to discount the likelihood contribution in 3 0 1 3 USING THE MODEL FOR TESTING During 
testing the P tjd distributions computed for the training documents can be discarded as they will not apply to new documents used for testing Rather we examine all the words seen so far in the document and calculate an estimate of P tjh for the current history using only the topic factors P wjt The mixing proportions P tjh can be determined during testing by holding the probabilities P wjt constant while estimating P tjh and iterating 3 and 5 only over the words seen previously in the current document Rather than doing the full EM calculation for P tjhi at each step during testing we use an online approximation calculated as follows For our initial experiments we used the TDT 1 corpus of newspaper text and transcribed broadcast news stories The corpus contains 6 797 659 words in 15 862 documents We formed our vocabulary by selecting all words occurring at least twice which gave a vocabulary of 49 225 words The data was augmented with sentence beginning and ending markers but the symbols themselves were not 
counted in calculating perplexities We used of the data as a training set holding out every 10th article for use in testing In a first series of experiments we investigated the different schemes to combine the topic based model with a conventional n gram built from the same training data The following table describes our test results on 4274 words comprising 10 stories The number of factors in the topic model was 256 a restriction which was made due to complexity considerations Although allowing a larger number of factors could in principle lead to overfitting we found in practice that by using the control parameter the number of topics could be increased with no drop in test set performance 90 0 wi jtP tjhi i P tjhi 6 PtPP wi jt0 P t0 jhi i 1 Pw d nw d P tjd Pw d P tjh P t 7 nw d P tjhi 1 i 1 1 1 1 1 Model Unigram Topic model Trigram Linear interpolation Ptri Ptopic P 1 Log scale interpolation Ptri topic Ptopic Unigram rescaling Ptri Punigram 1 Perplexity 1140 6 829 1 205 2 189 2 180 8 170 1 Table 1 Results 
on the TDT 1 corpus The rescaling method does not require an additional parameter fit and is nevertheless consistently superior than the interpolation schemes with optimized for linear and for log scale interpolation Using rescaling a reduction of 17 in perplexity was achieved over the trigram model which is relatively close to the 27 reduction from overall unigram to topicbased unigram perplexities To get a more reliable estimate of the model s perplexity we ran the unigram scaling method over a larger test set of 24 850 words in 50 stories Trigram perplexity was 180 8 whereas the combined model s perplexity was 147 2 a reduction of 18 6 Perplexity reductions on individual stories ranged from 8 to 36 Reductions on the five groups of ten stories ranged from 17 1 to 20 3 The improved perplexity results come with an increase in the computational load normalizing over the vocabulary makes the computation of probabilities with the combined model slow This is essentially an online EM algorithm of the type 
discussed in 14 but here only a single iteration is performed reducing the computational complexity in the test stage to a minimum Experiments using full EM iterations showed negligible improvements with higher computational costs Once the topic mixing proportions P tjh have been determined word probabilities can be calculated according to 1 As mentioned in the introduction the topic model does not take advantage of short range syntactic structure Thus we propose to combine the topic model with a standard language model which contributes a different type of information For simplicity we focus on combining it with a n gram model The combination scheme we favor is based on an intuition from maximum entropy model fitting by Iterated Proportional Scaling 6 We interpret the topic model probabilities as marginal word distributions that should be preserved in the combined model while leaving the higher order structure unaffected Under the assumption that the 09 08 Average log ratio of topic to ngram prob 
effectively increasing the complexity by a factor of the vocabulary size Running on a 296MHz Ultrasparc roughly 10 words could be processed per minute while the evaluation of a trigram model alone consists primarily of simple table lookups and can process thousands of words per second Experiments with a reduced 20 000 word vocabulary achieved a rate of 2 words per second 4 2 Perplexity Results on the Wall Street Journal Corpus In order to compare the performance of our probabilistic topic model with models based on standard LSA we performed experiments using the same training test data as in 4 The training data consisted of 29 327 337 words in 81 553 articles taken from the Wall Street Journal from 1987 1988 and 1989 The development test data consisted of 159 632 words from the same years and the final test data consisted of 234 120 words from 1995 and 1996 We used the 19 979 word vocabulary provided with the corpus augmented with sentence markers Perplexity results are shown in Table 2 Model Unigram Topic 
Bigram Bigram Topic Trigram Trigram Topic Dev Test Perpl Change 1046 8 621 1 41 174 3 134 5 20 108 8 89 8 17 Perpl 1107 7 681 9 235 5 187 3 171 0 143 7 Test Change 38 20 16 2 1 5 1 0 5 0 0 5 0 10 10 1 10 10 Position in word frequency list 2 3 10 4 10 5 Figure 2 Relative performance of the topic model by word frequency 0 4 0 3 Average log ratio of topic to ngram prob 0 2 0 1 0 0 1 0 2 Table 2 Results on the Wall Street Journal corpus The 20 improvement over bigram perplexity is significantly higher than the 12 improvement reported by 4 on the same data This stresses the advantage of our probabilistic factor model that has also been verified in other applications 9 10 4 3 Analysis When Does the Model Help It is interesting to consider which words the topic model helps in predicting One might expect that because extremely common function words such as and of and the occur with approximately equal frequency in all documents so that the topic model would be of little use in predicting them In order to test this 
hypothesis we calculated for each vocabulary item in our test data the average log ratio of the probabilities assigned by the two models 0 3 0 10 10 1 10 Position in story 2 10 3 10 4 Figure 3 Relative performance of the topic model by position in story for the standard method vs 89 4 for the function word method However the function word method has an beneficial side effect in terms of computational complexity because it avoids the costly normalization when evaluating a function word Another interesting way of analyzing the performance of the model is to look at how well it performs as a function of how many words of history are provided to the topic model Such a graph is shown in Figure 3 As expected the longer the history the more reliable the estimate of the article s topic and the better the performance The data also show that the combined model performs worse then the trigram for word 2 through 5 of a story The gain from the topic model plateaus after the 19th word in the story 5 APPLICATION TO SPEECH 
RECOGNITION RESULTS ON BROADCAST NEWS In order to determine how effective the topic based language model is in a real world application we put it to use in a large vocabulary continuous speech recognition system We used the SPRACH recognition system for broadcast news described in detail in 5 For this experiment we combined the trigram language model with the topic based language model The topic model used for the Broadcast News experiments was trained on 1996 CSR Hub 4 Language Model corpus collected by the Linguistic Data Consortium For efficiency the 100 most frequent words were removed from the training data After removing these words the corpus training set consisted of 60 328 305 words spread over 124 814 documents The trigram used had a vocabulary of 65 432 words however for efficiency the topic model was trained on a vocabulary of only the 20 000 most com N 1 X log Ptopic i ngram wi Pngram wi One simple approximation of the distinction between function and content words is a word s overall frequency 
We grouped vocabulary items by frequency to examine the correlation between the improvement yielded by the topic model and the word frequency Results are shown in Figure 2 As can be seen the topic based language model actually performs less well than a trigram for roughly the 100 most frequent words in the data Grouping words by their entropy over documents rather than frequency yields similar results These results suggest a simple modification to better handle function words for function words use the n gram probability directly for other words combine the topic and n gram probabilities as before but now normalized only over the non function words The normalization constant is chosen such that the probability assigned to all non function words by the combined model is the same as with the n gram Experiments showed that this approach did not in fact significantly lower the perplexity 89 8 mon words which covers 98 of the data Perplexity results were calculated both on test data from the CSR Hub 4 Language 
Model corpus and from two episodes of broadcast news for which acoustic data were available These complete episodes were segmented into stories by hand The trigram training data included the broadcast news transcripts used in training the topic model as well as newswire text for a total of roughly 450 million words Perplexity results are shown in Table 3 Sprach 98 Trigram 155 6 228 4 224 0 Topic Trigram 134 3 14 205 0 10 194 7 13 words in test set 76 260 3412 7554 Acknowledgments Daniel Gildea was supported by a National Defense Science and Engineering Graduate Fellowship Thomas Hofmann was supported by a DAAD postdoctoral fellowship 7 REFERENCES 1 J Bellegarda A latent semantic analysis framework for large span language modeling In Eurospeech 97 Rhodes Greece September 1997 2 C Chelba and F Jelinek Exploiting syntactic structure for language modeling In COLING ACL 1998 3 P R Clarkson and A J Robinson Language model adaption using mixtures and an exponentially decaying cache In IEEE ICASSP 97 pages LDC test 
data CNN episode A CNN episode B Table 3 Perplexity results on Broadcast News The percentage improvement over trigram perplexity is not as high as achieved with the WSJ corpus probably because the trigram used in the WSJ experiments was trained on a relatively small amount of data This result does show however that the topic model can still provide significant improvement over a state of the art trigram model The improvement on the handsegmented episode was smaller no doubt due to the large number of extremely short aritcles in the data Both of these shows contain many short headlines summing up the news of the day something not found in the HUB 4 traiing data Recognition results on CNN episode A actually deteriorated from 35 6 WER with the trigram model to 36 5 with the topic model combined with the trigram One reason this topicbased model may not help as much as perplexity gains would indicate is that the model would tend to improve performance on longer content words which are more easily acoustically 
distinguishable by the recognizer to begin with In order to test this hypothesis we categorized the recognizer s error according to the word s frequency Frequent Rare Out of Vocab Insertions topic w e r 27 4 27 0 73 8 276 3gram w e r 26 2 27 4 72 1 244 words 1474 1816 122 Table 4 Word error rate by word frequency Table 4 shows results broken down into the 100 most frequent words the words for which trigram probabilities are used the remaining words in the topic model s vocabulary words out of the topic model s vocabulary and insertions This analysis shows that the shorter frequent words are not in fact harder to recognize 
40	a	Lecture Notes in Artificial Intelligence Edited by J G Carbonell and J Siekmann 4885 Subseries of Lecture Notes in Computer Science Mohamed Chetouani Amir Hussain Bruno Gas Maurice Milgram Jean Luc Zarader Eds Advances in Nonlinear Speech Processing International Conference on Nonlinear Speech Processing NOLISP 2007 Paris France May 22 25 2007 Revised Selected Papers 13 Series Editors Jaime G Carbonell Carnegie Mellon University Pittsburgh PA USA Library of Congress Control Number 2007941810 CR Subject Classification 1998 I 2 7 J 5 C 3 LNCS Sublibrary SL This work is subject to copyright All rights are reserved whether the whole or part of the material is concerned specifically the rights of translation reprinting re use of illustrations recitation broadcasting reproduction on microfilms or in any other way and storage in data banks Duplication of this publication or parts thereof is permitted only under the provisions of the German Copyright Law of September 9 1965 in its current version and permission for 
use must always be obtained from Springer Violations are liable to prosecution under the German Copyright Law Springer is a part of Springer Science Business Media springer Preface We present in this volume a collection of revised selected papers from the ISCA Tutorial and Research Workshop on Nonlinear Speech Processing NOLISP 2007 held in Paris France VI Preface Organization NOLISP 2007 was organized by the Institute of Intelligent Systems and Robotics CNRS FRE2507 University Pierre and Marie Curie Paris 6 Scientific Committee Organizing Committee Mohamed Chetouani Bruno Gas Amir Hussain Maurice Milgram Mich ele Vie Jean Luc Zarader UPMC Paris France UPMC Paris France University of Stirling Scotland UK UPMC Paris France UPMC Paris France UPMC Paris France Sponsoring Institutions University Pierre and Marie Curie Paris 6 International Speech Communication Association ISCA European Association For Signal Speech And Image Processing EURASIP IEEE UKRI Industry Applications Society Chapter Table of Contents Non 
Linear and Non Conventional Techniques Phase Based Methods for Voice Source Analysis Christophe d Alessandro Baris Bozkurt Boris Doval Thierry Dutoit Nathalie Henrich Vu Ngoc Tuan and Nicolas Sturmel Some Experiments in Audio Visual Speech Processing G Chollet R Landais T Hueber H Bredin C Mokbel P Perrot and L Zouari Exploiting Nonlinearity in Adaptive Signal Processing Phebe Vayanos Mo Chen Beth Jelfs and Danilo P Mandic 1 28 57 Speech Synthesis Mixing HMM Based Spanish Speech Synthesis with a CBR for Prosody Estimation Xavi Gonzalvo Ignasi Iriondo Joan Claudi 86 Speaker Recognition On the Usefulness of Linear and Nonlinear Prediction Residual Signals for Speaker Recognition Marcos Faundez Zanuy Multi Filter Bank Approach for Speaker Verification Based on Genetic Algorithm Christophe Charbuillet Bruno Gas Mohamed Chetouani and Jean Luc Zarader Speaker Recognition Via Nonlinear Phonetic and Speaker Discriminative Features Lara Stoll Joe Frankel and Nikki Mirghafori Perceptron Based Class Verification 
Michael Gerber Tobias Kaufmann and Beat Pfister 95 105 114 124 X Table of Contents Speech Recognition Manifold Learning Based Feature Transformation for Phone Classification Andrew Errity John McKenna and Barry Kirkpatrick Word Recognition with a Hierarchical Neural Network Xavier Domont Martin Heckmann Heiko Wersing Frank Joublin Stefan Menzel Bernhard Sendhoff and Christian Goerick Hybrid Models for Automatic Speech Recognition A Comparison of Classical ANN and Kernel Based Methods Ana I 142 152 161 169 179 Speech Analysis Non stationary Self consistent Acoustic Objects as Atoms of Voiced Speech Friedhelm R Drepper The Hartley Phase Cepstrum as a Tool for Signal Analysis Ioannis Paraskevas and Maria Rangoussi Voiced Speech Analysis by Empirical Mode Decomposition 204 213 221 230 Table of Contents XI Exploitation of non linear techniques An Efficient VAD Based on a Generalized Gaussian PDF Oscar 255 263 273 283 Phase Based Methods for Voice Source Analysis Christophe d Alessandro1 Baris Bozkurt2 Boris 
Doval1 Thierry Dutoit3 Nathalie Henrich4 Vu Ngoc Tuan1 and Nicolas Sturmel1 LIMSI CNRS Orsay France Izmir Institute of Technology Izmir Turkey 3 TCTS FPMs Mons Belgium 4 DPC GIPSA Lab Grenoble cda limsi fr barisbozkurt iyte edu tr boris doval limsi fr thierry dutoit fpms ac be Nathalie Henrich gipsa lab inpg fr vnt limsi fr sturmel limsi fr 2 1 Abstract Voice source analysis is an important but difficult issue for speech processing In this talk three aspects of voice source analysis recently developed at LIMSI Orsay France and FPMs Mons Belgium are discussed In a first part time domain and spectral domain modelling of glottal flow signals are presented It is shown that the glottal flow can be modelled as an anticausal filter maximum phase before the glottal closing and as a causal filter minimum phase after the glottal closing In a second part taking advantage of this phase structure causal and anticausal components of the speech signal are separated according to the location in the Z plane of the zeros of 
the Z Transform ZZT of the windowed signal This method is useful for voice source parameters analysis and source tract deconvolution Results of a comparative evaluation of the ZZT and linear prediction for source tract separation are reported In a third part glottal closing instant detection using the phase of the wavelet transform is discussed A method based on the lines of maximum phase in the time scale plane is proposed This method is compared to EGG for robust glottal closing instant analysis 1 Introduction Voice source analysis is an important issue for speech and voice processing with many applications such as source tract decomposition formant estimation pitch synchronous processing low rate speech coding speaker characterisation singing speech synthesis phonetic and prosodic analyses voice pathology and voice quality evaluation etc However voice source analysis is also a difficult issue for speech processing There is generally no measurable reference to the true source and vocal tract components 
Speech and voice signals are rapidly time varying and subject to large individual and inter subject variations Finally source tract interactions are not well known to date but they may render voice source decomposition questionable in the situations where strong interactions are likely to occur e g source tract adjustments in singing The aim of this tutorial is to present some aspects of the authors recent works in the domain of voice source analysis A common feature of this line of research is the specific attention paid to the phase structure of the voice source signal Two aspects M Chetouani et al Eds NOLISP 2007 LNAI 4885 pp 2 C d Alessandro et al of the voice source phase are explored the spectral phase of the glottal pulse itself and cross scale instantaneous phases in a time scale space This paper presents the concepts without much technical details The general reader will get the main ideas out of this paper As the most significant references to published literature are pointed out at the beginning 
of each section the interested reader will easily find more detailed presentations of the material described herein 1 1 Definitions of Phase Phase is a highly polysemic word in the general language In signal processing also the meaning of phase is manifold for a review see Alsteris Paliwal 2007 Starting from the more basic periodic signals sine waves phase is defined as the argument of the sinusoidal function This first definition of phase in time domain or instantaneous phase is useful for dealing with waveforms For instance maxima of sine waves are located at phases 2 modulo 2 Mathematically instantaneous phase and instantaneous envelopes are defined using the Hilbert Transform They are useful for describing the time evolution of signals This first definition of phase can be extended to time frequency or time scale representations This will be used below for time scale analysis of the glottal closings instants of the voice source As spectral representation is a decomposition of the signal on a basis of 
complex exponentials i e sine and cosine waves a second definition of phase is the argument of the complex spectrum This spectral phase or phase spectrum is often difficult to deal with On the one hand spectral phases computed using the Fourier transform are obtained modulo 2 and must be unwrapped On the other hand even the smallest delay in the signal changes dramatically the phase spectrum because it introduces a linear component note that the phase spectrum derivative in frequency or group delay is a more robust representation Yegnanarayana Murthy 1992 This second definition of phase will be used below for glottal flow analysis and synthesis 1 2 Phase Structure of the Glottal Pulse Let s write the linear speech production model of voiced speech in the time domain and in the spectral domain s t e t v t l t t nTo g t v t l t n 1 S f E f n 2 Where t represents times f frequency s the speech signal e the voiced excitation component v the vocal tract impulse response l the lip radiation component T0 F0 the 
fundamental period frequency and g the glottal flow component Depending on the application the time domain 1 or spectral domain 2 model is preferred But it goes generally unnoticed that time domain and spectral domain approaches may not be equivalent because of a different underlying phase structure implicitly assumed for the glottal flow component Let us examine this point in more detail Phase Based Methods for Voice Source Analysis 3 In the early years of the source filter theory of speech production the effect of the voice source was mainly studied in the spectral domain like in Equation 2 the glottal flow signal being considered as the output of a low pass system to an impulse train For instance in a transmission line analogue Fant 1970 four poles sr1 sr2 sr3 sr4 on the negative real axis were used with sr1 sr2 2100Hz and sr3 22000Hz sr4 24000Hz Note that two poles are low pass we shall interpret these poles later on in terms of glottal formant and that two poles sr3 and sr4 are fixed we shall interpret 
these poles later on in terms of spectral tilt This simple form entailed important practical consequences because it has been used for discrete time signals for deriving the linear prediction equations see for instance Markel and Gray 1976 In this later case only two poles are used because the linearity of this acoustic model only holds for frequencies below about 4000 Hz G z 1 1 pz 1 1 p z 1 3 The corresponding impulse response magnitude and phase spectra of are plotted in Fig 1 Fig 1 All pole glottal flow model as assumed by the Linear Prediction synthesis model From left to right glottal waveform spectral phase and spectral magnitude On the other hand for e g formant synthesis the time domain model like in equation 1 is generally preferred using time domain models of the glottal flow component A neglected dimension of this glottal flow component is its phase structure In time domain models glottal flow models are generally not viewed as filters or linear systems but are rather described by ad hoc 
equations based on polynomials or trigonometric functions An example of such a model is the KLGLOTT88 model Klatt Klatt 1990 described by the following equations when there is no additional spectral tilt component at 2 bt 3 Ug t 0 0 t OqTo OqTo t To 4 The corresponding impulse response magnitude and phase spectra of are plotted in Fig 2 Note that as far as spectral magnitude is concerned Figure 1 and Figure 2 are very close the same glottal flow parameters being used However both waves are reversed in time or equivalently their phases are opposed in sign corresponding to a symmetry 4 C d Alessandro et al Fig 2 KLGLOTT88 glottal flow model as assumed by the Klatt synthesizer Klatt Klatt 1990 From left to right glottal waveform spectral phase and spectral magnitude relative to the glottal closing instant GCI Then it is argued in the following that the specific phase structure of glottal flow models can be used for source tract separation and for designing new linear glottal flow models Section 2 give details 
on the spectrum of glottal flow models and presents a new model the Causal Anticausal Linear model In Section 3 a new method that takes advantage of this causal anticausal model is presented along with some application to voice source analysis 1 3 Glottal Pulse Phases in the Time Scale Space A second noticeable aspect of the phase of the voice source signals is the instant of glottal excitation or glottal closing According to Equation 1 the source component can be split in two parts 1 a linear and thus linearly predictable using a small set of preceding samples glottal flow filter and 2 excitation by a train of Dirac pulses which is not linearly predictable at all using a small set of preceding samples at the GCI GCIs correspond to singularities in the signal Time scale representation using the wavelet transform is well suited to the problem of singularity detection However in the case of glottal pulses the phase structure of the glottal pulse also influences instantaneous phases in the time scale domain A 
specific method for following the phases across scales is proposed the lines of maximum amplitude LOMA of the wavelet transform This method is applied to GCI detection and compared to direct GCI measurement using electroglottography EGG in Section 4 Finally Section 5 summarizes the main results obtained 2 Time Domain and Spectral Glottal Flow Models1 2 1 Glottal Source Several mathematical glottal flow models abbreviated as GFM hereafter have been proposed over the past decades such as the well known LF model Fant Liljencrants 1985 the KLGLOTT88 model Klatt Klatt 1990 the Rosenberg s models Rosenberg 1971 or the R model Veldhuis 1998 Figure 3 gives a typical example of a glottal flow model and its time derivative 1 The main references for this Section are Doval d Alessandro Henrich 2006 Doval d Alessandro Henrich 2003 Phase Based Methods for Voice Source Analysis 5 Fig 3 Example of glottal flow model top and differential glottal flow mode bottom See text for explanation of the symbols from Sturmel et al 2007 
Generally voice quality is better described by spectral parameters such as the spectral tilt the relative amplitude of the first harmonics Hanson 1997 the harmonic richness factor Childers 1991 the parabolic spectral parameter Alku et al 2002 A remarkable spectral feature of GFM is the spectral peak that can be observed on the glottal flow derivative spectrum in the region of the first harmonics This peak has been coined the glottal formant although it is not a resonance like vocal tract formants The link between spectral voice quality and glottal source parameters has not previously been addressed systematically For answering this problem one must study the position variation and properties of the glottal formant and derive closed form equations for relating time domain glottal flow parameters to the glottal formant This work has been conducted in Doval et al 2006 with the following aims 2 2 Glottal Flow Models Among the GFMs proposed in the literature we have studied the following ones the parameters 
mentioned in this paragraph are explained later in the paper 6 C d Alessandro et al Fig 4 Comparison of several glottal flow models in time and magnitude spectral domains Left top GFM waveform Left bottom GFM derivative waveform Right GFM Magnitude spectrum from Doval et al 2006 R model Veldhuis 1998 the glottal flow is composed of a fourth order polynomial for the open phase followed by an exponential return phase There are five parameters K an amplitude coefficient T0 Te Tp and Ta The glottal flow is computed so that it returns exactly to 0 at the end of the cycle Rosenberg C Rosenberg 1971 the glottal flow is composed of two sinusoidal parts The 4 parameters are Av T0 Tpand Tn Te Tp Noticed that the smooth closure case is not handled LF model Fant Liljencrants 1985 the glottal flow derivative is modelled by an exponentially increasing sinusoid followed by a decreasing exponential There are five parameters Ee E the maximum excitation T0 Te Tp Ta The glottal flow is computed so that it returns exactly to 0 
at the end of the cycle For that two implicit equations must be solved Figure 4 shows an example of the four GFMs left top and their derivatives left bottom with abrupt closure and with a common set of parameters T0 8ms Oq 0 8 m 2 3 and E 1 KLGLOTT88 and R models are identical for this parameter set Note that Av differs between models when E and the other parameters are fixed However these differences are hardly audible All GFMs share some common timedomain features Phase Based Methods for Voice Source Analysis 7 during a fundamental period the glottal flow derivative is positive then negative then null the glottal flow and its derivative are continuous and differentiable functions of time except in some situations at the glottal closing instant Furthermore GFMs are described in terms of phases in the time domain 8 C d Alessandro et al Fig 5 Magnitude spectrum of a GFM derivative in a log log representation and straight line stylization from Doval et al 2006 2 4 Effects of Open Quotient and Asymmetry 
Coefficient on the Glottal Formant The glottal formant frequency is mainly inversely proportional to the open quotient It depends only marginally on m The glottal formant is roughly found between the first and the 4th harmonics Its relative amplitude depends mainly on m An example of synthetic speech is given in Figure 6 Fig 6 Glottal formant and open quotient synthetic speech Top row magnitude spectrum of the speech signals Bottom row time domain signals with glottal excitation superimposed to speech arrows are showing the corresponding waves Left column small open quotient tense phonation right column large open quotient lax phonation from Bozkurt 2005 Figure 7 right panels shows the influence of Oq 4th column right and m 3rd column right on the GFM 3rd row and the GFM derivative 4th row spectra Several points may be observed Phase Based Methods for Voice Source Analysis 9 Fig 7 Effect of amplitude of voicing spectral tilt asymmetry and open quotient on the waveform and spectra of GFM and GFM derivatives 
Top row GFM Second row GFM derivative Third row GFM log log magnitude spectrum Bottom row GFM derivative log log magnitude spectrum First column effect of amplitude of voicing Second column effect of the return phase and spectral tilt Third column effect of GFM asymmetry Last column effect of open quotient from Doval et al 2006 Oq mainly changes the glottal formant frequency m mainly changes the glottal formant amplitude or rather its bandwidth 2 5 Spectral Tilt The voice spectral tilt describes the GFM spectral profile in mid to high frequencies It is related to the return phase in the case of smooth closure of the vocal folds From a modelling point of view two methods can be applied either a time domain decreasing exponential leading to the return phase as described above noted return phase method or a low pass first or second order filter applied to the whole open phase noted low pass filter method For example R and LF are using the return phase method while KLGLOTT88 is using the low pass filter method 
The spectral tilt parameter is Qa Its main effect is to add an additional 6dB oct attenuation above the cut off frequency Fc Figure 7 second column illustrates the effect of Qa The main vocal quality effect of spectral tilt is voice loudness a low or null Qa corresponds to a minimum spectral tilt and a loud voice Conversely a high close to 1 Qa corresponds to a high spectral tilt and a weak voice 2 6 Causal Anticausal Glottal Flow Model and Application to Speech Synthesis The preceding discussion shows that the source log magnitude spectrum can be stylized by three linear segments with 6dB octave 6dB octave and 12dB octave or sometimes 10 C d Alessandro et al Fig 8 Poles pattern left for the Causal Anticausal Linear Model left Corresponding GFM middle and GFM derivative right 18dB oct slopes respectively like in Figure 5 The two breakpoints in the spectrum correspond to the glottal spectral peak and the spectral tilt cut off frequency For synthesis in the spectral domain it is possible to design an all pole 
filter which is comparable to e g the LF model This filter is a 3rd order low pass filter with a pair of conjugate complex poles and a simple real pole The simple real pole is given directly by the spectral tilt parameter It is mainly effective in the medium and high frequencies of the spectrum The pair of complex conjugate poles is used for modelling the glottal formant see Figure 8 If one wants to preserve the glottal pulse shape and thus the glottal flow phase spectrum it is necessary to design an anticausal filter for this pole pair The spectral model is then a Causal spectral tilt Anti causal glottal formant Linear filter Model CALM Doval et al 2003 This model is computed by filtering a pulse train by a causal second order system according to the frequency and bandwidth of the glottal formant whose response is reversed in time to obtain an anti causal response Note that if one wants to preserve the finite duration property of the glottal pulse it is necessary to truncate the impulse response of the 
filter Otherwise the decay of the filter response may continue longer than a single period and get mixed with the next period Spectral tilt is introduced by filtering this anti causal response by the spectral tilt component of the model The waveform is then normalized in order to control accurately the intensity parameter E The CALM has been recently used successfully for real time gesture controlled voice synthesis D Alessandro et al 2007 3 Zero of the Z Transform ZZT Representation of Speech 2 3 1 Principle of the ZZT ZZT is a new representation of signals ZZT means Zeros of Z Transform and is defined by the set of roots of the Z transform of any signal frame Mathematically speaking if x n n 0 N 1 is a signal frame its ZZT is the set of N complex roots or zeros Z m of its Z transform X z 2 The main references for this Section are Bozkurt 2005 Bozkurt Doval d Alessandro Dutoit 2005 Sturmel d Alessandro Doval 2007 Phase Based Methods for Voice Source Analysis 11 Fig 9 ZZT applied to a differential glottal 
flow signal a signal d corresponding magnitude spectrum c Zeros of the Z transform in Cartesian coordinates b Zeros of the Z transform in polar coordinates from Bozkurt et al 2005 X z x n z n 0 N 1 x 0 z N 1 z Z m 1 N 1 5 The ZZT is then an all zero representation of the Z Transform It can be represented on the complex plane either in the classical Cartesian coordinates see Figure 9c or in the more readable polar coordinates see Figure 9b To compute the ZZT an algorithm for polynomial root extraction is needed like such as the root function in MatlabTM since it is known that there is no general closed form expression to calculate the roots of a polynomial whose degree is larger than 5 from its coefficients Galois theorem 3 2 ZZT and the Linear Speech Production Model The ZZT and the source filter model have strong relationships The ZZT shows different patterns for each contribution of the source filter model and particularly for each part of the glottal flow signal the speech signal is simply the union of 
the ZZT of each contribution These results indicate that ZZT is well suited for source filter deconvolution and especially for the study of the source parameters Let us consider the LF GFM with equations g t E0 et sin g t 0 t t e 6 g t E e t t e e e tc te t e t t c T0 t a 7 12 C d Alessandro et al g t 0 t c t T0 8 The corresponding ZZT patterns are displayed in Figure 9 Two rows of zeros are obtained one for the causal part and the other for the anticausal part Gaps appear in each row corresponding to the poles of the CALM model According to Equation 1 a voiced speech frame s n can be written as the convolution product of an impulse train excitation signal e n by the differential glottal flow waveform g n followed by the vocal tract filter with impulse response v n As usual for source filter model the lip radiation contribution is approximated as a derivation and is incorporated into the source contribution as a differentiation Therefore it is the differential glottal flow rather than the glottal flow itself 
which is represented An example is given in Figure 10 Fig 10 A pictorial view of the speech production model of Equation 1 from Bozkurt 2005 Fig 11 A pictorial view of the speech production model of Equation 2 from Bozkurt 2005 It is well known that the magnitude spectral representation in dB transforms the convolution into a sum log S log E log V log L 9 For instance this is the first step of cepstrum source filter deconvolution An example is given in Figure 11 In contrast the ZZT representation transforms the convolution into a union ZZT S ZZT E ZZT V ZZT L 10 This can be clearly seen on Figure 12 where the sets of zeros of each contribution left three plots are simply copy pasted in the speech signal ZZT right plot Fig 12 A pictorial view of the speech production model in terms of ZZT from Bozkurt 2005 Phase Based Methods for Voice Source Analysis 13 Let us describe more precisely the different ZZT patterns encountered in each part of the source filter model like in Figure 12 Finally the ZZT pattern of 
the speech signal right plot is the union of all the zeros that appear in each part of the source filter model This is the key to source filter deconvolution using separation of the zeros into two subsets 3 3 Source Parameter Estimation Using the ZZT An interesting property of ZZT decomposition is that it can distinguish between the minimum and maximum phase parts of a signal Note that the glottal formant is maximum phase this can be seen on the group delay because the corresponding peak is negative or on the ZZT because the corresponding zeros are outside the unit circle Bozkurt et al 2004 or on the CALM model where the corresponding poles are outside the unit circle Since all other speech components are minimum phase or causal the outside zeros belong to the glottal flow component Therefore they can be extracted to estimate the glottal flow component and the glottal formant frequency Using this position the open quotient can be deduced using the theory exposed in Section 2 Henrich et al 2001 Figure 13 
gives an example of open quotient estimation using ZZT This example is a natural vowel changing from lax to pressed voice quality It has been recorded together with the EGG signal in order to extract the open quotient Oq 3 For a pressed voice the open quotient is low Figure 13 shows the estimated glottal formant frequency Fg together with a curve proportional to 1 Oq the value of k has been chosen to fit the first part of the Fg curve and the fundamental frequency which is 3 The EGG signal is proportional to the electrical current across the vocal fold When the glottis is open this current is relatively weak conversely when the glottis is closed this current is relatively strong The variation of the EGG current is more important at the GCI than at the opening instant In the derivative of the EGG current a large peak indicates the closing instant and a smaller peak indicates the opening instant Henrich et al 2004 Notice that these two peaks have opposite signs 14 C d Alessandro et al Fig 13 Open quotient 
estimation using the ZZT from Bozkurt 2005 Fig 14 Source tract decomposition using ZZT from Sturmel et al 2007 approximately constant Fg follows well the curve in 1 Oq which shows that ZZT is a promising means of estimating Oq Experiments on synthetic signals have shown that Fg can be estimated with good precision if it does not coincide with the first formant Sturmel et al 2006 Phase Based Methods for Voice Source Analysis 15 3 4 Source Tract Deconvolution Using the ZZT and Comparative Evaluation with Inverse Filtering Source filter deconvolution using ZZT is explained in Figure 14 Bozkurt et al 2004 Sturmel et al 2006 The principle is to compute the ZZT of a speech frame to separate its zeros in two sets according to their radius and then to compute two components from these sets of zeros These two components correspond to the source dominated and vocal tract dominated signals According to ZZT theory the zeros outside the unit circle correspond to the anticausal part of the speech signal The source filter 
model predicts that the anticausal part corresponds to the glottal formant However spectral tilt corresponds to a causal signal a decreasing exponential in the time domain Then the ZZT decomposition method separates the glottal formant contribution from the vocal tract and spectral tilt contributions Figure 15 shows an example of decomposition Fig 15 Source tract decomposition using the ZZT from Bozkurt 2005 The top plot shows the spectrum of a speech frame where formants can be seen at around 600Hz 1200Hz 2300Hz 3300Hz and 4000Hz The bottom plot shows the spectra of the two signals obtained from ZZT decomposition one mainly corresponds to the vocal tract response showing the formants the other corresponds to the glottal flow spectrum showing the glottal formant and a global spectral slope It must be pointed out that ZZT decomposition and inverse filtering are based on different principles Inverse filtering requires the vocal tract filter identification and is based on passing the speech signal through the 
inverse filter Inverse filtering achieves source filter decomposition mainly on the basis of properties of the filter On the contrary the ZZT representation is based on a very specific property of the source i e its mixed phase nature Therefore source tract decomposition using ZZT is not another inverse filtering method Four state of the arts commonly used inverse filtering methods have been compared to ZZT for source filter separation Sturmel et al 2007 All methods are based on LP Markel Gray 1976 the last one requiring additional processing steps All these methods are well documented in the literature 1 Linear prediction autocorrelation 16 C d Alessandro et al method unlike ZZT Autocorrelation is an asynchronous method 2 linear prediction covariance like ZZT covariance LP is a pitch synchronous method 3 Linear prediction lattice filter asynchronous method based on the Burg s algorithm which ensures a stable lattice filter this is an asynchronous method 4 Iterative Adaptive Inverse Filtering IAIF 
asynchronous method Alku 1992 As the source signal is unknown in natural speech the evaluation procedure is mainly based on analyzing synthetic speech in a first part and then on simultaneous recording of natural speech and electroglottographic signals that give partial information on the voice source The synthetic speech database contains a large number of test signals Automatic procedures for comparisons of synthetic and estimated voice sources are also proposed More formal evaluation with the help of a spectral distance were conducted using a large number of experimental conditions Fig 16 Comparison of ZZT and LP inverse filtering for source tract decomposition of a synthetic speech signal Top Left synthetic speech Top right synthetic differential glottal flow Middle left LP correlation Middle right ZZT bottom left LP covariance bottom right IAIF from Sturmel et al 2007 Source estimation examples are presented in figures 16 and 17 Figure 16 presents the estimated source waveforms for a synthetic speech 
signal a Note that the original synthetic source is known and can be compared directly to the estimated source waveforms Figure 17 presents source estimations for a real speech signal Both glottal flow and its derivative are shown for each method An electroglottographic reference is available for this example showing that the open quotient is about 0 5 i e the closed phase of the source is about half of the period In the example the ZZT is the only method giving a closed phase of about 0 5 Spectral distance results and visual inspection of the waveforms lead to the following conclusions o The pitch synchronous covariance linear prediction seems the worst differential glottal wave estimator Since it is performed on a very short signal segment the autoregressive filter order may probably be too small for accurate estimation of the vocal tract filter Nevertheless the overall low frequency restitution of the glottal formant seems realistic Phase Based Methods for Voice Source Analysis 17 Fig 17 Comparison of ZZT 
and LP inverse filtering for source tract decomposition of a natural speech signal Top center natural speech vowel a Left column estimated differential glottal flow Right column estimated glottal flow o o o The IAIF method seems the most robust one tested in the sense that it gives good results in almost every case the adaptive part of the algorithm appears to be useful for fitting even the most difficult signals However noise and ripples on the estimated differential glottal waveform make it hardly suited to parameter extraction or analysis The auto correlation linear prediction is surprisingly the best LP based source estimation in this benchmark However tests on signals are using long analysis windows exploiting the time invariance assumed by the method This is not always realistic for actual time varying real speech signals Furthermore we observed that the worst cases are those where the pre emphasis does not completely suppress the glottal formant low Oq values leading to a glottal formant at two or 
three times F0 and low values for m leading to a more resonant formant The ZZT inverse filtering outperforms LP based methods both in spectral measurements and time domain observations The absence of ripples in the glottal closed phase together with the very good benchmark results are the strongest arguments in favour of this method On real signals it is the only one to present a clearly visible closed phase on glottal flow waveforms 18 C d Alessandro et al figure 17 The low error values achieved during benchmark make ZZT the best choice for glottal parameter estimation by model fitting However the method relies heavily on precise glottal closing instants determination and it seems also relatively weak for low signal to noise ratio Computational load is heavier than for LP based methods because it is based on roots extraction from a high degree polynomial 4 Lines of Maximum Amplitude of the Wavelet Transform4 4 1 Glottal Closing Instants for Soft and Strong Voices In this section another aspect of the phase 
of the glottal source is explored The periodicity of the voice source signal is defined by the positions in time of the GCI In addition to periodicity another important prosodic parameter is the degree of voicing in the simplest form a voiced unvoiced decision For speech synthesis the GCI are needed in methods based on pitch period or pitch synchronous processing The preceding discussions also pointed out that the GCI is important for determining the anticausal and the causal parts of the glottal source signals Glottal closings are often points of sharp variations or singularities in the speech signal This is particularly the case for abrupt glottal closings when there is no additional spectral tilt component In this situation GCI are corresponding to a discontinuity in the glottal flow derivative and methods for discontinuity detection would be desirable On the other hand for soft voices with low vocal effort the glottal closing instants do not correspond to well marked discontinuities in the glottal flow 
derivative The waveform is smooth and the spectral tilt is large The waveform resembles a sine wave Then instead of searching for discontinuities in the signal it seems more important to follow the signal instantaneous phase Fig 18 Signal analysis using a non uniform filterbank Two extreme situations for glottal excitation signals signal at the top of the Figures and time scale analysis Left panel sinusoidal excitation without any singularity at glottal closing Right panel impulsive excitation The Wavelet Transform demonstrates excellent capabilities for detection of singularities in signals Mallat Wang 1992 This feature has been applied to pitch detection Kadambe and Boudreaux Bartels 1992 Their work is based on the dyadic 4 The main references for this section are Vu Ngoc Tuan d Alessandro 1999 2000 Phase Based Methods for Voice Source Analysis 19 wavelet transform of the speech signal This transform is computed only for two or three small scales high frequencies typically 24 25 and 26 Then GCI are 
detected by locating the local maxima of the transform which are above a threshold level across two dyadic scales This method works well when the speech signal contains singularities at glottal closing Figure 18 right panel This is not always the case and the singularity detection is questionable for quasi sinusoidal voice Figure 18 left panel When voiced speech is seen using a non uniform filterbank characteristic tree like patterns were obtained for voiced as can be seen in Figure 18 Long lines pointing to the singularities are obtained for strong voices or signal containing singularities On the contrary only the first filters give a significant response to soft voice or smooth signals However both situations are actually encountered in speech This is an indication that one could take advantage of the length of lines in the time scale space for improving GCI detection A new algorithm for GCI detection with the help of the wavelet transform has been presented Vu Ngoc Tuan d Alessandro 1999 Contrary to 
previous works all the scales are actually used for analysis Then the high frequency features related to abrupt closures as well as low pass quasi sinusoidal speech signals of soft voices can be analyzed with accuracy This is achieved by a new concepts the lines of maximum amplitude LOMA which are linking amplitude maxima across scales in the wavelets transform domain 4 2 Line of Maximum Amplitude of the Wavelet Transform A wavelet filter bank 6 band pass filters centered on 4000 2000 1000 500 250 and 125 Hz with bandwidths proportional to center frequencies is used for signal decomposition The purpose of the filter bank is to detect the most important periodic peaks Small peaks due to noise are present only in few high frequency HF filters and are uncorrelated On the contrary large periodic peaks are likely to produce large amplitudes for filters at all scales Lines of maximal amplitude LOMA are defined by following the amplitude maxima of the filter responses starting at HF filters and ending at low 
frequency LF filters The LOMA for voiced and unvoiced segments have rather different shapes Unvoiced segments result in short HF lines On the contrary voiced segments are represented by long lines starting from HF filters and ending at the LF filters For each voicing period the LOMA are drawing a kind of tree pattern The GCI for each period is associated to the position of the principal LOMA taken in the highest filter The analysis algorithm can be summarized as follows 1 Compute a wavelet transform The basic wavelet is chosen in such a way that the transform is equivalent to a zero phase filter bank Each filter is a band pass filter with a bandwidth proportional to its center frequency The wavelet transform WT can be considered as the convolution between the signal and a dilated compressed mother wavelet Let x t be the speech signal its WT yi t at scale i is given by yi t x t h t Si 11 The mother wavelet is a band pass impulse response in the form 20 C d Alessandro et al t2 h t cos 2f 0 t exp 2 2 _ 12 Note 
the minus sign in the cosine Then the wavelet analysis will have a maximum response to negative peaks The filters impulse responses are not causal because this is a zero phase filter bank Thus the signal and its response are in phase and the phase of the signal can be read in the phases of the filters at each scale The filters frequency responses are displayed in Figure 19 Fig 19 Wavelet filterbank The frequency response of the mother wavelet is H H 2 1 2 f 0 2 13 Signals at the output of these filters have local maxima see Figure 21 These maxima are tracked across scales starting from the highest frequency HF 4000 Hz towards the lowest frequencies BF 125 Hz Several LOMA are starting from the highest filter in a period the number of LOMA at a given scale is roughly equal to the center frequency of this scale However all these lines are joined together at a unique instant in the lowest filter for each voicing period as there is one LOMA tree per period Thus one can gather these lines into groups with only one 
group per period of voiced Phase Based Methods for Voice Source Analysis 21 Fig 20 Filterbank output right panel positive amplitudes only for a consonant vowel transition More filters are used for the sake of display from Vu Ngoc Tuan d Alessandro 1999 Fig 21 Lines of Maximum Amplitude across scales All the lines are displayed in the left panel In the right panel the optimal line for each period are shown 3 signal In each group the strength of each LOMA is computed by adding all the amplitudes along the lines Optimal selection of maxima across scale is achieved using a dynamic programming algorithm Then the set of optimal LOMA for a period is computed The line with the highest cumulated amplitude is then chosen for representing the period The GCI is then computed as the instant where the selected line starts at the smallest scale highest frequency see Figure 21 right panel 4 3 Comparison with Electroglottography For GCI detection algorithm evaluation a reference is needed The electroglottographic EGG 
reference is chosen because it is an accurate and non invasive GCI measurement method A database of speech including various productions like vocal fry modal and falsetto voices spontaneous and read speech male and female voices has been recorded Most of the GCI peaks in the EGG derivative signal are well defined but some of them are too close the time interval between two successive peaks is shorter than the period of the highest possible fundamental frequency So we developed a simple algorithm for selection of the most prominent peaks that represent GCI The EGG signal and the EGG derivative signal are represented in Figure 22 22 C d Alessandro et al Fig 22 Simultaneous EGG and acoustic signals analyses EGG and DEGG top panels acoustic signal middle panel and response of the wavelet filterbank from Vu Ngoc Tuan d Alessandro 2000 For experiments the speech signal is sampled and discrete time signals are analyzed Then the GCI cannot be determined with accuracy greater than the sampling period and there is a 
slight difference between the discrete time signal maximum and the corresponding continuous time signal maximum To increase time domain accuracy parabolic interpolation is used Near a GCI a parabola passing by this maximum and two adjacent points is computed The GCI is taken at the parabola maximum Parabolic interpolation is applied to both methods Acoustic signals were recorded in a sound proof room using a condenser microphone Phase Based Methods for Voice Source Analysis 23 4 5 The mean difference between both sets of GCI is computed Dm The procedure is repeated varying Td The optimal delay Do between the DEGG and the speech signal is obtained for the value of Td corresponding to minimum Dm The results of this analysis are summarized in Table 1 for 8 sustained vowels Table 1 Comparison of GCI detection using the EGG and LOMA from Vu Ngoc Tuan d Alessandro 2000 Tr s 1 3 2 5 1 8 3 0 1 8 1 5 3 0 2 3 Do ms 0 4 2 2 1 4 3 2 0 6 0 1 1 8 0 2 Dm ms 0 039 0 011 0 012 0 003 0 038 0 010 0 132 0 010 N Degg 206 349 175 
474 380 282 517 599 diff 1 9 O 8 0 5 1 2 0 2 7 8 2 1 34 0 N Speech 210 346 176 480 379 306 506 908 Where Tr represents the sentence duration Ndegg is the number of GCIs detected on the DEGG signal Nspeech is the number of GCIs detected using wavelets and diff the percentage of difference between the two measures Except for the last example Ndegg and Nspeech are generally very close In the last example in many cases the second harmonic octave is much stronger than the first harmonic fundamental frequency In this situation the wavelet algorithm tends to detect two peaks for each voicing period instead of only one When fundamental frequency is known which is actually the case these extra peaks are removed by a simple post processing procedure After this procedure the mean value of Dm is 0 028 ms standard deviation 0 3 ms This indicates that the LOMA method correctly detects the GCI when peaks due to the second harmonic are removed by post processing Fig 23 Examples of EGG and wavelet GCI detection Left panel 
soft voice head register female voice right panel chest register male voice from Vu Ngoc Tuan d Alessandro 2000 24 C d Alessandro et al Figure 23 is presents short segments extracted from sentences in the data base All the figures are presented in the same way from top to bottom EGG signal DEGG signal speech signal wavelet filter bank output with a line indicating the position of the GCI detected using LOMA The right panel Figure 23 shows a male voice in modal register which is the normal register for this speaker The algorithm takes advantage of small scales high frequencies for accurate GCI detection Figure 23 shows another female speaker using her normal head voice register In Figure 23 left GCI detection takes advantage of large scales low pass frequencies 4 4 LOMA and Glottal Flow Parameters In general the LOMA are not straight lines Figure 24 shows 7 bands decomposition for a derivative glottal flow with open quotient 0 8 left panel and 0 3 right panel LOMA differ between these two conditions and then 
glottal flow parameter analysis using LOMA should in principle be possible It is straightforward to obtain a first parameter amplitude of voicing Amplitude of voicing is computed using the energy carried by the best LOMA of each tree When the signal is unvoiced the lines carry very little energy this is because in this situation amplitude maxima are not wellorganized in the time scale space and no strong and long lines are likely to occur The energy is spread in a wide tree with no strong trunk and many small branches On the contrary the best LOMA i e the strongest trunk corresponding to a period of voicing carries a large amount of the signal energy The voiced unvoiced decision can be carried out by using a simple threshold on the amplitude carried by the trunk of each tree This simple measure is surprisingly robust Fig 24 Analysis of glottal pulses with different open quotients left panel Oq 0 8 right panel Oq 0 3 In future work LOMA will be used to investigate the shape of speech signal periods 
particularly near GCI It is well known that the shape of glottal closing is a strong correlate of spectral tilt and is important for studying voice quality LOMA will also be used for speech signal modification e g time and pitch scaling Phase Based Methods for Voice Source Analysis 25 5 Conclusions In this article recent work by the authors on voice source analysis and synthesis using methods based on the spectral phase or instantaneous phase are presented The main results obtained are the following References 1 Alku P Glottal wave analysis with pitch synchronous iterative adaptive inverse filtering Speech Communication 11 26 C d Alessandro et al 6 Bozkurt B Doval B d Alessandro C Dutoit T A method for glottal formant frequency estimation In Interspeech 2004 ICSLP 8th International Conference on Spoken Language Processing Jeju Island Korea October 4 8 2004 4 pages 2004 7 Bozkurt B Doval B d Alessandro C Dutoit T Zeros of Z Transform ZZT decomposition of speech for source tract separation In Interspeech 2004 
ICSLP 8th International Conference on Spoken Language Processing Jeju Island Korea October 4 8 2004 5 pages 2004 8 Bozkurt B Zeros of z transform ZZT representation and chirp group delay processing for analysis of source and filter characteristics of speech signals PhD Thesis Phase Based Methods for Voice Source Analysis 27 25 Sturmel N d Alessandro C Doval B A comparative evaluation of the Zeros of Z Transform representation for voice source estimation In Proceedings of Interspeech 2007 Antwerp Belgium August 27 31 2007 26 Veldhuis R A computationally efficient alternative for the Liljencrants Fant model and its perceptual evaluation J Acous Soc Am 103 Some Experiments in Audio Visual Speech Processing G Chollet1 R Landais1 T Hueber1 2 H Bredin1 C Mokbel4 P Perrot1 3 and L Zouari1 2 3 CNRS LTCI TSI Paris 46 rue Barrault 75634 Paris Cedex 13 France Laboratoire d Electronique ESPCI 10 rue Vauquelin 75005 Paris France Institut de Recherche Criminelle de la Gendarmerie Nationale IRCGN 93110 Rosny sous bois 
France 4 University of Balamand Po Box 100 Tripoli Lebanon 1 Abstract Natural speech is produced by the vocal organs of a particular talker The acoustic features of the speech signal must therefore be correlated with the movements of the articulators lips jaw tongue velum For instance hearing impaired people and not only them improve their understanding of speech by lip reading This chapter is an overview of audiovisual speech processing with emphasis on some experiments concerning recognition speaker verification indexing and corpus based synthesis from tongue and lips movements 1 Introduction A talking face is more intelligible expressive recognisable attractive than acoustic speech alone Natural speech is produced by the vocal organs of a particular talker The acoustic features of the speech signal must therefore be correlated with the movements of the articulators lips jaw tongue velum For instance hearing impaired people and most of us improve their understanding of speech by lip reading Lip reading 
also increases understanding in adverse environment All these reasons motivate the research done on audiovisual speech processing This chapter is an overview of audio visual speech processing The combined use of facial and speech information improves speech recognition identity verification and robustness to forgeries Multi stream models of the synchrony of visual and acoustic information have applications in the analysis coding recognition and synthesis of talking faces SmartPhones VisioPhones WebPhones SecurePhones Visio Conferences Virtual Reality worlds are gaining popularity This defines several applications of audiovisual speech processing e g M Chetouani et al Eds NOLISP 2007 LNAI 4885 pp Some Experiments in Audio Visual Speech Processing 29 2 Features Extraction Audiovisual applications analyse video data and take benefit from information extracted from the two available signals the audio and the visual signals Features extraction from these two signals is the preliminary step to any further analysis 
The most common features used in the five applications mentionned previously are detailed here Apart from the description of the features this section also addresses issues related to temporal and spatial segmentation to the sampling of signals and to the dimension of features vectors 2 1 Temporal Segmentation One of the main difference between the audio and the visual signal extracted from a video stream is the temporal sampling while the visual stream is divided into frames which could be directly handled audio samples are generally grouped together to form larger units which allow to extract reliable features Audio samples can be grouped using a sliding analyis window e g of 10ms This window moves over the signal overlapping may be allowed and each position leads to the extraction of relevant audio features to characterize the temporal segment attached to the window As speech oriented applications are considered the basic signal unit is the phone or a subword based unit as speech synthesis is concerned 
Phonetic segmentation is generally performed in the same time as their recognition For example phones are modeled into three to five states within a Hidden Markov Models HMM framework and the features extracted from a sliding window are used as observations to estimate the current state Whenever the signal leaves the last state of a given phone a phonetic temporal boundary is added More details are further given concerning HMM and speech recognition in section 4 2 Whenever phonetic modeling is adopted all or a part of speech training databases must be manually segmented into phones Unfortunately such a manual phonetic segmentation of the speech signal is difficult and time consuming For applications where text output is not needed an alternative segmental decomposition of speech called ALISP Automatic Language Independant Speech 30 G Chollet et al Processing techniques has been introduced in 1 This decomposition is computed in three main steps First speech signal is decomposed into variable length units 
using the temporal decomposition algorithm described in 2 This algorithm is based on the detection of quasi stationary segments in the parametric representation of the signal Then unit classes are built by gathering together acoustically similar speech segments using an unsupervised vector quantization algorithm 3 This decomposition is driven only by the data and is independent from the language and from the text but correspondence of ALISP segmentation with phonetic transcriptions has been studied 4 A consistent mapping was found which was however far from a one to one correspondence Applications using the ALISP segmentation are discussed later 2 2 Managing Sampling Rates and Alignement Two issues arise when first comparing an audio stream with a visual one the difference in sampling rates and the alignement Concerning sample rates any easy solution to recover a common sampling rate is to choose for a reference which may be the audio rate or the visual one In the first case the visual signal must be over 
sampled For instance if a sliding window of 10ms is considered to produce audio observations while video frames are observed every 40 ms interpolation must be provided to produce new visual features leading to the same number of frames per second cf 4 2 4 3 Another problem is alignement Audio and visual streams may not be synchronised at a particular time due to co articulation effects and articulator inertia In fact the articulators sometimes move in anticipation of a phonetic event before the phone is produced In these cases the visual information may be available before the acoustic evidence Many methods for modelling audio visual asynchrony have been proposed and are detailed in section 3 2 3 Spatial Segmentation Prior to any feature computation stage video frames areusuallyspatiallysegmented in order to focus on particular regions of interest Applications reported in this chapter mainly deal with speech processing Most of these regions of interest are therefore related to faces that is either faces or 
face features like eyes or lips In most of the cases face features are localized within a face area which has been previously determined A complete survey about face detection may be found in 5 Two face detection systems have been experimented The first one the Viola and Jones algorithm 6 may be qualified as a classical one considering that its use is widely spread over the community It is based on the estimation of a strong classifier composed of a cascade of many weak classifiers each of these weak classifiers being attached to a particular Haar feature A stage of learning is thus required to produce this strong classifier The nature of the data included in the learning base then influences the type of faces which can be correctly detected afterwards As a consequence different cascades must be learnt Some Experiments in Audio Visual Speech Processing 31 to allow the detection of faces under different orientations Typically a cascade is dedicated to frontal faces detection and another one to profile faces 
detection This system has been used in the framework of the VMike project concerning audiovisual speech recognition cf section 4 2 Faces are first extracted thanks to a frontal cascade A mouth cascade that is to say a classifier which has been learnt over a database containing samples of mouth is applied on the lower part of the detected face The second system may be considered as a probabilistic equivalent of the Viola and Jones method 7 While this system still relies on the estimation of a strong classifier the difference is that the underlying classifier function is then used to estimate the distribution of the object of interest faces in our case that is to model the generation of such objects within images such a model is called a generative model As this distribution is computed many partitions of the input images are considered and the patches they are composed of are assigned a label object of interest versus background depending on the estimation of likelihoods As for the Viola and Jones method any 
object may be considered A two stage process then allows to detect faces and eyes within faces This algorithm has been applied prior to features extraction within the framework of asynchrony detection 4 3 and within the framework of face verification Concerning the asynchrony detection application eyes position allows to determine a region of interest where to look for the mouth knowing the geometrical structure of the human face Then the actual mouth detection step is performed using a Viola and Jones detector 6 it was developped by Once eyes position is obtained within a face a geometrical normalization is performed in order to make the line between the eyes horizontal Then a mask 32 G Chollet et al Fig 1 Face detection and normalization is applied in order to remove artifacts that might appear at the border of the face Finally image pixels are normalized by histogram equalization Figure 1 shows an example on a face from the BANCA database see section 4 1 Given that the rotation of the face its partial 
occlusion or bad lightning conditions can lead to a poor quality detected face whose features are not representative of the person a method has been designed to keep only the best faces This selection is obtained by removing all detection results that might lead to degraded results regarding the aimed application e g authentication For each frame f of the video a reliability score r f is computed as the inverse of the euclidean distance between the detected face and its projection into the eigenface space see figure 2 Consequently a threshold is applied on r f in order to keep only the best faces within the video sequences Face f is selected if and only if r f f N f 1 where Nf is the set of all faces detected in the video sequence 2 3 has been used in our experiments Only the selected faces are then used for authentication Figure 3 shows an example of the application of this method 2 5 Audio Features Most speech recognition and speaker verification systems use short term cepstral features The two most 
popular sets of features are cepstrum coefficients obtained with a Mel frequency cepstrum coefficient MFCC 10 analysis and the ones whose computation relies on a perceptual linear predictive PLP 11 analysis In both cases a short term power spectrum is estimated on a fixed frame 20 30 milliseconds with the most used frame rate being 100 hz Some Experiments in Audio Visual Speech Processing 33 Fig 2 Reliability based on the distance from face space To get MFCC coefficients a cosine transform is applied to the log power spectrum A root linear prediction cepstral coefficient LPCC analysis is used to obtain the PLP cepstrum parameters 2 6 Visual Features A difference must be made between local features which are attached to a particular set of points within the region of interest that must be characterized and global which produce a new representation of the region of interest treating it as a whole Both features are detailed thereafter Local features SIFT Scale Invariant Feature Transform descriptors 12 are 
known to be among the best local descriptors 13 Their extraction can be coarsely summarized into three stages extraction of keypoint candidates filtering and descriptors calculation Keypoint candidates extraction relies on the scale space theoretical background 14 15 Once these candidates are extracted their location is refined and their scale is determined Keypoints are then filtered according to some constraints on contrast and geometrical properties ratio of principal curvatures Each remaining keypoint is finally represented by a 128 dimensional vector by computing gradient orientation and magnitude over its neighbourhood and by quantizing values spatially reducing to a 4x4 array and regarding orientation 34 G Chollet et al Fig 3 Face with maximum r left selected center and rejected face right 8 bins Each keypoint is also defined regarding three other data its spatial location its scale and its orientation SIFT descriptors have been used for face verification cf section 4 3 Global features Local methods 
require a precise localization of particular points within the region of interest Depending on illumination occlusions such a localization may not be easily obtained Global methods then allow to overcome this drawback The first kind of global features rely on the Discrete Cosine Transformation DCT which are used for asynchrony detection cf section 4 3 and audiovisual speech recognition cf section 4 2 Their extraction is illustrated in figure 4 Only the 28 coefficients corresponding to the low spatial frequency are kept as shown in figure 5 The eigenfaces method 16 may also be used to code the relevant information in the region of interest The main principle is to project face images viewed as intensity vectors in a space where data scattering is maximized Such a space is obtained by applying Principal Component Analysis PCA over a training set composed of numerous face images Its direction vectors are called eigenfaces as they refer to eigenvectors of the training data covariance matrix Such a method may 
easily be extended to any visual object given that enough learning data are available It has thus been applied to lips eigenlips and tongues eigentongues within the framework of our experiments concerning audiovisual speech recognition OUISPER project The control points of the optimal snake are good features of the object edges 2 7 Audiovisual Features and Decision Fusion Each of the audiovisual applications detailed in the next sections are related to an underlying decision process transcribing speech deciding whether a person claiming he she is person is effectively deciding whether an audio stream is synchronised with the visual one etc All these decision processes may take benefit from considering in the same time audio features vectors and visual ones Some Experiments in Audio Visual Speech Processing 35 Fig 4 Visual speech features extraction Fig 5 Visual speech features extraction 28 low spatial frequency DCT coefficients are extracted in a zigzag manner Two main approaches may then be adopted The 
first one is called early fusion and is based on the computation of audiovisual features vectors from audio and visual features vectors for instance concatenation The second one is called late fusion and relies on the fusion at the decision level Many different methods may be applied to combine the outputs of all the classifiers used in the modeling process 17 majority voting max min sum We will also mention here the use of Support Vector Machines SVMs to perform fusion at the score level e g for audiovisual identity verification involving scores given by each modality Early and late fusion methods have been experimented within the framework of several audiovisual applications and are detailed thereafter 2 8 Dimension Reduction The size of the learning databases required to compute models is a function of the dimension of the feature vectors chosen to represent audio segments visual 36 G Chollet et al regions As a consequence handling high dimensional feature vectors may be difficult if not enough learning 
data are available Dimension reduction may then be used to overcome this issue Many methods are available Principal Component Analysis PCA or Linear Discriminant Analysis LDA PCA has already been presented within the section concerning eigenfaces LDA is much more appropriate for classification Typically its properties are interesting for audiovisual speech recognition cf section 4 2 since classes are then known phones On the difference of PCA which tends only to maximize intra classes scattering LDA also tends in the same time to minimize interclasses scattering Another method for dimension reduction is the Co Inertia Analysis CoIA 18 This method is a multivariate statistical analysis that aims at jointly transforming two signal the acoustic and the visual one when performing audiovisual synchrony analysis in order to maximise their covariance Denoting X Rn and Y Rm the acoustic and visual features vectors CoIA can be summarized by the following equation a and b are column vectors of A and B optimal 
projection matrices 2 a b argmax cov a Rn b Rm Details for A and B calculation can be found in 18 It will be shown in section 4 3 how to derive synchrony measures from A and B matrices using their first K vectors 3 Modeling and Classification Once features vectors have been extracted from the audio and the video stream a modeling stage is applied to compute representations which will be used to make the final decision Most of the time these models are statistics For instance Gaussian Mixture Models model the distribution of observation vectors as a combination of gaussian distributions These models may be used for modeling phones observation distribution as the speech of a given speaker see sections 4 2 4 3 Hidden Markov Models HMMs then allow to model a statistical process involving different states These models are at the heart of many audiovisual applications and are detailed in this section 3 1 Gaussian Mixture Models As already mentionned GMM distribution is a mixture whose components are classical 
Gaussian distributions This results in the following form for the GMM distribution K K 1 2 1 X k k p X k 1 wk Nk X k k k 1 wk 2 p 2 k e 2 X k 1 T 3 Some Experiments in Audio Visual Speech Processing 37 where K X wk k k are respectively the number of components in the GMM the speech feature vector the weight of the kth component in the mixture i e its probability of appearance the mean vector and the covariance matrix of this k th Gaussian component Given a GMM to model the speech a sequence of T speech feature vectors will have the following likelihood t 1 p X 1 X T T p X t 4 This supposes that the speech feature vectors are independent given the GMM model Therefore the same likelihood will be obtained if we take a random order of the same sequence of T vectors Large GMM distributions have been used to represent speech in general in speaker recognition systems 19 The number of components K can take large values sometimes more than 2048 Given a set of feature vectors the estimation of the GMM parameters i e 
the components weights wk and the mean and covariance matrices k k does not have a direct analytical solution The estimation of the distribution parameters is then based on the Estimation Maximization EM algorithm It is an iterative algorithm that adjusts in each iteration the model parameters while ensuring a non decrease of the likelihood of the training data In some cases the amount of data available for training is not large enough to estimate the GMM parameters A constrained training is applied and is called adaptation Actually starting from an existing GMM the parameters are adjusted in order to better describe based on a criterion the training data The adjustment is constrained either by an a priori distribution function like in the Maximum A Priori MAP or Bayesian adaptation or by a transformation function applied on the models parameters like in the Maximum Likelihood Linear Regression MLLR adaptation A unified adaptation theory has been proposed in 20 3 2 Hidden Markov Models A Markov Model is a 
finite state machine composed of N states It changes state once every time unit In Hidden Markov Models states are not observed and each time a state is entered it emits an observation according to a statespecific probability distribution Formally an HMM is defined as si aij bj si state ii 1 2 N aij transition probability between i and j bi ok emission probability of observation ok at state i Looking at a series of observations O o1 o2 oT does not directly indicate the sequence of states S s1 s2 sN which are hidden However knowing the emission probabilities bi ok and the transition probabilities aij allows to 38 G Chollet et al a11 a22 a33 state 1 p1 o1 a12 state 2 p2 o2 a23 state 3 p3 o3 o1 o2 o3 Fig 6 Example of a HMM model with three states estimate the associated states sequence thanks to the Viterbi algorithm 21 All these probabilies are thus required to compute the sequence of hidden states The very first stage is then to estimate them using the Baum Welch algorithm over a training set 3 3 HMMs 
Extensions Two other kinds of statistical models may be derived from the classical HMMs to facilitate audiovisual process modeling namely the Multistream HMMs and the coupled HMMs CHMMs 22 23 24 Multistream HMMs may be considered as a late fusion method In this approach each modality here the audio one and the visual one is independently processed and pre classified The final classification is based on the fusion of the outputs of both modalities Multistream HMMs derive the most likely class by taking the product of the likelihoods of the two single modality classifier decisions using appropriate weights The models for each mode are estimated separately In the case of state synchronous decision fusion the scores weighted likelihoods are multiplied after eachtime unit in order to find a new audio visual likelihood of the observation being generated by a state P oav t s P oa t s a P ov t s v An advantage of decision fusion over early fusion is the possibility of weighting the importance of the two modes 
independently the weights a and v may be chosen so as to model the reliability of each modality However assigning the good weights to different streams is a critical step and if the weights are not chosen properly the system might perform poorly In fact the weights can be defined in a static manner by using a priori knowledge or they can be estimated and learned on a validation dataset For a more complete description of Some Experiments in Audio Visual Speech Processing 39 this dynamic weighting technique we refer the reader to 25 in which multistream combination is used to improve the noise robustness of automatic speech recognition ASR systems It has already been explained that audio and visual streams may not be synchronised at a given time due to co articulation effects and articulator inertia Many methods for modelling audio visual asynchrony have been proposed in the literature including multistream HMMs we presented above product HMMs and coupled HMMs The product HMM is a generalisation of the state 
synchronous multistream HMMs that combines the stream log likelihoods at an higher level A CHMM 24 can be considered as a set of HMMs in which each HMM is dedicated to one stream In the common topology of coupled HMM the discrete nodes at time t for each HMM are conditionned by the discrete nodes at time t 1 of all the HMMs of the set Thanks to this property CHMM can model the audio and visual state asynchrony while preserving their natural correlation over time 4 Applications Four main on going experimentations will be detailed afterwards audiovisual speech recognition audiovisual identity verification speaker indexing and speech reconstruction from silent speech All these applications make use of the features and models which have been presented in the previous section 4 1 The BANCA Database The BANCA database 26 has been used for our experiments concerning audiovisual speech recognition and for our audiovisual identity verification system Here is a brief overview of its content The BANCA database contains 
audiovisual recordings of 52 persons talking in front of a camera equipped with a microphone Two disjoint groups G1 and G2 of 26 persons each are made of 13 females and 13 males Each person recorded 12 sessions divided in 3 different conditions In each session one true and one false identity claims were recorded The difference between true and false identity claims only stays in what the person says his her name and address and a personal PIN for true identity claims and the name and address and the personal PIN of the target for false identity claims Concerning identity verification seven evaluation protocols for identity verification are defined for the BANCA database The Pooled protocol which contains 232 client accesses and 312 impostor accesses per group from any recording conditions has been chosen for our evaluation 4 2 Speech Recognition Most state of the art Automatic Speech Recognition ASR systems make use of the acoustic signal only and ignore visual speech cues while visual information has been 
shown to be helpful in improving the quality of speech recognizers 40 G Chollet et al especially under noisy conditions 27 28 29 The system described in this section involves information extracted from both modalities to improve recognition performances Audiovisual recognition units Audio only ASR systems generally use phones as basic recognition units As the visual signal only provides partial information about the underlying sequence of phones as all the articulators are not visible usually only the lips various sets of phones that are acoustically distinct may be visually indistinguishable A possible solution is to consider visemes the linguistically minimal units which are visually distinguishable However having different classes in the audio and the video system components complicates audiovisual integration identical classes for both modalities will then be used afterwards and both components will recognize phones Audiovisual integration The concept behind bimodal ASR is to combine the information from 
each mode in order to increase performances which could be obtained considering each mode separately 29 30 31 Both early and late fusion have been tested Early fusion the vectors of each single mode are concatenated Given timesynchronous audio and visual feature vectors oa t and ov t feature fusion considers oav t oa t ov t Rlav where lav la lv as the joint audio visual observation So a single classifier is trained on the concatenated vector It is also possible to process the concatenated vectors with any transformation such as Linear Discriminant Analysis LDA in order to reduce the increased number of coefficients and facilitate classification see Figure 7 Late fusion as already explained multistream HMMs derives the most likely speech class by taking the product of the likelihoods produced using models learned for each mode The final likelihood is then P oav t s P oa t s a P ov t s v Experiments Data and features This work is done within the framework of the VMike project 32 VMike is a video microphone 
which includes both a microphone and an optical retina Experiments have been led on the BANCA database 208 subjects were recorded in three different scenarios controlled degraded and adverse over 12 different sessions During each recording the subject was prompted to say a random 12 digit number his her name address and date of birth In the scope of this work only the 12 digit sequences of the scenario controlled are extracted In order to test the performance of the developed audiovisual speech system under noisy conditions those utterances are combined with samples of babble noise at several signal to noise ratios SNR The babble sample is taken from the NOISEX database 33 The retina has been simulated for evaluation First a detection algorithm is applied on every frame and outputs the position of the mouth as an image of Some Experiments in Audio Visual Speech Processing Audio FrontEnd 1 41 state 39 1 1 79 1 40 P o c state LDA state P o c 40 Decision Fusion Video FrontEnd Feature Fusion Fig 7 Example of 
discriminative feature fusion and state synchronous multistream integration 130 125 X Y 120 115 110 105 0 20 40 60 80 100 120 140 160 180 200 150 140 130 120 110 100 90 80 0 20 40 60 80 100 120 140 160 180 200 Y X x1 x2 x200 Fig 8 Horizontal and vertical profiles size 200x200 thanks to the Viola Jones mouth detector described in section 2 3 Horizontal and vertical projection profiles are then computed cf fig 8 The 200 projections along the X axis and 200 projections along the Y axis are concatenated to a single vector whose dimension is reduced to 40 using an LDA after a feature mean normalization So as to capture dynamic speech information each vector is then extended by concatenating its 7 chronologically preceding and the 7 following vectors The resulting 600 features per sample are finally transformed into vectors of 40 using LDA In order to compare these features to state of the art features DCT coefficients of the detected mouths are also computed these zones are firstly scaled to a 64x64 image and the 
100 most energetic coefficients are then selected The same process as the one described for profiles features is then applied resulting in a DCT feature vector of size 40 The computation of all the visual features is summarized in figure 9 Concerning the audio features 13 feature mean normalized MFCC coefficients are extracted and extended with first and second derivatives of each coefficient In order to obtain audio and visual features synchronicity a simple element wise 42 G Chollet et al 1 64 1 t 1 1 1 40 1 1 64 DCT Extraction 4096 Extract 100 most Energetic Feature 1 100 1 1 100 Mean Normalization LDA 40 1 40 40 LDA 40 E DCT 15 Frames 600 DCT2 1 200 1 200 Upsampling with Linear Interpolation Rate 100Hz Video Signal Rate 25Hz PRO t 130 125 120 115 110 105 1 1 40 1 PRO2 1 X Profile 0 20 40 60 80 100 120 140 160 180 200 1 400 Feature Mean Normalization 1 400 1 150 140 130 120 110 100 90 80 0 20 40 60 80 100 120 140 160 180 200 LDA 40 1 40 40 LDA 40 Y Profile E 15 Frames 600 Fig 9 Visual features extraction 
process linear interpolation of the visual features to the audio frame rate is applied From the up sampled pictures the Discrete Cosine Transform DCT coefficients and profiles are extracted Models and Fusion The acoustic models are context independent Each monophone consists of 3 states which are modeled by 16 Gaussians each The HTK Toolkit 34 software is used for model training and testing For the feature fusion the 39 dimensional audio vectors are simply concatenated with the 40 dimensional visual features DCT or profiles respectively The combined vectors are then LDA transformed before being used for model estimation Decision fusion is obtained by combining separately trained models for the audio and the visual coefficients to two stream models with specific weights on each stream We assume state synchronous fusion for combining the stream likelihoods The optimal weighting is found through by trial and error Results Two speech recognizers are then evaluated video only and audio visual The terminology for 
the different visual features is the following see figure 9 DCT PRO and DCT2 PRO2 correspond to the features without and with dynamic concatenation respectively 1 Visual only speech recognition the results of all four different parametrization experiments do not exceed 45 accuracy Fig 10 Using 15 consecutive vectors to include feature dynamics did not improve performance The results for single DCT PRO and for concatenated DCT2 PRO2 vectors respectively do not differ significantly 2 Audio visual speech recognition figure a in fig 11 shows the performance for all decision fusion recognizers at 5 db Both DCT2 and PRO2 improve word recognition but only DCT2 does so significantly compared to the audio only system When feature fusion is applied figure b in fig 11 under noisy conditions the recognition is improved by up to 12 percent with respect to audio only recognition Some Experiments in Audio Visual Speech Processing 43 Fig 10 Visual only ASR 4 3 Audiovisual Identity Verification Biometrics identity 
verification systems have been proven to be much more effective when information extracted from several modalities are merged together 35 The system presented in this section relies on the fusion of three different modalities the visual modality based on face verification the audio one based on speaker verification and the synchrony modality based on the analysis of the correspondence between the audio and the visual stream in a region located around the lips We will also deal with the issue of speech conversion which may be considered as a high effort attack against the verification system Face verification Face verification may rely either on global face features as in the eigenfaces approach 16 or on local ones approach using facial keypoints The latter are able to capture geometrical relations between particular parts of the face and are thus more efficient when geometrical distortions occur On the other hand global features are easier to compute and takes the whole face into account no information is 
lost We propose to benefit from the complementarity of these two approaches in a fusion framework where two algorithms based on global and local features respectively will be fused at scores level The first algorithm uses classical eigenfaces global features cf section 2 6 and the second one involves local SIFT descriptors cf section 2 6 The comparison stage is the same for both type of features and is based on an SVD matching process 36 37 SIFT descriptors have already been used together with the SVD based matching method in 38 which deals with object matching Concerning face authentication in particular SIFT descriptors have been tested in 39 where the matching between two images relies on the minimum euclidian distance between their SIFT descriptors Unfortunately this method relies on a manual registration of the different images The main advantage of our method is then to propose an end to end system which does not suppose to know the position of faces before applying verification 44 G Chollet et al a 
Audio visual decision fusion results at 5db b Audio visual parameter fusion results Fig 11 Audiovisual ASR results SVD based matching method was introduced for spatial matching between keypoints 36 and relies on the proximity and exclusion principles enunciated by Ullman 40 which impose one to one correspondences Let us consider two sets of keypoints and R be the distance matrix between them The matching consists in searching for pairs i j that minimize Rij Searching for one to one correspondences may be facilitated if some projection matrix Q allows to make R closer to the identity matrix I Such a problem is referred as the orthogonal procrustes problem find the orthogonal matrix Q that minimizes R IQ 41 It is proven that Q can be computed as follows 1 Compute R Singular Values Decomposition SVD R U DV 2 Replace D by the identity I to get Q Q U V The last step is then to extract good pairings i j searching for the elements of Q that are the greatest both of their row and their column Some Experiments in 
Audio Visual Speech Processing 45 This main principle is further improved by using a gaussian weighted distance to compute the proximity matrix Gij exp Rij 2 2 where quantifies the maximal tolerated distance between two keypoints This parameter is known to have very little influence on the final results 42 and will be set to 1 8 of the width of the image as this value has been already successfully tested 42 A first extension was defined in 42 to take local descriptions around keypoints into account SVD is then performed on the matrix G defined as Gij f Cij g Rij where Cij denotes the correlation between gray levels around i and j keypoints and where g is the gaussian function previously defined Two different f functions may be used 37 Exponential f Cij exp Cij 1 2 2 2 Linear f Cij Cij 1 2 5 6 where 0 4 37 A second improvement has been experimented in 38 where gray level correlation is replaced with SIFT descriptors correlation only the linear form for f function is tested At test time pairings i j are 
filtered according to their associated correlation Cij and the number of pairings with Cij Corrth is taken as the authentication score Considering a video as a set of faces the same SVD matching process is used to search for correspondences between two videos whatever face representation is used the global one based on eigenfaces or the local one using SIFT descriptors Concerning SIFT matching our system is the same as the one in 38 Position vectors used to compute R proximity matrix include the spatial location the scale and the orientation of SIFT descriptors A test will refer afterwards to an authentication test involving two videos V of a person claiming she he is person and V the enrollment SIFT video of person Let then Nf be the number of detected faces selected cf section 2 4 in each video SIFT descriptors are extracted from each of these faces Resulting video representations will be denoted afterwards as follows k k SIFT and S k 1 N SIFT where S k Sk k 1 Nf Ndesc repk k si i 1 Ndesc f resents the 
number of 128 dimensional SIFT descriptors sk i extracted from face k Matching is performed between each pair Sk Sl related to SIFT descriptors extracted from faces k and l retained from V and V respectively In this case l Cij and Rij elements are computed between sk i and sj descriptors An authentication score i e the number of matchings between descriptors is obtained for each pair Sk Sl These scores are firstly normalized according to the number of SIFT descriptors and their mean then produces a single score S V V 1 SIFT 2 Nf SIFT SIFT Nf Nf k 1 l 1 M Sk Sl min N k N l desc desc 7 where M Sk Sl is the number of matchings between Sk si k i 1 Ndesc k and Sl s l l i i 1 Ndesc 46 G Chollet et al Pairwise matching is performed between each Ek and Ek that is between faces directly As these features treat faces as a whole location information is lost and the G matrix is reduced to its description part Gij f Cij These Cij elements are computed between Ei and Ej This differs with SIFT matching since a single 
authentication score will be obtained for each test Let us consider the same authentication test to set out the matching PCA process for global representations The same number Nf of detected faces is kept in each video Their eigenface features will be denoted afterwards as PCA and E k 1 N PCA respectively Ek k 1 Nf k f S V V M E E 8 P CA and E El l 1 Nf Parameters have been set in the following manner during our experiments SIFT SIFT 5 Nf 100 Ei R97 i e we chose to keep the 97 most influent Nf directions to compute global representations Corrth 0 4 The f function is linear for global matching and exponential for local matching The form of f function has been chosen by cross validation between groups G1 and G2 of the BANCA database P CA where M E E is the number of matchings between E Ek k 1 Nf Speaker verification Speaker verification is based on GMM modeling cf section 3 1 of each speaker included in the BANCA database To overcome the lack of training data dedicated to each speaker adaptation of a world or 
universal model is performed using the MAP algorithm The verification score is computed as the following likelihood ratio S V V 1 Nx log x P x P x where x denotes an observation vector in the audio stream of V the world model and Nx the number of observation vectors considered in the whole speech sequence Synchrony modality Speaker conversion and face animation can be considered as high effort forgeries Some Experiments in Audio Visual Speech Processing 47 Fig 12 Example of a simple replay attack Audio and visual speech features are respectively MFCC and DCT coefficients extracted as explained in sections 2 5 and 2 6 In order to equalize the sample rates of acoustic and visual features initially 100 Hz and 25 Hz respectively visual features are linearly interpolated Using the acoustic and visual features X and Y extracted from the enrollment sequence CoIA cf section 2 8 is applied in order to compute the clientdependent synchrony model A B At test time acoustic and visual feature vectors X and Y of the test 
sequence are extracted and a measure Sc of their synchrony is computed using the synchrony model A B of the claimed identity Sc V V 1 D D corr a k X bk Y k 1 t t 9 where D is the number of dimensions actually used to compute the correlation In our case we chose D 3 Scores fusion The scores provided by each modality are finally fused in a late fusion framework involving SVM cf section 2 7 Results obtained on groups G1 and G2 of the BANCA database are depicted in figure 13 which validates the initial idea of taking benefit from different modalities to improve performances It has already been explained that the synchrony modality is appropriate whenever robustness to high effort attacks is required In order to test synchrony modality superiority some work has then been dedicated to generate forgeries which would defeat traditional modalities Speech conversion is one of the possible high effort attack and will be adressed in the next section 48 G Chollet et al Fig 13 Performances of mono modal and multimodal 
verification systems Speaker conversion Automatic voice conversion may be defined as the process of transforming the characteristics of speech uttered by a source speaker such that a listener would believe the speech was pronounced by a target speaker Different kinds of information are included in the speech signal environmental noise speech message speaker identity The question of voice conversion is firstly to establish the most characteristic features of a source individual to transform them to their target counterpart The analysis part of a voice conversion algorithm focuses on the extraction of speaker identity Secondly it will calculate the transformation function to apply Both operations must be performed independently of the environment and of the message At last a synthesis step will be achieved to replace the source speaker characteristics by the target speaker characteristics Consider a sequence Xs x1 x2 xn of spectral vectors pronounced by the source speaker and a sequence pronounced by the 
target speaker composed by the same words Yt y1 y2 yn Voice conversion is based on the calculation of a conversion function F that minimizes the mean square error mse E y F x 2 where E is the expectation Two steps are useful to build a conversion system a training step and a conversion step In the training phase speech samples from the source and the target speaker are analysed to extract the main features Then these features are time aligned and a conversion function is estimated to map the source and the target features The aim of the conversion step is then to apply the estimated conversion function rule to the source speech signal so that the new utterance sounds like the speech of the target speaker The last step is the re synthesis of the signal in order to reconstruct the speech segment of the source voice after the conversion The most representative techniques of voice conversion are based on vector quantization 44 on Gaussian Mixture Models and derived 45 46 47 48 on Some Experiments in Audio Visual 
Speech Processing 49 Multiple Linear Regression 49 and on an indexation in a client memory 50 Two of these conversion methods will be developed afterwards and their influence on an automatic speaker recognition system will be evaluated The first one is based on ALISP cf section 2 1 50 One hour of speech pronounced by the target speaker is available This speech signal is segmented and vector quantization allows to extract 64 classes which will constitute the target codebook As this speech signal is now annotated regarding these 64 recognition units or classes a HMM may be trained and applied on the source signal Once the source signal has been segmented the synthesis stage is applied each segment is replaced by one of its closest counterpart in the same class i e the one with the same index among target classes This counterpart is selected comparing prosodic parameters Harmonic plus Noise 48 between the source segment and all the segment contained in the target class cf figure 14 thanks to the Dynamic Time 
Warping DTW Fig 14 Conversion step This technique of conversion provided interesting results on the NIST 2004 corpus 50 as the recognition rate effectively decreased when applying speech conversion The second technique we experimented consists in modifying all the shape of the source spectrum to correspond to the target spectrum 49 The different stages of this techniques are depicted in the figure 15 In the first time the source segment and the target segment they contain the same utterance are aligned using DTW Vector Quantization is then applied on each segment to extract 64 classes Mappings between source and target classes is then estimated using DTW mapping codebook After a normalization stage over each class conversion matrices from source class i to target class j are then estimated using Multiple Linear Regression These matrices finally allow to transform a new source segment so that it corresponds to target speech in the feature space 50 G Chollet et al Fig 15 Obtaining the Mapping Codebook As in 
the previous conversion method a significant decrease of the automatic speaker recognition is demonstrated on the DET curve cf figure 16 The impact of speech conversion over a speaker verification system has been clearly established The next stage will be to test whether a modality like the synchrony could help to deal with such attacks 4 4 OUISPER The audio visual speech based applications discussed before use the video stream in addition to the audio stream to improve speech or speaker recognition However for some applications the audio stream cannot be used at all whenever audio is too much corrupted by noise or at the opposite in the context of speech communication in situations where silence and privacy must be maintained These applications address the issue of speech recognition and or speech reconstruction from silent speech that is normal speech without glottal activity Speech recognition from silent speech using electromyographic sensors to monitor the articulatory muscles has been introduced in 51 
In 52 an isolated word recognition task from whispered speech is investigated using a special acoustic sensor called non audible microphone NAM In 53 Denby proposes to use ultrasound acquisition of the tongue and video sequences of the lips as visual inputs of an artificial neural network and predict a relative robust LSF Line Spectral Frequency representation of voiced parts of speech This envisioned ultrasound based speech synthetiser could be helpful for patient having undergone a laryngectomy because it could provide an alternative to the tracheooesophageal speech In 54 an approach based on visual speech recognition and concatenative synthesis driven by ultrasound and optical images of the voice organ is introduced This system is based on the building of a one hour audiovisual corpus of phonetic units which associates visual features extracted from Some Experiments in Audio Visual Speech Processing Protocol Mc Group 1 original forged 40 51 30 Miss probability in 20 15 10 5 2 2 5 10 15 20 30 False Alarm 
probability in 40 a DET curve 45 40 35 30 25 20 15 10 5 0 1 Original Client Original Impostor Original Client Forged Impostor 0 5 0 0 5 1 1 5 2 2 5 3 b Score distribution Fig 16 Results obtained using the Multiple Linear Regression approach video to acoustic observations Ultrasound and optical images are coded using a PCA based approach similar to the EigenFaces approach described previously As the visual and audio streams are synchronized the initial phonetic segmentation of the video sequences can be obtained from the temporal boundaries of the phonemes in the audio signal These labels are generated using speech forced alignment techniques Then HMM based stochastic models trained on these visual features sequences are used to predict phonetic targets from video only data Finally a Viterbi unit selection algorithm is used to find the optimal sequence of acoustic units given this phonetic prediction The system is already able to perform phonetic transcription from visual speech data with over 50 correct 
recognition Figure 17 presents an overview of this system and figure 18 shows a typical image of the database in which a lip profile image is embedded into the 52 G Chollet et al Fig 17 Ouisper corpus based synthesis system overview ultrasound image The use of a large word dictionary and the introduction of a Language Model will help improving the rendered signal 4 5 Speaker Indexing One of the most promising on going experiments concerns speaker indexing The goal of this application is to answer automatically the question Who is speaking in a video sequence taking benefit from information extracted from the audio channel and from the video stream This application is clearly audiovisual and is based on many of the tools detailed in the previous sections First faces are located within each frame of the considered video Given a sliding temporal window audio energy is computed A visual feature vector is then attached to each pixel within the image its values over time The audio feature vectors are sampled to 
match with the frame rate and both vectors are normalized Correlations between all these vectors the single audio vector and visual feature vectors attached to each pixel are computed The mean correlation is then computed for each detected face and the one with the greatest value is defined as locating the current speaker First results are depicted in figure 19 While very simple this first method has proven to perform quite well Further experiments are under way focusing especially on the choice of appropriate visual features The idea would then be to fuse the obtained segmentation with face tracking recognition and the speaker segmentation to obtain better results and to be able to extract voice over speech segments Some Experiments in Audio Visual Speech Processing 53 Fig 18 Example of an ultrasound vocal tract image with embedded lip profile Fig 19 Some good localizations of the current speaker green rectangles 5 Conclusion and Perspectives Speech is not only an acoustic signal It is produced by a speaker 
moving his articulators The observation of these movements helps in all aspects of speech processing coding recognition synthesis This chapter described a few ongoing experiments exploiting the correlation between acoustic and visual features of speech It is demonstrated that the correlation of audio and visual information can be exploited usefully in many applications Aknowledgments The research leading to this chapter was supported by the European Commission K SPACE BioSecure SecurePhone by regional fundings InfoM gic and by the Franco Lebanese program CEDRE 54 G Chollet et al References 1 Chollet G Cernocky J Constantinescu A Deligne S Bimbot F Towards ALISP a Proposal for Automatic Language Independent Speech Processing In Computational Models of Speech Pattern Processing NATO ASI Series Series F Computer and System Sciences vol 169 pp Some Experiments in Audio Visual Speech Processing 55 21 Rabiner L A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition Proceedings of the 
IEEE 77 2 56 G Chollet et al 41 Golub G Loan C V Matrix Computations 3rd edn The Johns Hopkins University Press Baltimore MD 1996 42 Pilu M A Direct Method for Stereo Correspondence based on Singular Value Decomposition In Proceedings of CVPR pp Exploiting Nonlinearity in Adaptive Signal Processing Phebe Vayanos Mo Chen Beth Jelfs and Danilo P Mandic Department of Electrical and Electronic Engineering Imperial College London London SW7 2AZ UK foivi vayanos mo chen beth jelfs d mandic ic ac uk Abstract Quantitative performance criteria for the analysis of machine learning architectures and algorithms have been long established However the qualitative performance criteria e g nonlinearity assessment are still emerging To that end we employ some recent developments in signal characterisation and derive criteria for the assessment of the changes in the nature of the processed signal In addition we also propose a novel online method for tracking the system nonlinearity A comprehensive set of simulations in both 
the linear and nonlinear settings and their combination supports the analysis 1 Introduction M Chetouani et al Eds NOLISP 2007 LNAI 4885 pp 58 P Vayanos et al Fig 1 Sketch of the variety of signals spanned by the properties nonlinearity and stochasticity Areas where theoretical knowledge and technology for the analysis of time series are available are outlined such as Chaos and ARMA Modified from Schreiber 1999 significantly changed after processing e g prediction within compression algorithms and denoising the application of such filters will be greatly limited As a consequence such architectures and algorithms will not be suitable in situations where the signal nature is of critical importance for instance in speech processing Notice that the very core of adaptive learning is the change in the shapes of signal spectrum this reflects only the performance in terms of second order statistics and no account of other signal characteristics is provided To that cause we propose a new framework for the assessment 
of qualitative performance in machine learning and set out to investigate whether an improvement in the quantitative performance is necessarily followed by the improvement in the qualitative performance For generality and to illustrate this trade off this is achieved for both the linear and nonlinear filters On the other hand the existing signal modality characterisation algorithms in this area are typically based on hypothesis testing 2 3 4 and describe the signal changes in a statistical manner However there are very few online algorithms which are suitable for this purpose Therefore in this chapter we will also propose to demonstrate the possibility of online algorithms which can be used not only to identify the nature of the signal but also to track changes in the nature of the signal signal modality detection 2 Background Theory Before introducing new criteria for the analysis of qualitative performance in machine learning and the online algorithm for tracking system nonlinearity we set out to provide 
some necessary background focusing on some recent results on signal characterisation Exploiting Nonlinearity in Adaptive Signal Processing 59 2 1 Nature of a Signal By the signal nature 5 6 we adhere to a number of signal properties such as i Linear strict definition Research on signal nonlinearity detection started in physics in the 1990s and out of the several proposed methods the so called surrogate data method introduced by Theiler et al 7 has been extensively used in the context of statistical nonlinearity testing A surrogate time series or surrogate for short is a realisation of a composite null hypothesis In our case this null hypothesis is that the original signal is linear i e generated by a linear stochastic system driven by white Gaussian noise measured by a static monotonic and possibly nonlinear observation function Then a discriminating statistic is calculated for both the original time series and a set of surrogate data If the statistics for the original time series do not lie in the range of 
those for the surrogate data the null hypothesis is rejected and the original data is judged to be nonlinear otherwise it is judged to be linear There exist many discriminating statistics the commonly used ones include the so called third order auto covariance C3 4 and the asymmetry due to time reversal REV 4 In order to increase the power of the surrogate test and decrease the spurious rejections of the null hypothesis several modified methods for the generation of surrogate data have been proposed In this chapter we adopt the iterative amplitude adjusted Fourier Transform iAAFT surrogate method 8 The iAAFT surrogate data have their amplitude spectra similar and their amplitude distribution identical to those of the original time series Techniques described in this chapter rest upon the method of time delay embedding for representing a time series in so called phase space i e by a set of delay vectors DVs x k of a given embedding dimension m that is x k xk m xk T where is a time lag which for simplicity is 
set to unity in all simulations In other words x k is a vector containing m consecutive time samples From Figure 2 a although the wave form and the power spectrum of the two signals are similar to one another distinct difference can be observed in two 60 P Vayanos et al a Time domain waveform power spectrum and the phase space scatter plot of a chaotic signal left column and its surrogate right column b Phase space scatter plot for phonemes ao and s Fig 2 Phase space scatter plot phase space scatter plots There is some sort of structure in the scatter plot for the chaotic signal whereas the surrogate displays randomness in the scatter plot The reason for this is that during the surrogate generation process the temporal and spatial correlations were completely destroyed due to the randomisation scheme Figure 2 b illustrates the attractors for vowel ao and consonant s in phase space scatter plot From the Figure it is clear that these two phonemes differ from one another in nature 2 3 Signal Characterisation 
Tool Delay Vector Variance DVV Method Many methods for detecting the nonlinear structure within a signal have been proposed such as the above mentioned surrogate data with different choices of discriminating statistics Deterministic versus Stochastic DVS plot 9 Method 10 For our purpose it is desirable to have a method which is straightforward to visualise and which makes use of some notions from nonlinear dynamics and chaos theory e g embedding dimension and phase space One such method is our recently proposed Delay Vector Variance DVV method Exploiting Nonlinearity in Adaptive Signal Processing 61 2 which is based upon examining the predictability of a signal in the phase space and examines simultaneously the determinism and nonlinearity within a signal The DVV algorithm can be summarised in the following way For a given optimal embedding dimension1 m where Ntv denotes how fine the standardised distance is uniformly spaced and nd is a parameter controlling the span over which to perform the DVV analysis 2 
rd 1 N N 2 k 1 k rd 2 x 2 We only consider a variance measurement valid if the set k rd contains at least N0 30 DVs since too few points for computing a sample variance yields unreliable estimates of the true variance For more details please refer to 2 5 For a predictable signal the idea behind the DVV method is if two DVs lie close to one another in terms of their Euclidean distance they should also have similar targets The smaller the Euclidean distance between them the more similar targets they have Therefore the presence of a strong deterministic component within a signal will lead to small target variances for small spans rd The minimal 2 target variance min minrd 2 rd is a measure for the amount of noise 2 has an upper present within the time series Besides the target variance min bound which is unity This is because when rd becomes large enough all the DVs belong to the same set k rd Thus the variance of the corresponding target of those DVs will be almost identical to that of the original time series 
In the following step the linear or nonlinear nature of the time series is examined by performing the DVV test on both the original and a number of 1 In this chapter the optimal embedding dimension is calculated by Cao s method 11 since this method is demonstrated to yield robust results on various signals 62 P Vayanos et al DVV plot for AR 4 signal 1 1 DVV plot for Narendra Model 3 Surrogate target variance 2 0 8 Surrogate target variance 2 0 8 0 6 0 6 0 4 Original Signal 0 4 Original Signal 0 2 0 2 0 3 2 1 0 1 2 3 0 6 4 2 0 2 4 6 standardised distance standardised distance a AR 4 Model 1 1 b NARENDRA Model Three 0 8 0 8 surrogates 0 6 surrogates 0 0 2 0 4 0 6 0 8 1 0 6 0 4 0 4 0 2 0 2 0 0 0 0 2 0 4 0 6 0 8 1 original original c d Fig 3 Nonlinear and deterministic nature of signals The upper two diagrams DVV plot are obtained by plotting the target variance as a function of standardised distance The lower two diagrams DVV scatter diagram are obtained by plotting the target variance of the original data 
against the mean of the target variances of the surrogate data surrogate time series2 7 using the optimal embedding dimension of the original time series Due to the standardisation of the distances the DVV plots can be conveniently combined3 within a scatter diagram where the horizontal axis corresponds to the DVV plot of the original time series and the vertical axis to that of the surrogate time series If the surrogate time series yield DVV plots similar to that of the original time series the DVV scatter diagram coincides with the bisector line and the original time series is judged to be linear as shown 2 3 In this chapter all the DVV tests are performed using 19 surrogate data realisations The reason for that is that with the increase in the number of surrogate data DVV test does not yield a much better result whereas the computational complexity is much increased In fact target variance 2 of the original data is plotted against the mean of the d target variance of 19 surrogate data for all 
corresponding distances rd d Exploiting Nonlinearity in Adaptive Signal Processing 63 in Figure 3 c If not the original time series is judged to be nonlinear as depicted in Figure 3 d Since the minimal target variance indicates a strong deterministic component within the signal we conclude that in DVV scatter diagrams the more the curve approaches the vertical axis the more deterministic the nature of the signal This can be employed as a convenient criterion for estimating the level of noise within a signal 3 Qualitative and Quantitative Performance Analysis To assess the quantitative performance of learning algorithms it is convenient to use the standard one step forward prediction gain 12 Rp 10 log10 2 s dB 2 e 3 2 and estiwhich is a logarithmic ratio between the estimated signal variance s 2 mated prediction error variance e On the other hand to assess the qualitative performance that is a possible change in the signal nature introduced by a filter we proposed to compare DVV scatter diagrams of the output 
signals with those of the original signal In the prediction setting the target variances Eq 2 for the predicted signal and its surrogates are obtained by performing the DVV test on the predicted signal For robustness these steps are repeated 100 times If the considered filters yield high prediction gain Rp the quantitative performance of the filters is judged to be good As for the qualitative performance the more similar the DVV scatter diagram for the filtered signal is to that for the original signal the better the qualitative performance of the considered prediction architecture For generality we illustrate the usefulness of the proposed methodology for both linear and nonlinear neural networks adaptive filters and their combinations 4 Experimental Settings To illustrate the effect of the chosen mode of processing linear nonlinear etc we have chosen a general hybrid architecture which is shown to be able to improve the overall quantitative performance as compared to the performance of single modules In 
particular it has been suggested that a cascaded combination of a recurrent neural network RNN and finite infinitive response FIR filter can simultaneously model the nonlinear and linear component of a signal 12 The nonlinear neural filter can model the nonlinearity and a portion of the linearity within a signal while the subsequent linear FIR filter models the remaining linear part of the signal The nonlinear neural filters used in simulations were the dynamical perceptron nonlinear FIR filter trained by the nonlinear gradient descent algorithm NGD and a recurrent perceptron trained by the real time recursive learning RTRL 64 P Vayanos et al 13 algorithm The linear filters considered were standard FIR filters trained by least mean square LMS and recursive least squares RLS algorithms The inputs were a benchmark linear AR 4 signal given by x k 1 79x k 1 1 85x k 2 1 27x k 3 0 41x k 4 n k 4 where n k N 0 1 and a benchmark nonlinear signal the Narendra Model Three given by 14 z k z k 1 r3 k 1 z 2 k 1 r k 1 79 r 
k 1 1 85 r k 2 1 27 r k 3 0 41 r k 4 n k 5 where n k N 0 1 For these signals their DVV scatter diagrams are shown as Figure 3 c and 3 d clearly indicating the linear nature of 4 DVV scatter diagram on the bisector line and nonlinear nature of 5 DVV scatter diagram deviating from the bisector line 4 1 Simulations The first experiment was conducted for prediction of the linear benchmark signal 4 The DVV scatter diagrams show the nonlinearity information about the output of such filters From Figure 4 in terms of preserving the nature of the signal linear in this case both the nonlinear filters and hybrid filters performed well on a linear AR 4 signal indicated by the fact that all the DVV scatter diagrams in Figure 4 lie on the bisector line In terms of the prediction gain Rp the NGD and RTRL performed similarly and as expected the hybrid filters performed better than single nonlinear filters The hybrid filter realised as a cascaded combination of a dynamical perceptron trained by RTRL and FIR filter trained by 
RLS gave the best performance as illustrated in bottom right diagram of Figure 4 Figure 5 illustrates a similar experiment performed on prediction of a much more complex benchmark nonlinear signal 5 From Figure 5 both nonlinear filters trained by NGD and RTRL performed poorly on their own in terms of the prediction gain However from the change of the nature of the original signal seen in Figure 3 d they preserved the nature of the benchmark nonlinear signal better than the hybrid filters even though the quantitative gain Rp for hybrid filters was higher For instance the recurrent perceptron trained by the RTRL exhibited worse quantitative performance but better qualitative performance A hybrid filter consisting of a combination of a dynamical perceptron trained by NGD and an FIR filter trained by LMS showed a considerable increase in gain however the signal was considerably linearised as illustrated by the DVV scatter diagram approaching the bisector line The bottom right diagram in Figure 5 shows the 
performance of a hybrid filter consisting of a recurrent perceptron Exploiting Nonlinearity in Adaptive Signal Processing NGD Standard Single Neural Filter surrogates 1 Rp 4 3364 dB surrogates 0 8 0 6 0 4 0 2 0 0 0 5 original NGD LMS 1 Rp 5 5375 dB Hybrid Filter surrogates surrogates 0 8 0 6 0 4 0 2 0 0 0 5 original 1 1 RTRL 1 Rp 4 5223 dB 0 8 0 6 0 4 0 2 0 0 0 5 original RTRL RLS 1 Rp 5 7917 dB 0 8 0 6 0 4 0 2 0 0 0 5 original 1 1 65 Fig 4 Qualitative and quantitative comparison of the performance between nonlinear neural and hybrid filters for a linear benchmark signal 4 The top panels denote the DVV scatter diagrams for single neural filters feedforward and feedback trained by NGD and RTRL algorithm respectively The bottom panel diagrams relate to hybrid filters trained by RTRL followed by a FIR filter trained by the RLS algorithm This case gave best balance between the quantitative and qualitative performance out of all the combinations of hybrid filters considered The quantitative performance gain for 
this combination was the second best of all the combinations whereas the nature of the signal was preserved reasonably well We now investigate whether exchanging the order of filters within a hybrid filter will affect the overall performance Given the highly nonlinear nature of the problem it is expected that the performances will be significantly different To this end we re ran the experiments for the nonlinear benchmark signal The results of the experiments are shown in Figure 6 Figure 6 confirms that exchanging the order of the modules within a hybrid architecture does not provide the same performance both quantitatively and qualitatively Indeed the quantitative performance are considerably worse and also the nature of the predicted signal changed significantly towards a linear one This can be explained in the following way When a linear filter is placed at the first stage of the hybrid filter it linearise the input signal significantly and the subsequent nonlinear filter will not be able to recover the 
lost information as the system is not aware of presence of a nonlinear signal However if a nonlinear 66 P Vayanos et al NGD RTRL 1 surrogates 0 8 0 6 0 4 0 2 Standard Single Neural Filter 1 surrogates 0 8 0 6 0 4 0 2 0 Rp 0 39167 dB 0 0 5 original NGD LMS 1 0 Rp 0 18712 dB 0 0 5 original RTRL RLS 1 Hybrid Filter 1 surrogates surrogates 0 8 0 6 0 4 0 2 0 1 0 8 0 6 0 4 0 2 Rp 1 78167 dB 0 0 5 original 1 0 Rp 1 12365 dB 0 0 5 original 1 Fig 5 Qualitative and quantitative performance comparison of the performance between nonlinear neural and linear filters for a nonlinear benchmark signal 5 The dotted line denotes the DVV scatter diagram for the original nonlinear benchmark signal Eq 5 whereas the solid line denotes that for the output of the filters filter is placed as the first module it will capture the input signal nonlinearity and inform the system that the upcoming signal is nonlinear in nature so that the subsequent linear filter is able to refine the output 5 Online Nonlinearity Tracking Using Hybrid 
Filters We have shown a novel framework of evaluating the qualitative performance of adaptive filters This is achieved based upon examining the change in signal nature in terms of nonlinearity and determinism which is considered an offline method It is natural to ask whether it is possible to track the system nonlinearity online In 15 one such online approach is considered which relies on parametric modeling to effectively identify the signal in hand Figure 7 shows an implementation of this method which uses a third order Volterra filter nonlinear subfilter and a linear subfilter trained by the normalised LMS NLMS algorithm with a step size 0 008 to update the system parameters The system was fed with the signal y k obtained from I u k i 0 ai x k i where I 2 and a0 0 5 a1 0 25 a2 0 125 6 7 y k F u k k k Exploiting Nonlinearity in Adaptive Signal Processing 67 LMS NGD 1 0 8 surrogates 0 6 0 4 0 2 0 surrogates 1 0 8 0 6 0 4 0 2 RLS RTRL Rp 0 01132 dB 0 0 5 original 1 0 Rp 0 02581 dB 0 0 5 original 1 Fig 6 
Qualitative and quantitative comparison of the performance between inverse order hybrid filters for a nonlinear benchmark signal 5 The filter order is interchanged from the one in previous experiments The dotted line denotes the DVV scatter diagram for the original nonlinear benchmark signal 5 whereas the solid line denotes that for the output of the filters where x k are independent identically distributed and uniformly distributed over the range 0 5 0 5 and k N 0 0 0026 The function F u k k varies with the range of k as follows 3 u k for 10000 k 20000 8 F u k k u2 k for 30000 k 40000 u k elsewhere The signal y k can be seen in the first trace of Figure 7 The second and third traces show the residual estimation errors of the optimal linear system and Volterra system respectively The final trace is the estimated degree of system nonlinearity Whilst these results show that this approach can detect changes in nonlinearity and is not affected by the presence of noise this may be largely due to nature of the 
input signal being particularly suited to the Volterra model In this chapter we propose to overcome this problem be making use of the concept of convexity A convex combination can be described as 16 x 1 y where 0 1 9 as illustrated on Figure 8 The point resulting from the convex combination of x and y will lie somewhere on the line defined by x and y between the two The benefits of using convex optimisation are threefold 68 P Vayanos et al Fig 7 NLMS with Volterra series x x 1 y Fig 8 Convexity y hybrid filter has been proposed in 17 18 in a form that adaptively combines the outputs of the subfilters based on their instantaneous output error In 19 this approach has demonstrated to yield considerable improvement in the steady state and convergence capabilities of the resultant filter While previous applications of hybrid filters have focused on the improved performance they can offer over the individual constituent filters our approach relies on the observation of the evolution of the so called mixing 
parameter over time For example in the standard setting would vary so as to initially favour the faster subfilter learning and finally the filter with the best steady state properties4 In this section we consider hybrid combinations of filters The analysis of then provides insight into the nature of the signal under consideration In particular we focus on quantifying the degree of nonlinearity in a signal As a subset of nonlinearity we also consider the degree of sparsity as sparse signals occur naturally in many real world applications The benchmark signals considered are linear AR 4 or nonlinear by design whereas the real world signals considered in this case are speech data 5 1 Hybrid Adaptive Filter for Signal Modality Characterisation Figure 9 shows a block diagram of a hybrid adaptive filter aimed at signal modality characterisation In this Chapter we focus on tracking the degree of 4 This differs from the traditional search then converge approach since it caters for potentially nonstationary data 
Exploiting Nonlinearity in Adaptive Signal Processing 69 d k e 1 k Nonlinear Filter w1 k y k y k 1 e k x k y k Linear Filter w2 k e 2 k y k 2 y 1 k Fig 9 Block diagram of a hybrid adaptive filter for nonlinearity tracking nonlinearity in a signal by combining the outputs of a linear and a nonlinear subfilter in a hybrid fashion Following the approach from 17 19 the output of such a structure was obtained as y k k y1 k 1 k y2 k T T 10 where y1 k x k w1 k and y2 k x k w2 k are the outputs of the two T T k and w2 k and where x k is subfilters with respective weight vectors w1 the common input vector For simplicity w1 k and w2 k are assumed to be of equal length L 10 and are adapted independently using their own design rules and depending on the property we aim at tracking Parameter k is a mixing scalar parameter which is adapted using a stochastic gradient rule that minimises the quadratic cost function J k e2 k of the overall filter where e k is the output error given by e k d k y k Using LMS type adaptation 
to minimise the error of the overall filter the generic form the the update can be written as 11 k 1 k J k k where is the adaptation step size of the hybrid filter Using 10 and the expression for the output error the partial derivative of the cost function with respect to k can be written as J k k e k y1 k y2 k Then equation 11 can be rewritten as k 1 k e k y1 k y2 k 13 12 70 P Vayanos et al To ensure the combination of adaptive filters remains a convex function was kept in the range 0 k 1 For this purpose in 17 the authors used a sigmoid function to bound k in the range 0 1 Since in order to determine the changes in the modality of a signal we are not interested in the overall performance of the filter but in the variable the use of a sigmoid function would interfere with the true values of k and was therefore not used Instead a hard limit on the set of allowed values for was implemented 5 2 Performance of the Combination on Benchmark Signals In order to illustrate the operation of the convex combination 
aimed at signal modality tracking simulations were initially performed on a set of synthetic signals made by alternating between blocks of linear and nonlinear data 100 runs of independent trials were performed and averaged in the one step ahead prediction setting The linear signal used was a stable AR 4 process given by x k 1 79x k 1 1 85x k 2 1 27x k 3 0 41x k 4 n k 14 where n k N 0 1 is white Gaussian noise WGN The benchmark nonlinear input signal was 14 x k x2 k 1 x k 1 2 5 n k 1 1 x k 1 2 x k 2 2 15 For the experiments the linear adaptive filter was the NLMS Normalised Least Mean Square while the nonlinear filter was the Normalised Nonlinear Gradient Descent NNGD 20 NNGD and NLMS were used as opposed to the standard NGD and LMS algorithms in order to overcome the issue of high dependence of the convergence of the individual subfilters and hence of the combination on input signal statistics Furthermore since these subfilters exhibit a rate of convergence that is potentially faster this alternative also 
increased the speed of adaptation of The nonlinearity at the output of the nonlinear filter was the logistic sigmoid function given by 1 z IR 16 z 1 e z with a slope of 1 Intuitively we expect the linear filter to take over i e 0 when the modality of the input signal is more linear while the output is expected to follow the more nonlinear filter i e 1 when the input is nonlinear see Fig 9 Figure 10 shows the evolution of at the output of the hybrid combination from Figure 9 for a signal alternating between linear and nonlinear every 200 and 100 samples respectively The combination proved robust to changes in stepsizes within the combination and was always capable of tracking the degree of nonlinearity in the input signal provided N LMS N N GD and provided the step size values were such that both subfilters converged Having demonstrated the ability of the combination at tracking the degree of nonlinearity in synthetically generated data we next perform simulations on real world speech data Exploiting 
Nonlinearity in Adaptive Signal Processing 1 0 9 0 8 0 7 Variation of k 0 6 0 5 0 4 0 3 0 2 0 1 0 0 200 400 600 Number of Iteration k 800 1000 Variation of k 1 0 9 0 8 0 7 0 6 0 5 0 4 0 3 0 2 0 1 0 0 200 400 600 Number of Iteration k 800 1000 71 a Input signal nature alternating every b Input signal nature alternating every 200 samples 100 samples Fig 10 Mixing parameter at the output of the hybrid combination from Figure 9 with 20 for input signal nature alternating between linear 14 and nonlinear 15 5 3 Tracking the Degree of Nonlinearity in Speech Data In this section we aim at giving a flavour of the potential of the hybrid adaptive filtering approach on speech data The area of speech modality characterisation is only emerging and in fact only little is known about the nature of speech Recently much effort has been devised in developing accurate models for the speech production system and for characterising the modality of speech It is believed that the accurate knowledge of speech characteristics will 
lead significant advances in several areas of speech processing including speech coding and speech synthesis Typically the vocal tract5 is modeled as an all pole filter i e using a linear difference equation This is mainly due to the solid theory underlying linear systems and to the corresponding decrease in computational complexity However the physical nature of the vocal tract is itself an indication of the potentially nonlinear nature of the radiated speech In fact several studies have suggested that linear models do not sufficiently model the human vocal tract 21 22 Due to its nonstationary nature the characterisation of speech is a complex task and much research has been done recently to study the nonlinear properties of speech and to find an efficient model for the speech signal These studies have typically been based on a classification between vowels and consonants or between voiced and unvoiced sounds6 It is known that all vowels and certain consonants are voiced i e highly periodic in nature with a 
periodic excitation source In the case of unvoiced consonants the folds may be completely open e g for the s sh and f sounds or partially open e g for h sound resulting in a noise like waveform 23 24 25 5 6 The vocal tract is the cavity where sound that is produced at the sound source is filtered It consist of the laryngeal cavity the pharynx the oral and nasal cavities it starts at the vocal folds vocal cords A sound is referred to as being voiced when the vocal folds are vibrating whereas it is voiceless or unvoiced in a contrary case 72 P Vayanos et al H OPE speech S1 0 5 0 0 5 0 1 1000 2000 IS STRONG IN A LL 3000 4000 5000 6000 7000 8000 9000 10000 k nonlinearity 0 5 0 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 1 k sparsity 0 5 0 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 Number of Iterations k Fig 11 Speech signal S1 and corresponding variation of for determining the degree of nonlinearity and sparsity In 26 Kubin shows that there are several nonlinearities in the human vocal tract 
whereas he demonstrates that linear autoregressive models are fully adequate for unvoiced speech In 27 28 29 chaotic behaviour is found in voiced sounds such as vowels and nasals like n and m In 30 the speech signal is modeled as a chaotic process Finally hybrid methods combining linear and nonlinear structures have previously been applied to speech processing 31 32 33 While the majority of the studies so far have suggested a nonlinear nature of voiced speech the form of fundamental nonlinearity is still unknown In 34 it is suggested that speech may contain different types of linear nonlinear characteristics and that for example vowels may be modeled by either chaotic features or types of higher order nonlinear features while consonants may be modeled by random processes For the simulations speech signals S1 and S3 from 35 were first analysed Finally a randomly selected recording from the APLAWD database 36 was considered together with the corresponding laryngograph7 signal All amplitude 7 A laryngograph 
monitors vocal fold closure by measuring variations in the conductance between a transmitting electrode delivering a high frequency signal to the neck on one side of the larynx and a receiving electrode on the other side of the larynx Exploiting Nonlinearity in Adaptive Signal Processing 73 I LL speech S3 0 5 0 0 5 0 1 1000 BE TRYING TO WIN THE 2000 3000 4000 5000 6000 7000 8000 9000 10000 k nonlinearity 0 5 0 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 1 k sparsity 0 5 0 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 Number of Iterations k Fig 12 Speech signal S3 and corresponding variation of for nonlinearity and sparsity tracking signals were standardised so that the amplitude range was between 0 5 0 5 For generality the values of step sizes were kept as in the simulations on stationary data namely N LMS N N GD 0 4 and was varied according to the aim of the experiment larger used for demonstrating the correlation between the laryngograph signal and the evolution of The nonlinearity used in 
the complex NNGD CNNGD algorithm was the hyperbolic tanh function given by x tanh x expx exp x sinh x x IR cosh x expx exp x 17 Prediction was performed in the one step ahead setting short term prediction One may in the future perform simulations using long term prediction i e using a prediction delay of one pitch period as in 37 In order to investigate the potential of using hybrid filters for the purpose of determining the degree of nonlinearity and sparsity in a speech waveform the combination of CNLMS and CNNGD complex linear and nonlinear subfilters and NLMS and SSLMS signed sparse LMS following the approach from 38 were both fed with the speech waveforms S1 and S3 in turn The first trace from Figures 11 and 12 shows the speech waveform while the second and third traces respectively show the corresponding variations of for tracking the degree of 74 P Vayanos et al m 0 5 Speech 0 0 5 2500 500 2600 2700 2800 2900 3000 Lx 0 500 2500 2600 2700 2800 2900 3000 differencial Lx 200 100 0 100 2500 1 2600 2700 
2800 2900 3000 k 0 5 0 2500 2600 2700 2800 2900 3000 Number of Iterations k Fig 13 Speech waveform for letter m corresponding Lx waveform and differential Lx variation of nonlinearity and sparsity in the waveform In the second trace and as above a value of close to 1 indicated the predominantly nonlinear nature of speech and vice versa for 0 Finally in the third trace and for consistency a 1 showed the predominantly sparse nature of the waveform From the figures the expected correlation between nonlinearity and sparsity is confirmed We note that certain parts of a speech signal are better modeled using nonlinear structures 1 while for others linear structures Exploiting Nonlinearity in Adaptive Signal Processing 75 are sufficient 0 Furthermore voiced speech appears to be indicated by regions where exhibits a spiky behaviour From Figure 11 it can be noticed that the noise like sounds z around samples 2800 3200 and s around samples 4100 4200 are linear which agree with previous findings in the field From 
Figure 12 it can be inferred that highly voiced sounds such as a in trying is more nonlinear 5 4 Correlation Between Laryngograph Signal and the Variation of In this section we aim at exploring the relationship between the variation of and the laryngograph waveform Lx For this purpose simulations are performed on the randomly selected speech waveform from the APLAWD database 36 the letter m read by a male speaker Figure 13 shows the speech and corresponding laryngograph waveforms and the evolution of at the output of the hybrid combination of CNLMS and CNNGD From this figure it is clear that there is some correlation between the two waveforms during certain periods of voiced speech In particular it appears that sharp transitions in and in the derivative of the Lx waveform indicating glottal opening instant occur simultaneously the delay between the two waveforms is due to the larynxto microphone delay and estimated in 39 to be of approximately 0 95ms i e 20 References 1 Schreiber T Interdisciplinary 
application of nonlinear time series methods Phys Rep 308 1 76 P Vayanos et al 10 Kaplan D Exceptional events as evidence for determinism Physica D 73 1 Exploiting Nonlinearity in Adaptive Signal Processing 77 30 Townshend B Nonlinear prediction of speech In ICASSP 1991 Proceedings of the International Conference on Acoustics Speech and Signal Processing pp Mixing HMM Based Spanish Speech Synthesis with a CBR for Prosody Estimation Xavi Gonzalvo Ignasi Iriondo Joan Claudi GPMM Grup de Recerca en Processament Multimodal Enginyeria i Arquitectura La Salle Universitat Ramon Llull Quatre Camins 2 08022 Barcelona Spain gonzalvo iriondo jclaudi falias cmonzo salle url edu http www salle url edu tsenyal Abstract Hidden Markov Models based text to speech HMM TTS synthesis is a technique for generating speech from trained statistical models where spectrum pitch and durations of basic speech units are modelled altogether The aim of this work is to describe a Spanish HMMTTS system using an external machine learning 
technique to help improving the expressiveness System performance is analysed objectively and subjectively The experiments were conducted on a reliably labelled speech corpus whose units were clustered using contextual factors based on the Spanish language The results show that the CBR based F0 estimation is capable of improving the HMM based baseline performance when synthesizing non declarative short sentences while the durations accuracy is similar with the CBR or the HMM system 1 Introduction One of the main interests in TTS synthesis is to improve quality and naturalness in general purpose applications Concatenative speech synthesis for a limited domains e g Virtual Weather man 1 presents drawbacks when used in a different domain and new recordings become time consuming and expensive In contrast the main benefit of HMM TTS is the capability of modelling voices in order to synthesize different speaker features styles and emotions In that sense voice transformation with concatenative speech synthesis 
still requires large databases in contrast to HMM which can obtain better results with smaller databases 2 e g speaker interpolation 3 or eigenvoices 4 Furthermore language is a key topic during the design of a TTS and HMM synthesis has also been used to design polyglot systems 5 The HMM TTS scheme based on contextual factors for clustering can be used for any language e g English 6 Portuguese 7 or Japanese Spanish for the polyglot system 5 Basic synthesis units Thanks to Prof Dr Eric Keller University of Lausanne for kindly spending a time on verifying this paper This work has been partially supported by the European Commission project SALERO FP6 IST 4 027122 IP M Chetouani et al Eds NOLISP 2007 LNAI 4885 pp Mixing HMM Based Spanish Speech Synthesis with a CBR 79 i e phonemes and their context attributes values pairs are the main language dependent information As a result HMM TTS systems provide a stable synthesis performance though it sometimes presents a lower quality and a plain expressiveness in 
comparison with concatenative systems For the former drawback some techniques have been shown to improve the quality i e Global Variance or GV 8 and for the latter this work presents a mixed F0 approach HMM CBR for improving the expressiveness for delivering different types of sentences with the original speaker style The HMM TTS system presented in this work is based on a source filter model approach to generate speech directly from HMM itself in contrast to other approaches that unified concatenative and HMM approaches 9 It uses a decision tree based on context clustering in order to improve model training and to characterize phoneme units introducing a counterpart approach with respect to English 6 As the HMM TTS system is a complete technique to generate speech this research presents objective results to measure its performance as a prosody estimator and subjective measures to test the synthesized speech It is compared with a tested Machine Learning strategy based on case based reasoning CBR for prosody 
estimation 10 This paper is organized as follows Section 2 describes the HMM system workflow parameter training and synthesis Section 3 concerns CBR for prosody estimation Section 4 describes the mixed F0 Section 5 presents measures and discusses the results and section 6 presents the concluding remarks and future planned research 2 2 1 HMM TTS System Training and Synthesis As in any HMM TTS system two stages are distinguished training and synthesis Figure 1 depicts the classical training and synthesis workflow dotted lines stand for the optional F0 generation from the CBR module First HMM for isolated phonemes each HMM represents a contextual phoneme are estimated and each of these models are used as an initialization of the contextual phonemes Then similar phonemes are clustered by means of a decision tree using contextual information and specific questions Unseen units during the training stage can be synthesized using these decision trees Each contextual phoneme HMM definition includes spectrum F0 and 
state durations Topology used is a 5 states left to right with no skips Each state is represented with 2 independent streams one for spectrum and another for pitch Both types of information are completed with their delta and delta delta coefficients Spectrum is modelled by 13th order mel cepstral coefficients which can generate speech with the MLSA Mel Log Spectrum Approximation filter 11 The spectrum model is a multivariate Gaussian distribution 2 The Spanish corpus was pitch marked using the approach described in 12 This algorithm refines mark up to get a smoothed F0 contour in order to reduce discontinuities in the generated 80 X Gonzalvo et al curve for synthesis The model is a multi space probability distribution 2 that may be used in order to store continuous logarithmic values of the F0 contour and a discrete indicator for voiced unvoiced State durations of each HMM are modelled by a Multivariate Gaussian distribution 13 Its dimensionality is equal to the number of states in the corresponding HMM 
Speech corpus Parameter analysis HMM Training Text to synthesize Context clustering Context Label HMMs Excitation generation MLSA filter CBR Model Synthetic speech Parameter generation Fig 1 Training and synthesis workflow Once the system has been trained it provides a set of phonemes represented by a contextual factor The first step in the synthesis stage produces a complete contextualized list of phonemes from a text to be synthesized Chosen units are converted into a sequence of HMMs Durations are estimated to maximize the probability of state durations and then using the algorithm proposed by Fukada in 11 spectrum and F0 parameters are generated from HMM models using dynamic features The excitation signal is generated from the F0 curve and the voiced and unvoiced information Finally in order to reconstruct speech the system uses spectrum parameters as the MLSA filter coefficients and excitation as the input signal 2 2 Context Based Clustering The decision trees used to perform the clustering are based on 
information referring to spectrum F0 and state durations and are designed independently because they are affected by different contextual factors As the number of contextual factors increases there is less data to train the models To deal with this problem the clustering scheme presented in 2 will be used to provide the HMMs with enough samples as some states can be shared by similar units Text analysis for HMMTTS based decision tree clustering was carried out by Festival 15 updating an existing Spanish voice for linguistic analysis Spanish HMM TTS required the design of specific questions to use in the tree Table 2 2 enumerates the main features and contextual factors taken into account Correct questions will determine clusters to reproduce a fine F0 contour in relation with the real intonation 3 CBR System Description As shown in figure 1 CBR system for prosody estimator can be included as a module in any TTS system i e excitation signal can be created using either Mixing HMM Based Spanish Speech Synthesis 
with a CBR Table 1 Spanish phonetic features and contextual factors Unit Vowel Phoneme Features Frontal Back Half open Open Closed lateral Rhotic palatal labio dental Interdental Prepalatal plosive nasal fricative Syllable Word Phrase Stress position in word vowel POS syllables End Tone Preceding next stress phonemes stressed syllables Preceding next POS syllables Preceding next syllables Consonant Dental velar bilabial alveolar Contextual factor 81 Preceding next Position in syllable HMM CBR or the mixed system proposed in this work It was shown in 10 that a CBR approach is appropriate to create prosody even with expressive speech The CBR strategy was originally designed for retrieving mean phoneme information related to F0 energy and duration but this research compares the F0 and the duration with the HMM based estimator This CBR system is a corpus oriented method for the quantitative modelling of prosody Analysis of texts is carried out by the SinLib library 14 an engine developed to Spanish text analysis 
Prosody cases are built from features extracted from texts i e a set of attribute value pair These are based on the accentual group AG that incorporates syllable influence and is related to speech rhythm and the intonational group IG Structure at IG level is reached by concatenating AGs Main features to characterize each intonational unit are position of AG in IG number of syllables accent type IG phrase position IG as interrogative declarative or exclamative stressed syllable position duration values or polynomial coefficients for the F0 estimation The system training can be divided in two stages selection and adaptation Each sentence is analysed in order to convert it into a new case i e a set of attribute value pairs In order to optimize the system case reduction is carried out by grouping similar attributes Once the memory of cases is created the goal is to obtain a solution that best matches the new problem i e the most similar case Mean F0 per phoneme is retrieved by first estimating phoneme durations 
normalizing temporal axis and computing the mean pitch for each phoneme using the retrieved polynomial Attributevalue extraction CBR training Speech corpus Parameter extraction CBR Model Texts Fig 2 CBR Training workflow 82 X Gonzalvo et al 4 Mixed F0 The aim of using an external F0 estimation is to build a better excitation signal able to produce an improved expressiveness The standard HMM F0 estimation is over smoothed due to the statistical processing during the training stage and the CBR approach produces a F0 that can be too high to be synthesized by the HMM TTS Moreover in the work presented in 8 it was not clearly shown that the GV technique was able to improve the quality when applied only to F0 In this work the final F0 contour is produced as the mean value of both approaches hence it fuses the advantages of stability HMM and high expressiveness CBR Notice that the CBR estimation can be viewed as the global variance for a specific sentence 5 Experiments Experiments were conducted on the corpus to 
evaluate objective and subjective measures On the one hand objective measures compare real prosody F0 and duration with HMM TTS and CBR system estimations On the other hand subjective results validate the Spanish synthesis1 Results are presented for various phrase types and lengths number of phonemes Phrase lengths classification is referenced to the corpus average length Thus a short S and a long L sentence are below and above the standard deviation Since both fundamental frequency and duration accuracy estimations are crucial in a source filter model approach and are important factors for the expressiveness modelling the evaluation of these parameters becomes a key step 17 Objectives 1 See http www salle url edu gonzalvo hmm for some synthesis examples Mixing HMM Based Spanish Speech Synthesis with a CBR 83 CBR 60 50 40 30 20 10 0 HMM MixedF0 52 9 50 CBR 44 30 43 1 36 3 36 55 32 2 40 30 Hz z HMM 35 6 MixedF0 37 9 39 6 37 21 29 0 28 2 27 53 42 2 34 7 33 0 32 65 32 01 34 2 33 0 32 04 Hz 20 10 0 DEC EXC INT 
VS S L VL Fig 3 RMSE for F0 contour a phrase type b phrase length for interrogative type 25 23 23 44 23 98 23 05 22 60 23 05 23 83 22 37 mSeconds 35 30 25 20 15 10 5 0 27 96 27 65 27 96 27 65 mSeconds 24 34 24 94 27 74 27 74 23 16 22 98 23 83 21 19 17 15 DEC EXC INT VS S L VL CBR HMM CBR HMM Fig 4 RMSE for duration a phrase type b phrase length measures evaluate the RMSE i e estimated vs real of the F0 contour figure 3 and the mean duration of each phoneme figure 4 In the one hand figure 3 a shows that both systems CBR and HMM present a similar performance except for the case of interrogative sentences For this kind of sentence short ones are worse see figure 3 b On the other hand the HMM and the CBR system have a similar RMSE for duration figure 4 though interrogative are worse with HMM and VL worse with CBR A real example is presented in figure 5 b For the short declarative part frames 350 both HMM and CBR estimate a similar F0 contour However for the short and interrogative part frames 350 the CBR 
approach becomes a better approach because it reproduces fast changes better The mixed F0 contour presents variations when the expressiveness must be improved thanks to the CBR system hence the final excitation signal produces a better intonation 5 2 Subjective Evaluation The aim of the subjective measures see figure 5 a is to assess synthesized speech from HMM TTS using HMM based F0 estimators and a mixed F0 system using HMM and CBR The preference of the users is biased towards the mixed system with INT or EXC sentences In the case of EXC sentences the preference for the mixed system decreases and the non distinction option is bigger than for the INT case Although the expressiveness is also improved in this case some of the synthesized sentences were harder to be distinguished and even the mixed system intonation was subjectively considered not appropriate in some cases though it 84 X Gonzalvo et al HMM 100 Pref ference 80 60 40 20 0 INT EXC 10 00 23 33 30 56 66 67 50 00 Hz Equals MixedF0 350 300 250 200 19 
44 150 100 50 0 100 200 300 400 Frames 500 Original CBR HMM Mixed 600 Fig 5 a Preference for phrase type b Example of F0 estimation for HMM TTS Su sistema de was closer to the original recording DEC sentences were not included in the test due to the similarity of the synthesis i e similar RMSE in figure 3 a 6 Conclusions and Future Work A Spanish HMM TTS was presented here and its performance was compared with CBR for F0 and duration estimation The HMM system performance has been analysed by objective and subjective measures Objective measures showed that HMM prosody reproduction depends on the phrase type and length For declarative sentences HMM and CBR has a similar RMSE performance Exclamative and interrogative sentences i e intonational variations are better reproduced by the F0 CBR estimator This can be explained since the CBR approach uses AG and IG attributes in a polynomial interpolation to retrieve a changing F0 contour that becomes a better solution in non declarative phrases and low contextual 
information cases Moreover HMM and CBR are similar estimators for durations so the CBR estimation contributes little though more research will be carried out in this respect The final mixed F0 contour improved the expressiveness when we used HMM as the stable information as well as CBR for the desirable variance Subjective measures validated the HMM TTS synthesis using the mixed F0 In addition notice that the CBR approach involves a low computational cost and that HMM training process is able to model all parameters together in a HMM taking advantage of voice analysis and transformation Future HMM TTS system could include AG and IG in its features to improve F0 estimation Furthermore vocoded speech produced by the HMM TTS system will be improved when a mixed excitation technique is applied using well defined models of the parametrized residual excitation that will be implemented using a multi band mixing structure Also the main power of HMM TTS is the statistical modelling of information Regarding this the 
prosody study shown in this work serves as the starting point for future systems using voice modification Furthermore the HMM TTS systems are also able to model a voice with a reduced corpus so it would be interesting to set the minimum number of sentences needed to produce a good synthesis in terms of naturalness and expressiveness 17 Mixing HMM Based Spanish Speech Synthesis with a CBR 85 References 1 Objective and Subjective Evaluation of an Expressive Speech Corpus Ignasi Iriondo Santiago Planet Joan Claudi GPMM Grup de Recerca en Processament Multimodal Enginyeria i Arquitectura La Salle Universitat Ramon Llull C Quatre Camins 2 08022 Barcelona Spain iriondo splanet jclaudi falias salle url edu Abstract This paper presents the validation of the expressiveness of an acted oral corpus produced to be used in speech synthesis Firstly an objective validation has been conducted by means of automatic emotion identification techniques using statistical features extracted from the prosodic parameters of speech 
Secondly a listening test has been performed with a subset of utterances The relationship between both objective and subjective evaluations is analyzed and the obtained conclusions can be useful to improve the following steps related to expressive speech synthesis 1 Introduction There is a growing tendency toward the use of speech in human machine interaction by incorporating automatic speech recognition and speech synthesis The recognition of emotional states or the synthesis of emotional speech can improve the communication by doing it more natural 1 Therefore one of the most important challenges in the study of the expressive speech is the development of oral corpora with authentic emotional content that enable robust analysis according to the task for which they are developed It is not the objective of the present work to carry out an exhaustive summary of the available databases for the study of emotional speech since recently complete studies have appeared in the literature In 2 a new compilation of 48 
databases is presented showing a notable increase of multimodal databases In 3 the databases used in 14 experiments of automatic detection of the emotion are summarized Finally in 4 a revision of 64 databases of emotional speech is done providing a basic description of each one and its application Section 2 introduces different aspects about expressive speech Section 3 explains the production of our corpus Section 4 details the process of the objective validation carried out using techniques of automatic emotion identification Section 5 concerns subjective evaluation by means of a listening test and finally the conclusions are presented in Section 6 This work has been partially supported by the European Commission project SALERO FP6 IST 4 027122 IP M Chetouani et al Eds NOLISP 2007 LNAI 4885 pp Objective and Subjective Evaluation of an Expressive Speech Corpus 87 2 Building Emotional Speech Corpora According to 5 four aspects have to be considered for building emotional speech corpora i the scope number 
genre and age of speakers language dialects and emotional states ii the context where an utterance takes place emotional significance related to semantics prosody facial expression and gestures iii the descriptors that represent the linguistic emotional and acoustic content of the speech and iv the naturalness which will depend on the strategy followed to obtain the emotional speech With respect to the latter the main debate is centered on the compromise between authenticity and audio quality Campbell 1 and 88 I Iriondo et al This work combines methods of both types of studies On the one hand the production of the corpus follows the guidelines of the studies centered on the listener since it is oriented to speech synthesis On the other hand we apply techniques of emotion recognition in order to validate its expressive content 3 Our Expressive Speech Corpus We considered the development of a new expressive oral corpus for Spanish due to lack of availability of a corpus with the suitable characteristics within 
the framework of our research in expressive speech synthesis This corpus had a twofold purpose to learn the acoustic models of emotional speech and to be used as the speech unit database for the synthesizer This section describes the steps followed in the production of the corpus For the recording a female professional speaker was chosen to read texts from different categories with the suitable style stimulated acted speech For the design of texts semantically related to different expressive styles an existing textual database of advertisements was used Based on a study of the voice in the audio visual publicity 8 five categories of the text corpus were chosen and the most suitable emotion style was assigned to each one new technologies neutral mature education joy elation cosmetic style sensual sweet automobiles aggressive hard and trips sad melancholic A set of phrases was selected from each category by means of a greedy algorithm 9 allowing phonetic balance for each style This type of algorithms take the 
locally optimum choice at each stage with the hope to find an adequate global solution Therefore the application of this algorithm to the raised problem will obtain a valid solution although may be not the optimum one In addition sentences that contain exceptions e g foreign words abbreviations were avoided to make easy the automatic phonetic transcription and labeling The recording of the oral corpus was carried out in a professional recording studio Speech signals were sampled at 48 KHz and quantized using 24 bits per sample and stored in WAV files A forced time alignment using Hidden Markov Models from the phonetic transcription was conducted for the corpus segmentation in phrases and later there was a manual review The result of this alignment also was used to segment the phrases in phonemes The recorded database has 4638 sentences and it is 5 h 12 min long 4 Objective Validation The goal of this experiment was to validate the expressiveness of the corpus by means of automatic emotion identification 
using different data mining techniques applied to statistical features computed from prosodic parameters of speech An exhaustive subjective evaluation of the full corpus more than 5 hours of speech would be unfeasible Objective and Subjective Evaluation of an Expressive Speech Corpus 89 4 1 Acoustic Analysis Prosodic features of speech fundamental frequency energy duration of phones and frequency of pauses are related to vocal expression of emotion 10 In this work an automatic acoustic analysis of the utterances is performed using information from the previous phonetic segmentation F0 related parameters The analysis of the fundamental frequency F0 is based on the result of the pitch marker described in 11 This system assigns marks over the whole signal The unvoiced segments and silences are marked using interpolated values from the neighboring voiced segments For each phrase three sequences of local F0 values are computed complete excluding silences and unvoiced sounds and only the stressed vowels The 
information about the boundaries of voiced unvoiced V UV segments and silences is obtained from the corpus labeling Notice that if the phonetic segmentation was not available an automatic voice activity detector VAD and a V UV detector would be required 12 Moreover F0 is calculated in linear and logarithmic scales Energy related parameters For energy speech is processed with 20 ms rectangular windows and 50 of overlap calculating the energy linear and dBs every 10ms Following the same idea that for F0 three sequences per utterance are generated complete excluding silences and only in the stressed vowels Rhythm related parameters The duration of phones is an important cue for vocal expression of emotion However some studies omit this parameter due to the difficulty to obtain it automatically 12 In the present work we have incorporated this information to generate datasets with and without this information in order to contrast its relevance Usually z score has been employed for duration modeling in text to 
speech synthesis to predict individual segment duration and to control the speed of the delivery Therefore we incorporate rhythm information using the z score duration of each phoneme as a means to analyze the temporal structure of the speech 13 Also a sequence with only the values for the stressed vowels is computed Moreover two pausing related parameters are added for each utterance the frequency and duration of pauses 4 2 Statistical Analysis and Datasets The prosody of an utterance is represented by 5 sequences of values by phoneme F0 linear and logarithmic energy linear and dB and normalized durations zscore For each sequence the first and the second derivative are calculated For all these sequences the following statistics are obtained mean variance maximum minimum range skew kurtosis quartiles and interquartilic range As a result 464 parameters by utterance are calculated considering both parameters related to the pausing This set of parameters was divided into different subsets according to different 
strategies to reduce the dimensionality see figure 1 The first criterion was to omit the second derivative from Data1 to Data2 in order to assess the 90 I Iriondo et al UHGXFWLRQ 7 RJ 1 G 1DYDVHW DO RJ 1 G 7 UHGXFWLRQ 7 7 7 1 7 7 2QO 6WUHVVHG RPSOHWH UHGXFWLRQ 2QO 6WUHVVHG RPSOHWH 7 6 7 7 1 7 6 7 LWKRXW QG GHULYDWLYH Fig 1 Generation of different datasets Table 1 Learning Algorithms used for the automatic recognition experiment Name J48 B J48 Part B Part DT B T IB1 IBk NB SMO1 SMO2 Description mean 95 CI max Data Decision tree based on C4 5 93 significance of this function Secondly two new datasets were generated without the linear versions of both F0 and energy due to preliminary experiments showed better results for the logarithmic versions Also both Data1L and Data2L were divided in two new sets considering all the phonemes or only the stressed vowels Moreover an automatic reduction of both initial datasets was carried out by means of the simple genetic algorithm GA implemented in Weka 14 Data1G and 
Data2G Finally two similar datasets to 12 were generated to test the significance of omitting the timing parameters Data1N and Data1NG 4 3 Experiments and Results Numerous schemes of automatic learning can be used in a task such as classifying the emotion from the speech analysis The objective evaluation of expressiveness in our speech corpus is based on 15 where a large scale data mining experiment about the automatic recognition of basic emotions in short utterances was conducted After different preliminary experiments the set of machine learning algorithms shown in table 1 was selected in order to be tested with the different datasets Some algorithms were completed with their boosted versions that achieve better results although they present a greater computational cost All the experiments were carried out using Weka software 14 by means of tenfold cross validation Both tried versions of SMO Support Vector Machine of Weka obtain the best results so much on average as in maximum value see table 1 SMO 
algorithms achieve the highest results with Data1G showing Objective and Subjective Evaluation of an Expressive Speech Corpus J48 100 00 98 00 96 00 91 BoostJ48 PART BoostP DT BoostDT IB1 IBk NaiveBayes SMO1 SMO2 Identification 94 00 92 00 90 00 88 00 86 00 84 00 82 00 80 00 DATA1G DAT A1L DATA1LC DATA1N DATA1NG DATA1LS J48 100 00 98 00 96 00 BoostJ48 PART BoostP DT BoostDT IB1 IBk NaiveBayes SMO1 SMO2 Identific ation 94 00 92 00 90 00 88 00 86 00 84 00 82 00 80 00 DATA2G DATA2L DATA2LC DAT A2LS Fig 2 Identification percentage for the ten tested datasets that the dimensionality reduction based in GA helps to these systems although differences with Data1L and Data1LC are minimum However other algorithms i e J48 IB1 and IBk work better with datasets generated by two consecutive reductions without 2nd derivative and latter GA reduction And finally we can observe that there is a third group of algorithms that work better if the linear logarithmic redundancy of F0 and energy is removed Also we can observe that 
the boosted versions improve significantly the results respect to their corresponding algorithms Figure 2 shows a comparison between different datasets depending on the algorithm Notice that Data1LC obtains almost the same results than Data1G and Data1L but with less than the half of parameters The same effect is presented in the datasets without the 2nd derivative Results experiment a slight loss when timing parameters are removed Data1N and Data 1NG However results worsen significantly when parameters are calculated only in the stressed vowels Data1LS and Data2LS Table 2 shows the confusion matrix with the average results for the eleven classifiers with Data2G that has achieved the best mean percentage of identification 97 02 5 Subjective Evaluation Subjective evaluation allows to validate the expressiveness of acted speech from the user viewpoint An exhaustive evaluation of corpus would be excessively tedious the corpus has 4638 utterances For each style 96 utterances were chosen having done a total of 
480 This test set was divided in 4 subsets having 120 utterances each one An ordered pair of subsets was assigned to each subject Therefore 12 different tests were generated The allocation of ordered pairs tries to compensate the fact that the second test could be easier to evaluate due to the previous training 92 I Iriondo et al AGR 1 100 AGR 2 HAP 1 HAP 2 S AD1 S AD2 NEU1 NEU2 S EN1 S EN2 AVG 1 AVG 2 90 100 95 90 85 80 75 70 65 60 55 50 Identific ation Identification 80 70 60 AGR T es t1 HAP T es t2 S AD T es t3 NE U T es t4 Avg S EN 50 40 Fig 3 Percentage of identification de Fig 4 Boxplots for first second round depending on the test and average pending on the style and the average Table 2 Average confusion matrix for Table 3 Average confusion matrix for the the automatic identification Data2G subjective test AGR HAP SAD NEU SEN Agr 99 1 1 6 0 2 0 2 0 0 Hap 0 8 97 1 0 1 0 9 0 1 Sad 0 1 0 0 99 3 0 4 0 2 Neu 0 0 1 2 0 4 93 9 4 9 Sen 0 0 0 2 0 1 4 5 94 8 AGR HAP SAD NEU SEN Agr 82 7 15 6 0 0 5 3 0 0 Hap 14 
2 81 0 0 0 1 3 0 1 Sad 0 1 0 1 98 8 0 7 5 7 Neu 1 8 1 9 0 5 86 4 4 7 Sen 0 1 0 2 0 6 3 6 86 8 Dk A 1 1 1 2 0 1 2 7 2 6 A forced answer test was designed with the Objective and Subjective Evaluation of an Expressive Speech Corpus 93 by the subjects Also the influence of order has been studied In average the second round obtains better results than the first especially for neutral sensual and aggressive styles see figure 4 6 Conclusion and Future Work In this paper the production of an oral corpus oriented to expressive speech synthesis has been presented We performed subjective listening test and objective automatic emotion identification evaluation in order to validate its expressive content showing good results The advantage of the automatic experiments is that they are performed over the whole corpus while the listening test comprises a subset of utterances In future we will introduce voice quality parameterization in addition to prosody to minimize the confusion between sensual and neutral styles Moreover 
this work should serve to analyze the bad classified utterances in order to eliminate them and to improve the latter modeling and synthesis processes References 1 Campbell N Databases of emotional speech In Proceedings of the ISCA Workshop on Speech and Emotion pp 94 I Iriondo et al 12 Navas E On the Usefulness of Linear and Nonlinear Prediction Residual Signals for Speaker Recognition Marcos Faundez Zanuy Escola Abstract This paper compares the identification rates of a speaker recognition system using several parameterizations with special emphasis on the residual signal obtained from linear and nonlinear predictive analysis It is found that the residual signal is still useful even when using a high dimensional linear predictive analysis On the other hand it is shown that the residual signal of a nonlinear analysis contains less useful information even for a prediction order of 10 than the linear residual signal This shows the inability of the linear models to cope with nonlinear dependences present in 
speech signals which are useful for recognition purposes 1 Introduction Several parameterization techniques exist for speech 17 and speaker 15 recognition cepstral analysis and its related parameterizations such as Delta Cepstral features Cepstral Mean Subtraction etc being the most popular There are two main ways to compute the cepstral coefficients and one important drawback in both cases relevant information is discarded as follows 1 LP derived cepstral coefficients The linear prediction analysis produces two main components the prediction coefficients synthesis filter and the residue of the predictive analysis This latter signal is usually discarded However experiments exist 9 where it is shown that human beings are able to recognize the identity of the speaker listening to residual signals of LP analysis Based on this fact several authors have evaluated the usefulness of the LPC residue and have found that although the identification rates using this kind of information alone does not perform as well as 
the LP derived cepstral coefficients a combination of both can improve the results 20 12 14 22 11 Fourier Transform derived cepstral coefficients Instead of working out a set of Linear prediction coefficients are based on the power spectrum information where phase information has been discarded 19 proposed the use of new acoustic features based on the short term Fourier phase spectrum The results are similar to the LP derived cepstral coefficients Although these phase spectrum features cannot outperform the classical cepstral parameterization the results are improved using a combination of both features 2 In this paper we will focus on the first kind of parameterization because they are a clear alternative to the nonlinear predictive models which have shown an improvement M Chetouani et al Eds NOLISP 2007 LNAI 4885 pp 96 M Faundez Zanuy over the classical linear techniques in several fields for a recent overview about these techniques 7 In 4 6 we proposed a new set of features and models based on these types 
of nonlinear models and an improvement was also found when this information was combined with the traditional cepstral analysis but so far the relevance of the residual signals from linear and nonlinear predictive analysis has not been studied and compared In this paper we will study if the relevance of the residual signal is due to an insufficient linear predictive analysis order or because of the incapability of the linear analysis to model nonlinearities present in speech and demonstrate is usefulness for speaker recognition purposes This important question has not been solved in previous papers that focus on a typical 8 to 16 prediction order 2 Experiment Setup 2 1 Database For our experiments we have used the Gaudi database 16 We have used one subcorpora of 49 speakers acquired with a simultaneous stereo recording with two different microphones The speech is in wav format with a sampling frequency fs 16 kHz 16 bit sample and the bandwidth is 8 kHz From this database we have generated narrow band signals 
using the potsband routine that can be downloaded from 21 This function meets the specifications of G 151 for any sampling frequency Thus our study has been performed on telephone bandwidth 2 2 Identification Algorithm In this study we are only interested in the relative performance between linear and nonlinear analyses Thus we have chosen a simple algorithm for speaker recognition In the training phase we compute for each speaker empirical covariance matrices based on feature vectors extracted from overlapped short time segments of the speech signals As features representing short time spectra we use both linear prediction cepstral coefficients LPCC and mel frequency cepstral coefficients melceps 3 In the speaker recognition system the trained covariance matrices for each speaker are compared with an estimate of the covariance matrix obtained from a test sequence from a speaker An arithmetic harmonic sphericity measure is used in order to com1 1 pare the matrices 1 d log tr Ctest C j tr C j Ctest 2 log l 
where tr denotes the trace operator l is the dimension of the feature vector Ctest and Cj is the covariance estimate from the test speaker and speaker model j respectively 2 3 Parameterizations We have used the following parameterizations 1 2 3 LP derived cepstral coefficients LPCC Fourier transform derived cepstral coefficients melceps LP residue coefficients The first two first parameterizations can be found for instance in 17 15 3 while the third is proposed in 11 and will be described in more detail next On the Usefulness of Linear and Nonlinear Prediction Residual Signals 97 Feature extraction from the LP residual signal We will use the Power Difference of Spectrum in Subband PDSS obtained as follows 11 1 2 3 4 Calculate the LP residual signal using the Pth order linear prediction coefficients Calculate the Fast Fourier Transform fft of the LP residual signal using zero padding in order to increase the frequency resolution S fft residue Group power spectrum into P subbands Calculate the ratio of the 
geometric to the arithmetic mean of the power spectrum 2 in the ith Hi Ni S k k L subband and subtract it from 1 PDSS i 1 i Hi where 1 S k N i k Li th 1 N i H i Li 1 is the sample number of frequency points in the i subband and Li Hi is the lower and upper limit of frequency in ith subband respectively We have used the same bandwidth for all the bands x n Inverse filtering FFT Channel 1 V 1 1 Linear predictor a1 T T a2 T aP Channel 2 Channel M Hi Ni S k k L 1 i Hi 1 S k N i k Li V 2 V M Fig 1 LP residual signal parameterization PDSS can be interpreted as the subband version of spectral flatness measure for quantifying the flatness of the signal spectrum Figure 1 summarizes the procedure 3 New Possibilities Using Non linear Predictive Analysis Although the relevance of residual NL predictive analysis for speaker recognition has not been studied previously nonlinear predictive analysis has been widely studied in the context of speech coding For instance 5 revealed that a forward ADPCM scheme with nonlinear 
prediction can achieve the same Segmental Signal to Noise Ratio SEGSNR as the equivalent linear predictive system same prediction order with one less quantization bit We propose an analogous scheme replacing the linear predictor with a nonlinear predictor Figure 2 shows the scheme We have used a Multi Layer Perceptron MLP The structure of the neural net has 10 inputs 2 neurons in the hidden layer and one output The selected training algorithm was the Levenberg Marquardt 10 The number of epochs has been set up to 6 First layer and hidden layer transfer functions are tansig while the output layer is linear 98 M Faundez Zanuy x n Inverse filtering NNET predictor Channel 1 FFT V 1 1 Channel 2 Channel M Hi Ni S k k L 1 i Hi 1 S k N i k Li V 2 V M T T T Fig 2 Block diagram used to calculate PDSS parameters from NL prediction residual signal 4 Experimental Results Obviously one important question when dealing with residual LP signals is Is the information contained in this residual signal coming from an 
insufficient predictive analysis order That is what happens when the prediction analysis order is so high that it is not possible to extract more relevant information using a linear analysis The experimental approach used to solve this question is to use a number of LP coefficients higher than usual Two possible results can be obtained 1 When the analysis order is increased the discriminative power of the residual signal is reduced to simple chance results This means that there is potential for speaker recognition rate improvements through extraction of the LP coefficients in a more efficient manner probably by increasing the number of coefficients When the analysis order is increased the residual signal still contains useful information This means that a linear analysis is unable to extract this information and there is room for improvement combining parameterizations defined on the LP coefficients and the residual signal In order to obtain the optimal results both signals should be extracted and optimized 
jointly 100 90 80 Identification rate 70 60 50 40 30 20 5 10 opinion fusion MEL residue melcepstrum LPC P residue LPCC opinion fusion LPCC residue LPC 80 residue MLP 10x2x1 residue 15 20 25 30 P Vector dimension 35 40 2 Fig 3 Identification for several parameterization algorithms On the Usefulness of Linear and Nonlinear Prediction Residual Signals 99 Figure 3 shows the results obtained with the following parameterizations Melcepstrum The residual signal of an LPC 80 analysis can produce a recognition rate higher than 80 for a 15 dimensional vector extraction Thus it was found that the residual signal of a LP analysis contains relevant information and this is due to the inability to extract this information using a linear analysis 80th order analysis is enough to model short term and long term dependencies between samples but if the analysis is linear it is limited to linear dependencies The residual signal of a nonlinear predictive analysis as expected produces the lower recognition rates because the 
relevant information has been retained in the predictor coefficients However a maximum of 70 recognition rate is possible 4 1 Opinion Fusion One way to improve the results is by means of a combination of different classifiers opinion 13 In our case we will use the same classifier scheme but different parameterizations In order to study the complementarity of the parameterizations studied we have computed the correlation coefficient and scatter diagrams Table 1 shows the correlation coefficients between distances of several parameterizations The higher the correlation the smaller the complementarity of both measures Figure 2 shows a scatter diagram which represents points on a two dimensional space The coordinates correspond to the obtained distance measures which correspond to each parameterization one in each axis Looking at the diagram we observe that the points diverge from a strip Thus they have complementary information and can be combined in order to improve the results Table 1 Correlation coefficients 
between obtained distance values for P 20 LPCC melceps LP 20 resid LP 80 resid MLP 10x2x1 LPCC Mel ceps 0 79 0 79 0 68 0 69 0 52 0 56 0 55 0 62 LP 20 resid 0 68 0 69 0 78 0 64 LP 80 resid 0 52 0 56 0 78 0 60 MLP 10x2x1 0 55 0 62 0 64 0 60 When combining different measures special care must be taken for the range of the values If they are not commensurate some kind of normalization must be applied We have tested the following based on a sigmoid function 18 oi where ki oi mi 2 i 2 i 1 1 e ki oi 0 1 and oi is the initial opinion of the ith classifier mi i are the mean and standard deviation of the opinions of the i classifier obtained with data from the authentic speakers intra model distances 100 M Faundez Zanuy Table 2 Identification rates combinations with sum rule P Param LPCC Melceps LP P resid LP 80 resid MLP resid LPCC LP P LPCC MLP 5 46 9 65 7 44 1 32 7 20 0 64 9 51 0 10 90 6 89 8 75 9 72 2 65 3 89 8 91 4 15 93 5 92 7 84 1 78 0 70 2 94 7 95 1 20 97 1 95 5 78 4 78 0 66 1 97 6 97 1 25 98 0 93 5 82 0 73 5 
65 3 97 1 97 96 30 98 8 91 8 78 8 68 6 62 9 97 1 98 4 40 94 7 87 4 77 1 68 6 53 5 95 1 95 1 We have limited the combinations to the outputs of two different classifiers and the sum and product combination rules 13 We have experimentally observed that slightly better results are obtained without normalization Looking at figure 4 it can be seen that the distance values obtained with the residual signal parameterization have less amplitude about 2 to 3 times Thus if the normalization is not done it is equivalent to a weighted combination where the LPCC distances have more influence over the combined result than the residual signal Figure 3 and table 2 summarize the identification rates for several vector dimensions P and different combined parameters Fig 4 Scatter diagram of distances for observing the correlation between parameters On the Usefulness of Linear and Nonlinear Prediction Residual Signals m1 m2 sqrt std2 std2 1 2 2 2 2 1 8 1 6 1 4 1 2 1 0 8 MLP 10x2x1 LPCC MELCEPS LP P LP 80 101 5 10 15 20 25 30 P 
vector dimension 35 40 Fig 5 Discrimination measure for several feature extractors LP 80 residue 0 2 0 18 0 16 0 14 0 12 0 1 0 08 0 06 0 04 0 02 0 5 10 15 20 25 30 P vector dimension 35 1 0 9 0 8 0 7 0 6 0 5 0 4 0 3 0 2 0 1 0 10 20 30 40 50 P vector dimension 60 70 80 LPCC melceps 0 8 0 7 0 6 0 14 0 5 0 4 0 3 0 2 0 04 0 1 0 5 10 15 20 25 30 P vector dimension 35 0 02 0 5 10 15 0 12 0 1 0 08 0 06 0 2 0 18 0 16 MLP 10x2x1 residue 20 25 30 P vector dimension 35 40 Fig 6 Mean and variance for inter and intra distances for several parameterization orders and feature extractors We have experimentally observed that slightly better results are obtained without normalization Looking at figure 4 it can be seen that the distance values obtained with the residual signal parameterization have less amplitude about 2 to 3 times 102 M Faundez Zanuy Thus if the normalization is not done it is equivalent to a weighted combination where the LPCC distances have more influence over the combined result than the residual signal 
Figure 3 and table 2 summarize the identification rates for several vector dimensions P and different combined parameters Figure 5 shows a discrimination capability measure based on the ratio m1 m2 2 12 2 for several feature extractors The higher this ratio is the better the recognition rates are Figure 6 shows the mean solid line and standard deviation vertical bar for several feature extractors and vector dimensions For a good biometric recognition we are looking for no overlap between intra and interdistances and as much separation as possible We can see that there is much more overlap for nonlinear residual signal than for the linear one Thus nonlinear residual signal has lesser potential for speaker recognition 5 Conclusions So far several papers have established that a combination between classical parameters LPCC melceps with some kind of parameterization computed over the residual analysis signal can yield improvements in recognition rates In our experiments we have found that this is only true when 
the analysis order ranges from 8 to 16 These values have been selected mainly because a spectral envelope can be sufficiently fitted with this amount of data so there was no reason to increase the number of parameters Although we consider that this is true for speech analysis synthesis and coding it is interesting to observe that the parameterization step for a speaker recognition system is twofold 1 2 We make a dimensionality reduction so it is easier to compute models distances between vectors etc We make a transformation from one space to another one In this new domain it can be easier to discriminate between speakers and some parameterizations are better than others Thus we are not looking for good quality representation of the speech signal or a compromise between good representation with the smallest number of parameters We are just looking for good discrimination capability In our experiments we have found that for parameter vectors of high order although the residual signal has a significant 
discriminative power among speakers this signal seems to be redundant with LPCC or melceps and it is not useful If instead of using the residual signal of a linear analysis a nonlinear analysis is used both combined signals are more uncorrelated and although the discriminative power of the NL residual signal is lower the combined scheme outperforms the linear one for several analysis orders The results show that there is just a marginal improvement on the results when increasing the number of parameters the identification rate plot saturates but the residual signal is whiter when increasing the prediction order especially for the nonlinear analysis This is a promising result because although a good parameterization based on nonlinear analysis has not yet been established this paper reveals that On the Usefulness of Linear and Nonlinear Prediction Residual Signals 103 the NL analysis can extract more relevant information with the same prediction order as a linear analysis Thus it opens a new way for 
investigation that has started to provide successful results 2 and is a promising approach for improving biometric systems 23 8 We think that the flow chart in order to evaluate the relevance of residual signals is the one depicted in figure 7 It is not enough to achieve a reasonable recognition rate We must check that this information is complementary to the classical one Otherwise it is worthless because it cannot help to improve recognition rates It is fundamental to take into account the second question Is possible to obtain good recognition rates Using the residual signal NO unuseful YES Is it reduntant or it can be combined in order to obtain an improvement NO unuseful YES useful Fig 7 Flow chart of deciding if a residual signal is suitable for improving speaker recognition rates Acknowledgements This work has been supported by FEDER and MEC TEC2006 13141 C03 02 TCM References 1 Bimbot F Mathan L Text free speaker recognition using an arithmetic harmonic sphericity measure In Eurospeech 1993 pp 104 M 
Faundez Zanuy 9 Feustel T C Velius G A Human and machine performance on speaker identity verification In Speech Tech 1989 pp Multi Filter Bank Approach for Speaker Verification Based on Genetic Algorithm Christophe Charbuillet Bruno Gas Mohamed Chetouani and Jean Luc Zarader Abstract Speaker recognition systems usually need a feature extraction stage which aims at obtaining the best signal representation State of the art speaker verification systems are based on cepstral features like MFCC LFCC or LPCC In this article we propose a feature extraction system based on the combination of three feature extractors adapted to the speaker verification task A genetic algorithm is used to optimise the features complementarity This optimisation consists in designing a set of three non linear scaled filter banks Experiments are carried out using a state of the art speaker verification system Results show that the proposed method improves significantly the system performances on the 2005 Nist SRE Database Furthermore the 
obtained feature extractors show the importance of some specific spectral information for speaker verification 1 Introduction Speech feature extraction plays a major role in speaker verification systems State of the art speaker verification systems front end are based on the estimation of the spectral envelope of the short term signal e g Mel scale Filterbank Cepstrum Coefficients MFCCs Linear scale Filter bank Cepstrum Coefficients LFCCs or Linear Predictive Cepstrum Coefficients LPCCs Even if these extraction methods achieve good performances on speaker verification they do not take into account specific information about the task to achieve To avoid this draw back several approaches have been proposed to optimize the feature extractor to a specific task These methods consist to simultaneously learn the parameters of both the feature extractor and the classifier 1 This procedure consists in the optimisation of a criterion which can be the Maximisation of the Mutual Information MMI 2 or the Minimisation of 
the Classification Error MCE 3 In this paper we propose to use a genetic algorithm for the design of feature extraction system adapted to the speaker verification task Genetic algorithms GA were first proposed by Holland in 1975 4 and became widely used in various domains as a new mean of complex systems optimization Recently their have been successfully applied to speech processing Chin Teng Lin and al 5 proposed to apply a GA to the feature transformation problem for speech recognition and M Zamalloa and al 6 worked on a GA M Chetouani et al Eds NOLISP 2007 LNAI 4885 pp 106 C Charbuillet et al Fig 1 Feature extraction optimisation based feature selection algorithm for speaker recognition The most attractive quality of GAs is certainly their aptitude to avoid local minima However our study relies on another quality which is the fact that GAs are unsupervised optimisation methods So they can be used as an exploration tool free to find the best solution without any constraint In a previous work 7 we used this 
approach to show the importance of specific spectral information for the speaker diarization task State of the art speaker verification systems are based on a cepstral feature extraction front end LFCC MFCC LPCC follow by a GMM 8 or an hybrid GMM SVM classifier 9 Nowadays an alternative an increasingly used approach consists in fusing different systems This technique can be divided in two main categories depending on the source of this difference The systems based on a classifier s variety 10 and the systems based on different features Our study deals with the second principle We can quote the work of M Zhiyou and al 11 which consist of combining the LFCC and MFCC features or the study of Poh Hoon Thian al 12 who proposed to complete the LFCC s with spectral centroids sub bands features In this paper we proposed to fuse three systems based on different feature extractors A genetic algorithm is used to optimise the feature extractor s complementarity Figure 1 describes this approach In the second section a 
description of the feature extraction method is given Afterwards we describe the genetic algorithm we used followed by its application to complementary feature extraction Then the experiments we made and the obtained results are presented 2 Filter Bank Based Feature Extractors The conventional MFCC and LFCC feature extractor process mainly consists of modifying the short term spectrum by a filter bank This process has four steps Multi Filter Bank Approach for Speaker Verification 107 Fig 2 Linear scaled filter bank Fig 2 presents the linear scaled filter bank used for the LFCC s computation This feature extractor is known to be the most robust for telephone signals representation The purpose of our study is to find a set of three cepstrum based feature extractors design for high level fusion To this end we propose to use a genetic algorithm to optimise the number of filters on the bank the scaled of the filter bank and the number of cepstral output coefficients 3 Genetic Algorithm A genetic algorithm is an 
optimisation method Its aim is to find the best values of the system s parameters in order to maximise its performance The basic idea is that of natural selection i e the principle of the survival of the fittest A GA operates on a population of systems In our application each individual of the population is a feature extractor defined by its genes Genes consists in a condensed an adapted representation of the feature extractor s parameters 3 1 Gene Encoding Parameter s encoding plays a major role in genetic algorithms By an adapted parameter representation this method can strongly increase the speed convergence of the algorithm Moreover it reduces the over fitting effect by reducing the parameters dimension The parameters we chose to optimise are the followin ones Parameters C and B are encoded with two polynomial functions described by the equations 1 and 2 This encoding method reduces the parameter s dimension from 50 to 12 average case and guaranties the filter bank s regularity The parameter N f and N c 
are not encoded and will be directly muted Ci gc0 108 C Charbuillet et al Fig 3 Genetic algorithm Where gc0 gcN and gb0 gbN are the genes relative to the parameters C0 CN f and B0 BN f N is the polynomial order N f represents the number of filter 3 2 Genetic Algorithm Description The algorithm we used is made of four operators Mutation Decoding Evaluation and Selection M D E S These operators are applied to the current population p t to produce a new generation p t 1 by the relation p t 1 S E D M p t 3 Fig 3 represents this algorithm The first step consists on a random initialisation of the feature extractor s genes Then the operators are iteratively applied The Mutation operator consists in a short random variation of the genes The Decoding operator aims at decoding the genes to obtain the operational feature extractor s parameters The Evaluation operator s goal is to evaluate each feature extractor performances The evaluation criterion we used is defined on the next section The Selection operator selects 
the N s better feature extractors of the current population These individuals are then cloned according to the evaluation results to produce the new generation p t 1 of N p feature extractors As a consequence of this selection process the average of the performance of the population tends to increase and in our application adapted feature extractors tend to emerge 3 3 Application to Complementary Feature Extraction The objective is to obtain a set of three complementary feature extractors The main idea is to evolve three isolated populations of feature extractors and to select the best combination At each generation the fusion is done for all combination of feature extractors and the resulting Equal Error Rate EER is memorised At the end of this process the fitness of an individual is defined as the lower Multi Filter Bank Approach for Speaker Verification 109 EER obtained e i the EER corresponding to the best combination including this feature extractor As a consequence of this process each population tends 
to specialise on specific feature complementary with the others 4 4 1 Experiments and Results Databases The databases used are extracted from the 2005 Nist SRE corpus 13 This corpus is composed of conversational telephone speech signals passed through different channels land line cordless or cellular and sampled to 8 kHz We used 10 males and 10 females with one utterance of 2 min 30s per speaker for the evolution phase 30 males and 30 females for the cross validation base and 50 males and 50 females for the test This three sets are speaker independent 4 2 Speaker Verification System All experiments we made are based on a state of the art GMM UBM speaker verification system This system called LIA SpkDet 14 was provided by the University of Avignon France We used a system with 16 Gaussian per mixture with diagonal covariance matrix 4 3 Genetic Algorithm Parameters The genes gc0 gcN and gb0 gbN are initialised with a Gaussian normalised random variable The parameter N f are initialised to 24 and N c to 16 The 
parameter we used for the feature extractor s evolution are Population size N p 20 Number of selected individuals N s 5 Polynomial order for the genes encoding N 5 Mutation method for the polynomials coefficients Gaussian random variation The over fitting effect is a common problem in machine learning A formal definition was given by T M Mitchell 15 Given a hypothesis space H a hypothesis h H is said to overfit the training data if there exists some alternative hypothesis h H such that h has smaller error than h over the training examples but h has a smaller error than h over the entire distribution of instances 110 C Charbuillet et al To avoid this effect several approaches were proposed for evoltionary computation such as cross validation CV early stopping ES complexity reduction CR noise addition NA or random sampling technic RST 16 17 18 In our application we used a combination of the cross validation and random sampling technic RST consists of using a random selected subset of trainning data to evaluate 
the individual performance A new sub set of data is used for each generation We selected a subset of 10 males and 10 females from a global train set of 30 speakers of each gender CV technic consists of evaluating the generalisation capacity of an individuals by the use of unseen data For each generation we evaluate and memorise the perfomaces of the best individual of the population on a cross validation base The speakers involved on this set are independent of both the train and test set The algorithm is stopped when a stagnation of the performances is observed Then the best individual of the best generation on the cross validation base is evaluated on the test database 4 5 Results In this section obtained feature extractors are presented and analysed Fig 4 b presents the obtained filter banks In order to interpret the obtained solution a statistical analysis of the fundamental frequency and formants was done on a database composed of 20 males and 20 females Fig 4 a presents the probability distributions of 
these measures Table 1 details both the feature extractor s characteristics and the results obtained on the test base The combination method used is an arithmetic fusion as illustrated by the Fig 1 Table 2 presents the correlation coefficients between the compared system and the EER obtained by fusion The correlation is based on the log likelihood outputs of the compared systems for the whole tests of the test database A test consists to measure the log likelihood between a speaker model and test signal The r correlation coefficient is defined by r Nt i 1 S 1i Nt i 1 Nt i 1 4 Where S 1i represents the log likelihood obtained by the system 1 on ith test N t is the number of test The correlation coefficient which takes value in 1 1 is a measure of the system s decision similarity In our application the classifiers are identical As a consequence this measure can be interpreted as the similarity between the information provided by the feature extractors A correlation of 1 means that the information supplied by 
the feature extractors are equivalent i e they lead to the same decision A correlation of 0 means that the information supplied Multi Filter Bank Approach for Speaker Verification 111 are independent Taking into account these different information we can high light some key points a Formant and fundamental frequency distributions b Obtained filter banks for C1 top C2 midle and C3 bottom Fig 4 Spectral analyse and obtained solutions 112 C Charbuillet et al Table 1 Comparative results Feature extractor LFCC MFCC C1 C2 C3 C1 C2 C3 Nf 24 24 23 25 19 N c Freq min Hz Freq max Hz EER 16 300 3400 14 44 16 300 3400 14 88 15 360 1145 22 90 20 266 3372 14 79 19 156 3309 16 07 12 69 Table 2 Fusion analysis Feature extractor Correlation EER obtained by fusion C1 C2 0 51 13 21 C2 C3 0 83 13 45 C1 C3 0 64 15 39 5 Conclusion In this paper we proposed to use a genetic algorithm in order to optimise a feature extraction system adapted to the speaker verification task The proposed system is based on a combination of three 
complementary feature extractors Obtained results show that the proposed method improves significantly the system performance Furthermore the obtained feature extractors reveal the importance of specific spectral information relatives to the first formant Our future work will consist in study the robustness of the obtained solutions according to both the initial conditions and the corpus used for the evolution phase References 1 Chetouani M Faundez Zanuy M Gas B Zarader J L Non linear Speech Feature Extraction for Phoneme Classification and Speaker Recognition In Chollet G Esposito A Multi Filter Bank Approach for Speaker Verification 113 6 Zamalloa M Bordel G Rodriguez J L Penagarikano M Feature selection based on genetic algorithms for speaker recognition In IEEE Odyssey vol 1 pp Speaker Recognition Via Nonlinear Phoneticand Speaker Discriminative Features Lara Stoll1 2 Joe Frankel1 3 and Nikki Mirghafori1 1 International Computer Science Institute Berkeley CA USA 2 University of California at Berkeley CA 
USA 3 Centre for Speech Technology Research Edinburgh UK lstoll nikki icsi berkeley edu joe cstr ed ac uk Abstract We use a multi layer perceptron MLP to transform cepstral features into features better suited for speaker recognition Two types of MLP output targets are considered phones Tandem HATS MLP and speakers Speaker MLP In the former case output activations are used as features in a GMM speaker recognition system while for the latter hidden activations are used as features in an SVM system Using a smaller set of MLP training speakers chosen through clustering yields system performance similar to that of a Speaker MLP trained with many more speakers For the NIST Speaker Recognition Evaluation 2004 both Tandem HATS GMM and Speaker SVM systems improve upon a basic GMM baseline but are unable to contribute in a score level combination with a state of the art GMM system It may be that the application of normalizations and channel compensation techniques to the current state of the art GMM has reduced 
channel mismatch errors to the point that contributions of the MLP systems are no longer additive 1 Introduction The speaker recognition task is that of deciding whether or not a previously unseen test utterance belongs to a given target speaker for whom there is only a limited amount of training data available The traditionally successful approach to speaker recognition uses low level cepstral features extracted from speech in a Gaussian mixture model GMM system Although cepstral features have proven to be the most successful choice of low level features for speech processing discriminatively trained features may be better suited to the speaker recognition problem We utilize multi layer perceptrons MLPs which are trained to distinguish between either phones or speakers as a means of performing a feature transformation of acoustic features There are two types of previous work directly related to our research both involving the development of discriminative features In the phonetically discriminative case the 
use of features generated by one or more MLPs trained to distinguish between phones has been shown to improve performance for automatic speech recognition ASR At ICSI Zhu and Chen et al developed what they termed Tandem HATS MLP features which incorporate longer term temporal information through the use of MLPs whose outputs are phone posteriors 1 2 M Chetouani et al Eds NOLISP 2007 LNAI 4885 pp Speaker Recognition 115 In the area of speaker recognition Heck and Konig et al focused on extracting speaker discriminative features from MFCCs using an MLP 3 4 They used the outputs from the middle layer of a 5 layer MLP which was trained to discriminate between speakers as features in a GMM speaker recognition system The MLP features when combined on the score level with a cepstral GMM system yielded consistent improvement when the training data and testing data were collected from mismatched telephone handsets 3 A similar approach was followed by Morris and Wu et al 5 They found that speaker identification 
performance improved as more speakers were used to train the MLP up to a certain limit 6 In the phonetic space we use the Tandem HATS MLP features in a GMM speaker recognition system The idea is that we can use the phonetic information of a speaker in order to distinguish that speaker from others More specifically a person s speech contains more information than just vocal quality the speech also encapsulates variations on a phonetic level in terms of the phonetic articulation tendencies of a speaker Examples of speaker recognition systems that successfully utilize the phonetic information of a speaker include the MLLR 7 and phone n gram 8 systems In the speaker space we train 3 layer Speaker MLPs of varying sizes to discriminate between a set of speakers and then use the hidden activations as features for a support vector machine SVM speaker recognition system The intuition behind this method is that the hidden activations from the SpeakerMLP represent a nonlinear mapping of the input cepstral features into 
a general set of speaker patterns Our Speaker MLPs are on a larger scale than any previous work we use more training speakers training data and input frames of cepstral features and larger networks To begin Section 2 outlines the experimental setup The results of our experiments are reported in Section 3 Finally we end with discussion and conclusions in Section 4 2 2 1 Experiments Overall Setup The basic setups of the Tandem HATS GMM and Speaker SVM systems are shown in Figures 1 and 2 respectively Frames of perceptual linear prediction PLP coefficients as well as frames of critical band energies in the former case are the inputs to the MLPs A log is applied to either the output or hidden activations and after either dimensionality reduction or calculation of mean standard deviation histograms and percentiles the final features are used in a speaker recognition system GMM or SVM 2 2 Baseline GMM Systems We make use of two types of GMM baselines for purposes of comparison The first is a state of the art GMM 
system which was developed by our colleagues 116 L Stoll J Frankel and N Mirghafori Tandem MLP Output activations 9 frames of PLP Phone 1 Phone N Speaker MLP Combination HATS MLP Hidden activations Log Dimensionality reduction Phone 1 Output activations 21 frames of PLP Speaker 1 Speaker N Hidden activations Log 51 frames of critical band energy SVM feature calculations Fig 1 Tandem HATS GMM System at SRI and which we will refer to as SRI GMM 9 It utilizes 2048 Gaussians CMS T norm H norm and channel mapping to improve its results We use this system for score level combinations in which the scores from SRI s GMM system are combined with the scores from our MLP features systems For more details see Section 2 6 The second system on the other hand is a very basic GMM system with 256 Gaussians and which includes only CMS without any other normalizations This system which we will refer to as Basic GMM is useful for the purpose of feature level combination where we use MFCC features augmented with MLP features as 
features in the GMM system as well as for score level combination 2 3 Tandem HATS MLP Features There are two components to the Tandem HATS MLP features namely the Tandem MLP and the HATS MLP The Tandem MLP is a single 3 layer MLP which takes as input 9 frames of PLPs 12th order plus energy with deltas and double deltas contains 20 800 units in its hidden layer and has 46 outputs corresponding to phone posteriors The hidden layer applies the sigmoid function while the output uses softmax The HATS MLP is actually two stages of MLPs that perform phonetic classification with long term 500 1000 ms information The first stage MLPs take as input 51 frames of log critical band energies LCBE with one MLP for each of the 15 critical bands each MLP has 60 hidden units with sigmoid applied and the output layer has 46 units with softmax corresponding to phones For the HATS Hidden Activation TRAPS features the hidden layer outputs are taken from each first stage critical band MLP and then input to the secondstage merger 
MLP which contains 750 hidden units and 46 output units The Tandem MLP and HATS MLP features are then combined using a weighted sum where the weights are a normalized version of inverse entropy for the Tandem HATS GMM The log is applied to the output and a KarhunenLoeve Transform KLT dimensionality reduction is applied to reduce the output Phone N GMM System SVM System Hidden activations Fig 2 Speaker SVM System Speaker Recognition 117 feature vector to an experimentally determined optimal length of 25 This process is illustrated in Figure 1 The Tandem HATS MLP system is trained on roughly 1800 hours of conversational speech from the Fisher 10 and Switchboard 11 corpora 2 4 Speaker MLP Features Speaker Target Selection Through Clustering As a contrast to using all speakers with enough training data available with the idea that including more training speakers will yield better results we also implemented MLPs trained using only subsets of specifically chosen speakers These speakers were chosen through 
clustering in the following way First a background GMM model was trained using 286 speakers from the Fisher corpus Then a GMM was adapted from the background model with the data from each MLP training speaker These GMMs used 32 Gaussians with input features of 12th order MFCCs plus energy and their first order derivatives The length 26 mean vectors of each Gaussian were concatenated to form a length 832 feature vector for each speaker Principal component analysis was performed keeping the top 16 dimensions of each feature vector accounting for 68 of the total variance In this reduceddimensionality speaker space k means clustering was done using the Euclidean distance between speakers for k 64 and k 128 Finally the sets of 64 and 128 speakers were chosen by selecting the speaker closest to each of the 64 or 128 cluster centroids MLP Training A set of 64 128 or 836 speakers was used to train each Speaker MLP with 6 conversation sides per speaker used for training and 2 for cross validation CV The training 
speaker data came from the Switchboard 2 corpus 11 The set of 836 speakers included all speakers in the Switchboard2 corpus with at least 8 conversations available The smaller sets of speakers selected through clustering used training and CV data that was balanced in terms of handsets ICSI s QuickNet MLP training tool 12 was used to train the Speaker MLPs The input to each Speaker MLP is 21 frames of PLPs 12th order plus energy with first and second order derivatives appended The hidden layer applies a sigmoid while the output uses softmax Table 2 shows the sizes of MLPs varying in the number of hidden units trained for each set of speakers 2 5 SVM Speaker Recognition System The GMM system is well suited to modeling features with fewer than 100 dimensions However problems of data sparsity and singular covariance matrices soon arise in trying to estimate high dimensional Gaussians Previous work in speech recognition HATS has shown that there is a great deal of information in the hidden structure of the MLP 
Preliminary experiments also showed that reducing the dimensionality of the hidden activations using principal component analysis PCA or linear discriminant analysis LDA so that the features could be used 118 L Stoll J Frankel and N Mirghafori in a GMM system yielded poor results In order to take advantage of the speaker discriminative information in the hidden activations of the Speaker MLPs we use an SVM speaker recognition system which is better suited to handle the high dimensional sparse features is naturally discriminative in the way it is posed and has proven useful in other approaches to speaker verification Since the SVM speaker recognition system requires the same length feature vector for each speaker whether a target an impostor or a test speaker we produce a set of statistics to summarize the information along each dimension of the hidden activations These statistics mean standard deviation histograms of varying numbers of bins and percentiles are then used as the SVM features for each speaker 
For our experiments the set of impostor speakers used in the SVM system is a set of 286 speakers from the Fisher corpus designed to be balanced in terms of gender channel and other conditions 2 6 System Combinations Using LNKnet In order to improve upon the baseline of the SRI GMM system we choose to combine our various systems on the score level with the SRI GMM using LNKnet software 13 We use a neural network with no hidden layer and sigmoid output non linearity which takes two or more sets of likelihood scores as input We use a round robin approach and divide our test data into two subsets for development and evaluation 3 3 1 Results Testing Database In order to compare the performance of our systems we use the database released by NIST for the 2004 Speaker Recognition Evaluation SRE 14 This database consists of conversational speech collected in the Mixer project and includes various languages and various channel types We use only telephone data containing a variety of handsets and microphones One 
conversation side roughly 2 5 minutes is used for both the training of each target speaker model and the testing of each test speaker As performance measures we use the detection cost function DCF of the NIST evaluation and the equal error rate EER The DCF is defined to be a weighted sum of the miss and false alarm error probabilities while the EER is the rate at which these error probabilites are equal 3 2 Tandem HATS GMM For NIST s SRE2004 the DCF and EER results are given in Table 1 for the Basic GMM system the Tandem HATS GMM system and their scoreand feature level combinations as well as for the SRI GMM system and its combination with the Tandem HATS GMM Changes relative to each baseline where a positive value indicates improvement are shown in parentheses Speaker Recognition 119 Table 1 Tandem HATS GMM system improves upon Basic GMM system especially in combination but there is no improvement for SRI GMM system Alone the Tandem HATS GMM system perform slightly better than the Basic GMM system Feature 
level combination of MFCC and Tandem HATS features in a GMM system as well as score level combination of the Tandem HATS GMM system with the Basic GMM both yield significant improvements When the Tandem HATS GMM system is combined on the score level with the SRI GMM system there is no gain in performance over the SRI GMM alone In order to consider phonetic features comparable to those produced by the Speaker MLP we also tested the Tandem MLP features individually i e without combination with the HATS MLP features The Tandem GMM system actually yields better results than the Tandem HATS GMM although in combinations with the Basic GMM and SRI GMM there is no improvement Thus we may conclude that the added complexity of the HATS MLP component does not lead to any significant difference in speaker recognition results for the Tandem HATS GMM system For more detailed results see 15 3 3 Speaker SVM Both the cross validation and SRE2004 results for the Speaker MLPs are shown in Table 2 for each size MLP It is clear 
that the CV accuracy increases with respect to the number of hidden units for each training speaker set The accuracy increase on adding further hidden units does not appear to have reached a plateau at 2500 hidden units for the 836 speaker net though for the purposes of the current study the training times became prohibitive With the computation shared between 4 CPUs it took over 4 weeks to train the MLP with 2500 hidden units Similar to the CV accuracy the speaker recognition results improve with an increase in the size of the hidden layer when considering a given number of training speakers In Table 3 the results are given for the score level combination of the 64 speaker 1000 hidden unit Speaker SVM system with the Basic GMM and SRIGMM systems For the SRI GMM the best combination is yielded when the Speaker MLP is trained with 64 speakers and 1000 hidden units although the 128 speakers with 2000 hidden units does somewhat better in combination with 120 L Stoll J Frankel and N Mirghafori Table 2 Speaker 
SVM results improve as the number of hidden units as well as the CV accuracy increase spkrs Hid units 64 400 64 1000 128 1000 128 2000 836 400 836 800 836 1500 836 2500 CV acc Table 3 System combination with 64 speaker 1000 hidden unit Speaker SVM improves Basic GMM results but not the SRI GMM Table 5 Breakdown of results for matched and mismatched conditions for the MLPbased systems and their score level fusions with the SRI GMM System Alone Matched Mismatched EER EER SRI GMM 5 74 10 71 Tandem HATS GMM 12 53 21 54 Speaker SVM 1000hu 64ou 13 93 23 56 Fusion with SRI GMM Matched Misatched EER EER Speaker Recognition 121 the Basic GMM There is a reasonable gain made when combining the SpeakerSVM system with the Basic GMM but there is no significant improvement for the combination of the Speaker SVM and SRI GMM systems 3 4 Mismatched Train and Test Conditions We now consider matched same gender and handset and mismatched different gender or handset conditions between the training and test data Such a breakdown 
is given for the Tandem HATS GMM and Speaker SVM systems and their score level combinations with the Basic GMM and SRI GMM in Tables 4 and 5 respectively For each combination changes relative to the appropriate baseline system are given in parentheses When considering a score level fusion with the Basic GMM system gains are made in the matched and especially the mismatched conditions for both the Tandem HATS GMM and Speaker SVM For the SRI GMM baseline combination with the Tandem HATS GMM and Speaker SVM systems has marginal impact in either the matched or mismatched case 4 Discussion and Conclusions For the first time phonetic Tandem HATS MLP features were tested in a speaker recognition application Although developed for ASR the Tandem HATS MLP features still yield good results when used in a GMM for a speaker recognition task In fact the Tandem HATS GMM performs slightly better than a basic cepstral GMM system with even more improvement coming from score and feature level combinations of the two Prior 
related work used discriminative features from MLPs trained to distinguish between speakers Motivated by having a well established infrastructure for neural network training at ICSI we felt that there was potential for making greater gains by using more speakers more hidden units and a larger contextual window of cepstral features at the input Even though preliminary experiments confirmed this ultimately however a smaller subset of speakers chosen through clustering proved similar in performance and could be trained in less time Although the MLP based systems do not improve upon the SRI GMM baseline in combination this result could be explained by considering the difference in the performance between the two types of systems standalone each MLPbased system performs much more poorly than the SRI GMM The addition of channel compensating normalizations like T norm 16 to an MLP based system should help reduce the performance gap between the MLP based system and the SRI GMM It may then be possible for the MLP 
based system to improve upon the state of the art cepstral GMM system in combination in the event that the performance gap is narrowed sufficiently Similar to results observed in prior work the Speaker SVM system improved speaker recognition performance for a cepstral GMM system lacking sophisticated normalizations such as feature mapping 17 speaker model synthesis 122 L Stoll J Frankel and N Mirghafori SMS 18 and T norm such a result was also true for the Tandem HATSGMM system However no gains were visible in addition with the SRI GMM which is significantly improved from the Basic GMM as well as the GMM systems of Wu and Morris et al and Heck and Konig et al by the addition of feature mapping T norm as well as increasing the number of Gaussians to 2048 As shown in Table 4 combinations of the Basic GMM with the phonetic and speaker discriminant MLP based systems of this paper do yield larger improvements for the mismatched condition which refers to the training data and test data being different genders or 
different handset types However as seen in Table 5 such a result does not hold for combinations of the MLP based systems with the SRI GMM The previous work of Heck and Konig et al completed prior to the year 2000 showed that the greatest strength of an MLP based approach was for the case when there is a handset mismatch between the training and test data however the state of the art has since advanced significantly in normalization and channel compensation techniques As a result the contributions of the MLP based systems without any normalizations applied to a state of the art cepstral GMM system are no longer significant for the mismatched condition Acknowledgements This material is based upon work supported under a National Science Foundation Graduate Research Fellowship and upon work supported by the National Science Foundation under grant number 0329258 This work was also made possible by funding from the EPSRC Grant GR S21281 01 and the AMI Training Programme We would also like to thank our colleagues 
at ICSI and SRI References 1 Chen B Zhu Q Morgan N Learning long term temporal features in LVCSR using neural networks In ICSLP 2004 2 Zhu Q Chen B Morgan N Stolcke A On using MLP features in LVCSR In ICSLP 2004 3 Heck L P Konig Y Speaker Recognition 123 7 Stolcke A Ferrer L Kajarekar S Shriberg E Venkataraman A MLLR transforms as features in speaker recognition In EUROSPEECH 2005 pp Perceptron Based Class Verification Michael Gerber Tobias Kaufmann and Beat Pfister Speech Processing Group Computer Engineering and Networks Laboratory ETH Zurich Switzerland gerber tik ee ethz ch Abstract We present a method to use multilayer perceptrons MLPs for a verification task i e to verify whether two vectors are from the same class or not In tests with synthetic data we could show that the verification MLPs are almost optimal from a Bayesian point of view With speech data we have shown that verification MLPs generalize well such that they can be deployed as well for classes which were not seen during the training 1 
Introduction Multilayer perceptrons MLPs are successfully used in speech processing For example they are used to calculate the phoneme posterior probabilities in hybrid MLP HMM speech recognizers see for example 1 In this case their task is to output for every phoneme the posterior probability that a given input feature vector is from this phoneme They are thus used to identify a feature vector with a given phoneme Expressed in more general terms the MLPs are used for the identification of input vectors with a class from within a closed set of classes There are applications however where the identification of input vectors is not necessary but it has to be verified whether two given input vectors x and y are from the same class or not In Section 2 we present two verification tasks in the domain of speech processing In this work we show that MLPs have the capability to optimally solve verification problems Furthermore we have observed in a task with real world data that the verification MLPs can even be used 
to discriminate between classes which were not present in the training set This is an especially useful property for two reasons M Chetouani et al Eds NOLISP 2007 LNAI 4885 pp Perceptron Based Class Verification 125 of performing the verification task in an optimal way from a Bayesian point of view we made experiments with synthetic data These experiments and their results are described in Section 5 The results of experiments with speech data are shown in Section 6 Finally our conclusions are summarized in Section 7 2 Motivation Our method to decide whether two speech signals are spoken by the same speaker or not includes the following 3 steps First equally worded segments are sought in the two speech signals This results in a series of frame pairs where both frames of a pair are from the same phoneme In a second step for each frame pair the probability that the two phonetically matching frames come from the same speaker is computed Finally the global indicator that the two speech signals were spoken by the 
same speaker can be calculated from these framelevel probabilities See e g 2 for a more detailed description of the speakerverification approach We used the verification MLPs for the following two tasks 3 Verification MLP Since the MLP has to decide whether two given input vectors x and y are from the same class the MLP has to process vector pairs rather than single vectors The target output of the MLP is os if the two vectors of the pair are from the same class and od if they are from different classes The vectors are decided to belong to the same class if the output is closer to os and to different classes otherwise The sizes of the 3 layer perceptrons used for the experiments described in Sections 5 and 6 are as follows input size 1st hidden layer 2nd hidden layer output layer synthetic data 126 M Gerber T Kaufmann and B Pfister The verification MLPs were trained by means of the backpropagation algorithm The weights were randomly initialized and a momentum term was used during the training For a 
hyperbolic tangent output neuron a good choice for the output targets is os 0 75 and od 0 75 such that the weights are not driven towards infinity see for example 3 With these settings we experienced that at the beginning of the training the difference between desired and effective output decreased quite slowly but that the training was never stuck in a local minimum 4 Performance Evaluation In order to evaluate a verification MLP we measure its verification error rate for a given dataset and compare it to a reference error rate which is optimal in a certain sense By formulating our verification task as a classification problem we can use the Bayes error as a reference The Bayes error is known to be optimal for classification problems given the distribution of the data To reformulate a verification task as a classification problem each pair of vectors is assigned one of the following two groups GS group of all vector pairs where the two vectors are from the same class GD group of all vector pairs where both 
vectors are from different classes Provided that the same classes which are present in the tests are used to estimate the distributions of GS and GD the Bayes error is optimal since the two distributions are modeled properly Otherwise there is a mismatch which leads to ill modeled GS and GD and thus the Bayes classifier is not necessarily optimal any more In the case of synthetic data it is possible to calculate the Bayes verification error since the data distributions are given in a parametric form For real world problems the data distributions are not given in a parametric form and hence the Bayes verification error can t be computed directly In this case we can use a k nearest neighbor KNN classifier to asymptotically approach the Bayes error as described below vector 1 vector 2 output targets os same class od different classes Fig 1 Structure of the verification MLPs output value Perceptron Based Class Verification 127 1 5 1 0 5 c 0 5 1 1 5 2 3 2 0 2 1 c1 0 1 2 Fig 2 Synthetic data 2 classes with 2 
dimensional non Gaussian distributions The KNN approach is a straightforward means of classification The training set for the KNN algorithm consists of training vectors with known classification atr i btr i where atr i is the training vector and btr i is its associated class A test vector atst j is classified by seeking the k nearest training vectors atr i and it is assigned to the class which is most often present among the k nearest neighbors The KNN classifier is known to reach the Bayes error when an infinite number of training vectors is available see e g 4 and is therefore a means to approximate the Bayes error if the data distributions are not known in a parametric form 5 Experiments with Synthetic Data The aim of the experiments with synthetic datasets i e datasets with known data distributions was to test if the verification MLP achieves the lowest possible verification error from a Bayesian point of view The data sets had 2 to 4 classes and were 2 to 5 dimensional We illustrate these investigations 
by means of an experiment with a 2 dimensional dataset with 2 classes that are distributed as shown in Figure 2 The number of training epochs which were necessary to train the verification MLP depended largely on the type of the dataset We observed the following dependencies 128 M Gerber T Kaufmann and B Pfister 0 35 Verification with KNN Verification with ANN Bayes verification error 0 3 Rate of wrong verification 0 25 0 2 0 15 0 1 0 05 0 2 10 10 10 Number of KNN training vectors 3 4 10 5 Fig 3 Class verification error for the test set as shown in Figure 2 the KNN verification error is shown in function of the training set size As expected with increasing size it approximates the Bayes limit which is indicated by the dotted line The error rate of the verification MLP is close to the Bayes error parallel stripes or classes that had a non linear Bayes decision boundary such as those shown in Figure 2 required many epochs Figure 3 shows the error rates of different verification methods for data distributed as 
shown in Figure 2 It can be seen that the error of the verification MLP is almost as low as the Bayes error Note that the MLP was trained with a fixed number of 20 000 vector pairs We are only interested in the best possible verification error for a given task and not in the verification error in function of the number of training vectors see Section 1 Therefore the MLP training set was chosen as large as necessary For all investigated datasets the verification error achieved with the verification MLP was not significantly higher than the Bayes verification error 6 6 1 Experiments with Speech Data Data Description and Feature Extraction For a speaker verification task with single speech frames we used speech signals from 48 male speakers recorded from different telephones From short speech segments 32 ms frames the 13 first Mel frequency cepstral coefficients MFCCs were extracted and used as feature vectors for our experiments For the phoneme Perceptron Based Class Verification 0 5 0 45 0 4 Rate of wrong 
verification 0 35 0 3 0 25 0 2 0 15 0 1 0 05 0 Verification with KNN input coding Verification with KNN no input coding Verification with ANN input coding Verification with ANN no input coding 129 10 10 Number of KNN training vectors 2 4 10 6 Fig 4 Phoneme verification task The KNN error rates decrease with increasing number of KNN training vectors The error rates of the verification MLPs are shown as dots The error rates for both KNN and MLP are given for coded and uncoded input vectors verification task the first derivatives of the MFCCs were used in addition to the static MFCCs The data from all speakers was divided into 3 disjoint sets i e no speaker was present in more than one set The MLP and KNN training vector pairs were extracted from the training set 26 speakers The validation set 10 speakers was used to stop the MLP training at the optimal point and to find the optimal value of k for the KNN classifier The test vector pairs were taken from the test set 12 speakers Note that all vector pairs were 
formed in a way that the two vectors of a pair were always from the same phoneme In every set the number of pairs with vectors of the same speaker and the number of pairs with vectors from different speakers were equal 6 2 Phoneme Verification In this task the objective was to decide whether two speech feature vectors originate from the same phoneme In this task the same classes phonemes are present in all 3 datasets since all signals have similar phonetic content Yet all sets are extracted from different speakers as is described in Section 6 1 Because of the large number of KNN training vectors which were necessary that the KNN algorithm converged we used two types of input namely pairs of concatenated vectors pin x y as mentioned above and coded vector pairs pin x y x y see 5 for details about the input coding This input coding sped up the training of the MLPs and led to a steeper descent of the KNN verification error in function of the number of training vectors 130 M Gerber T Kaufmann and B Pfister 0 55 
0 5 Rate of wrong verification 0 45 0 4 0 35 0 3 Verification with KNN input coding Verification with KNN no input coding Verification with ANN input coding Verification with ANN no input coding 10 10 Number of KNN training vectors 2 4 0 25 10 6 Fig 5 Speaker verification The KNN error rates decrease with increasing number of KNN training vectors The error rates of the verification MLPs are shown as dots The error rates for both KNN and MLP are given for coded and uncoded input vectors The verification error of an MLP trained with 580000 vector pairs is shown in Figure 4 For comparison also the KNN error rate in function of the training set size is drawn It can be seen that with this data the verification KNN converges only with a large number training vectors This was expected because of the more complex nature of the problem It can only be guessed where the asymptote and therefore the Bayes error will be It seems that the verification error of the MLP is close to the Bayes verification error however Since 
we did not have enough training data we could not prove this assumption Furthermore it does not seem that the input coding had a big effect on the optimal verification result 6 3 Speaker Verification In this task the objective was to decide whether two speech frames are from speech signals of the same speaker or not In this case all 3 sets of classes speakers were disjoint Therefore a good generalization of the verification MLP is required The experiment results are shown in Figure 5 It can be seen that the KNN verification error in function of the training set size decreases much less steeply than in the experiments done with synthetic data and does not even get as low as the verification error of the MLP This is possible since the training and test set have some mismatch because the speaker sets are disjoint see Section 4 Here it can be seen very well that the KNN which is based on coded vector pairs Perceptron Based Class Verification 131 converged with much less training vectors In this case the 
verification MLP which used coded vector pairs was a bit worse however The verification error of the MLP is quite low if it is considered that the feature vectors x and y were extracted from single speech frames only If all phonetically matching frame pairs of two equally worded speech segments of about 1 s length are fed separately into the MLP and the output values of the MLP are averaged the verification error rate is about 6 More detailed results can be found in 5 6 and 2 7 Conclusions By means of experiments we have shown that the error rate of an appropriately configured and trained verification MLP is close to the Bayes error rate Depending on the class distributions the training can be fairly time consuming however This is not critical in our application since the MLP is class independent and does not need to be retrained whenever new classes are added to the application For speech data with a virtually unlimited set of classes as it is for example the case in speaker verification MLP based class 
verification has shown to be very efficient not only in terms of verification error but also with respect to computational complexity For a speaker verification task the good generalization property of the verification MLP could be shown Thus the verification MLPs are able to learn a general rule to distinguish between classes rather than class specific features Acknowledgements This work was partly funded by the Swiss National Center of Competence in Research IM2 References 1 Hermansky H Ellis D P W Sharma S Tandem connectionist feature extraction for conventional HMM systems In Proceedings of the ICASSP 2000 vol 3 pp Manifold Learning Based Feature Transformation for Phone Classification Andrew Errity John McKenna and Barry Kirkpatrick School of Computing Dublin City University Dublin 9 Ireland andrew errity john mckenna barry kirkpatrick computing dcu ie Abstract This study aims to investigate approaches for low dimensional speech feature transformation using manifold learning It has recently been shown 
that speech sounds may exist on a low dimensional manifold nonlinearly embedded in high dimensional space A number of manifold learning techniques have been developed in recent years that attempt to discover this type of underlying geometric structure The manifold learning techniques locally linear embedding and Isomap are considered in this study The low dimensional representations produced by applying these techniques to MFCC feature vectors are evaluated in several phone classification tasks on the TIMIT corpus Classification accuracy is analysed and compared to conventional MFCC features and those transformed with PCA a linear dimensionality reduction method It is shown that features resulting from manifold learning are capable of yielding higher classification accuracy than these baseline features The best phone classification accuracy in general is demonstrated by feature transformation with Isomap 1 Introduction Feature transformation is an important part of the speech recognition process and can be 
viewed as a two step procedure Firstly relevant information is extracted from short time segments of the acoustic speech signal using a procedure such as Fourier analysis cepstral analysis or some other perceptually motivated analysis The resulting D dimensional parameter vectors are then transformed to a feature vector of lower dimensionality d d D The aim of this dimensionality reduction is to produce features which are concise low dimensional representations that retain the most discriminating information for the intended application and are thus more suitable for pattern classification Dimensionality reduction also decreases the computational cost associated with subsequent processing Physiological constraints on the articulators limit the degrees of freedom of the speech production apparatus As a result humans are only capable of producing sounds occupying a subspace of the entire acoustic space Thus speech data can be viewed as lying on or near a low dimensional manifold embedded in the M Chetouani et 
al Eds NOLISP 2007 LNAI 4885 pp Manifold Learning Based Feature Transformation 133 original acoustic space The underlying dimensionality of speech has been the subject of much previous research using many different approaches including classical dimensionality reduction analysis 1 2 nonlinear dynamical analysis 3 and manifold learning 4 The consensus of this work is that some speech sounds particularly voiced speech are inherently low dimensional Dimensionality reduction methods aim to discover such underlying low dimensional structure These methods can be categorised as linear or nonlinear Linear methods are limited to discovering the structure of data lying on or near a linear subspace of the high dimensional input space Two of the most widely used linear dimensionality reduction methods are principal component analysis PCA 5 and linear discriminant analysis LDA 6 These methods have been successfully applied to feature transformation in speech processing applications in the past 7 8 However if speech data 
occupies a low dimensional submanifold nonlinearly embedded in the original space as proposed previously 2 4 linear methods will fail to discover the low dimensional structure A number of manifold learning also referred to as nonlinear dimensionality reduction algorithms have been developed 9 10 11 which overcome the limitations of linear methods Manifold learning algorithms have recently been shown to be useful in a number of speech processing applications including low dimensional visualization of speech 11 12 13 4 14 and limited phone classification tasks 15 14 In this study we build upon previous work and apply two manifold learning algorithms locally linear embedding LLE 9 and isometric feature mapping Isomap 10 to extract features from speech data These features are evaluated in phone classification experiments using a support vector machine SVM 16 classifier The classification performance of these features is compared to baseline Mel frequency cepstral coefficients MFCC and those resulting from the 
classical linear method PCA These classification experiments are primarily used as a means of evaluating how much meaningful discriminatory information is contained in the low dimensional representations produced by each method These experiments also serve to display the potential value of these methods in speech processing applications The remainder of this paper is structured as follows In Section 2 the manifold learning algorithms LLE and Isomap are briefly described Section 3 details the experimental procedure data set parameter extraction feature transformation and classification technique used Results are examined and discussed in Section 4 with conclusions presented in Section 5 2 2 1 Manifold Learning Algorithms Locally Linear Embedding LLE 9 is an unsupervised learning algorithm that computes low dimensional embeddings of high dimensional data The principle of LLE is to compute a low dimensional embedding with the property that nearby points in the high dimensional space remain nearby and similarly 
co located with respect to one 134 A Errity J McKenna and B Kirkpatrick another in the low dimensional space In other words the embedding is optimised to preserve local neighbourhoods The LLE algorithm can be summarised in three steps 1 For each data point Xi compute its k nearest neighbours based on Euclidean distance or some other appropriate definition of nearness 2 Compute weights Wij that best reconstruct each data point Xi from its neighbours minimising the reconstruction error E E W i Xi j Wij Xj 2 1 3 Compute the low dimensional embeddings Yi best reconstructed by the weights Wij minimising the cost function Y i Yi j Wij Yj 2 2 In step 2 the reconstruction error is minimised subject to two constraints first that each input is reconstructed only from its nearest neighbours or Wij 0 if Xi is not a neighbour of Xj second that the reconstruction weights for each data point sum to one or j Wij 1 i The optimum weights for each input can be computed efficiently by solving a constrained least squares problem 
The cost function in step 3 is also based on locally linear reconstruction errors but here the weights Wij are kept fixed while optimising the outputs Yi The embedding cost function in 2 is a quadratic function in Yi The minimisation is performed subject to constraints that the outputs are centered and have unit covariance The cost function has a unique global minimum solution for the outputs Yi This is the result returned by LLE as the low dimensional embedding of the high dimensional data points Xi 2 2 Isomap The Isomap algorithm 10 offers a differently motivated approach to manifold learning Isomap is a nonlinear generalisation of multidimensional scaling MDS 6 that seeks a mapping from a high dimensional dataset to a low dimensional dataset that preserves geodesic distances between pairs of data points that is distances on the manifold from which the data is sampled While Isomap and LLE have similar aims Isomap is based on a different principle than LLE In particular Isomap attempts to preserve the 
global geometric properties of the manifold while LLE attempts to preserve the local geometric properties of the manifold As with LLE the Isomap algorithm consists of three steps 1 Construct a neighbourhood graph Determine which points are neighbours on the manifold based on distances dist Xi Xj between pairs of points Xi Xj in the input space as in step 1 of LLE These neighbourhood relations are then represented as a weighted graph over the data points with edges of weight dist Xi Xj between neighbouring points Manifold Learning Based Feature Transformation 135 2 Compute the shortest path between all pairs of points among only those paths that connect nearest neighbours using a technique such as Dijkstra s algorithm 3 Use classical MDS to embed the data in a d dimensional Euclidean space so as to preserve these geodesic distances 3 Experiments The objective of these experiments is to evaluate how much meaningful discriminatory information is contained in the low dimensional representations produced by the 
manifold learning and linear dimensionality reduction methods 3 1 Classification Tasks Phone classification experiments were performed using four different feature types baseline MFCC vectors and features produced by applying PCA Isomap and LLE to the baseline MFCC vectors Two types of baseline MFCC vectors were used standard static MFCCs only and static MFCCs concatenated with dynamic information in the form of delta coefficients The experimental procedure detailed below was repeated separately for the static and dynamic baseline MFCCs Each of the four feature types were evaluated in three phone classification experiments The first experiment involves distinguishing between a set of five vowels aa iy uw eh and ae Phones are labeled using TIMIT symbols 17 In the second test a further five vowels ah ax ao ih and ow were added to the previous vowel set forming a more complex ten class vowel classification problem The final test involves classifying a set of 19 phones into their associated phone classes The 
phone classes and phones used were vowels listed above fricatives s sh stops p t and k nasals m n and semivowels and glides l y 3 2 Data The speech data used in this study was taken from the TIMIT corpus 17 This corpus contains 6300 utterances 10 spoken by each of 630 American English speakers The speech recordings are provided at a sampling frequency of 16 kHz 3 3 Parameter Extraction Based on the phonetic transcriptions and associated phone boundaries provided in TIMIT all units of a subset of phones listed in Section 3 1 were extracted from the corpus For each phone unit frames of length 40 ms were extracted with a frame shift of 20 ms Units of duration less than 100 ms were discarded The raw speech frames were preemphasized with the filter H z 1 0 98z 1 and Hamming windowed Following this preprocessing 13 dimensional MFCC vectors including the zeroth cepstral coefficients were computed for each frame Standard delta coefficients were also computed These MFCC vectors and those concatenated with their 
deltas MFCC serve as both baseline features and high dimensional inputs for PCA Isomap and LLE 136 A Errity J McKenna and B Kirkpatrick 3 4 Feature Transformation For each of the three phone classification experiments 250 units representing each of the required phones were chosen at random from those extracted above to make up the data set PCA Isomap and LLE were individually applied to the equivalent sets of MFCC and MFCC vectors In order to examine the ability of the feature transformation methods to compute concise representations of the input vectors retaining discriminating information the dimensionality of the resulting feature vectors was varied from 1 to 13 for static MFCC features and from 1 to 26 for MFCC features A separate classifier was subsequently trained and tested using feature vectors with each of the different dimensionalities Thus the ability of these feature transformation methods to produce useful low dimensional features could be evaluated and changes in performance with varying 
dimension analysed As a baseline the original MFCC and MFCC vectors were used also varying in dimensionality as detailed above The number of nearest neighbours k used in Isomap and LLE was set equal to 14 and 6 respectively These values were chosen empirically by varying k and examining classification performance 3 5 Support Vector Machine Classification Initially a number of classifiers including K nearest neighbours Gaussian mixture models and support vector machines SVMs were tested in phone classification tasks SVMs 16 yielded the best performance and thus were used in the following experiments SVM is a binary pattern classification algorithm For our experiments it is necessary to construct a multiclass classifier This was achieved using a one against one training scheme training one classifier for every possible pair of classes The final classification result was determined by majority voting It is also necessary to choose an appropriate kernel function to be used in the SVM In order to select an 
effective kernel different SVM models using linear polynomial and radial basis function RBF kernels were evaluated in a number of phone classification tasks SVM with RBF kernel demonstrated the best classification accuracy and was used throughout this work In all classification experiments 80 of the data was assigned as training data with the remaining 20 withheld and used as testing data The data was partitioned such that the training and test sets had no speakers in common thus ensuring speaker independence 4 4 1 Results Static Features Firstly the results of experiments conducted using 13 dimensional MFCCs as baseline feature vectors and inputs to the dimensionality reduction methods are presented In each experiment the classifier was evaluated on each of the four Manifold Learning Based Feature Transformation Training Data 95 85 75 65 55 45 35 137 Classification Rate PCA MFCC Isomap LLE 0 2 4 6 8 10 12 14 Classification Rate Testing Data 85 75 65 55 45 35 0 2 4 6 8 Feature Dimension 10 12 14 Fig 1 Five 
vowel classification results for baseline MFCC PCA Isomap and LLE features on the TIMIT database feature types baseline MFCC vectors and PCA LLE and Isomap embeddings of these baseline vectors The dimensionality of the feature vectors used in the experiment vary from 1 to 13 the original full dimension Results are presented for evaluation on both the training data and testing data Figure 1 shows the results of the five vowel classification task using the baseline MFCC PCA Isomap and LLE features The percentage of phones correctly classified is given on the vertical axis The horizontal axis represents the dimensionality of the feature vector The results in Fig 1 can be summarized as follows 138 A Errity J McKenna and B Kirkpatrick Training Data 80 70 60 50 40 30 20 Classification Rate PCA MFCC Isomap LLE 0 2 4 6 8 10 12 14 Classification Rate Testing Data 65 55 45 35 25 15 0 2 4 6 8 Feature Dimension 10 12 14 Fig 2 Ten vowel classification results for baseline MFCC PCA Isomap and LLE features on the TIMIT 
database Table 1 Mean classification accuracy in the ten vowel classification task for MFCC PCA Isomap and LLE features Dimensions Manifold Learning Based Feature Transformation Training Data 85 75 65 55 PCA MFCC Isomap LLE 0 2 4 6 8 10 12 14 139 Classification Rate Classification Rate Testing Data 85 80 70 60 50 0 2 4 6 8 Feature Dimension 10 12 14 Fig 3 Phone class classification results for baseline MFCC PCA Isomap and LLE features on the TIMIT database Classification Rate Five Vowel 85 75 65 55 45 35 0 4 8 12 Ten Vowel 65 60 50 40 30 20 0 4 8 12 Phone Class 85 80 70 60 50 0 4 8 12 16 Feature Dimension 20 24 26 16 20 24 26 16 20 PCA MFCC Isomap LLE 24 26 Fig 4 Classification results for baseline MFCC PCA Isomap and LLE features used in three different phone classification tasks 4 2 Dynamic Features As detailed in Section 3 experiments were also performed using 26 dimensional MFCC vectors as high dimensional inputs to the three dimensionality reduction methods The results of performing phone classification 
using the features output by each of these methods and the original MFCC vectors are shown Classification Rate Classification Rate 140 A Errity J McKenna and B Kirkpatrick in Fig 4 These results are based on tests carried out on the testing data It can be seen that these results are very similar to those using the static features as detailed in Section 4 1 The manifold learning methods offer improved performance over both MFCC and PCA derived features in low dimensions In general features output by Isomap offer the best performance outperforming all other feature types in 74 36 of the classification tests shown in Fig 4 Classification performance on the training set was also found to be consistent with the static feature results 5 Conclusions In this study a phone classification approach using nonlinear manifold learningbased feature transformation was proposed and evaluated against a baseline linear dimensionality reduction method PCA and conventional MFCC features All of the dimensionality reduction 
methods presented outperform the baseline MFCC features for low dimensions This illustrates the capability of these methods to extract discriminating information from the original MFCC features Higher classification accuracy is shown for manifold learning derived features compared to baseline MFCC and PCA transformed features for low dimensions This indicates that manifold learning algorithms are more capable of retaining information required to discriminate between phones especially in low dimensional space This may be due to the ability of these methods to exploit nonlinear structure in the speech space In general Isomap was found to yield superior performance to both MFCC and PCA features Comparing the two manifold learning methods Isomap generally demonstrates better classification accuracy than LLE This indicates that preserving global structure rather than local relationships may be more important for speech feature transformation Acknowledgments Andrew Errity would like to acknowledge the support of 
the Irish Research Council for Science Engineering and Technology grant number RS 2003 114 References 1 Klein W Plomp R Pols L C W Vowel spectra vowel spaces and vowel identification J Acoust Soc Amer 48 4 Manifold Learning Based Feature Transformation 141 5 Jolliffe I Principal Component Analysis Springer Series in Statistics SpringerVerlag New York 1986 6 Duda R O Hart P E Pattern Classification and Scene Analysis Wiley New York 1973 7 Wang X Paliwal K K Feature extraction and dimensionality reduction algorithms and their applications in vowel recognition Pattern Recognition 36 10 Word Recognition with a Hierarchical Neural Network Xavier Domont1 2 Martin Heckmann1 Heiko Wersing1 Frank Joublin1 Stefan Menzel1 Bernhard Sendhoff1 and Christian Goerick1 Honda Research Institute Europe GmbH D 63073 Offenbach am Main Germany martin heckmann honda ri de Technische 1 2 Abstract In this paper we propose a feedforward neural network for syllable recognition The core of the recognition system is based on a 
hierarchical architecture initially developed for visual object recognition We show that given the similarities between the primary auditory and visual cortexes such a system can successfully be used for speech recognition Syllables are used as basic units for the recognition Their spectrograms computed using a Gammatone filterbank are interpreted as images and subsequently feed into the neural network after a preprocessing step that enhances the formant frequencies and normalizes the length of the syllables The performance of our system has been analyzed on the recognition of 25 different monosyllabic words The parameters of the architecture have been optimized using an evolutionary strategy Compared to the Sphinx 4 speech recognition system our system achieves better robustness and generalization capabilities in noisy conditions Keywords speechrecognition robustfeatures feed forwardarchitecture 1 Introduction Conventional speech recognition systems perform very well in clean scenarios but their performance 
drastically decreases in noisy environments This poor performance in adverse conditions prohibits the application of such systems for many scenarios especially our target scenario the control of a humanoid robot In contrast to this human speech perception is far less susceptible to such distortions 1 In this article we present a speech recognition system with a higher robustness towards noise and reverberation This system is based on a feedforward neural network inspired from an object recognition system Several studies have shown that auditory and visual primary cortices show substantial similarities In 1988 Sur et al have shown that the primary auditory cortex of young ferrets is plastic enough to allow the ferrets to attain visual M Chetouani et al Eds NOLISP 2007 LNAI 4885 pp Word Recognition with a Hierarchical Neural Network 143 Speech Signal Syllable Hypothesis Preprocessing FeatureSelective Layer Combination Layer SyllableTuned Units Hierarchical Architecture Fig 1 Overview of the word recognition 
system perception via the auditory cortex 2 More recently Shamma determined the shape of the time frequency receptive fields in the primary auditory cortex of newborn ferrets 3 They are selective to modulations in the time frequency domain and as in the visual cortex have Gabor like shapes These receptive fields have been modeled by Chin 4 and used for source separation 5 and speech detection 6 As Gabor like filters are extensively used in object recognition systems 7 8 we decided to develop a system for speech recognition by adapting the feedforward neural network initially developed by Wersing and 2 Preprocessing of the Spectrogram The preprocessing mainly aims at transforming a previously segmented speech signal corresponding to one syllable into an image that is fed into the hierarchical recognition architecture A two dimensional representation of a signal is obtained by computing its spectrogram In addition to the phonetic information the speech signal also contains many speaker and recording specific 
information As the phonetic information is chiefly conveyed by the formant trajectories we enhance them in the spectrograms prior to recognition 144 X Domont et al 6 5 3 8 2 1 1 2 0 6 0 3 0 0 1 0 2 0 3 Time s 0 4 6 5 3 8 2 1 1 2 0 6 0 3 0 0 1 0 2 0 3 Time s 0 4 Frequency kHz a Response of the basilar membrane Frequency kHz 0 1 0 2 0 3 Time s 0 4 Frequency kHz b Low pass filtering 6 5 3 8 2 1 1 2 0 6 0 3 0 6 5 3 8 2 1 1 2 0 6 0 3 0 0 1 0 2 0 3 Time s 0 4 Frequency kHz c Preemphasis d Mexican Hat filtering along the frequency axis Fig 2 Overview of the preprocessing step for the word list spoken by a female American speaker The 128 channels logarithmically span the frequency range from 80 Hz to 8 kHz The harmonic structure has been removed using a filtering along the frequency axis We used a Gammatone filterbank to compute the spectrogram of the signal It models the response of the basilar membrane in the human inner ear and is therefore adapted to a biology inspired system The signal s sampling frequency is 
16 kHz The filterbank has 128 channels ranging from 80 Hz to 8 kHz and follows the implementation given in 10 Figure 2 shows the response of the Gammatone filterbank after rectification a and low pass filtering b To compensate for the influence of the speech excitation signal the high frequencies are emphasized by 6 dB per octave resulting in a flattened spectrogram Fig 2 c Next the formant frequencies are enhanced by filtering along the channel axis using mexican hat filters Fig 2 d only the positive values are kept For the filtering the size of the kernel is channel dependent varying from 90 Hz for low frequencies to 120 Hz for high frequencies This takes the logarithmic arrangement of the center frequencies in the Gammatone filterbank into account Finally the length of the spectrogram is scaled using linear interpolation so that all the spectrograms feeding the recognition hierarchy have the same size The sampling rate is then reduced to 100 Hz By doing so syllables of different Word Recognition with a 
Hierarchical Neural Network 145 lengths are scaled to the same length This relies on the assumption that a linear scaling can handle variations in the length of the same syllable uttered at different speaking rates However these are known to be non linear In particular some parts of the signal like vowels are more affected by variation in the speech rate than other parts e g plosives The generalization over these variations is a main challenge in the recognition task In order to also assess the performance of the recognition hierarchy independent of this non linear scaling we applied the Dynamic Time Warping DTW method to the spectrograms For each syllable we selected one single repetition as reference template and aligned the other by DTW Afterwards the syllables were again scaled to the same length and downsampled At the output of the preprocessing stage the spectrograms feeding the recognition hierarchy have all the size of 3 The Recognition Hierarchy The preprocessed two dimensional spectrogram is from 
now on considered to be an image and feeds into a feedforward architecture initially aimed at visual object recognition However the structure of spectrograms differs from the structure of images taken from objects and while keeping the overall layout of the network described in 8 the receptive fields and the parameters of the neurons were retrained for the task of syllable recognition The recognition hierarchy is illustrated in Fig 3 3 1 Feature Selective Layer The first feature matching stage consists of a linear receptive field summation a Winner Take Most WTM and a pooling mechanism The preprocessed spectrogram is first filtered by eight different Gabor like filters The purpose of these filters is to extract local features from the spectrogram In 8 the receptive fields were chosen as four first order even Gabor filters For syllable recognition 8 receptive fields were learned using independent component analysis on 3500 randomly selected local patches of preprocessed spectrograms The WTM competition 
mechanism between features at the same position introduces nonlinearity into the system The value rl t f of the spectrogram in the lth neuron of the feature selective layer after the WTM competition is given at the position t f by the following equation rl t f 0 ql t f 1 M t f 1 1 if ql t f M t f 1 or M t f 0 else 1 where ql t f is the value of the spectrogram before the WTM competition M t f maxk qk t f the maximal value at position t f over the eight neurons and 0 1 1 is a parameter controlling the strength of the competition 146 X Domont et al Combination Layer Feature Selective Layer Syllable Tuned Units HAVE CHART LIST SHIPS HOW Fig 3 The system is based on a feedforward architecture with weight sharing and a succession of feature sensitive matching and pooling stages It comprises three stages arranged in a processing hierarchy A threshold 1 is applied to the activity rl t f This threshold is common for all the neurons in the layer The pooling performs a downsampling of the spectrogram by four in both 
time and frequency direction It is done by a Gaussian receptive field with width 1 The feature selective layer transforms the The goal of the combination layer is to detect relevant local feature combinations in the first layer Similar to the previous layer it consists of a linear receptive field summation a Winner Take Most and a pooling mechanism These combination cells are learned using the non negative sparse coding method NNSC as in 8 however no invariance transformations have been implemented at this stage Similarly to Non Negative Matrix Factorization NMF the NNSC method decomposes data vectors Ip into linear combinations with non negative weights sp i of non negative features wi by minimizing the following cost function E p Ip i sp i wi 2 p i sp i NNSC differs from NMF by the presence of a sparsity enforcing term in the cost function controlled by the parameter which aims at limiting the number of non zero coefficients required for the reconstruction Consequently if a feature appears often in the 
data it will be learned even if it can be obtained by a combination of two or more other features Therefore the NNSC is expected Word Recognition with a Hierarchical Neural Network 147 to learn complex and global features appearing in the data An comprehensive description of this method can be found in 11 For the proposed syllable recognition system 50 complex features wi have been learned from image patches extracted from the output of the feature selective layer At last a WTM competition 2 2 and pooling 2 are applied to the 50 neurons and their size is reduced to In the last stage of the architecture linear discriminant classifiers are learned based on the output of the combination layer A classical gradient descent is used for this supervised learning including an early stopping mechanism to avoid overfitting The obtained classifiers are called Syllable Tuned Units STUs in reference to the View Tuned Units used in 7 and 8 Due to the high dimensionality 640 and sparseness of the features after the 
combination layer learning the STUs is unproblematic 4 Optimization of the Architecture The performance of the recognition highly depends on the choice of the nonlinearities present in the hidden layers of the architecture i e the coefficients and the thresholds of the WTM competitions Eq 1 and the width of the poolings The six parameters 1 2 1 2 and 1 2 have to be tuned simultaneously and the receptive field of the combination layer as well as the Syllable Tuned Units have to be learned at each iteration similarly to the method used in 12 Practically this tuning of the model parameter set has been realized within an evolutionary optimization aiming at maximizing the recognition performance in a clean speech scenario Due to the stochastic components and the use of a population of solutions evolutionary algorithms need more quality evaluations than other algorithms but on the other hand they allow for a global search and are able to overcome local optima In the present context an evolutionary strategy with 
global step size adaptation GSA ES has been applied relying on similar ranges of the object variables Initially standard values see 13 14 have been used and then tuned in some test experiments to this specific task Based on these experiments we have chosen a population size of 32 individuals Each generation the two individuals with the best performance have been chosen as parents for the next generation The optimization parameters have been scaled and the initial global step size was set to 0 003 Although the evolutionary optimization used a clean scenario for the performance evaluation of each individual we will show that the optimized parameters are robust with respect to noisy signals 5 Recognition Performance In order to evaluate the performance of the system a database was built using 25 very frequent monosyllabic words extracted from the DARPA Resource 148 X Domont et al Word Error Rates 100 80 60 40 20 0 0 5 Step by step tuning Evolution Strategy 10 15 SNR dB 20 clean Fig 4 Improvement of the 
recognition performance using an evolutionary algorithm to tune the parameters compared to manual tuning one layer after the other The spectrograms are scaled using a linear interpolation Management RM database Isolated monosyllabic words have been chosen in lack of a syllable segmented database with sufficient size The words were segmented using forced alignment For each of the monosyllabic words we selected 140 occurrences from 12 different speakers 6 males and 6 females from the speaker dependent part of the database For training 70 repetitions of each word were used 20 for the early stopping validation of the Syllable Tuned Units and 50 for testing The performance of our system has been compared to the Sphinx 4 speech recognition system an open source speech recognition system that performs well on the whole RM corpus 15 MFCC features were used as front end for the HMMs 13 cepstral coefficients plus delta and double delta were computed using the default parameters of Sphinx Cepstral Mean Normalization 16 
has been used in order to improve the robustness of the MFCC features SphinxTrain was employed to train triphones HMMs Each model had 3 states without skip over states and each state used a mixture of 8 Gaussians The Hidden Markov Models were trained on the segmented monosyllabic words The robustness towards noise has been investigated by adding babble noise white noise and factory noise from the NOISEX database to the test database at different signal to noise ratios SNR while training was still performed on clean data Figure 4 illustrates the gain in performance on babble noise obtained using the evolutionary algorithm compared to a manual tuning of the parameters one layer after the other Following the notation introduced in 8 the optimal parameters given by the evolution strategy are 1 0 82 1 2 66 1 3 16 for the first layer and 2 0 84 2 2 78 2 1 87 for the second layer when linear interpolation is used to scale the signals Using a DTW the optimal set of parameters is 1 0 99 1 0 32 1 4 for the first layer 
and 2 0 89 2 0 99 2 1 93 As can be seen the performance increased due to the optimization at all SNR levels With clean speech we observe an improvement Word Recognition with a Hierarchical Neural Network 149 Word Error Rates Word Error Rates 100 80 60 40 20 0 0 5 SPHINX NN Input Aud Hierarchy 100 80 60 40 20 0 0 5 SPHINX Aud Hierarchy Aud Hier DTW 10 15 SNR dB 20 clean 10 15 SNR dB 20 clean a Spectrograms scaled using a linear interpolation b Spectrograms scaled using Dynamic Time Warping Fig 5 Comparison of the Word Error Rates WER between the proposed system and Sphinx 4 in the presence of babble noise Word Error Rates 80 60 40 20 0 0 5 Word Error Rates 100 SPHINX Aud Hierarchy Aud Hier DTW 100 80 60 40 20 0 0 5 SPHINX Aud Hierarchy Aud Hier DTW 10 15 SNR dB 20 clean 10 15 SNR dB 20 clean a White noise b Factory noise Fig 6 Comparison of the Word Error Rates WER between the proposed system and Sphinx 4 in the presence of white and factory noise from 6 72 to 5 44 19 relative The largest improvement was 
achieved at 15 dB SNR from 30 72 to 17 04 44 5 relative Figure 5 summarizes the performance of both Sphinx 4 and the proposed system in presence of a babble noise To measure the baseline similarities of the image ensemble we also give the performance of a nearest neighbor classifier NN that matches the test data against all available training views An exhaustive storage of examples is however not a viable model for auditory classification With clean signals the STUs show better generalization capabilities and perform better than a nearest neighbor on the input layer Fig 5 a For noisy signals the STUs are slightly worse however at a strong reduction of representational complexity 150 X Domont et al With a simple linear time scaling our system only outperforms Sphinx 4 in noisy conditions but shows inferior performance on clean data When Dynamic Time Warping is used to properly scale the signals the STUs improve the already good performance obtained directly after the preprocessing in all the cases and our 
system outperforms Sphinx 4 even for clean signals Fig 5 b With clean data Sphinx obtains a 3 1 Word Error Rate WER our system achieves 0 9 WER with the DTW and 5 4 without the DTW Figure 6 shows that the performance is very similar when adding white or factory noise 6 Discussion and Summary In this paper we presented a novel approach to speech recognition interpreting spectrograms as images and deploying a hierarchical object recognition system To optimize the main free parameters of the system we used an evolutionary algorithm which allows us to quickly change the system without the need for manual parameter tuning We could show that our system performs better than a state of the art system in noisy conditions even when we applied a simplistic linear scaling of the input for time alignment When we aligned the current utterance with the DTW to a known representation in an optimal non linear way we obtained better than state of the art results for all cases tested However in its current form the DTW makes 
use of information not available in real situations From this we conclude that our architecture and the underlying features are more robust against noise than the commonly used mel frequency cepstral coefficients MFCCs This robustness against noise is very important for real world scenarios which are usually characterized by significant background noise and variations in the recording conditions A similar robustness was also observed for visual recognition in clutter scenes 8 Our comparison between the linear scaling and the DTW shows that the performance of the model could be significantly improved by better temporal alignment We therefore consider methods for improving this alignment as interesting future research directions The complexity of our recognition task is very low Therefore it remains an open question how our system will scale to more complex tasks We can expect that our system generalizes well to larger vocabulary In fact the high dimensionality and the sparseness of the vector space at the 
output of the combination layer should allow to train STUs for a large number of syllables In order to process continuous speech syllable segmentation is required One way to obtain this segmentation is to implement a syllable segmentation system prior to the recognition This would allow to keep the advantages of the recognition hierarchy its fast implementation and the capacity to train or update STUs on the fly Another possibility is to use the architecture as a front end for Hidden Markov Models similarly to 17 Word Recognition with a Hierarchical Neural Network 151 References 1 Lippmann R Speech recognition by machines and humans Speech Communication 22 1 Hybrid Models for Automatic Speech Recognition A Comparison of Classical ANN and Kernel Based Methods Ana I Department of Signal Theory and Communications EPS Universidad Carlos III de Madrid Avda de la Universidad 30 28911 Abstract Support Vector Machines SVMs are state of the art methods for machine learning but share with more classical Artificial 
Neural Networks ANNs the difficulty of their application to input patterns of non fixed dimension This is the case in Automatic Speech Recognition ASR in which the duration of the speech utterances is variable In this paper we have recalled the hybrid ANN HMM solutions provided in the past for ANNs and applied them to SVMs performing a comparison between them We have experimentally assessed both hybrid systems with respect to the standard HMM based ASR system for several noisy environments On the one hand the ANN HMM system provides better results than the HMM based system On the other the results achieved by the SVM HMM system are slightly lower than those of the HMM system Nevertheless such a results are encouraging due to the current limitations of the SVM HMM system Keywords Robust ASR Additive noise Machine Learning Hybrid systems Artificial Neural Networks Support Vector Machines Hidden Markov Models 1 Introduction Hidden Markov Models HMMs have become the most employed core technique for Automatic 
Speech Recognition ASR After several decades of intense research work in the field it seems that HMM based ASR systems are very close to reach their limit of performance Some alternative approaches most of them based on Artificial Neural Networks ANNs were proposed during the late eighties and early nineties Among them it is worth to mention hybrid ANN HMM systems see 1 for an overview since the reported results were comparable or even slightly superior to those achieved by HMMs On the other hand during the last decade a new tool appeared in the field of machine learning that has proved to be able to cope with hard classification problems in several fields of application the Support Vector Machines SVMs 2 The M Chetouani et al Eds NOLISP 2007 LNAI 4885 pp Hybrid Models for Automatic Speech Recognition 153 SVMs are effective discriminative classifiers with several outstanding characteristics namely their solution is that with maximum margin they are capable to deal with samples of a very high dimensionality 
and their convergence to the minimum of the associated cost function is guaranteed Nevertheless it seems clear that the application of these kernel based machines to the ASR problem is not straightforward In our opinion there are three main difficulties to overcome 1 SVMs are originally static classifiers and have to be adapted to deal with the variability of duration of speech utterances 2 the SVMs were originally formulated as binary classifiers while the ASR problem is multiclass and 3 current SVM training algorithms are not able to manage the huge databases typically used in ASR In order to cope with these difficulties some researchers have suggested hybrid SVM HMM systems 3 4 that notably resemble the previous hybrid ANN HMM systems 5 In this paper we comparatively describe both types of hybrid systems SVM and ANN HMM highlighting both their common fundamentals and their special characteristics and conduct an experimental performance comparison for both clean and noisy speech recognition tasks 2 Hybrid 
Systems for ASR As a result of the difficulties found in the application of ANNs to speech recognition mostly motivated by the duration variability of the speech instances corresponding to the same class a variety of different architectures and novel training algorithms that combined both HMMs with ANNs were proposed in the late eighties and early nineties For a comprehensive survey of these techniques see 1 In this paper we have focused on those that employ ANNs and SVMs to estimate the HMM state posterior probabilities proposed by Bourlard and Morgan 5 6 The starting point for this approach is the well know property of using feedforward networks such as multi layer perceptrons MLPs for estimating aposteriori probabilities given two conditions 1 there must be high enough number of input samples to train a good approximation between the input and output layers and 2 a global minimum error criterion must be used to train the network for example mean square error or relative entropy The fundamental advantage 
of this approach is that it introduces a discriminative technique ANN into a generative system HMM while retaining their ability to handle the temporal variability of the speech signal However this original formulation had to be modified to estimate the true emission likelihood probabilities by applying the Bayes rule Therefore the aposteriori probabilities should be normalized by the class priors to obtain what is called scaled likelihoods This fact was further reinforced by posterior theoretical developments in the search of a global ANN optimization procedure see 7 Thus systems of this type keep being locally discriminant given that the ANN was trained to estimate a posteriori probabilities However it can also be shown that in theory HMMs can be trained using local posterior probabilities 154 A I as emission probabilities resulting in models that are both locally and globally discriminant The problem is that there are generally mismatches between the prior class probabilities implicit to the training data 
and the priors that are implicit to the lexical and syntactic models that are used in recognition In fact some experimental results show that for certain cases the division by the priors is not necessary 7 Among the advantages of using hybrid approaches we highlight the following from 7 3 3 1 Experimental Setup Database We have used the well known SpeechDat Spanish database for the fixed telephone network 8 This database comprises recordings from 4000 Spanish speakers recorded at 8 KHz over the fixed PSTN using an E 1 interface in a noiseless office environment In our experiments we have used a large vocabulary more than 24000 words continuous speech recognition database The training set contains approximately 50 hours of voice from 3146 speakers 71000 utterances The callers spoke 40 items whose contents are varied comprising isolated and connected digits natural numbers spellings city and company names common applications words phonetically rich sentences etc Most items are read and some of them are 
spontaneously spoken The test set corresponding to a connected digits task contains approximately 2122 utterances and 19855 digits 5 hours of voice from 499 different speakers 3 2 Parameterization In our experiments we have used the classical parameterization based on 12 MFCCs Mel Frequency Cepstral Coefficients plus energy and the first and second derivatives These MFCCs are computed every 10 ms using a time window of 25 ms Thus the resulting feature vectors have 39 components In this work we have considered a per utterance normalization that is every parameter is normalized in mean and variance according to the following expression x i n xi n f f 1 Hybrid Models for Automatic Speech Recognition 155 where xi n represents the ith component of the feature vector corresponding to frame n f is the estimated mean from the whole utterance and f is the estimated standard deviation As a result per utterance normalization will be more appropriate in the case of noisy environments where training and testing 
conditions do not match 3 3 Database Contamination We have tested our systems in clean conditions and in presence of additive noise For that purpose we have used two different kinds of noises white and babble extracted from the NOISEX database 9 These noises have been added to the clean speech signals at four different signal to noise ratios SNRs namely 12 dB 9 dB 6 dB and 3 dB Only the testing subset has been corrupted in the way previously stated whereas the acoustic models GMMs Gaussian Mixture Models in the case of the baseline HMM system and the MLPs and the SVMs in the case of the hybrid systems have been estimated or trained using only clean speech 3 4 Baseline Experiment with HMMs The recognition rates achieved by a left to right HMM based recognition system based in the COST 249 SpeechDat Reference Recognizer will be our reference results We use 18 context dependent phones this is the number of phones usually used for digits recognition tasks in Spanish with 3 states per phone Emission probabilities 
for each state were modeled by a mixture of 32 Gaussians 3 5 Experiments with Hybrid Recognition Systems In this work we consider two different hybrid recognition systems an ANN HMM system and a SVM HMM one Both of them use a Viterbi decoder with aposteriori probabilities as local scores as discussed in section 2 The whole hybrid recognition system is composed of two stages The first one estimates initial evidences for phones in the form of a posteriori probabilities using an MLP or an SVM The second stage consists of a classical Viterbi decoder where we replace the likelihoods estimates provided by the reference HMM based recognition system by the posteriors obtained in the first stage For the hybrid systems presented in this paper we have partitioned every phone into three segments For this purpose we have obtained a segmentation of the training database by performing a forced alignment with the HMM baseline system considering each segment delimited by the state transitions Experimental results with both 
ANN HMM and SVM HMM hybrid systems show significant improvements in the word recognition rate due to the use of three classes per phone especially for the case of the SVM based system see 4 Whereas the reference HMM based recognition system uses the whole training data set 71000 utterances the hybrid SVM based recognition system only uses a small portion of the available training data due to a practical limitation regarding the number of training samples that the SVM software can consider 156 A I Thus in order to compare the two hybrid systems we decided to use the same small quantity of training data in the ANN HMM hybrid system although we also obtained some results using the whole training data set Therefore we have considered useful to evaluate the evolution of the accuracy of each system performing incremental tests using balanced subsets of the available training data set equal number of frames per class three classes per phone randomly selected from the whole training set between 250 and 20000 frames 
per class Experiments with ANNs A posteriori probabilities used by the Viterbi decoder are obtained using a MLP trained on either a smaller version of the training data set or the whole training data set as we mentioned before The MLP has one hidden layer with 1800 units There are 39 input units corresponding to the feature vector dimension described in section 3 2 and 54 output units each of them corresponding to one of the three parts of the 18 phones considered as we described in section 3 5 The MLP is trained using the relative entropy criterion and the back propagation factor was experimentally fixed at 0 02 by using a separate tuning set Experiments with SVMs In this case a multiclass SVM using the 1 vs 1 approach is used to estimate posteriors for each frame using Platt s approximation 10 The SVM uses a RBF Radial Basis Function kernel whose parameter must be tuned by means of a cross validation process as well as a parameter C which establishes a compromise between error minimization and 
generalization capability in the SVM The values we have used in our experiments are C 2 and 0 03125 also obtained empirically using the tuning set we already mentioned in 3 5 For more details about the hybrid SVM HMM system please refer to 4 4 Results and Discussion This section is devoted to the presentation and discussion of the results obtained by the systems described in the previous section Preliminary experiments show a similar behaviour of both SVMs and ANNs at a frame classification level We can also see in figure 1 that better results are achieved when more samples are added to the training database up to a final frame recognition rate around 72 obtained for the maximum number of input samples that our SVM based system can handle 1080000 20000 frames per class Nevertheless when we use our MLP based system that can handle the whole and not balanced set of input samples 16378624 frames we manage to improve this frame recognition rate up to 78 47 Similar behaviour is expected for the SVM HMM hybrid 
system if the employed software could process such an amount of input samples We compare our hybrid systems to the standard HMM based speech recognition system at word and sentence levels in the different noise environments described in section 3 3 We can see in figures 2 and 3 the Word Recognition Rate WRR and the Sentence Recognition Rate SRR respectively of the Hybrid Models for Automatic Speech Recognition 157 Fig 1 Frame recognition rate of ANNs and SVMs Word Re cognition Rate 1 080 000 training sample s for ANN HM M and SVM HM M HMM 100 95 90 85 80 75 70 65 60 55 50 12 dB Clean 9 dB 6 dB 3 dB 12 dB 9 dB 6 dB 3 dB A NN HMM SV M HMM WRR White Noise Environm e nt Babble Noise Fig 2 Word recognition rate of HMMs and Hybrid ANN and SVM based systems three systems Thus we can notice that using only a 6 6 of the available data samples our hybrid systems get results which are comparable or even better in the case of the ANN HMM system than the standard HMM based system trained using the entire database 158 A I 
Se nt Re cognition Rate 1 080 000 training sample s for ANN HM M and SVM HM M HMM 100 90 80 70 60 50 40 30 20 10 0 12 dB Clean 9 dB 6 dB 3 dB 12 dB 9 dB 6 dB 3 dB A NN HMM SV M HMM SRR White Noise Environm e nt Babble Noise Fig 3 Sentence recognition rate of HMMs and Hybrid ANN and SVM based systems The SVM software used in the experiments 11 which requires to keep the kernel matrix in memory is the responsible for the limit in the SVM HMM training data set However without this restriction we have observed in the ANN HMM system that the more samples we add to the training database the higher is the improvement in WRR and SRR with regard to the baseline HMM based system as we could see at the frame classification level In addition as we have stated in Section 2 both SVMs and ANNs provide a posteriori probabilities to the Viterbi decoder whereas what we really need and HMMs compute are likelihoods 5 We tried to use likelihoods in the ANN HMM system but we achieved worse results than that of posterior 
probabilities in all the cases except for the case we train the ANN HMM hybrid using the entire database If we analyze these results we can see that in fact when we train the MLP using a balanced set of samples the a priori probability is the same for all the classes and posteriors provided by the MLP are actually scaled likelihoods 5 Conclusions The clear success of SVMs classifiers in several fields of application has called the attention of researchers in the field of ASR The first attempts to use SVMs for connected digit recognition have resulted in hybrid SVM HMM systems 4 that resemble the hybrid systems based on ANN proposed during the last decade Consequently it becomes relevant to compare the performance achieved by both types of systems Furthermore since the robustness of the ASR systems is one Hybrid Models for Automatic Speech Recognition 159 of the current open problems the comparative assessment of hybrid systems should be carried out in a noisy environment The ANN HMM and SVM HMM hybrid 
systems presented in this work are inspired in the work due to Bourlard and Morgan 5 Our more significant contribution with respect to this reference consists of using sub phone units Specifically three classes parts per phone are considered instead of one Some limitations regarding the publicly available software implementation of the SVMs have prevented us to train our hybrid SVM HMM ASR system using the whole training set Therefore in order to carry out a fair comparison the hybrid ANN HMM has been trained using the same small subset of the training set In this conditions the achieved results can be summarized as follows References 1 Trentin E Gori M A Survey of Hybrid ANN HMM Models for Automatic Speech Recognition Neurocomputing 37 160 A I 5 Bourlard H Morgan N Connectionist Speech Recognition a Hybrid Approach Boston Kluwer Academic Norwell MA USA 1994 6 Morgan N Bourlard H Continuous Speech Recognition An Introduction to the Hybrid HMM Connectionist Approach IEEE Signal Processing Magazine Towards 
Phonetically Driven Hidden Markov Models Can We Incorporate Phonetic Landmarks in HMM Based ASR Guillaume Gravier and Daniel Moraru Institut de Recherche en Informatique et Syst emes Abstract Automatic speech recognition mainly relies on hidden Markov models HMM which make little use of phonetic knowledge As an alternative landmark based recognizers rely mainly on precise phonetic knowledge and exploit distinctive features We propose a theoretical framework to combine both approaches by introducing phonetic knowledge in a non stationary HMM decoder To demonstrate the potential of the method we investigate how broad phonetic landmarks can be used to improve a HMM decoder by focusing the best path search We show that assuming error free landmark detection every broad phonetic class brings a small improvement The use of all the classes reduces the error rate from 22 to 14 on a broadcast news transcription task We also experimentally validate that landmarks boundaries does not need to be detected precisely and 
that the algorithm is robust to non detection errors 1 Data Driven vs Knowledge Based Speech Recognition In hidden Markov models HMM based speech recognition systems the decoding process consists in compiling a graph which includes all the available sources of knowledge language model pronunciations acoustic models before finding out the best path in the graph in order to obtain the best word sequence w arg max p y w p w w 1 At the acoustic level this approach relies on data driven methods that learn from examples Therefore integrating explicit phonetic knowledge in such systems is difficult Alternately various studies aim at explicitly relying on phonetic knowledge to represent the speech signal for automatic speech recognition 1 2 3 These approaches are most of the time based on the extraction of a set phonetic features a k a landmarks on top of which a model either rule based or statistical based is build for the purpose of recognition Phonetically driven ASR relies on fine grain phonetic features such as 
onset and offset times 2 and distinctive features 1 3 However in practice automatically detecting such features might be M Chetouani et al Eds NOLISP 2007 LNAI 4885 pp 162 G Gravier and D Moraru difficult and error prone in particular in the case of noisy signals or spontaneous speech This work presents a preliminary study which aims at bridging these two paradigms in order to make use of explicit phonetic knowledge in the framework of HMM While landmark based systems use phonetic landmarks as a feature describing the signal the idea of our approach is to use landmarks in order to guide the search for the best path during Viterbi decoding in an HMM based system Hence prior knowledge on the nature of the signal is used as anchor points during decoding We will use indistinctly the two terms landmark and anchor to designate constraints on the search The aim of this study is twofold The first aim is to define a theoretical framework to incorporate phonetic knowledge in HMM based systems using anchor points and 
to experimentally validate this approach This framework allows for uncertainty in the landmark detection step though this is not validated in the study as of now The second aim is to study which landmarks effectively complements the data driven knowledge embedded in HMM systems We believe that detecting fine grain phonetic features is a particularly challenging 2 Landmark Driven Viterbi Decoding Most HMM based systems rely on the Viterbi algorithm in order to solve 1 along with pruning techniques to keep the search tractable for large vocabularies We briefly recall the basics of the Viterbi algorithm before extending this algorithm for the integration of phonetic anchors 2 1 Beam Search Viterbi Decoding The Viterbi algorithm aims at finding out the best alignment path in a graph using dynamic programming DP on a trellis The DP algorithm proceeds incrementally by searching for the best hypothesis reaching the state j t of the trellis according to S j t max S i t 1 ij ln p yt j i 2 Towards Phonetically Driven 
Hidden Markov Models 163 where j is the state in the decoding graph and t the frame index in the observation sequence In 2 ij denotes the weight for the transition from state i to j in the graph while p yt j denotes the likelihood of the feature vector yt conditional to state j Hence S i t represents the score of the best partial path ending in state i at time t In practice not all paths are explored in order to keep the algorithm tractable on large decoding graphs Unlikely partial hypotheses are pruned according to the score of the best path ending at time t 2 2 Introducing Landmarks Landmarks can be considered as hints on the best path For example if a landmark indicates that a portion of an utterance corresponds to a vowel then we can constrain the best path to be consistent with this piece of information since nodes in the decoding graph are linked to phonemes One easy way to do this is to penalize or even prune all the paths of the trellis which are inconsistent with the knowledge brought by the 
landmark Assuming confidence measures are associated with the landmarks the penalty should be proportional to the confidence Formally the above principle can be expressed using non stationary graphs i e graphs whose transition weights are dependent on time The idea is that if a transition leading to state i t of the trellis is inconsistent with the landmark knowledge then the transition cost increases In order to do this we replace in 2 the transition weights ij by ij t ij t Ij t 3 Ij t is an indicator function whose value is 0 if node j is compatible with the available anchor information and 1 otherwise The penalization term t 0 reflects the confidence in the anchor available at time t if any Hence if no anchor is available or if a node is consistent with the anchor no penalty is applied In the opposite case we apply a penalty where the higher the confidence in the landmark the higher the penalty In the ideal case where landmark detection is perfect setting t enables to actually prune paths inconsistent 
with the landmarks In 3 one can notice that the penalty term only depends on the target state j and hence the proposed scheme is equivalent to modifying the state conditional probability p yt j to include a penalty However introducing the penalty at the transition level might be useful in the future to introduce phonological constraints or word level constraints A by product of the proposed method is that decoding should be much faster with landmarks as adding a penalty will most likely result in inconsistent paths being pruned In this preliminary study we use manually detected landmarks in order to investigate whether or not broad phonetic landmarks can help and to what extent in an ideal case We will therefore set t t in all the experiments described in section 4 164 G Gravier and D Moraru 3 Baseline System Before describing the experiments we briefly present the data and baseline system used Experiments are carried out on a radio broadcast news corpus in the French language The data used is a 4 hour 
subset of the development data for the ESTER broadcast news rich transcription evaluation campaign 9 The corpus mostly contains high fidelity planned speech from professional radio speakers Interviews however contain more spontaneous speech from non professional speakers sometimes in degraded acoustic conditions The entire data set was labeled phonetically based on the reference orthographic transcription using our ASR system to select pronunciation variants Two reference systems were used in this study Both systems are two pass systems where a first pass aims at generating a word graph which is then rescored in a second pass with more sophisticated acoustic models The two systems differ in the complexity of the acoustic models used for the word graph generation context independent models are used in the first system while word internal context dependent ones are used in the second one Clearly using landmarks to guide the decoding is more interesting when generating the word graph as it should enable better 
and smaller word graphs which already take into account the landmark knowledge Therefore the reason for comparing two systems for word graph generation is to determine to what extent phone models capture broad phonetic information Both transcription passes are carried out with a trigram language model Monophone acoustic models have 114 states with 128 Gaussians per state while the word internal triphone models have 4 019 distinct states with 32 Gaussians each Cross word triphones models are used for word graph rescoring with about 6 000 distinct states and 32 Gaussians per state 4 Broad Phonetic Landmarks The experiments described in this section are performed using manually detected broad phonetic landmarks the goal being to measure the best expected gain from the use of such landmarks The main motivation for using this type of landmarks as opposed to distinctive features is that we believe that reliable and robust automatic broad phonetic landmark detectors can be build For example in 6 7 8 to cite a few 
good results are reported on the detection of nasals and vowels Fricatives also seems relatively easy to detect using energy and zero crossing rate information Moreover we observed that most of the time the heap of active hypotheses in the ASR system contains hypotheses corresponding to different broad phonetic classes Though this is normal since hypotheses correspond to complete partial paths rather than to local decisions this observation indicates that a better selection of the active hypotheses based on locally detected landmarks is bound to improve the results Towards Phonetically Driven Hidden Markov Models 165 Table 1 Word error rate in as a function of the landmarks used The landmark ratio indicates the amount of signal in for which a landmark is available landmarks landmark ratio none all 43 6 VSF 34 6 vow stop fri nas gli 18 3 9 0 7 3 2 8 6 2 monophones passe 1 passe 2 triphones passe 1 passe 2 29 2 22 3 27 3 21 3 15 3 21 7 13 9 17 6 19 6 23 9 15 0 18 2 26 6 26 5 27 5 27 8 25 1 21 2 20 7 21 0 21 5 
20 1 27 0 26 3 26 0 26 4 24 9 20 7 20 4 20 3 20 7 19 6 4 1 Landmark Generation Five broad phonetic classes are considered in this study namely vowels fricatives stops nasal consonants and glides Landmarks are generated from the available phonetic alignments obtained from the orthographic transcription For each phone a landmark corresponding to the broad phonetic class to which the phone belongs is generated centered on the phone segment The landmark duration is proportional to the phone segment length In the first set of experiments the landmark length is set to 50 of the phone segment length We study in section 4 3 the impact of the landmark duration 4 2 Which Landmarks The first question to answer is what is the optimal improvement that can be obtained using each broad phonetic class separately Results are given in table 1 for the monophone and triphone systems after the first and second pass with each landmark type taken separately Results using all the landmarks or only vowel stop and fricative landmarks 
are also reported Results show a small improvement for each type of landmarks thus clearly indicating that the transcription system is not misled by phones from a particular broad phonetic class The best improvement is obtained with landmarks for glides that correspond to highly transitory phones which are difficult to model in particular because of co articulation effects More surprisingly vowel landmarks yield a small but significant improvement in spite of the fact that the phone models used in the ASR system do little confusions between vowels and other phones This result is due to the fact that the DP maximization not only depends on the local state conditional probabilities but also on the score of the entire path resulting in an hypothesis In other words even if the local probabilities p yt i are much better for states corresponding to a vowel than for states corresponding to some other class some paths incompatible with the knowledge of a vowel landmark might get a good cumulated score and are 
therefore kept in the heap of active hypotheses Using the landmark driven version of the Viterbi 166 G Gravier and D Moraru algorithm actually removes such paths from the search space thus explaining the gain obtained with vowel landmarks Clearly using all the available landmarks strongly improves the WER for both systems the improvement being unsurprisingly better for the monophone based system One interesting point to note is that when using all the landmarks the two systems exhibit comparable levels of performance with a slight advantage for the monophone system This advantage is due to the fact that the word graph generated with the monophone system contains more sentence hypotheses than the one generated with the triphone system though both graphs have roughly the same density A last point worth noting is the rather good performance obtained after the first pass using the monophone system This result suggest that combining landmark driven decoding with fairly simple acoustic models can provide good 
transcriptions with a limited amount of computation Indeed the average number of active hypotheses and hence the decoding time is divided by a factor of four when using landmarks In a practical setting the reliable detection and segmentation of a signal into broad phonetic classes is somewhat unrealistic the detection of nasals and glides being a rather difficult problem However detecting vowels stops and fricatives seems feasible with a great accuracy We therefore report results using only landmarks from those three classes VSF results in table 1 Using such landmarks a nice performance gain can still be expected in particular with a monophone based word graph generation These results show the optimal gain that can be obtained using broad phonetic landmarks as anchors in a Viterbi decoding thus justifying further work on landmark detection 4 3 Landmark Precision Two questions arise regarding the precision of the landmark detection step The first question is to determine whether a precise detection of the 
landmark boundaries is necessary or not The second question concerns the robustness to detection errors of the proposed algorithm Temporal Precision Table 2 shows the word error rate for the two systems as a function of the landmark extent where the extent is defined as the relative duration with respect to the phone used to generate the landmark An extent of 10 therefore means that the duration of a landmark is 0 1 times that of the corresponding phone All the landmarks are considered in these experiments Unsurprisingly the longer the landmarks the better the transcription It was also observed that longer landmarks reduce the search space and yield smaller yet better word graphs In spite of this most of the improvement comes from the fact that landmarks are introduced no matter their extent Indeed with a landmark extent of only 5 the word error rate decreases from 22 3 to 14 3 with the monophone system When increasing the landmark extent to 50 the gain is marginal with a word error rate of 13 9 Note that 
with an extent of 5 the total duration of landmarks corresponds to 4 4 of the total Towards Phonetically Driven Hidden Markov Models Table 2 WER in as a function of the landmark duration extent in monophones triphones 0 5 10 20 167 22 3 14 3 14 4 14 3 20 14 2 13 8 13 5 15 2 15 1 15 0 14 8 14 4 14 3 duration of the signal and therefore landmark based pruning of the hypotheses heap happens only for 4 4 of the frames Similar conclusions were obtained using only the vowel landmarks This is a particularly interesting result as it demonstrates that landmark boundaries do not need to be detected precisely Reliably detecting landmarks on some very short portion of the signal one or two frames is sufficient to drive a Viterbi decoder with those landmarks Robustness to non detection errors In the absence of confidence measures landmark driven Viterbi is highly sensitive to detection errors Clearly false alarms i e insertion and confusion errors have detrimental effects on the system However miss detection errors are 
less disastrous Therefore automatic broad phonetic landmark detection systems should be designed to have as low as possible a false alarm rate However lower false alarm rates unfortunately imply higher miss detection rates We tested the robustness of our landmarkdriven decoder by simulating miss detection errors at various rates assuming a uniform distribution of the errors across the five broad phonetic classes Results show that the word error rate is a linear function of the miss detection rate For example with the monophone system the word error rate is 17 9 resp 15 8 for a miss detection error rate of 50 resp 25 5 Discussion The preliminary experiments reported in this paper are encouraging and prove that integrating broad phonetic landmarks in a HMM based system can drastically improve the performance assuming landmarks can be detected reliably These results also validate the proposed paradigm for the integration of various sources of knowledge phonetic knowledge via landmarks and data driven knowledge 
acquired by the HMM However results are reported in an ideal laboratory setting where landmark detection is perfect The first step is therefore to work on robust detectors of broad phonetic landmarks at least for vowels stops and fricatives in order to validate the proposed paradigm in practical conditions Experiments carried out with broad phonetic segmentation using HMM on top of cepstral coefficients along with a trigram model resulted in an accuracy of 76 6 which did not prove sufficient to provide reliable landmarks Indeed landmarks extracted from this segmentation with a landmark extent of 20 are incorrectly labeled for 10 8 of the time which resulted in a small increase of the WER from 22 3 to 23 2 with vowels stops and fricative landmarks 168 G Gravier and D Moraru However the segmentation system is naive in the sense that it relies on the same features and techniques than the ASR system and therefore does not bring any new information Still HMM based broad phonetic segmentation used as is does not 
seem reliable enough for landmark detection Promising results were obtained with support vector machines applied to the classification of isolated frames into broad phonetic classes with miss rates between 2 and 5 for a false alarm rate of 0 5 These results still need to be References 1 Liu S A Landmark detection for distinctive feature based speech recognition PhD thesis Massachusetts Institute of Technology 1995 2 Juneja A Speech recognition based on phonetic features and acoustic landmarks PhD thesis University of Maryland 2004 3 John Hopkins University Center for Language and Speech Processing Landmarkbased speech recognition report of the 2004 John Hopkins Summer Workshop John Hopkins University Center for Language and Speech Processing 2005 4 McDermott E Hazen T Minimum classification error training of landmark models for real time continuous speech recognition In Proc IEEE Intl Conf Acoust Speech Signal Processing vol 1 2004 5 Schutte K J G Robust detection of sonorant landmarks In European Conf on 
Speech Communication and A Hybrid Genetic Neural Front End Extension for Robust Speech Recognition over Telephone Lines Sid Ahmed Selouani1 Habib Hamam2 and Douglas O Shaughnessy3 1 3 Abstract This paper presents a hybrid technique combining the Karhonen Loeve Transform KLT the Multilayer Perceptron MLP and Genetic Algorithms GAs to obtain less variant Mel frequency parameters The advantages of such an approach are that the robustness can be reached without modifying the recognition system and that neither assumption nor estimation of the noise are required To evaluate the effectiveness of the proposed approach an extensive set of continuous speech recognition experiments are carried out by using the NTIMIT telephone speech database The results show that the proposed approach outperforms the baseline and conventional systems 1 Introduction Adaptation to the environment changes and artifacts remains one of the most challenging problems for the Continuous Speech Recognition CSR systems The principle of CSR 
methods consists of building speech sound models based on large speech corpora that attempt to include common sources of variability that may occur in practice Nevertheless not all situations and contexts can be exhaustively covered As speech and language technologies are being transferred to real applications the need for greater robustness in recognition technology becomes more apparent when speech is transmitted over telephone lines when the signal to noise ratio SNR is extremely low and more generally when adverse conditions and or unseen situations are encountered To cope with these adverse conditions and to achieve noise robustness different approaches have been studied Two major approaches have emerged The first approach consists of preprocessing the corrupted speech input signal prior to the pattern matching in an attempt to enhance the SNR The second approach attempts to establish a compensation method that modifies the pattern matching itself to account for the effects of noise Methods in this 
approach include noise masking the use of robust distance measures and HMM decomposition For more details see 5 M Chetouani et al Eds NOLISP 2007 LNAI 4885 pp 170 S A Selouani H Hamam and D O Shaughnessy As an alternative approach we propose a new enhancement scheme based on the combination of subspace filtering the Multilayer Perceptron MLP and Genetic Algorithms GAs to obtain less variant Mel frequency parameters The enhanced parameters are expected to be insensitive to the degradation of speech signals due to telephone channel degradation The main advantages of such an approach over the compensation method are that the robustness can be reached without modifying the recognition system and without requiring assumption or estimation of the noise This paper is organized as follows In section 2 we describe the basis of the signal subspace approach namely the Karhonen Loeve Transform KLT and the extension we proposed to enable the use of the technique in the Mel frequency space In section 3 we briefly describe 
the principle of MLP based enhacement method Then we proceed in section 4 with the description of the evolutionarybased paradigm that we introduced to perform noise reduction In section 5 we evaluate the hybrid MLP KLT GA based front end technique in the context of telephone speech Finally in section 6 we conclude and discuss our results 2 Signal and Mel frequency Subspace Filtering The principle of the signal subspace techniques is based on the construction of an orthonormal set of axes These axes point in the directions of maximum variance thus forming a representational basis that projects on the direction of maximum variability Applied in the context of noise reduction these axes enable decomposing the space of the noisy signal into a signal plus noise subspace and a noise subspace The enhancement is performed by removing the noise subspace and estimating the clean signal from the remaining signal space The decomposition of the space into two subspaces can be performed by using KLT eigendecomposition Let 
x x1 x2 xN T be an N dimensional noisy observation vector which can be written as the sum of an additive noise distortion vector w and the vector of clean speech samples s The noise is assumed to be uncorrelated with the clean speech Further let Rx Rs and Rw be the covariance matrices from x s and w respectively The eigendecomposition of Rs is given by Rs Qs QT where s diag s 1 s 2 s N is the diagonal matrix of eigenvalues given in a decreasing order The eigenvector matrix Q of the clean speech covariance matrix is identical to that of the noise Major signal 2 2 I where w is subspace techniques assume the noise to be white with Rw w the noise variance and I the identity matrix Thus the eigendecomposition of 2 I QT The enhancement is performed by asRx is given by Rs Q s w suming that the clean speech is concentrated in an r N dimensional subspace the so called signal subspace whereas the noise occupies the N r dimensional observation space Then the noise reduction is obtained by considering only the signal 
subspace in the reconstruction of the enhanced signal Mathematically it consists of finding a linear estimate of s given by s Fx Fs Fw where F is the enhancement filter This filter matrix F can be written as follows F Qr Gr QT r in which the diagonal matrix Gr contains the weighting factors A Hybrid Genetic Neural Front End Extension 171 gi with i 1 r for the eigenvalues of the noisy speech Perceptually meaningful weighting functions exist to generate gi These functions are empirically guided in order to constitute an alternative choice for gi which results in a more or less aggressive noise suppression depending on the SNR In 1 the linear estimation of the clean vector is performed using two perceptually meaningful weighting functions The first function is given by gi x i 2 x i w i 1 r 1 where 1 The second function constitutes an alternative choice for gi which results in a more aggressive noise suppression gi exp 2 w xi i 1 r 2 The value of the parameter is to be fixed experimentally Instead of dealing 
with the speech signal we chose to use the noisy MelFrequency Cepstral Coefficients MFCC vector C as well The reason is that these parameters are suited to speech recognition due to the advantage that one can derive from them a set of parameters which are invariant to any fixed frequency response distortion introduced by either the adverse environments or the transmission channels 6 The main advantage of the approach proposed here is that we do not need to define weighting functions In this approach the filter matrix F can be written as follows Fgen QGgen QT in which the diagonal matrix Ggen contains now weighting factors optimized using genetic operators Optimization is reached when the Euclidian distance between the noisy and clean MFCCs is minimized To improve the enhancement of noisy MFFCs we introduce a preprocessing level which uses the MLP As depicted in Figure 1 the noisy MFCC vectors C are first enhanced by MLP Then a KLT is performed Finally the space of feature representation on the output of MLP 
denoted by C is reconstructed by using the eigenvectors weighted by the optimal factors of the Ggen matrix 3 MLP Based Enhancement Preprocessing of the KLT Numerous approaches were proposed in the literature to incorporate acoustic features estimated by the MLP under noisy conditions 6 13 The connectionist approaches offer inherent nonlinear capabilities as well as easy training from pairs of corresponding noisy and noise free signal frames Because the front end is very modular the MLP estimator can be introduced at different stages in the feature processing stream For instance the MLP can estimate robust filterbank log energies that will then be processed with the traditional Distrete Cosine Transform to get the unnormalized cepstral coefficients Alternatively we can estimate the cepstral features directly with an MLP Yet another possibility 172 S A Selouani H Hamam and D O Shaughnessy Clean Speech MFCCs NNs KLT GAs a11 a22 S2 S3 a33 MFCCs S1 a13 a23 Telephone Speech GA KLT HMMs Fig 1 The proposed MLP KLT 
GA based CSR system is to estimate filterbank log energies but to measure the feature distortion at the cepstrum level and optimize the filterbank log energy estimator accordingly 13 The fact that the noise and the speech signal are combined in a nonlinear way in the cepstral domain led us to choose the second alternative described above MLP can approximate the required nonlinear function to some extent 6 Hence the input of the MLP is the noisy MFCC vector C while the actual is computed during a training phase by using a response of the network C convergence algorithm to update the weight vector in a manner to minimize the and the desired clean cepstrum value C The weights error between the output C of this network are calculated during a training phase with a back propagation training algorithm using a mean square error criterion The noisy 13 dimensional vector 12 MFCCs energy is fed to an MLP network in order to reduce the noise effects on this vector Once the enhanced vector is obtained it is fed to the 
KLT GA module This latter module refines the enhanced vector by projecting its components in the subspace generated by a genetically weighted version of the eigenvectors of the clean signal The motivation behind the use of a second level of enhancement after using the MLP network is to compensate for the limited power of the MLP network for enhancement outside the training space 6 4 Hybrid MLP KLT GA Speech Front end gives the diagonal The KLT processing on the MLP enhanced noisy vectors C matrix Gr containing the weighting factors gi with i 1 r In the classical subspace filtering approaches a key issue is to determine the rank r from which the high order components those who are supposed to contain the noise are removed In the evolutionary based method we propose all components are used in the optimization process Only the performance criterion will determine the final components that are retained to perform the reconstruction of the space of enhanced features A Hybrid Genetic Neural Front End Extension 173 
The evolution process starts with the creation of a population of the weight factors gi with i 1 N which represent the individuals The individuals evolve through many generations in a pool where genetic operators are applied 4 Some of these individuals are selected to reproduce according to their performance The individuals evaluation is performed through the use of an objective function When the fittest individual best set of weights is obtained it is then use in the test phase to project the noisy data Genetically modified MFCCs their first and second derivatives are finally used as enhanced features for the recognition process As mentioned earlier the problem of determining optimal r is not needed since the GA considers the complete space dimension N 4 1 Initialization Termination and Solution Representation A solution representation is needed to describe each individual in the population For our application the useful representation of an individual for function optimization involves genes or variables 
from an alphabet of floating point numbers with values within the variables upper and lower bounds noted ai bi respectively Concerning the initialization of the pool the ideal zero knowledge assumption is to start with a population of completely random values of weights These values follow an uniform distribution within the upper and lower boundaries The evolution process is terminated when a certain number of maximum generations is reached This number corresponds to a convergence of the objective function 4 2 Selection Function A common selection method assigns a probability of selection Pj to each individual j based on its objective function value Various methods exist to assign probabilities to individuals In our application the normalized geometric ranking is used 7 This method defines Pj for each individual by Pj q 1 q s 1 P 1 1 q 3 where q is the probability of selecting the best individual s is the rank of the individual 1 is the rank of the best and P is the population size 4 3 Crossover In order to 
avoid the extension of the exploration domain of the best solution a simple crossover operator can be used 7 It generates a random number l from a uniform distribution and does an exchange of the genes of the parents X and Y on the offspring genes X and Y It can be expressed by the following equations X lX 1 l Y Y 1 l X lY 4 174 S A Selouani H Hamam and D O Shaughnessy 4 4 Mutation The principle of the non uniform mutation consists of randomly selecting one component xk of an individual X and setting it equal to a non uniform random number xk xk xk bk xk f Gen if u1 0 5 xk ak xk f Gen if u1 0 5 5 where the function f Gen is given by f Gen u2 1 t Gen Genmax 6 where u1 u2 are uniform random numbers in the range 0 1 t is a shape parameter Gen is the current generation and Genmax is the maximum number of generations The multi non uniform mutation generalizes the application of the non uniform mutation operator to all the components of the parent X 4 5 Objective Function The GA must search all the axes generated 
by the KLT of the MEL frequency space to find the closest to the clean MFCCs Thus evolution is driven by a fitness function defined in terms of a distance measure between noisy MFCCs pre processed by MLP and projected on a given individual axis and the clean MFCCs The fittest individual is the axis which corresponds to the minimum of that distance The distance function applied to cepstral or other voice representations refers to spectral distortion measures and represents the cost in a representing two classification system of speech frames For two vectors C and C frames each with N components the geometric distance is defined as d C C N 1 l k l Ck C 7 k 1 For simplicity the Euclidian distance is considered l 2 which turned out is to be a valuable measure for both clean and noisy speech Note that d C C used as a distance measure because the evaluation function must be maximized 5 Experiments and Results Extensive experimental studies were carried out to characterize the impairment induced by telephone 
networks 3 When speech is recorded through telephone lines a reduction in the analysis bandwidth yields a higher recognition error particularly when the system is trained with high quality speech and tested using simulated telephone speech 9 A Hybrid Genetic Neural Front End Extension 175 Table 1 Percentages of word recognition rate CW rd insertion rate Ins deletion rate Del and substitution rate Sub of Mean normalized MFCCs their first and second derivatives denoted CMN KLT MLP MLP KLT KLT GA and MLP KLT GA CSR systems using a 1 mixture b 2 mixture c 4 mixture and d 8 mixture tri phone models Best rates are highlighted in boldface Sub Del Ins CW rd No processing 82 71 4 27 33 44 13 02 CMN 79 35 4 24 29 15 17 14 KLT 77 05 5 11 30 04 17 84 MLP 77 03 5 02 28 74 18 18 MLP KLT 76 84 5 06 28 89 20 17 KLT GA 58 11 5 28 24 48 39 50 MLP KLT GA 52 15 5 07 21 36 43 22 a Results using 1 mixture triphone models Sub Del Ins CW rd No processing 81 25 3 44 38 44 15 31 CMN 79 36 3 12 37 92 17 81 KLT 78 11 3 81 38 89 18 08 
MLP 78 47 3 56 40 78 17 94 MLP KLT 70 71 3 09 43 02 21 86 KLT GA 54 01 3 96 49 85 45 03 MLP KLT GA 49 78 3 68 49 40 46 48 b Results using 2 mixture triphone models Sub Del Ins CW rd No processing 78 85 3 75 38 23 17 40 CMN 77 50 3 96 39 87 19 12 KLT 76 27 4 88 39 54 18 85 MLP 75 42 4 05 38 44 19 97 MLP KLT 70 43 3 98 36 65 22 18 KLT GA 54 88 3 79 35 46 48 42 MLP KLT GA 50 95 3 58 22 98 49 10 c Results using 4 mixture triphone models Sub Del Ins CW rd No processing 78 02 3 96 40 83 18 02 CMN 77 48 4 95 34 75 19 46 KLT 77 36 5 37 34 62 17 32 MLP 77 13 5 04 32 32 21 58 MLP KLT 75 29 5 13 33 88 21 72 KLT GA 55 66 5 46 27 12 47 01 MLP KLT GA 47 85 5 86 25 39 50 48 d Results using 8 mixture triphone models 176 S A Selouani H Hamam and D O Shaughnessy In our experiments the training set composed of the dr1 and dr2 subdirectories of the TIMIT database described in 2 was used to train a set of clean speech models The speech recognition system used the dr1 subdirectory of NTIMIT as test set 2 HTK the HMM based speech 
recognition system described in 12 has been used throughout all experiments We compared seven systems a baseline system where no enhancement processing is done a system which performs the Cepstral Mean Normalization CMN in order to attenuate the effect of inserting a transmission channel on the input speech a KLT based system as detailed in 10 a system based on a MLP preprocessing a hybrid MLP KLT system a hybrid KLT GA system and finally the MLP KLT GAbased CSR system All the systems use the MFCCs first and second derivatives front end Note that in our KLT process the mean is globally normalized and the standard deviation is equal to one The architecture of the MLP network consists of three layers The input layer consists of 13 neurons while the hidden layer and the output layer consists of 26 and 13 neurons respectively The input to the network is the noisy 12dimensional MFCC vector in addition to the energy The weights of this network are calculated during a training phase with a back propagation 
algorithm with a learning rate equal to 0 25 and a momentum coefficient equal to 0 09 The obtained weight values are then used during the recognition process to reduce the noise in the enhanced obtained vector that is incorporated into the KLT GA module To control the run behaviour of a genetic algorithm a number of parameter values must be defined The initial population is composed of 250 individuals and was created by duplicating the elements of the weighting matrix The genetic algorithm was halted after 500 generations The percentages of crossover rate and mutation rate are fixed respectively at 28 and 4 The number of total runs was fixed at 70 After the GA processing the MFCCs static vectors are then expanded to produce a 39 dimensional static dynamic vector upon which the hidden Markov models HMMs which model the speech subword units were trained We found through experiments that using the MLP KLT GA as an approach to enhance the MFCCs that were used for recognition with N mixture Gaussian HMMs for N 1 
2 4 and 8 using tri phone models leads to an important improvement in the accuracy of the word recognition rate A correct rate of 50 48 is reached by the MLP KLT GA based CSR system when the second best result 48 42 is achieved by the KLT GA These two systems outperform significantly the systems that use either MLP or KLT solely or combined together As found in 10 11 in the context of additive car noise this result confirms that the use of GAs to optimize the space representation of noisy data leads to more robustness of the CSR process Note that an important improvement of more than 31 is achieved comparatively to the CMN based system when the 8 mixture tri phone model is used Expanding to more than 8 mixtures did not improve the performance The results in Table 1 show also that substitution and insertion A Hybrid Genetic Neural Front End Extension 177 errors are considerably reduced when the hybrid neural evolutionary approach is included leading to more effectiveness for the CSR system 6 Conclusion In 
this paper a hybrid genetic neural front end was proposed to improve speech recognition over telephone lines It is based on an MLP KLT GA hybrid enhancement scheme which aims to obtain less variant MFCC parameters under telephone channel degradation Experiments show that use of the proposed robust front end processing greatly increases the recognition rate when dr1 and dr2 TIMIT directories are used for the training and dr1 directory of NTIMIT for the test The MLP KLT GA system outperforms significantly the systems that use either MLP or KLT solely or combined together This indicates that both neural preprocessing subspace filtering and GA based optimization gained from their combination as front end for speech recognition over telephone lines It is worthy to note that the neural evolutionary based technique is less complex than many other enhancement techniques which need to either model or compensate for the noise For further work many other directions remain open Present goals include the improvement of 
the objective function in order to perform the online adaptation of the HMM based CSR system when it faces new and unseen contexts and environments References 1 Ephraim Y Van Trees H L A signal subspace approach for Speech Enhancement IEEE Transactions on Speech and Audio Processing 3 4 178 S A Selouani H Hamam and D O Shaughnessy 10 Selouani S A O Shaughnessy D Robustness of speech recognition using genetic algorithms and a Mel cepstral subspace approach Proc IEEE ICASSP I Efficient Viterbi Algorithms for Lexical Tree Based Models S Espa na Boquera M J Castro Bleda F Zamora Departamento de Sistemas Abstract In this paper we propose a family of Viterbi algorithms specialized for lexical tree based FSA and HMM acoustic models Two algorithms to decode a tree lexicon with left to right models with or without skips and other algorithm which takes a directed acyclic graph as input and performs error correcting decoding are presented They store the set of active states topologically sorted in contiguous memory 
queues The number of basic operations needed to update each hypothesis is reduced and also more locality in memory is obtained reducing the expected number of cache misses and achieving a speed up over other implementations 1 Introduction Most of large vocabulary Viterbi based recognizers for speech handwritten or other recognition tasks although speech terminology is used in this work with no loss of generality make use of a lexicon tree organization which has many advantages over a linear lexicon representation 1 2 As it is shown in the literature more compact representations are possible using a lexicon network 3 which is a minimized Finite State This work has been partially supported by the Spanish Government TIN2006 12767 and by the Generalitat Valenciana GVA06 302 M Chetouani et al Eds NOLISP 2007 LNAI 4885 pp 180 S Espa na Boquera et al specialized decoding of lexical tree models has a great impact in the overall performance Therefore it is not strange to find specialized algorithms which take 
advantage of the properties of tree based Hidden Markov Models HMM which integrate the tree lexicon and the acoustic HMM models In this work three specialized Viterbi algorithms based on contiguous memory queues FIFO data structures are proposed When performing a Viterbi step a new result queue is created with the help of one or several auxiliary queues The basic algorithm uses left to right HMM acoustic models with no skips This algorithm can be applied whenever acoustic left to right models without skips are used it can be used for isolated word or continuous speech recognition either with a one step or a two step approach with time start or language model history copies and also within word or across word context dependent units triphones quinphones etc A simple extension is presented to show how to use it with across word context dependent models 7 A second version of the algorithm extends the first one to allow the use of skips in the acoustic units with a negligible additional cost The last proposed 
algorithm performs an error correcting Viterbi decoding and is capable of analyzing a DAG instead of a sequence This algorithm can be used for instance to obtain a word graph from a phone graph 2 Left to Right without Skips Algorithm When the acoustic models are strict left to right without skips the resulting expanded tree based HMM model is acyclic if loops are ignored and every node has only zero or one preceding state to take into account in the dynamic programming equation If a lexicon tree is expanded with left to right acoustic HMM models without skips the following observations about the expanded tree models are straightforward Efficient Viterbi Algorithms for Lexical Tree Based Models 181 t beam threshold loop probability aux_child merge generate children aux_gchild generate grandchildren emission probability update best probability t 1 Fig 1 Viterbi M and Viterbi MS algorithms The queue aux gchild dotted part is only used in Viterbi MS 182 S Espa na Boquera et al 3 For every active state i s of the 
queue aux child extract it update the score s 3 Left to Right with Skips Algorithm This algorithm generalizes the previous one by allowing the use of Bakis HMM acoustic models Left to right units with skips are known as Bakis topology and have a widespread use as acoustic models in most recognizers When those models are used in conjunction with a tree lexicon the number of predecessors given an active state can be zero one or two 3 1 Model Representation The model representation is similar to the previous section The only difference is another vector skip prob which stores for every state the incoming skip transition probabilities In order to iterate over the set of grandchildren of a given state i the algorithm loops from first child first child i to first child first child i 1 1 Efficient Viterbi Algorithms for Lexical Tree Based Models 183 3 2 Viterbi Merge Algorithm with Skips The Viterbi MS algorithm is the same of previous section but another auxiliary queue aux gchild is used to store the active 
states with scores computed by means of the skip transitions Every time an active state is extracted from t the set of grandchildren is used to add items to the queue aux gchild just as the set of children is used to add items to the other auxiliary queue Now the resulting queue t 1 is obtained by merging the loop transition score of the processed active state with the states from the two auxiliary queues see Figure 1 This algorithm is linear with the number of active states and the number of children and grandchildren of active states The resulting cost is thus linear with the number of active states 3 3 Extension to Across Word Context Dependent Units The same observations of previous algorithm are also applicable here 4 Error Correcting Viterbi for DAGs Not expanding the acoustic models in the tree lexicon and maintaining a pure tree structure which matches a phone graph is another possibility In this case the input is no longer a sequence of acoustic frames but a phone graph a directed acyclic 184 S Espa 
na Boquera et al Fig 2 An example of phone graph A possible sequence of calls for this example is vertex step 0 edge step 0 1 a vertex step 1 edge step 1 2 a edge step 0 2 a vertex step 2 Another sequence is vertex step 0 edge step 0 2 a edge step 0 1 a vertex step 1 edge step 1 2 a vertex step 2 which are explained in detail below The vertex step must be applied to a given vertex once all edges arriving to it have been processed using the edge step procedure On the other hand the edge step must be aplied after the origin vertex has been processed with the vertex step procedure An example of a phone graph together with possible sequences of edge step and vertex step calls is shown in Figure 2 Edge Step For every edge a Viterbi step takes the set of active states of the origin vertex and use them to update the active states of the destination vertex This procedure only considers the cost of deletions and substitutions As can be observed in Figure 3 top this algorithm is similar to the Viterbi M algorithm 
where loop probability updating is replaced by the deletion operation the generation of children states corresponds to the substitution operation including a symbol by itself Another difference which origin beam threshold deletion operation aux merge generate children with substitution operation destination update best probability updated destination vertex active states updated vertex active states merge aux generate children with insertion operation compare with beam threshold at this point Fig 3 Viterbi MEC DAG edge step procedure top and vertex step procedure bottom Efficient Viterbi Algorithms for Lexical Tree Based Models 185 can be also used in Viterbi M and Viterbi MS to process a DAG as input data is the presence of second input queue which stores the active states already updated at the destination vertex by means of other edges of the DAG The cost of this procedure is linear with the number of active states in both input queues because the number of generated successor states grows linearly with 
the number of active states Vertex Step Once all edges arriving at a given vertex have been processed the insertion operation is considered This operation updates a set of active states without consuming any symbol As can be observed in Figure 3 bottom the output of the auxiliary queue is used to insert more active states in the same queue to take into account the possibility of several insertion operations The cost is not linear with the number of active states a sole active state at the root could in principle activate all the states of the model but most of them are expected to be pruned by the beam search depending on the cost of insertions and the beam width The cost of this operation is linear with the number of active states before applying the procedure plus the number of active states after the procedure which is bounded by the number of states in the model 5 Experimental Results A previous work related to lexical tree Viterbi decoding we were aware of after our algorithms were developed is the 
active envelope algorithm 9 This algorithm also uses a total order which subsumes the partial order of the states of the model and places siblings contiguously This algorithm is specified for left to right models without skips so it is only comparable to our first algorithm Viterbi M The active envelope algorithm uses a single linked list to perform a Viterbi step which is an advantage in memory usage Since this algorithm modifies the original set of active states it is restricted to sequential input data and cannot be used when the input data is a DAG In order to use only a list the active hypothesis in active envelope algorithm are traversed in reverse topological order The price to pay for this advantage is the need of linked lists instead of contiguous memory arrays Since the use of linked lists cannot assure memory locality and the cost of traversing them is greater than traversing memory arrays it is expected to perform worse than Viterbi M algorithm The memory occupied by an active hypothesis is an 
index state and a score if linked lists are used a pointer is also needed so a linked list needs approximately 50 or 100 more memory per active state depending on the computer architecture On the other hand the use of memory arrays needs an estimation of the number of active states In order to compare the performance of our Viterbi M and Viterbi MS algorithms three more algorithms have been implemented a conventional Viterbi algorithm based on hash tables with chaining to store and to look up the active states for HMM without and with skips and the active envelope algorithm All algorithms have been implemented in C and use the same data structures to represent the tree based HMM models as described in sections 2 1 and 3 1 The experiments were done on a Pentium D machine at 3GHz with 2 Gbytes of RAM using a Linux with kernel 2 6 18 and the gcc compiler version 4 1 2 with O3 optimization The lexical trees used in the experiments were obtained by expanding 3 state left to right without and with skips hybrid 
neural HMM acoustic models in 186 S Espa na Boquera et al Table 1 Experimental results in millions of active states or hypothesis updated per second Viterbi M Hash AEnvelope Viterbi M 3 00 16 08 28 54 2 76 12 92 28 50 1 92 6 44 26 57 Viterbi MS Hash Viterbi MS 2 06 27 60 1 41 27 78 0 90 25 99 States 9 571 76 189 310 894 the tree lexicon The size of these trees varies from 9 571 to 310 888 states Only the Viterbi decoding time has been measured the emission scores calculation and other preprocessing steps were not taken into account The results are shown in Table 1 The speed is measured in millions of active states updated per second 6 Conclusions In this paper three Viterbi algorithms specialized for lexical tree based FSA and HMM acoustic models have been described Two of these algorithms are useful to decode a set of words given a sequence of acoustic frames and the third one is useful to parse a phone graph with error correcting edition operations These algorithms are based on contiguous memory queues 
which contain the set of active states topologically sorted Although the asymptotic cost of these algorithms is the same as any reasonable implementation of the Viterbi algorithm the experimental comparison between the Viterbi M algorithm a conventional Hash table swapping algorithm and the active envelope algorithm shows that our algorithm is approximately 10 times faster than the hash table swapping implementation and from 2 to 4 times faster than the active envelope algorithm A decrease in speed with the size of the models is observed in the three algorithms which is possibly related with the main memory and the cache relative speeds Analogous conclusions can be drawn for the Viterbi MS algorithm For this reason more experimentation is needed in order to better understand this behaviour and also to study the effect of other parameters such as the beam width of the pruning during the search References 1 Klovstad J Mondshein L The CASPERS linguistic analysis system IEEE Trans on ASSP 23 1 Efficient Viterbi 
Algorithms for Lexical Tree Based Models 187 7 Kanthak S et al Within word vs across word decoding for online speech recognition 2000 8 Konig Y Bourlard H Morgan N REMAP Recursive estimation and maximization of A posteriori probabilities In Advances in NIPS vol 8 pp Non stationary Self consistent Acoustic Objects as Atoms of Voiced Speech Friedhelm R Drepper Zentralinstitut Abstract To account for the strong non stationarity of voiced speech and its nonlinear aero acoustic origin the classical source filter model is extended to a cascaded drive response model with a conventional linear secondary response a synchronized and or synchronously modulated primary response and a nonstationary fundamental drive which plays the role of the long time scale part of the basic time scale separation of acoustic perception The transmission protocol of voiced speech is assumed to be based on non stationary acoustic objects which can be synthesized as the described secondary response and which are analysed by introducing a 
self consistent filter stable part tone decomposition suited to reconstruct the hidden fundamental drive and to confirm its topological equivalence to a glottal master oscillator The filter stable part tone decomposition opens the option of a phase modulation transmission protocol of voiced speech Aiming at communication channel invariant acoustic features of voiced speech the phase modulation cues are expected to be particularly suited to extend and or replace the classical feature vectors of phoneme and speaker recognition Keywords signal analysis instantaneous part tone frequencies fundamental drive cascaded response generalized synchronization voiced continuants 1 Introduction In spite of the undisputedly high degree of non stationarity of speech signals the present day determination of its acoustic features is based on the assumption that speech production can be described as a linear time invariant LTI system on the time scale of about 20 ms 1 As an evolutionarily plausible supplement speech perception 
is also assumed to be focussed on acoustic features which can be analyzed by psycho acoustic experiments with LTI signals 2 The wide sense stationarity of an M Chetouani et al Eds NOLISP 2007 LNAI 4885 pp Non stationary Self consistent Acoustic Objects as Atoms of Voiced Speech 189 represent topological invariants 4 which are largely invariant with respect to changes of the geometry of the acoustic transmission and important cues for the distinction of vowels 1 3 However the conventional LTI system assumption turns out to be problematic in the case of voiced phones The vocal tract filter should not be assumed to be time invariant 1 3 and the source not to be generated by an autonomous linear dynamical system 5 7 The physiologically more plausible assumption of a nonlinear dynamical system 5 7 as underlying voice source does not exclude that near periodic motion in the basin of attraction of a stable limit cycle can successfully be approximated by transients of a stable or nearly stable LTI system 3 However 
there is empirical evidence that the complex neural control of the vocal fold dynamics leading to shimmer jitter and vocal tremor 8 impedes or excludes a low dimensional autonomous deterministic description of the phonation process Being hopelessly irregular from the point of view of acoustics the time evolution of pitch and loudness intonation and stress can partially be given phonological and paralinguistic emotion related interpretation 9 Pitch and loudness of human speech show relevant frequency components up to the range of at least 15 Hz 8 This invalidates or challenges the stationary stochastic process description of the voice source A physiologically and phonologically more consistent phenomenological description of the aero acoustics of voiced speech can be achieved by introducing an additional drive response step which describes the highly complex wideband acoustic source as stationary primary response of a non stationary band limited fundamental drive process in the frequency range of the pitch 10 
13 The importance generality and precision of the acoustic percept of pitch can be taken as a first hint that the hidden fundamental drive FD can directly be extracted from the speech signal This leads to a two level cascaded drive response model DR model of voiced speech production which describes non stationary acoustic objects of 30 40 ms as stationarily coupled response of a non stationary hidden FD Following common practice the potentially long transients of the secondary response are simplified by assuming time invariant stable linear response dynamics resulting from an all pole filter with a fixed point attractor 1 3 An attractor is an invariant set of states which homes the asymptotic long time behaviour of stationary dynamics 4 As complementary simplification the primary response is assumed to result from a strongly dissipative nonlinear response dynamics 4 which generates predominantly short transients 5 The resulting two response levels cannot only be interpreted more or less erroneously as source 
and vocal tract filter output but more consistently as two complementarily simplified steps of a cascaded response dynamics 13 The dissipative primary response dynamics can be simplified drastically by restricting the dynamics to the asymptotic invariant set which neglects the transient behaviour Stationary unidirectionally coupled DR systems with dissipative responses are known to have invariant sets which represent continuous synchronization manifolds lines or surfaces in the combined state space of drive and response 14 15 Being constrained to a continuous synchronization manifold the primary response can be expressed by a continuous coupling function which describes the momentary state of the response by a unique coupling function of a response related state of the fundamental drive 10 13 As a slightly more general 190 F R Drepper synchronization concept the algebraic relation may be limited to the phases of drive and response Synchronization of phases or phase locking is known to be a generic property 
of nonlinearly coupled DR dynamics 4 Due to the broader frequency range of the primary response the coupling function represents a noninvertible multimodal function As an important special case of generalized synchronization an invertible coupling function describes a coupling between two oscillators which are topologically equivalent behave like a single oscillator 4 14 1 1 History and Role of the Fundamental Drive FD The idea that the higher frequency acoustic modes of voiced speech song and music as well as the perception of their pitch are causally connected to a single acoustic mode in the frequency range of the pitch son fundamentale or fundamental bass can be traced back to Rameau 16 However Seebeck 17 could show that virtual pitch perception does not rely on a fundamental acoustic mode which is part of the heard signal Based on stationary voiced signals it has been shown in numerous studies that virtual pitch perception relies on a subband decomposition with harmonically related frequencies of 
several subbands or part tones 18 19 The present study replaces Rameau s son fundamentale by an abstract order parameter termed FD which is obtained by using harmonically related part tones which can be confirmed as topologically equivalent image of a glottal master oscillator of the voice source 10 13 and which is closely related to an acoustic correlate of non stationary virtual pitch perception In preliminary studies the principle feasibility of the receiver side reconstruction of the FD from a voiced speech signal as well as from a simultaneously recorded electro glottogramme has been demonstrated and compared 10 11 Inspired by the prevailing interpretation of the function of the cochlea the extraction of the FD had been based on part tones with time independent centre filter frequencies within the current window of analysis Similar part tones or sinusoids have also been used in several other studies 18 23 As will be explained below the restriction on time invariant filter frequencies leads to part tones 
of non stationary voiced speech which cannot be expected to be precisely related to respective acoustic modes in the vocal tract of the transmitter The introduction of self consistent filter frequency contours of the part tone filters opens the option to reconstruct topologically equivalent images of formant specific acoustic modes The higher frequency formant images are particularly suited to reconstruct the instantaneous frequency of the fundamental drive Whereas the confirmation of the topological equivalence of the images of formant oscillators is limited to analysis windows of about 30 40 ms the reconstruction of a coherent FD can be achieved for uninterrupted voiced segments of speech i e for time spans of more than 100 ms 11 13 The self consistent FD represents the long time scale part of the basic time scale separation of human acoustic perception which separates the phone or timbre specific fast dynamics from intonation and prosody Due to the hypothetically unique definition of their centre filter 
frequencies filter stable parttones of voiced phones are suited for a phase modulation transmission protocol The transmission of filter stable phases gives rise to additional topologically invariant cues of voiced speech which can be expected to be robust under variation of the Non stationary Self consistent Acoustic Objects as Atoms of Voiced Speech 191 acoustic communication channel In contrast to conventional approaches 1 3 the self consistent time scale separation does not rely on the assumption of stationary excitation of the vocal tract or a related gap in the relevant time scales or frequencies of speech production It cannot be excluded that the conventional assumption of a frequency gap suppresses important acoustic correlates of emotions being expressed during affective speech 2 Voice Adapted Part Tones It is well known that virtual pitch and loudness perception are based on a subband or part tone decomposition 2 and that the band pass filters of the peripheral auditory pathway can roughly be 
approximated by tone filters 24 or more simply by cascaded complex first order autoregressive filters 25 Cascaded first order filters have the nice property that they can be implemented efficiently as low dimensional linear dynamical systems 25 and can be described analytically by matrix recursion 12 13 By introducing the autoregressive order and the bandwidth parameters j which are simply related to the part tone specific equivalent rectangular bandwidths ERB j j exp a ERB j 25 the output of the non normalized complex bandpass filter of part tone j is obtained as 12 13 z j t t t exp i j k k t 1 t 0 tj t t t 1 S 1 t t t 1 from which the part tone carrier phase is determined as j t arctan im z j t re z j t For j 1 N the set of normalized part tones can be interpreted as an over critically sampled time frequency decomposition of the speech signal St into elementary impulse responses each being separated into complex phase factor and real amplitude To exclude an a priori correlation between neighboring part 
tones the set of centre filter frequency contours j k k 0 1 t is chosen in such a way that an oversampling with respect to frequency is avoided As a rather unconventional assumption the centre filter frequencies are assumed to be time variant even on the intrinsic time scale of the current window of analysis 2 1 A Priori Knowledge about the Centre Filter Frequencies The receiver side decomposition of voiced speech can be based on the a priori knowledge that the common origin of the transmitter side acoustic modes the pulsed airflow through the glottis and the nonlinearity of the aero acoustic dynamics in the vocal tract leads to a characteristic phase locking of some of these acoustic modes 13 Since at least the lower frequency transmitter side acoustic modes are well separated in frequency it can be expected that there exists a range of part tones for which an appropriate adaptation of the centre filter frequencies of their bandpass filters leads to part tones which are topologically equivalent to the 
corresponding transmitter side acoustic modes 192 F R Drepper The roughly audiological choice of the bandwidths of the part tone filters has the effect that we can distinguish a lower range of part tone indices 1 j 6 characterized by guaranteed single harmonic resolved part tones and a range of potentially multiple harmonic unresolved part tones 2 As a remarkable feature of speech segments which correspond to nasals or vowels it is typical that some of the optimally adapted part tones in the a priori unresolved frequency range are also dominated by a single harmonic acoustic mode The instantaneous frequency contours of at least a subset of the part tones of a voiced speech signal can thus be expected to be centered around rational multiples of the frequency contour of the glottal oscillator Other a posteriori resolved part tones can be expected to have minor phase shifts which are consistent with formant specific resonance properties of a stationarily coupled secondary response Excluding for a moment the so 
called diplophonic voice type 8 it is typical for voiced signals that there exists a fundamental phase velocity 0 for which the winding number ratios of some of the part tone phase velocities to 0 simplify to an integer harmonic number h j For stationary voiced signals the latter feature can be detected by using the harmonic frequency template introduced by Goldstein 19 or the subharmonic coincidence cluster of Terhardt 18 For non stationary voiced signals the detection of a strict n n synchronization of the phases of a priori independent part tone pairs with non overlapping spectral bands represents a phenomenon which has a low probability to happen by chance For such part tone pairs it can therefore be assumed that there exists an uninterrupted causal link between those part tones including the only plausible case of two uninterrupted causal links to a common drive which can be identified as a formant specific acoustic mode and or as the glottal master oscillator Since the n n phaselocking with n n is 
generated by the nonlinear coupling to the glottal oscillator a stable synchronization of a priori independent part tone phases can be taken as a confirmation of topological equivalence between these part tones and respective acoustic modes in the vocal tract of the transmitter The topologically equivalent parttones with higher frequency in particular the ones of the 3 formant are particularly suited to reconstruct the FD which is interpreted as topologically equivalent image of the glottal master oscillator 2 2 Time Frequency Atoms with Time Variant Frequency The impulse responses of equation 1 can be simplified by introducing a logarithmic expansion around the maximum of the amplitude of the impulse response at t t j For higher orders a second order expansion of the exponent Gaussian approximation is well suited to approximate the impulse response To be consistent with the second order expansion of the real part the imaginary part of the exponent should also include in general a second order term Following 
ideas of Gabor 26 such Gaussian wave packets SG t 1 2 2 exp i 0 t 1 c 2 t 1 t t j 2 2 2 2a Non stationary Self consistent Acoustic Objects as Atoms of Voiced Speech 193 represent time frequency atoms TFA which are optimally suited to describe simultaneously event particle and wave type properties of non stationary wave processes Contrary to the conventional one 26 this parametric set of non stationary time frequency atoms is characterized by a linear trend of the phase velocity with a non zero relative chirp c 0 t 0 1 c t 2b Numerous subband or part tone decompositions of speech signals like the ones introduced by McAuley and Quatieri 22 Terhardt 18 23 and all wavelet decompositions 3 are based on the additional assumption that the phase velocities instantaneous frequencies of the impulse responses time frequency atoms are constant within the extent of the time frequency atom or of the current window of analysis The zero chirp assumption is conventionally used to obtain orthogonal timefrequency atoms which 
have the advantage that their squared amplitude can be interpreted in terms of a time frequency energy distribution 3 The time variable centre filter frequencies of the present approach however have the alternative advantage that the part tones can be generated as topologically equivalent images of physically interpretable underlying non stationary acoustic modes The choice of a high order 5 in the present study guarantees a maximal time resolution which is compatible with a frequency resolution which is necessary to isolate a sufficient number of part tones to confirm their topological equivalence to the underlying acoustic modes In view of the important phonological and paralinguistic role of non stationary pitch fluctuations of voiced speech within human speech communication and in view of the well known efferent enervation of the outer hear cells of the cochlea it is plausible to assume that speech perception is based on a decomposition into near optimal time frequency atoms with a non stationary time 
variant instantaneous frequency as described in equation 1 As indicated in equations 2a 2b a near optimal perception equivalent analysis of non stationary speech signals can only be achieved with centre filter frequency contours with an at least linear trend of the phase velocity within the current window of analysis 2 3 Self consistent Centre Filter Frequencies For a constant amplitude input k S t A exp i k 0 k t with arbitrary instantaneous frequency 2 and a centre filter frequency k 2 chosen as identical to the instantaneous frequency the application of bandpass filter 1 generates the output zt t t A exp i k k 0 t 0 t t t t 1 1 t t 3 with a phase velocity which is identical to the one of the input signal In the limit t the sum in equation 3 represents an asymptotic gain factor g j Being exclusively dependent on the bandwidth parameter j and the order the gain factors can be used to obtain the normalized part tone amplitudes a j t z j t g j 194 F R Drepper For a given filter frequency contour k k 0 1 t 
input signals with k k experience a damping due to interference of the phase factors For a given input frequency contour other filter frequency contours generate a phase distortion of the filter output For sufficiently small distortions of the filter frequency contour the corresponding instantaneous frequency distortions are smaller than the preliminary deviation of the filter frequency contour The latter contractive feature represents a characteristic feature of voiced phones It motivates an iterative approach to determine optimally adapted self consistent centre filter frequencies With the help of a simple example it will be demonstrated that the iterative replacement of the filter frequency contour by a smoothed instantaneous frequency contour of the part tone outputs can be used to obtain stable bandpass filters whose centre filter frequency contour is identical to the instantaneous frequency contour of the respective filter output and thus also to obtain an instantaneous output frequency which is 
identical to the one of the respective dominant input Due to this self consistency such filter stable part tones are suited to transmit phase modulation cues to the receiver of a speech signal It is well known that human auditive perception is not limited to the frequency range of separable part tones and that the amplitudes envelopes of the higher frequency subbands show a modulation in the frequency range of the pitch 2 18 20 24 It is therefore plausible to extend the analysis of part tone phases to phases which can be derived from the envelopes of the part tones Being used preferentially for part tones in the unresolved frequency range the envelope phases are determined e g as Hilbert phases of appropriately highpass filtered scaled and smoothed 18 21 24 modulation amplitudes a j t of the part tones The relative importance of the envelope phases is expected to increase when the voice source changes from a modal ideal voice to a more breathy one 3 Stable Part Tones of a Pulsed Excitation To demonstrate the 
generation of self consistent part tones of a non stationary voiced acoustic object a sequence of synthetic glottal pulses with a chirped instantaneous frequency is chosen as example For simplicity the input pulses are chosen as constant amplitude saw teeth with a power spectrum which is roughly similar to the one of the glottal excitation The pulse shape is described by a coupling function of the form G t min mod t 2 s 2 mod t 2 4 where t represents the instantaneous phase of the fundamental drive The parameter s chosen to be 6 determines the ratio of the modulus of the downhill slope of the glottal pulses to the uphill one The chirp of the glottal oscillator is described by a t which is chosen in analogy to equation time dependent phase velocity t 2b however with potentially different chirp rate c and initial phase velocity 0 The fundamental phase is obtained by integrating the analogue of equation 2b with respect to time t replacing index k t 0 t c t 2 2 5 Non stationary Self consistent Acoustic Objects 
as Atoms of Voiced Speech 195 3 1 Time Scale Separation In the situation of signal analysis the filter frequency contour of the bandpass filter of part tone j has to be adapted algorithmically to the frequency contour of a suitable input process As a characteristic feature of voiced signals this can be done iteratively by adapting the filter frequency contour to the respective part tone output starting from an appropriate initial filter frequency contour For reasons of algorithmic stability it is advantageous to incorporate the a priori knowledge that the reconstructed frequency contour has a minimal degree of smoothness In accordance to chapter 2 2 the smoothing step of each iteration is based on the assumption that the frequency contour can be approximated within the current analysis window by a linear trend chirp as described in equation 2b To reduce the dependence of the estimated parameters on the size and position of the window of analysis and or to avoid the adaptation of the window length to the 
instantaneous period length the linear trend ansatz is extended by a 2 periodic function Pj j t h j of the respective normalized part tone phase j t h j j t Pj j t h j 6 The 2 periodic function Pj accounts for the oscillations of the part tone phase velocity around the long term trend which result from the characteristic auto phase locking The oscillations can be assumed to be periodic with respect to the formant or part tone specific fundamental normalized phase and can therefore be approximated by an appropriate finite order Fourier series The Fourier coefficients as well as the trend parameter j of time scale separation ansatz 6 are obtained by multiple linear regression 3 2 Graphical Adaptation of the Filter Chirp With the exception of the first analysis window within a voiced segment of speech the start value j 0 of the linear filter frequency contour can be assumed to be given as result of the adaptation of the filter chirp of the preceding analysis window As a first step we may therefore treat the 
latter parameter as given j 0 h j 0 In this situation the adaptation of the chirp parameter can be explained by a graph which shows the trend parameter j of equation 6 for several part tone indices j as function of the common filter chirp rate c To make figure 1 suited for the graphical analysis it gives the estimates of the relative trend j 0 c for the indices j 2 4 6 9 corresponding to the sequence of the fixed points from bottom to top as function of the relative filter chirp rate c c All chirp rates are given relative to the chirp rate of the input sawtooth process defined in equations 4 5 The adaptation of the chirp parameter of each filter frequency contour can be read off from figure 1 by an iteration of two geometric steps Project horizontally from one of the described curves to the diagonal of the first quadrant which indicates the line where the fixed points of any iteration are situated and project vertically down or up to the curve again As can be seen from figure 1 the chirp parameters of all 
four 196 F R Drepper part tones have a stable fixed point equilibrium within a well extended basin of attraction of the chirp parameter The fixed points indicate the final error of the filter chirp Due to the simple least squares regression of equation 6 the modulus of the trend is systematically underestimated 1 1 rel part tone chirp 1 05 1 0 95 0 9 0 85 0 8 0 8 0 9 1 1 1 1 2 1 3 1 4 1 5 rel filter chirp Fig 1 Estimated relative part tone chirp rates as function of the relative chirp rate of the centre filter frequentcies given for the envelope phase of part tone 9 circles and the three carrier phases of part tones 2 4 6 The extended basins of attraction of the filter adaptation processes of figure 1 are not at all typical for the more general situation characterized by j 0 h j 0 and by more complex voice sources In general we have to expect the coexistence of different basins of attraction of the filter adaptation process The stability regions of j 0 become necessarily smaller for higher part tone indices 
The number of stable fixed points with a macroscopic basin of attraction depends on the width of the power spectrum or on the steepness parameter s of the slope of the glottal pulse Resonances of the secondary response are suited to enlarge the basin of attraction of the respective part tones 3 3 Circle Maps Relating Part Tone Phases The mutual phase locking of the self consistently reconstructed part tones is shown in figure 2 It demonstrates that the precision of reconstructed synchronization manifolds is hardly influenced by the deficient estimate of the filter chirp The top row shows synchronization manifolds in the state space spanned by the part tones 5 or 6 being indicated on the ordinate and a fundamental drive being indicated on the ordinate The perfectly linear one dimensional manifolds are obtained by showing the projections in the direction of the respective phases The fundamental phase with arbitrary initial phase has been derived from the velocity of the carrier phase of parttone 4 Care has 
been taken that the wrapping of all phases happens simultaneously Non stationary Self consistent Acoustic Objects as Atoms of Voiced Speech 197 The bottom row shows the corresponding phase diagrams for the envelope phases of the part tones 6 and 7 In case of the circle maps which relate exclusively carrier phases top row the phase relations are precisely time invariant and result in precisely linear synchronization manifolds In case of circle maps which relate mixed type phases bottom row the time invariance is achieved only approximately by an open loop group delay correction and the circle maps become curved the shape being dependent on the mentioned smoothing and sublinear scaling of the modulation amplitudes 18 21 24 When appropriately smoothed the envelope phases are suited to be included into the cluster analysis of the phase velocity contours and potentially also to be used for a rough estimate of the phase velocity of the FD They are however less suited for the precise reconstruction of the 
fundamental phase over an extended voiced speech segment The more strict relation between the carrier phases is obviously advantageous for the precise adaptation and selection of the part tone filters The importance of an operational description of the adaptation of the centre filter frequencies to a specific voice and in particular the special role of the carrier phases of the a posteriori resolvable part tones for a precise adaptation appear to have been underestimated by conventional psychoacoustics 2 24 2 2 6 Carrier phase5 1 0 5 0 Carrier phase 0 0 5 1 1 5 2 1 5 1 5 1 0 5 0 0 0 5 1 1 5 2 2 2 Envelope phase6 1 5 1 0 5 0 Envelope phase 0 0 5 1 1 5 2 7 1 5 1 0 5 0 0 0 5 1 1 5 2 Fundamental phase Fundamental phase Fig 2 Circle maps phase relations of different part tone phases which are more or less suited to retrace the causal connection to the fundamental drive Both carrier phases are harmonically normalized All phases are given in units of 4 Multi Part Tone Stable Acoustic Objects It is well known that 
human pitch perception can be trained to switch between analytic listening to spectral pitch and synthetic listening to virtual pitch 2 18 It is 198 F R Drepper plausible to interprete the described single part tone stable acoustic objects with a macroscopic basin of attraction of the filter frequency contour parameters and with formant type underlying acoustic modes as potential candidates of spectral pitch perception From psychoacoustic experiments it is also known that virtual pitch is a more universal and robust percept than spectral pitch 2 18 Due to the characteristic auto phase locking mechanisms of voiced speech it is plausible that an observed carrier or envelope phase velocity of one part tone might be used to adjust the centre filter frequency of other part tones This encourages the introduction of a more robust multi part tone adaptation strategy 4 1 Definition In analogy to the single part tone stability of the last section we define multi part tone stability of an acoustic object by the 
existence of a common fundamental phase velocity contour which can be obtained as stable invariant set fixed point of the iteration of four cascaded mappings where the first mapping relates a preliminary fundamental phase velocity contour to a set of filter frequency contours the second mapping uses the set of filter frequency contours to generate a corresponding set of part tone phase velocity contours the third mapping executes the smoothing step and the forth mapping uses a subset of the smoothed part tone phase velocity contours to update the fundamental phase velocity contour The first mapping makes use of the fact that optimally adapted centre filter frequency contours of at least a subset of the part tones can be assumed as winding number multiples of the frequency contour of the common drive j k h j 0 k with integer or small denominator rational h j The second mapping uses filter 1 the third mapping refers to the estimation of time scale separation 6 and the fourth mapping uses some kind of cluster 
analysis to identify a mutually consistent set of winding number normalized part tone phase velocity j k h j k 0 1 t which are suited to reconstruct the phase velocity of the contours common drive As indicated above the existence of a cluster of at least three mutually phase locked part tones can be interpreted as a confirmation of topological equivalence of the respective part tones to underlying acoustic modes in the vocal tract of the transmitter being synchronized to a common formant or glottal oscillator However this confirmation relies on the a priori independence of those part tones It is therefore essential that the set of centre filter frequencies of the part tones taking part in the cluster analysis is pruned in the non resolvable frequency range of the part tone decomposition On the other hand it is allowed that single part tones enter the cluster analysis with different hypothetical harmonic numbers winding numbers being used to normalize the part tone phases A typical cluster analysis might 
result in the following set of harmonic numbers h j 1 2 6 8 10 12 15 4 2 Relation to EMD and to the Perception of Virtual Pitch In spite of the rather different generation mechanism the described filter stable parttone decomposition has some similarity to the empirical mode decomposition being introduced by Huang et al 27 and being applied to speech signals as part of the Non stationary Self consistent Acoustic Objects as Atoms of Voiced Speech 199 present volume 28 Both decompositions are aimed at the extraction of intrinsic modes from strongly non stationary signals The present approach might be interpreted as an intrinsic mode decomposition which has been evolutionarily optimized for the analysis of voiced speech For such signals the lower harmonic filter stable part tones are optimally chosen to reconstruct topologically equivalent images of underlying physical modes The topological equivalence of filter stable part tones gives a more solid basis for the physical interpretation of the intrinsic modes Due 
to their focus on the precise reconstruction of the instantaneous phase velocity the topologically equivalent part tones are better suited for the hypothetically relevant phase modulation transmission protocol of voiced speech The indicated reconstruction of the phase velocity of the FD has been based on the assumption that virtual pitch perception plays the role of a near optimal instrument of basic causality analysis of a voiced signal Virtual pitch perception of stationary signals has long been studied empirically and theoretically 18 19 In particular it has been established that the respective acoustic correlate depends not only on the phase velocities of the part tones but also on their amplitudes A corresponding modification of the fourth cluster analysis step of the iteration procedure can be expected to improve the robustness of the reconstruction of the fundamental phase The multi part tone adaptation is also expected to be complicated by co existence of different basins of attraction of the filter 
parameters Apart from the octave ambiguity different basins of attraction should now be interpreted in terms of different voice sources More precisely the choice of different start positions in different basins of attraction should correspond to an attention switching between different hypothetical voice sources as part of an auditory scene The parameters of a perception equivalent acoustic correlate of non stationary pitch should therefore also be chosen in view of numerous psycho acoustic results from monotic auditory scene analysis 4 3 Reconstruction of the Fundamental Drive As a further remarkable feature of the unexpectedly subtle voice transmission protocol the reconstruction of the phase velocity of the formant type acoustic modes and or the FD can typically be achieved for considerably larger speech segments than the one being used for the described confirmation of topological equivalence between part tones and respective acoustic modes of the vocal tract Whereas the piecewise detection of phase 
synchronization clusters is preferentially performed in a time window of about 200 F R Drepper respect to the glottal closure event and can thus be used to remove the arbitrariness of the initial phase of the FD of a voiced segment In fact it turns out that a high time resolution building block of the acoustic correlate of loudness perception is well suited to calibrate the wrapped up fundamental phase with respect to the glottal closure event 13 The coherent reconstruction of the fundamental phase turns out to be particularly useful for the analysis and synthesis of sustainable voiced consonants For the acoustic correlates of the latter phones the simple source filter interpretation in terms of a plane wave source in an unbranched vocal tract looses its validity and the nonlinearity of the aero acoustic dynamics becomes more apparent 5 7 Nasals are characterized by a sudden apparent time or fundamental phase shift of the glottal closure event induced pulse due to a sudden increase of the group delay of the 
acoustic transmission path from the glottis to the receiver 20 21 Voiced fricatives as the English z like in zoom are characterized by an acoustic source in the vicinity of a second constriction of the vocal tract which generates an intermittently turbulent jet The conversion of the kinetic energy of this pulsatile jet into acoustic energy e g at the edge of the teeth happens with a characteristic time delay which results from the comparatively slow subsonic convection speed of the relevant jet 7 The phoneme specific time shift of the second acoustic source with respect to the glottal closure event generates phase shifts of the instantaneous part tone phases which are no longer explicable by resonances of a stationarily coupled secondary response Such excessive phase shifts can be interpreted as further examples of topological invariants of voiced speech which are comparatively insensitive to variations of the acoustic communication channel Once the fundamental phase of a NAO has been reconstructed with the 
help of filterstable part tones two different paths can be chosen to continue the analysis of NAOs If the aim is given as a complete analysis optional transformation and re synthesis cycle there is probably no other choice than to determine also the amplitude of the FD and to estimate the parameters of the initially indicated two level cascaded drive response model with a given FD and a largely general fundamental phase triggered primary response As has been explained elsewhere 13 the reconstruction of the fundamental amplitude can be based on the assumption that loudness perception plays the role of a complementary instrument of causality analysis 10 13 4 4 Phase Related Acoustic Cues However if the aim is restricted to the analysis of a voiced signal like in the case of the auditive pathway the detailed knowledge of the filter stable part tones can directly be used to determine a favourable acoustic feature vector of voiced speech The main idea of this shortcut is that the frequency dependent resonant 
amplification or damping of the acoustic modes during the passage trough the vocal tract cannot only be analysed by looking at the amplitudes of the different part tones or formants but more directly by looking at their characteristic phase shifts In case of a resonant amplification the phase of the part tone response lags the one of the excitation and precedes it in case of a resonant damping As a second advantage of the phase related cues the magnitude of the phase shifts are proportional to the time derivative of the Non stationary Self consistent Acoustic Objects as Atoms of Voiced Speech 201 corresponding amplitudes which is known to play a major role in the acoustic feature vector of conventional speech recognition 1 The phase related acoustic cues are in open conflict to the conventional psychoacoustic theory originating from Ohm and Helmholtz which states that the amplitudes of subbands represent the primary acoustic cues 2 24 To explain the superior communication channel insensitivity of human 
acoustic perception and the efferent enervation of the cochlea it is hypothesized that the deviations of the carrier phases of self consistently determined part tones from the synchronization manifold of the ideal function type pulsed excitation triggered by a vowel nucleus anchored fundamental phase have a comparable or higher relevance for acoustic perception than the corresponding amplitudes 20 In particular it is expected that the rational winding numbers of self consistently determined part tones and the phone specific fundamental phase shifts of the receiver side reconstruction of the glottis induced pulses of nasals and other sustainable voiced consonants represent favorable cues which are suited to improve the distinction of the speakers as well as of their hypothetically intended voiced phonemes 13 In this context it is important to remind that many phones are so far defined exclusively by human perception of minimal differences between syllables Whereas vowels can alternatively be defined 
acoustically using the LTI model this does not apply to many of the non vocalic voiced continuants like nasals and sustainable voiced fricatives For these phones the shift of the glottal closure event induced pulses with respect to the wrapped up fundamental phase generates a phone specific long range correlation or co articulation effect which cannot be described by using a single LTI model Together with the gliding part tone coordination of the glides such phones can only be described by NAOs with non stationarily coupled responses As doubly non stationary NAOs the acoustic correlates of such elementary phones belong to the same complexity class as voiced syllables From the point of view of acoustics either singly non stationary acoustic objects which are consistent with a stationarily coupled response cascade or doubly non stationary acoustic objects which are suited to represent complete voiced sections of syllables appear as the more natural atoms of voiced speech 5 Conclusion A transmission protocol of 
non stationary voiced acoustic objects is outlined which are generated as response of a non stationary fundamental drive and which can be analysed as a superposition of time frequency atoms with non stationary partially phasesynchronized instantaneous frequencies For sustained voiced phones the partially synchronized set of time frequency atoms can be extracted by part tone filters with centre frequency contours which are iteratively adapted to the instantaneous frequencies of the respective part tones filter outputs Sets of mutually phase synchronized filter stable part tones can be interpreted as topologically equivalent images of underlying acoustic modes of the transmitter and are thus suited for the reconstruction of the phase velocity of the FD The latter properties qualify the 202 F R Drepper non stationary part tone stable voiced acoustic objects as most elementary atoms symbols of a voice transmission protocol being characterized by a time scale separation which offers a precise and robust decoding 
option The minimal acoustic description of the phone specific co articulation effects of sustainable voiced fricatives and nasals requires doubly non stationary voiced acoustic objects which do not only result from a non stationary fundamental drive but also from a non stationary coupling of the primary and or secondary response The incorporation of appropriate acoustic cues suited to describe these co articulation effects into present day automatic speech recognition LTI cue based discrete Hidden state Markov Models represents a non trivial task Acknowledgements The author would like to thank M Kob B References 1 Gold B Morgan N Speech and audio signal processing John Wiley Sons Chichester 2000 2 Moore B C J An introduction to the psychology of hearing Academic Press London 1989 3 Rabiner L R Schafer R W Digital Processing of Speech Signals Prentice Hall NJ Englewood Cliffs 1978 4 Kantz H Schreiber T Nonlinear time series analysis Cambridge Univ Press Cambridge 1997 5 Herzel H Berry D Titze I R Saleh M 
Analysis of vocal disorders with methods from nonlinear dynamics J Speech Hear Res 37 Non stationary Self consistent Acoustic Objects as Atoms of Voiced Speech 203 15 Afraimovich V S Verichev N N Rabinovich M I Stochastic synchronization of oscillation in dissipative systems Radiophys Quantum Electron 29 795 1986 16 Rameau J P Generation harmonique In Jacobi E ed Complete Theoretical Writings vol 3 American Institute of Musicology 1967 17 Seebeck A The Hartley Phase Cepstrum as a Tool for Signal Analysis Ioannis Paraskevas1 and Maria Rangoussi2 1 Centre for Vision Speech and Signal Processing CVSSP School of Electronics and physical Sciences University of Surrey Guildford GU2 7XH Surrey UK paraskevas env aegean gr 2 Department of Electronics Technological Education Institute of Piraeus 250 Thivon str Aigaleo Athens GR 12244 Greece mariar teipir gr Abstract This paper proposes the use of the Hartley Phase Cepstrum as a tool for speech signal analysis The phase of a signal conveys critical information which is 
exploited in a variety of applications The role of phase is particularly important for speech or audio signals Accurate phase information extraction is a prerequisite for speech applications such as coding synchronization synthesis or recognition However signal phase extraction is not a straightforward procedure mainly due to the discontinuities appearing in it phase wrapping effect Several phase unwrapping algorithms have been proposed to overcome this point when extraction of the accurate phase values is required In order to extract the phase content of a signal for subsequent utilization it is necessary to choose a function that can efficiently encapsulate it In this paper through comparison of three alternative non linear phase features we propose the use of the Hartley Phase Cepstrum HPC Keywords Fourier Phase Cepstrum Hartley Phase Cepstrum Speech signal phase Phase unwrapping algorithm 1 Introduction The phase of a signal as a function of frequency conveys meaningful information that is particularly 
useful for speech or audio signals Accurate phase extraction is crucial in various speech processing applications such as localization synchronization coding etc The major disadvantage of the computation of the phase spectrum via the Fourier transform is the heuristics employed for the compensation of the extrinsic discontinuities arising in it phase wrapping ambiguities The effect of these wrapping ambiguities is more severe in the presence of additive noise The phase spectrum obtained via the Hartley transform on the other hand is advantageous as a it does not exhibit extrinsic discontinuities and b due to its structure it is less affected by the presence of noise as justified through comparison of the shapes of the respective probability distribution functions 1 M Chetouani et al Eds NOLISP 2007 LNAI 4885 pp The Hartley Phase Cepstrum as a Tool for Signal Analysis 205 As signal localization applications show the phase content of a signal is more efficiently encapsulated in the Hartley Phase Cepstrum HPC 
rather than in the Fourier Phase Cepstrum FPC function Interesting advantages of the HPC such as localization capability and robustness to noise are based on the properties of the respective spectra which carry over to the cepstral domain thanks to the analytic relations holding between the two domains As it will be presented in the following sections the limited localization capability of the FPC as compared to the HPC is due to the ambiguities introduced by the use of the unwrapping algorithm The HPC advantage is preserved in the presence of additive noise 1 Another property of the HPC of interest in speech synthesis is its invertibility the non linear process for the evaluation of the HPC is invertible because no phase unwrapping algorithm is necessary Aiming towards a phase feature appropriate for accurate noisy signal localization we examine here three alternatives namely the FPC the whitened FPC and the HPC In conclusion the HPC is proposed as a promising and viable substitute to its Fourier 
counterpart 2 The Phase Cepstrum In general computation of the cepstrum of a discrete time signal x n belongs to a class of methods known as homomorphic deconvolution processes 2 The block diagram of such a process is given in Fig 1 non linear process x n 1 x n Fig 1 Summary of the homomorphic deconvolution process It is an invertible procedure in which the signal x n is transformed into another domain via an orthogonal transform a non linear process is applied to the transformed signal in the new domain and the result is transformed back to the original domain via the inverse transform 1 Discontinuities arising from the application of the non linear processing step have to be compensated before the inverse transform 1 is applied In the following paragraphs we will examine three alternative phase features produced by homomorphic deconvolution process using either the Fourier or the Hartley direct and inverse transforms and phase computation as the non linear step 2 1 The Fourier Phase Cepstrum If and 1 in 
figure 1 represent the Discrete Time Fourier Transform DTFT and the Inverse Discrete Time Fourier Transform IDTFT respectively while the nonlinear processing step is the evaluation of the Fourier phase spectrum 206 I Paraskevas and M Rangoussi S S arctan 1 where S and S are the real and imaginary components of the Fourier transform S of the signal s n respectively then we obtain the so called Fourier Phase Cepstrum FPC cF cF IDTFT 2 The Fourier Phase Spectrum FPS 1 experiences two types of discontinuities 2 The first type called extrinsic is due to the use of the arctan function employed in equation 1 and is overcome using a phase unwrapping algorithm 3 This step is necessary in order to apply the IDTFT and obtain the FPC However the heuristics involved in the extrinsic discontinuities unwrapping algorithm render the non linear step non invertible This results in loss of the signal localization capability of the FPC regardless of the SNR level Indeed experiments carried out on noisy synthetic signals 
containing one or more pulses show that the FPC suffers loss of its signal localization capability except for the ideal case of just one noise free pulse figure 2 The second type of discontinuities called intrinsic originates from the spectral properties of the signal itself and is overcome by appropriate compensation algorithms 4 5 2 2 The Whitened Fourier Phase Cepstrum An alternative for signal phase extraction proposed among others in 6 is the use of the whitened WF S S 2 S 2 3 Consequently the Whitened Fourier Phase Cepstrum WFPC is obtained by evaluating the IDTFT of the whitened Fourier spectrum which is a complex quantity by equation 3 The advantage of whitening of the Fourier spectrum is that it yields the phase content of a signal without using the arctan function and therefore without any extrinsic discontinuities i e no wrapping ambiguities arise However as to its intrinsic The Hartley Phase Cepstrum as a Tool for Signal Analysis Y cos sin 207 4 and iii the Inverse Discrete Time Hartley Transform 
IDTHT we then obtain the Hartley Phase Cepstrum HPC defined as cH IDTHT Y 5 The HPS Y of equation 4 unlike the conventional FPS of equation 1 does not need unwrapping and hence it does not suffer from the wrapping ambiguities Moreover for the Hartley case the homomorphic deconvolution process is invertible since the unwrapping algorithm is not used The HPS experiences only the intrinsic type of discontinuities which can be compensated as in 8 3 Comparison of the Three Alternative Cepstra on Synthetic Signals In order to illustrate the relative merits of the two alternative cepstral phase features defined earlier the WFPC is not considered after the comments in section 2 2 three signal cases are employed in this section namely 1 2 3 a single rectangular pulse signal noise free and noisy a sequence of five exponentially dumped sinusoidal pulses a sequence of rectangular pulses of varying widths and amplitudes simulating a transmitter receiver scenario Figure 2 a shows the location of a rectangular pulse in the 
time domain and figure 2 b shows the Fourier phase spectrum of the same signal As can be seen the Fourier phase spectrum is a ramp function The gradient of this ramp depends on the location of the signal in the time domain The amplitude of the cepstral function in figure 2 c has a linear relationship with the gradient of the FPS after unwrapping is applied and consequently corresponds to the location of the signal in the time domain Hence the maximum value amplitude of the cepstral function corresponds to the location of the pulse in the time domain The amplitude of the cepstral function does not always correspond to the starting point of the signal in the time domain the point it corresponds depends on the shape of the pulse in the time domain Figure 3 presents a rectangular pulse in the time domain in a along with the HPS and the HPC of the same pulse in b and c respectively The shape of the HPS is a cosinusoidal signal the rate of the zero crossings with respect to the axis frequency corresponds to the 
signal location in the time domain Thus the higher the rate of the zero crossings of the Hartley phase function the further the signal is shifted in the time domain The location of the highest peak s in the HPC yields the location of the pulse in the time domain From figure 3 c it is clear that apart from the two dominant peaks other peaks of lower amplitude appear in the HPC The majority of these additional peaks are the result of the intrinsic discontinuities that exist in the Hartley phase spectrum These additional peaks can be removed from the Hartley phase cepstrum by compensating the intrinsic discontinuities in the corresponding Hartley phase spectrum 208 I Paraskevas and M Rangoussi Fig 2 a Time domain b Frequency domain Fig 3 a Time domain b Frequency domain Figures 4 a and 4 b show the HPC before and after compensation of the intrinsic discontinuities while figure 4 c shows the reconstructed HPC after compensation The number and amplitudes of additional peaks are significantly reduced in figure 4 c 
as compared to figure 3 c In the same figure however it is clear that the HPC peak s do not correspond to the endpoints of the pulse in the time domain rather they yield the middle or central point of the pulse Hence compensation may be useful or not depending on the application The signal example employed next is a sequence of five exponentially dumped sinusoidal The Hartley Phase Cepstrum as a Tool for Signal Analysis 209 Fig 4 a Frequency domain Fig 5 a Time domain b Frequency domain speech signals As shown in figure 5 c peaks in the HPC indicate pulse positions along the time axis while spurious peaks arise due to discontinuities still present After compensation of the discontinuities figure 6 c show that as expected spurious peaks are suppressed yet HPC peaks correspond not to the individual pulses but to the central point of the pulse sequence The FPC cannot yield the location of more than one pulse due to the heuristic and non invertible nature of the unwrapping algorithm irrespective of the 
compensation of the spectral discontinuities Thus when more than one pulse exist in the time domain the amplitude of the 0th cepstral coefficient of the FPC corresponds to an indefinite point within the support of the pulses in the time domain Moreover 210 I Paraskevas and M Rangoussi simulations carried out with additive noise from SNR 20dB down to SNR 11dB for a single pulse either exponentially dumped sine wave or rectangular pulse show that even for high SNRs the amplitude of the 0th cepstral coefficient in the FPC does not correspond to the location of the pulse in the time domain irrespective of the compensation of the intrinsic discontinuities This shows the sensitivity of the unwrapping algorithm to the presence of even the lower possible noise level Fig 6 a Time domain b Frequency domain On the other hand simulations on the synthetic signals mentioned above indicate that the Hartley phase spectrum without the compensation of the intrinsic discontinuities is more immune to noise compared to the 
Hartley phase spectrum with the compensation of the intrinsic discontinuities It is important to mention that the presence of noise in the time domain increases the amount of the intrinsic discontinuities in the phase spectrum Consequently the task of the compensation of the intrinsic discontinuities is more demanding in the case where noise is present in the time domain Concluding the HPC compresses more efficiently than the FPC or the WFPC the signal s phase content and can therefore be used in speech processing applications such as compression or coding Preliminary compression coding experimentation on synthetic speech signals have already given encouraging results in 9 3 1 The Hartley Phase Spectrum of a Rectangular Pulse Signal The signal shown in figure 7 is produced by a rectangular pulse transmitted from a source Multiple reflections are received with time delays and varying amplitudes in a noisy environment As expected the starting and finishing points of the pulse reflection of the pulse in the 
time domain coincide with the highest cepstral peaks of the HPC The Hartley Phase Cepstrum as a Tool for Signal Analysis 211 Time domain 1 5 1 A m p l i t u d e 0 5 0 0 5 0 50 100 150 200 250 300 Time s amples Ceps tral domain Fourier phas e ceps trum 350 400 450 500 150 100 A m p l i t u d e 50 0 50 100 150 0 50 100 150 200 250 300 P has e ceps tral amples Ceps tral domain w hitened Fourier phase ceps trum 350 400 450 500 0 2 0 1 A m p l i t u d e 0 0 1 0 2 0 50 100 150 200 250 300 P has e ceps tral s amples Ceps tral domain Hartley phas e ceps trum 350 400 450 500 400 200 A m p l i t u d e 0 200 400 0 50 100 150 200 250 P has e ceps tral s amples 300 350 400 450 500 Fig 7 Time domain signal a and cepstral domain signals b FPC c WFPC and d HPC respectively of the transmitted and reflected pulse 4 Conclusions In this work we compare thre alternative non linear cepstral phase features as to their noise robustness and phase content encapsulation efficiency merits The feature proposed as advantageous in both 
aspects with practical interest for voiced speech signal application is the Hartley Phase Cepstrum The Hartley Phase Cepstrum may efficiently substitute the Fourier Phase Cepstrum in practical analysis of speech signals Acknowledgement This research has been conducted within the framework of the EnvironmentArchimedes II Funding of research groups in TEI of Piraeus project co funded by the European Union 75 and the Greek Ministry of Education 25 References 1 Paraskevas I Phase as a Feature Extraction Tool for Audio Classification and Signal Localisation Ph D thesis University of Surrey 2005 2 Rabiner L R Gold B Theory and Applications of Digital Signal Processing ch 12 Prentice Hall Englewood Cliffs New Jersey 1975 3 Tribolet J A new phase unwrapping algorithm Acoustics Speech and Signal Processing see also IEEE Transactions on Signal Processing IEEE Transactions on 25 2 212 I Paraskevas and M Rangoussi 6 Moreno I Kober V Lashin V Campos J Yaroslavsky L P Yzuel M J Colour pattern recognition with circular 
component whitening Optics Letters 21 7 Voiced Speech Analysis by Empirical Mode Decomposition Institut 1 Abstract Recently Empirical Mode Decomposition has been proposed as a nonlinear tool for the analysis of non stationary data This paper concerns Empirical Mode Decomposition EMD of speech signal into intrinsic oscillatory mode functions IMFs and their spectral analysis EMD is applied on speech signal spectrogram of speech and IMFs are analysed The different modes explored underline the band pass structure of IMFs LPC analysis of the different modes shows that formant frequencies of voiced speech signal are still preserved Keywords Empirical mode decomposition intrinsic mode functions speech signal spectral analysis 1 Introduction Basis decomposition techniques such as short time Fourier decomposition or wavelet decomposition have extensively been used to analyze non stationary speech signals 1 2 3 They are based on a complete and orthogonal decomposition of the signal into elementary components The 
amplitudes of such components can be interpreted in terms of time frequency or time scale energy distribution Spectrogram and scalogram which are respectively time frequency and time scale representation have been used in speech analysis The main drawback of these approaches is that the basis functions are predetermined and don t necessarily match varying nature of signals Linear prediction coding applied on speech signal is widely used for formant estimation and voice source analysis Recently the Empirical Mode Decomposition EMD has been proposed as a new tool for data analysis 2 This technique performs a time adaptive decomposition of a complex signal into elementary almost orthogonal components that don t overlap in frequency Practical applications of EMD are today broadly spread in numerous scientific disciplines and applied to a number of real life situations 5 6 7 In this paper we use the empirical mode decomposition EMD first introduced by N E Huang and al in 1998 4 in speech analysis This technique 
adaptively decomposes a signal into oscillating components The EMD is in fact a type of adaptive wavelet decomposition which sub bands are built as needed to separate the different components of the signal M Chetouani et al Eds NOLISP 2007 LNAI 4885 pp 214 A Bouzid and N Ellouze Motivated by the success of the EMD technique used in number of applications 6 7 and 8 we are interested by the decomposition of natural speech signal in order to explore speech parameters The outline of the present paper is as follows Second section presents the non linear decomposition EMD technique EMD of composite synthesized signal is given as academic example In the third section we analyze the spectral contribution of the first three IMFs of speech signal Linear Prediction Coding LPC analysis of different intrinsic mode functions is presented in section 4 This analysis provides formant representation of speech Last section concludes this work 2 Empirical Mode Decomposition The empirical mode decomposition is a signal 
processing technique proposed to extract the oscillatory modes embedded in a signal without any requirement of stationarity or linearity of the data The goal of this procedure is to decompose a time series into components with well defined instantaneous frequencies This technique can empirically identify the physical time scales intrinsic to the data that is the time lapse between successive maxima and minima of the signal 9 Each characteristic oscillatory mode extracted is named Intrinsic Mode Function IMF It satisfies the following properties an IMF is symmetric has unique local frequency and do not exhibit the same frequency at the same time for different IMFs In other words the IMFs are characterized by having the number of maxima and minima equal to the number of zero crossings or different at most by one The algorithm operates through the following steps 4 1 Identification of all the maxima and minima of input signal x k 2 Generation of the upper and lower envelope via cubic spline interpolation among 
all the maxima and minima respectively 3 Point by point averaging of the two envelopes to compute a local mean series m k 4 Subtraction of m k from the data to obtain an IMF candidate d k x k m k 5 Check the properties of d k x t i 1 n d i t m n t 1 Voiced Speech Analysis by Empirical Mode Decomposition 215 Here mn t is the residue and di t is the intrinsic mode function relative to i di has the same number of zero crossings and extrema and is symmetric with respect to the local mean Another way to explain how the empirical mode decomposition works is that it picks out the highest frequency oscillation that remains in the signal Thus locally each IMF contains lower frequency oscillations than the one extracted just before This property can be very useful to pick up frequency changes since a change will appear even more clearly at the level of an IMF 4 Figure 1 shows EMD of composite signal The signal taken in this example is composed by a superposition of a chirp and a sine wave 2 1 0 1 2 10 20 30 40 50 60 
70 80 90 100 110 120 residue 1 5 1 0 5 0 0 5 1 1 5 10 20 30 40 50 60 70 80 90 100 110 120 Fig 1 At the top the original signal composed by a chirp and a sine wave Below the first IMF witch is the chirp 12 Fig 2 EMD of a sum of 3 sine waves 100Hz 300Hz and 900Hz 216 A Bouzid and N Ellouze Figure 2 shows another example where the analyzed signal is the sum of three sine waves having respectively 100 Hz 300 Hz and 900 Hz as frequencies The signal is composed by 450 samples at a sampling frequency of 9 kHz EMD of this signal gives four IMFs and the residue We can see that each IMF has the same number of zero crossings as maxima and minima and is symmetric with respect to the zero line We also note that the first mode which corresponds naturally to the highest frequency shows clearly that the 900 Hz frequency is present in the signal Also the second mode depicts 300Hz frequency and the third IMF shows the lowest frequency which is 100Hz Fig 3 Spectrums of a composite signal 100Hz 300 Hz and 900 Hz and its 4 IMFs 
Spectral analysis of the composite signal and its IMFs as depicted in figure 3 shows that the highest frequency is identified by the first IMF and the lowest one is given by the third IMF This analysis permits to separate the composite signal into elementary components constituting the original signal 3 EMD Analysis of Voiced Speech Signal EMD is used to decompose the speech signal in order to analyze the formant frequencies characterizing the vocal tract Speech signals used in this work are extracted from the Keele University database sampled at 20 KHz 13 For better comprehension and evaluation of IMF decomposition of speech signal we compare the spectrogram of speech signal to spectrograms of the corresponding IMFs Figure 4 represents spectrogram of the sentence the north wind pronounced by the speaker f4 given by Keele database and spectrograms of the three first IMFs Spectrogram of speech is represented in figure 4a Spectrogram of IMF1 IMF2 and IMF3 are respectively represented in figures 4b 4c and 4d 
Spectrogram of IMF1 4b shows highest frequencies of speech signal The IMFs have band pass frequency structure they are characterized by decreasing frequency bandwidth and decreasing center frequency Voiced Speech Analysis by Empirical Mode Decomposition 217 Fig 4 Spectrograms of speech signal and its three first IMFs a spectrogram of speech signal b spectrogram of IMF1 c spectrogram of IMF2 d spectrogram of IMF3 4 LPC Analysis of Intrinsic Mode Functions For more exploration of IMFs we operate LPC analysis of IMFs We take as an exa mple of speech signal a vowel o pronounced by a female speaker f1 from Keele database 218 A Bouzid and N Ellouze Fig 5 Illustration of the EMD vowel o speaker f1 and the corresponding IMFs Fig 6 LPC analysis of vowel o speaker f1 dashed line and the 3 first IMFs solid lines Figure 5 shows the different modes obtained from the empirical mode decomposition Figure 6 represents LPC analysis of the signal and the three first IMFs The LPC analysis of the first IMF fits approximately the 
speech signal for frequencies higher than 2 5 KHz The first IMF does not depict the low frequencies of the signal but the highest frequencies These results can be interpreted as the frequency response of equivalent filters As shown in figure 5 the collection of all such filters tends to estimate the different resonant frequencies of the vocal tract Voiced Speech Analysis by Empirical Mode Decomposition 219 Fig 7 Illustration of the EMD vowel a speaker m3 and the corresponding IMFs Fig 8 LPC analysis of vowel a speaker m3 dashed line and the 3 first IMFs solid lines Even if EMD is a non linear processing technique the formants of speech signal are preserved and these frequencies are correctly evaluated We give a second example for the vowel a expressed by a male speaker m2 The resulting EMD and the LPC analysis are respectively depicted in figures 7 and 8 220 A Bouzid and N Ellouze 5 Conclusion In this work we have proposed a new method to decompose a speech signal into different oscillatory modes called 
empirical mode decomposition EMD Furthermore we can look for a new time frequency attributes obtained from the EMD analysis and based on an instantaneous frequency calculation of each component of the decomposition LPC analysis of the first IMFs of speech signal gives good estimation of formants which represents resonant frequencies of the vocal tract Perspectives of this work are noise reduction by EMD analysis and parameter estimation of speech signal References 1 2 3 4 Flandrin P Temps 5 6 7 8 9 10 11 12 13 Estimation of Glottal Closure Instances from Speech Signals by Weighted Nonlinear Prediction Karl Schnell Institute of Applied Physics Goethe University Frankfurt Max von Laue Str 1 60438 Frankfurt am Main Germany schnell iap uni frankfurt de Abstract In this contribution a method based on nonlinear prediction of speech signals is proposed for detecting the locations of the instances of glottal closures GCI For that purpose feature signals are obtained from the nonlinear prediction of speech using a 
sliding window technique The resulting feature signals show maxima caused by the glottal closures which can be utilized for the GCI detection To assess the procedure a speech database with corresponding EGG signals is analyzed providing GCIs as reference Keywords speech analysis nonlinear prediction GCI detection 1 Introduction Speech analysis is often performed using linear models and statistics However nonlinear components are also contained in the speech signal 1 The nonlinear components of speech signals are caused by several effects based on the speech production process and the excitation The nonlinearity can be described more statistically by components which are not produced by a linear system excited with white Gaussian noise WGN this is especially valid for example for the voiced excitation For speech analysis nonlinear systems and operators like the energy operator are used 2 3 In this contribution the nonlinearity of the speech signals is estimated by nonlinear prediction based on Volterra series 
The estimation is performed by an LSE approach yielding the optimum coefficients for a speech segment analytically 4 5 In 5 speech features based on the prediction gain by nonlinear components are discussed In this contribution features based on individual nonlinear predictor coefficients are used and additionally a post processing of the feature signals is carried out estimating the locations of the instances of glottal closures GCI The GCI detection from the speech signal is relevant for many applications therefore several methods exist 6 One group of algorithms uses a first processing based on linear prediction or related methods with a sliding window to obtain feature signals from which the GCIs are estimated 7 8 Other groups use for example wavelet based methods or an analysis of the LPC residual 9 The algorithm proposed in this contribution has a common ground with these of the firstmentioned group since it uses a sliding window technique too The contribution is organized in a way that firstly the 
nonlinear prediction and the feature signals are explained and secondly the GCI detection algorithm and its evaluation are discussed M Chetouani et al Eds NOLISP 2007 LNAI 4885 pp 222 K Schnell 2 Weighted Nonlinear Prediction The nonlinear prediction based on Volterra series estimates a signal value x n by a linear combination of last values and products of last values The first line of eq 1 n using the first and second Volterra shows the prediction error e n x n x kernels with the coefficients hi and hi j respectively For a segment wise analysis attaching different weights to the error values can be appropriate leading to a weighted nonlinear prediction defined by the second line of eq 1 e n x n h i x n i h i j x n i x n j i 1 i 1 j 1 N M i w n e n w n x n h i x n i h i j x n i x n j i 1 i 1 j 1 N M i 1 The weighting function w n determines how strong the values are considered for the estimation depending on their positions within the segment For the prediction of the segment values the finite signals can 
be described by vectors leading to a vectorbased description of eq 1 represented by N M u h i ui h i j ui j ew i 1 i 1 j 1 N M i with definitions N M w 0 e k w 1 e k 1 w L x k L ew T 2 u w 0 x k w 1 x k 1 w L x k L T ui w 0 x k i w 1 x k 1 i w L x k L i T ui j w 0 x k i x k j w 1 x k 1 i x k 1 j T implying the segment xk k L x k x k 1 x k L with the length l L 1 N M The vector ew includes the weighted prediction error values obtained from a prediction of orders N and M for the first and second kernel respectively Eq 2 can be solved for the vector u resulting in eq 3 The optimum predictor coefficients are given by the best possible approximation of the vector u by a linear combination of the vectors u i and u i j minimizing the length of the error vector u N M h i ui hi j ui j ew i 1 i 1 j 1 N M i N M ew 2 e N M n n w 3 min This optimization can be solved analytically by regression An example of performing the regression is given in 5 Estimation of Glottal Closure Instances from Speech Signals 223 3 Nonlinear 
Feature Signals The vector based prediction algorithm of the previous section is used to estimate the coefficients h i and h i j from overlapping speech frames xk k L The coefficients corresponding to the frame xk k L x k x k 1 x k L are denoted by the N M index k with hi k and hi j k analogously the same is valid for ew k For the analysis a sliding window technique is used since the positions of the frame boundaries affect the analysis results which is explained in the following The voiced speech can be described by the output of the speech production system excited by the voiced excitation The voiced excitation is highly affected by the glottal closures causing impulses in the excitation in comparison to that glottal openings have usually minor effects In 7 a short rectangular window is used as sliding window since in this case the prediction error can be assumed to be zero or minimum for windows within the closed glottis interval In comparison to that here nonlinear prediction is used and additionally a 
special window function is integrated Furthermore the frame length is chosen longer modifying the argumentation slightly The analysis results of the prediction are influenced by the numbers and locations of the glottal closures within in the frame Therefore the prediction results can be also very sensitive corresponding to the positions of the frame boundaries since a small shift of a boundary can decide whether a glottal closure is inside or outside of the frame Hence if the frame is shifted rightward a glottal closure can be introduced at the right side and or a glottal closure can be removed at the left side To remove the escaping glottal closure at the left side gradually an asymmetric window function wa is used which can be seen in fig 1 The left part 2 3 of the length of wa is equal to Fig 1 Asymmetric window function wa the left side of the Hann window The window function is not included to eliminate spectral distortions since the estimation algorithm corresponds to the covariance method Feature 
signals are derived from the products of the nonlinear prediction which is explained in the following Each frame xk k L yields a feature value F Since the shifting of the frame is one sample the series of features can be interpreted as a feature signal F k For each feature value F k a corresponding speech sample x k d can be assigned with 0 d L To synchronize the speech and feature signal d L is reasonable for the asymmetric window wa since it describes the right window side In 5 the prediction gain by the nonlinear components is used to define 224 K Schnell N M the feature signal Fgain k of eq 4 describing a measure of nonlinearity the numerator contains the error by the linear prediction whereas the denominator contains the error by the nonlinear prediction The analyses indicate that the prediction gain yields best results for order M 1 of the second kernel however it yields not reliably good results One reason lies in the fact that the voiced excitation comprises components in addition to the glottal 
closures Therefore the feature signal Fi Nj M k is proposed based on an individual nonlinear coefficient hi j and its sign depends on the polarity of the analyzed signal N M Fgain k log N 0 ew k N M ew k 4 Fi Nj M k 4 Detection of GCI To asses GCI detection algorithms the GCIs obtained from the EGG signal can be used as reference The locations of the GCI can be associated with the positive peaks of the DEGG signal which is the derivative of the EGG Since the EGG signals can be a little bit noisy the EGG signals are smoothed prior to applying the derivative For the GCI detection from speech the feature signals of the last section are used In the following the order selection and differences between the features signals for GCI detection are treated The analyses indicate that the feature signals with small prediction orders yield mostly better or equal results than with higher prediction 1 1 0 1 orders The features with smallest prediction orders are F1 1 and F1 1 the latter feature considers only nonlinear 
components Fig 2 shows the feature signals and the EGG based signals for a speech segment uttered by a female speaker The speech segment is from the Keele database which is converted to a sampling rate of 8 kHz The frame length for the analysis is l 150 It can be seen that the peaks of the 1 1 1 1 DEGG correspond to peaks of the feature signals Fgain and F1 1 Therefore GCI candidates can be recognized by the maxima of these feature signals The feature signal based on prediction gain shows additional maxima which complicates the choosing of the true GCIs from the candidates Since the additional maxima have 1 1 1 1 mostly a negative sign for the feature F1 1 the feature F1 1 has advantages This Estimation of Glottal Closure Instances from Speech Signals 225 circumstance is based on the fact that the prediction gain depends only on the absolute value of h1 1 provided that the coefficient is optimum Although the resulting feature 1 1 signal F1 1 k in fig 2 is suitable the analyses of different speakers indicate 
that the choice of a male speaker deteriorates often the results In this point the feature signal 0 1 F1 1 seems to be favorable since it delivers good results for female and male voices Fig 2 Analyzed speech with corresponding feature signals and EGG signal 4 1 Algorithm for GCI Detection In the following an automatic GCI detection algorithm based on the feature signal 0 1 1 1 F1 1 is explained In comparison to F1 1 the glottal closures are estimated by crossing a threshold from beneath The threshold is determined by a percentage of the maximum value of the feature signal the calculation of is performed adaptively resulting in k The estimation of GCI candidates is defined by 0 1 F1 1 k k and 0 1 0 1 0 1 with k max F1 1 k W F1 1 k W 1 F1 1 k W 0 1 F1 1 k 1 k k d is GCI candidate 5 The factor of the maximum is chosen with 0 3 and W determines the neighborhood for the calculation of the thresehold k this threshold is shown in fig 2 with the positive values of the feature signal representing the threshold for 0 
the function pos F yields the feature value F for F 0 and 0 for F 0 After determining the GCI candidates a post processing is performed estimating a 226 K Schnell realistic sequence of GCIs Only GCI candidates are chosen which have distances between themselves which are greater than a minimum distance This minimum distance is equal for all analyses however different for male and female speakers Each GCI candidate is weighted by the sum S of the following consecutive feature values which are greater than the threshold This value S can be used to decide between two or more possible GCI candidates which are close together The algorithm starts with two adjoining GCI candidates describing the initial conditions From the difference of the positions of the two candidates a provisional pitch period length p is estimated Then in a region of the position of the right GCI candidate plus p the GCI candidate with maximum S is chosen as next GCI position If no GCI candidate is in that region GCI candidates are used 
obtained with the threshold 0 0 This is repeated till no candidate can be found The same is performed on the left side The initial condition of the algorithm is varied for all possible two adjoining GCI candidates and the resulting GCI positions are arranged in the final set of GCI positions considering the minimum distance The final set of detected GCI 0 1 whereas the GCI positions obtained from the positions is denoted with k F1 1 EGG signal is denoted with i EGG 4 2 Analysis of Speech Database 0 1 The automatic GCI detection algorithm based on the feature signal F1 1 is used to estimate the GCIs of the speech signals from the Keele database The Keele database contains utterances from five male and five female speakers 6 Each utterance consists of the same text providing phonetically balanced English For the analysis the sampling rate of the speech and EGG signals are converted to 8 kHz To analyze only voiced speech parts firstly the voiced parts of the speech signals are aligned For the voice detection 
the short time energy of the speech low passed is used Then the GCIs are automatically determined from the speech and the EGG signals 0 1 and i EGG respectively It should be resulting in the GCI positions k F1 1 mentioned that the i EGG are actually obtained from the DEGG signal For 0 1 comparing the GCIs of k F1 1 and i EGG each EGG based GCI position is 0 1 assigned to a feature based GCI i EGG i F1 1 with i on condition that 0 1 i EGG of two positions is the absolute value the difference i F1 1 smaller or equal the constant is the allowed deviation This means that if is true the EGG based glottal closure is declared as not detected 0 1 Additionally no unassigned k F1 1 should exist between two adjacent i EGG In this way the percentage of correctly detected GCIs can be computed This percentage of correct detections depends on the permitted deviation Table 1 shows the resulting averaged values for the analysis of the Keele database depending on The values and are the means of the differences and their 
absolute values representing the bias of the detection and the averaged distance to the reference Estimation of Glottal Closure Instances from Speech Signals 227 GCI respectively Since the first and last GCIs of each voiced segment are sometimes imperfect the values are also computed ignoring these GCIs this is denoted by The resulting values of table 1 are averaged over all analyzed voiced segments of the speakers and then averaged for five female and four male speakers respectively The 4th male speaker of the Keele database is ignored since the EGG based GCI detection was not reliable however this was not valid for the feature based GCI detection The voiced parts of the analyzed nine speakers of the Keele database yield about 26000 EGG based GCIs which are considered for the evaluation From table 1 it can be seen that the GCI detection yields better estimations for female speakers This is also valid for the individual subjects for example the percentage of correctly estimated GCIs for 1 ms is 96 5 96 9 96 
9 94 2 and 91 7 for the five female speakers and 87 8 87 7 87 4 and 80 9 for the four male speakers The averaged offset of the GCI detection is different for male and female speakers This can be seen also for the individual subjects of the speakers for example for 1 ms the offsets is 0 13 0 18 0 13 0 35 and 0 16 ms for the female speakers and 0 11 0 18 0 11 and 0 27 ms for the male speakers It should me mentioned that for the calculation of the percentage of the correctly estimated GCIs the offset is not considered By a consideration of an offset the estimation results would be improved The averaged values give an evaluation respecting the whole database however an inspection of individual segments is necessary to judge several effects Fig 3 shows some individual examples cutting out from analyzed segments Fig 3 a shows that the algorithm works usually also for in stationary speech regions however if the regions are very corrupted with abrupt changes of the pitch the GCI detection can produce false 
detections Fig 3 d shows that multiple candidates can be produced by the threshold 0 3 for this case the candidates are unambiguous with the threshold with 0 The EGG and DEGG signals don t yield in all cases unambiguous and precise GCIs 10 The figs 3 b c and e show examples concerning the use of the EGG signal for the reference GCIs In some cases the EGG signal has a region which is very Table 1 Comparisons between estimated and reference GCIs obtained from Keele database is the deviation between EGG based and feature based GCIs is the percentage of correctly estimated GCIs with and are the means of the deviations of the correctly estimated GCIs means ignoring the first and last GCI of each voiced segment ms samples 0 375 0 375 0 625 0 625 1 0 1 0 1 5 1 5 2 0 2 0 3 3 5 5 8 8 12 12 16 16 female male ms female male ms female male 83 7 85 7 93 3 95 3 95 3 97 0 96 2 98 0 96 6 98 3 63 7 67 5 76 6 80 4 85 9 89 6 90 9 93 7 93 6 95 6 0 19 0 19 0 22 0 22 0 24 0 23 0 25 0 24 0 25 0 25 0 18 0 18 0 25 0 24 0 31 0 31 0 
37 0 35 0 41 0 38 0 157 0 016 0 163 0 014 0 185 0 092 0 19 0 086 0 19 0 167 0 2 0 157 0 2 0 22 0 21 0 2 0 25 0 25 0 21 0 22 228 K Schnell Fig 3 Examples of analyzed speech segments a in stationary speech segment b and c DEGG signal with double peaks d multiple GCI candidates for threshold with 0 3 e distorted EGG DEGG signal Estimation of Glottal Closure Instances from Speech Signals 229 low level and blurred as shown in fig 3 e In comparison to that the GCI detection based on the feature signal works for these regions usually as good as for the other regions The increasing regions of the EGG describing the GCIs can be relatively long Additionally this increasing part of one glottal cycle of the EGG can produce more than one peak in the DEGG which is shown in the figs 3 b and c This means that the evaluation by EGG based GCIs is a good method however it produces not continuously exact evaluations especially for a precise calculation of the deviations between the estimated and true GCIs 5 Conclusions In this 
paper a GCI detection algorithm based on nonlinear prediction is proposed For that purpose different speech features are discussed concerning the detection of GCIs The analyses show that small prediction orders are favorable and that for the feature definition individual nonlinear coefficients are advantageous over the prediction gain The proposed algorithm is evaluated by the analysis of the Keele database showing that the nonlinear statistics can be utilized for GCI detection Acknowledgments The author would like to thank C D Alessandro for discussion References 1 Faundez M et al Nonlinear Speech Processing Overview and Applications Int J Control Intelligent Syst 30 1 Quantitative Perceptual Separation of Two Kinds of Degradation in Speech Denoising Applications Anis Ben Aicha and Sofia Ben Jebara 1 Introduction Evaluation of denoised speech quality can be done using subjective criteria such as MOS Mean Opinion Score or DMOS Degradation MOS 1 However such evaluation is expensive and time consuming so that 
there is an increasing interest in the development of robust quantitative speech quality measures that correlate well with subjective tests Objective criteria can be classified according to the domain in which they operate We relate for example the Signal to Noise Ratio SN R and segmental SN R operating on time domain 2 the Cepstral Distance CD and Weighted Slope Spectral distance W SS operating in frequency domain 2 and Modified Bark Spectral Distortion M BSD operating in perceptual domain 3 Perceptual measures are shown to have the best chance of predicting subjective quality of speech and other audio signals since they are based on human auditory perception models The common point to all objective criteria is their ability to evaluate speech quality using a single parameter which embed all kinds of degradation after any processing Indeed speech quality measures are basing their evaluation on both original and degraded speeches according to the following application C C E2 R x y c M Chetouani et al Eds 
NOLISP 2007 LNAI 4885 pp 1 Quantitative Perceptual Separation of Two Kinds of Degradation 231 where E denotes the time frequency or perceptual domain x resp y denotes original speech resp observed speech altered by noise or denoised speech after processing and c is the score of the objective measure Mathematically C is not a bijection from E2 to R It means that it is possible to find a signal y which is perceptually different from y but has the same score as the one obtained with y c x y c x y We relate for example the case of an original signal x which is corrupted by an additive noise to construct the signal y Then x is coded and decoded using a CELP coder to obtain the signal y It is obvious that the degradation noticed in both y and y are not the same Degradation of y is heard as a background noise and the degradation of y is perceptually heard as signal distortion We aim improving speech quality evaluation by separating two kinds of degradation which are the additive residual noise and the speech 
distortion Each degradation will be evaluated using its adequate criterion so that the non bijection C will be avoided and replaced by a bijection one characterized by a couple of outputs instead of a single output Moreover thanks to the advantage of perceptual tools in the evaluation of speech quality the new couple of criteria will be based on auditor properties of human ear 2 Study Context Speech Denoising Before defining novel criteria of speech quality evaluation let s define the different kinds of degradation altering speech Without loss of generality we consider the speech denoising application and we use spectral denoising approaches They are viewed as a multiplication of noisy speech spectrum Y m k by a real positive coefficient filter H m k see for example 5 The estimated spectrum of clean speech is written m k H m k Y m k S where m resp k denotes frame index resp frequency index The estimation error spectrum m k is given by m k m k S m k S 3 2 We assume that speech and noise are uncorrelated Thus 
the estimated error power spectrum is given by E m k 2 H m f 1 2 E S m k 2 H m k 2 E N m k 2 4 where N m k 2 denotes the noise power spectrum Since 0 H m k 1 the first term of Eq 4 expresses the attenuation of clean speech frequency components Such degradation is perceptually heard as a distortion of clean speech However the second term expresses the residual noise which is perceptually heard as a background noise Since it is additive it is possible to formulate it as an accentuation of clean speech frequency components 232 A Ben Aicha and S Ben Jebara Fig 1 Power spectrum of two signals degraded differently 3 Classical Criteria Limitations To show the limitations of classic criteria for evaluating the denoised signal quality in terms of degradation nature i e residual background noise or distortion of clean speech we construct the two following signals 4 s1 n is a corrupted version of the clean speech s n by an additive white Gaussian noise with SN Rseg 15 dB s1 n is perceptually heard as the clean speech 
drowned in background noise without any noticeable distortion of the clean speech itself The background noise heard in s1 n express the residual noise after the denoising process s2 n is constructed in the short time spectral domain To simulate the speech distortion we artificially attenuate some frequency components of clean speech signal s2 n is heard as a distorted version of the clean speech s n without any noticeable residual background noise We represent in Fig 1 the power spectrum of the two signals They are completely different because of their different construction process The power spectrum of the first signal S1 m k 2 is above the clean one It can be seen as an accentuation of the clean speech power spectrum However the power spectrum of the second signal S2 m k 2 is below the clean one Hence it is considered as an attenuation of the clean speech power spectrum Regarding Fig 1 the two signals s1 n and s2 n must be evaluated differently using classic criteria To asses the speech quality of s1 n 
and s2 n we use the SN Rseg as temporal criterion the W SS as frequency criterion and M BSD as perceptual criterion Tab 1 summarize the assessment scores obtained by the mentioned criteria In terms of SN Rseg and W SS scores the two signals are similar which means that they are supposed to have the same quality However according to listening tests and to Fig 1 this is can t be true Thus for the same score obtained by an objective criterion it is possible to find many signals which Quantitative Perceptual Separation of Two Kinds of Degradation Table 1 Objective evaluation of s1 n and s2 n s1 n s2 n SN Rseg dB W SS M BSD 3 5 34 3 0 92 3 6 31 0 0 02 233 are perceptually different This fact shows the non bijection of classical objective criteria In terms of M BSD the best score is obtained for the distorted signal However subjective tests show that the degradation of the signal s2 n is more annoying than the degradation of signal s1 n Thus using a single parameter to evaluate the speech quality it is not 
possible to separate the two kinds of degradation 4 Proposed Perceptual Characterization of Audible Degradation We aim to perceptually characterize the degradation altering denoised speech Hence auditory properties of human ear are considered More precisely the masking concept is used a masked signal is made inaudible by a masker if the masked signal magnitude is below the perceptual masking threshold MT In our case both degradation can be audible or inaudible according to their position regarding the masking threshold We propose to find decision rules to decide on the audibility of residual noise and speech distortion by using the masking threshold concept If they are audible the audibility rate will be quantified according to the proposed criterion There are many techniques to compute masking threshold MT we use in this paper Johnston model which is well known for its simplicity and well used in coding context 6 4 1 Perceptual Characterization of Audible Noise According to MT definition it is possible to 
add to the clean speech power spectrum the MT curve considered as a certain signal so that the resulting signal obtained by inverse FFT has the same audible quality than the clean one The resulting spectrum is called Upper Bound of Perceptual Equivalence U BP E and is defined as follows U BP E m k s m k M T m k 5 where s m k is the clean speech power spectrum When some frequency components of the denoised speech are above U BP E the resulting additive noise is heard 4 2 Perceptual Characterization of Audible Distortion By duality some attenuations of frequency components can be heard as speech distortion Thus by analogy to U BP E we propose to calculate a second curve 234 A Ben Aicha and S Ben Jebara which expresses the lower bound under which any attenuation of frequency components is heard as a distortion We call it Lower Bound of Perceptual Equivalence LBP E To compute LBP E we used the audible spectrum introduced by Tsoukalas and al for audio signal enhancement 7 In such case audible spectrum is 
calculated by considering the maximum between clean speech spectrum and masking threshold When speech components are under MT they are not heard and we can replace them by a chosen threshold m k The proposed LBP E is defined as follows LBP E m k s m k if s m k M T m k m k otherwise 6 The choice of m k obeys only one condition m k M T m k We choose it for example equal to 0 dB 4 3 Usefulness of U BP E and LBP E Using U BP E and LBP E we can define three regions characterizing the perceptual quantity of denoised speech frequency components between U BP E and LBP E are perceptually equivalent to the original speech components frequency components above U BP E contain a background noise and frequency components under LBP E are characterized by speech distortion This characterization constitutes our idea to identify and detect audible additive noise and audible distortion As illustration we present in Fig 2 an example of speech frame power spectrum and its related curves U BP E upper curve in bold line and LBP E 
bottom curve in dash line The clean speech power spectrum is for all frequencies index between the two curves U BP E and LBP E We remark that the two curves are the same for most peaks It means that for these frequency intervals any kind of degradation altering speech will be audible If it is over U BP E it will be heard as background noise In the opposite case it will be heard as speech distortion 5 5 1 Audible Degradation Estimation Audible Additive Noise PSD Estimation Once U BP E is calculated the superposition of denoised signal power spectrum and U BP E leads to separate two cases The First one corresponds to the regions of denoised speech power spectrum which are under U BP E In such case there is no audible residual noise In the second case some denoised speech frequency components are above U BP E the amount above U BP E constitutes the audible residual noise As illustration we represent in Fig 3 an example of denoised speech power spectrum and its related U BP E curve calculated from clean speech 
The used denoising approach is spectral subtraction 5 From Fig 3 we Quantitative Perceptual Separation of Two Kinds of Degradation 235 Fig 2 An example of U BP E and LBP E in dB of clean speech frame notice that frequency regions between 1 kHz and 2 kHz are above U BP E they hence contain residual audible noise In terms of listening tests such residual noise is annoying and constitutes in some cases the musical noise Such musical noise is very popular and constitutes the main drawback of spectral subtraction approaches Once the U BP E is calculated it is possible to estimate the audible power spectrum density of residual noise using a simple subtraction when it exists Hence the residual noise power spectrum density PSD is written p m k n s m k U BP E m k if s m k U BP E m k 0 otherwise 7 where s m k denotes the PSD of denoised speech and the suffix p designs the perceptually sense of the PSD 5 2 Audible Speech Distortion PSD Estimation We use the same methodology as the one used for residual background noise 
We represent in Fig 4 an example of denoised speech power spectrum and its related curve LBP E calculated from the clean speech We notice that some regions are under LBP E for example regions between 1 5 kHz and 2 kHz they hence constitute the audible distortion of the clean speech In terms of listening tests they are completely different from residual background noise They are heard as a loss of speech tonality p as follows It is possible to estimate the audible distortion PSD d p d m k LBP E m k s m k if s m k LBP E m k 0 otherwise 8 236 A Ben Aicha and S Ben Jebara Fig 3 Superposition of a denoised speech power spectrum and its related clean speech U BP E 6 Audible Degradation Evaluation In this section we detail the proposed approach to quantify separately the two kinds of degradation The assessment of the denoised speech quality by means of two parameters permits to overcome the problem of non bijection of classic objective evaluation and to better characterize each kind of speech degradation Hence 
instead of the application defined in Eq 1 we develop a novel application from perceptual domain to R2 C E2 R2 x y P SAN R P SADR 9 where P SAN R and P SADR are two parameters related respectively to the residual noise and the distortion The definition of P SAN R and P SADR is inspired from the SN R definition which is the ratio between signal energy and noise energy Thanks to Parseval theorem it can be calculated in frequency domain Moreover since the U BP E and LBP E are perceptually equivalent to the original signal the proposed definition uses the energy of U BP E and LBP E instead of the energy of the clean speech The time domain signal related to U BP E is called upper effective signal whereas the time domain signal related to LBP E is called lower effective signal In the following subsection we define the proposed criteria 6 1 Perceptual Noise Criterion PSANR The perceptual residual noise criterion is defined as the ratio between the upper effective signal which is the U BP E and the audible residual 
noise The Perceptual Signal to Audible Noise Ratio P SAN R m of frame m is calculated in frequency domain due to the Parseval theorem and it is formulated as follows Quantitative Perceptual Separation of Two Kinds of Degradation 237 Fig 4 Superposition of a denoised speech frame and its related clean speech LBP E N k 1 U BP E m k N p k 1 n m k P SAN R m 10 where N denotes the total number of frames 6 2 Perceptual Distortion Criterion PSADR By the same manner we define the Perceptual Signal to Audible Distortion Ratio P SADR m of frame m as a ratio between the lower effective signal which is LBP E and the audible distortion The P SADR m is given by P SADR m 6 3 PSANDR Criteria N k 1 LBP E m k N p k 1 d m k 11 To compute the global P SAN R and P SADR of the total speech sequence we are referred to the segmental SN R SN Rseg thanks to its better correlation with subjective tests when compared to the traditional SN R The principle of segmental SN R consists in determining the SN R for each frame SN R m and then 
calculating their geometric mean over the total number of frames SN Rseg M m SN R m 2 Moreover since the SN R and SN Rseg are usually expressed in dB The geometric mean is equivalent to the arithmetic mean in log domain Using this approach we compute the global P SAN R and P SADR for a given sequence of speech Next the couple P SAN R P SADR defines the new criterion to evaluate both kinds of degradation We call it Perceptual Signal to Audible Noise and Distortion Ratio P SAN DR M 238 A Ben Aicha and S Ben Jebara 7 7 1 Experimental Results for Artificial Degradations Case of Artificial Additive Noise To show the ability of P SAN DR to take into account the perceptual effect of an additive noise we add to a clean speech an artificial noise constructed from the masking threshold by multiplying it with a factor 0 y n s n M T n 12 In Fig 5 we represent the evolution of SN Rseg P SAN R and P SADR versus calculated between clean speech s n and the noisy one y n We notice the following interpretations We propose to 
show the ability of P SADR to take into account the perceptual effect of the distortion impairing the denoised speech For simplicity reasons we deal with the case where the distortion is inaudible For such reason we built an artificial signal obtained from the clean one by multiplying its power spectrum which is under the M T by a factor 0 1 Y m k We hence define the distortion as the attenuation of frequency components of clean speech When we superpose the clean speech power spectrum and it s related masking threshold M T frequency components under M T are not audible Quantitative Perceptual Separation of Two Kinds of Degradation 239 Fig 5 Evolution of SN Rseg P SAN R and P SADR versus in case of additive noise By multiply these non audible components by a factor 1 we will not touch on the perceptual quality of the clean speech In Fig 6 we represent the evolution of SN Rseg and P SADR versus calculated between clean speech s n and the distorted one y n We notice the following interpretations 8 8 1 
Experimental Results with Subtractive Denoising Techniques Overview In order to show advantages of the proposed criterion for evaluating quality of denoised speech in real cases we propose to assess the performances of power subtraction technique using P SAN R P SADR and traditional criteria We recall that the denoised speech power spectrum is obtained from the noisy one using the following relationship 8 m k 2 240 A Ben Aicha and S Ben Jebara Fig 6 Evolution of SN Rseg and P SADR versus in case of artificial distortion m k 2 resp Y m k 2 denotes the denoised speech power spectrum where where IF F T is the Inverse Fast Fourier Transformation and m k is the phase of noisy speech 8 2 Perceptual Effect of and The choice of the two parameters and in Eq 14 must obey the classical tradeoff between residual noise and speech distortion In fact the parameters and operates as follows Quantitative Perceptual Separation of Two Kinds of Degradation 241 In this section we propose to study the influence of the factor on 
the quality of the denoised speech using P SAN R P SADR and classic criteria We seek to demonstrate that the proposed criterion can predict the impact of varying on the denoised speech quality when classic criteria are not able to do it We summarize in Tab 2 the evolution of W SS and M BSD versus the factor Table 2 Evolution of W SS and M BSD versus 0 1 2 3 4 5 6 7 8 9 10 W SS 45 8 55 56 72 65 110 3 170 2 222 8 229 6 200 3 158 3 123 98 17 M BSD 2 30 1 74 1 28 0 92 0 65 0 46 0 31 0 20 0 14 0 09 0 06 Fig 7 Evolution of SN Rseg P SAN R and P SADR versus in case of subtraction techniques 242 A Ben Aicha and S Ben Jebara 9 9 1 Experimental Results with Perceptual Denoising Techniques Perceptual Techniques Overview Let s now compare some perceptual denoising techniques by means of the new objective criteria We propose to denoise a corrupted signal by Gaussian noise with SN R 0 dB using the following techniques where SN Rprio m k is the a priori Signal to Noise Ratio 9 Hw m 1 k Y m 1 k 2 SN Rprio m k 1 P SN Rpost m 
k The major drawback of Wiener technique is the presence of a musical residual noise with unnatural structure in the enhanced speech Indeed an over or a sub estimation of noise power spectrum at a given frequency leads to the occurrence of an artificial tone which is called musical tone Such residual noise becomes in some cases more annoying than the original noise Quantitative Perceptual Separation of Two Kinds of Degradation 243 where HG m k is the considered denoising filter To make the residual noise inaudible HG m k must verify the following equation where h is the amplitude difference between the musical peaks and its neighbors and w is the width of musical tones The reduction of musical noise is done by multiplying the power spectrum of the detected musical tones by a factor 0 1 which controls the shape attenuation It leads to the improvement of many quantitative criteria temporal spectral and perceptual characteristics These results are confirmed by subjective tests 9 2 Perforamnces Comparison 
Evaluation of denoising quality is done using classical objective criteria segmental SN R W SS M BSD and the proposed P SAN DR Results are resumed in Tab 3 244 A Ben Aicha and S Ben Jebara Table 3 Evaluation of denoised signals noisy speech wiener technique modified wiener perceptual technique SN Rseg dB W SS M BSD P SAN R dB P SADR dB 4 30 1 05 1 13 1 62 46 07 74 25 69 63 2 32 0 28 0 19 3 90 5 04 5 54 12 71 17 27 7 53 7 01 6 93 45 41 0 15 10 Conclusion The spectral and perceptual analysis of the degradation in the case of denoised speech imposes to separate between residual noise and signal distortion We first propose two curves U BP E and LBP E to calculate the audible residual noise and audible distortion Next two parameters P SAN R and P SADR characterizing the two kinds of degradation are developed Simulation results comparing different denoising approaches and classical objective measures show a better characterization of degradation nature of denoised signal The calculation of the degree of 
correlation of the proposed criteria with MOS criterion constitutes the perspectives of our work References 1 Recommandation UIT T P 800 Quantitative Perceptual Separation of Two Kinds of Degradation 245 3 Yang W Benbouchta M Yantorno R Performance of the modified bark spectral distortion as an objective speech measure In ICASSP Proc Int Conf on Acoustics Speech and Signal Processing vol 1 pp An Efficient VAD Based on a Generalized Gaussian PDF O E T S I I T Universidad de Granada C Periodista Daniel Saucedo 18071 Granada Spain gorriz ugr es Abstract The emerging applications of wireless speech communication are demanding increasing levels of performance in noise adverse environments together with the design of high response rate speech processing systems This is a serious obstacle to meet the demands of modern applications and therefore these systems often needs a noise reduction algorithm working in combination with a precise voice activity detector VAD This paper presents a new voice activity detector VAD 
for improving speech detection robustness in noisy environments and the performance of speech recognition systems The algorithm defines an optimum likelihood ratio test LRT involving Multiple and correlated Observations MCO An analysis of the methodology for N 2 3 shows the robustness of the proposed approach by means of a clear reduction of the classification error as the number of observations is increased The algorithm is also compared to different VAD methods including the G 729 AMR and AFE standards as well as recently reported algorithms showing a sustained advantage in speech non speech detection accuracy and speech recognition performance 1 Introduction The emerging applications of speech communication are demanding increasing levels of performance in noise adverse environments Examples of such systems are the new voice services including discontinuous speech transmission 1 2 3 or distributed speech recognition DSR over wireless and IP networks 4 These systems often require a noise reduction scheme 
working in combination with a precise voice activity detector VAD 5 for estimating the noise spectrum during non speech periods in order to compensate its harmful effect on the speech signal During the last decade numerous researchers have studied different strategies for detecting speech in noise and the influence of the VAD on the performance of speech processing systems 5 Sohn et al 6 proposed a robust VAD algorithm based on a statistical likelihood ratio test LRT involving a single observation vector Later Cho et al 7 suggested an improvement based on a smoothed LRT Most VADs in use today normally consider hangover algorithms based on empirical models to smooth the VAD decision It has been shown recently 8 9 that incorporating long term speech information to the decision rule reports M Chetouani et al Eds NOLISP 2007 LNAI 4885 pp An Efficient VAD Based on a Generalized Gaussian PDF 247 benefits for speech pause discrimination in high noise environments however an important assumption made on these 
previous works has to be revised the independence of overlapped observations In this work we propose a more realistic one the observations are jointly gaussian distributed with non zero correlations In addition important issues that need to be addressed are i the increased computational complexity mainly due to the definition of the decision rule over large data sets and ii the optimum criterion of the decision rule This work advances in the field by defining a decision rule based on an optimum statistical LRT which involves multiple and correlated observations The paper is organized as follows Section 2 reviews the theoretical background on the LRT statistical decision theory Section 4 considers its application to the problem of detecting speech in a noisy signal Finally in Section 4 1 we discuss the suitability of the proposed approach for pair wise correlated observations using the experimental data set AURORA 3 subset of the original Spanish SpeechDat Car SDC database 10 and state some conclusions in 
section 6 2 Multiple Observation Probability Ratio Test Under a two hypothesis test the optimal decision rule that minimizes the error probability is the Bayes classifier Given an observation vector y to be classified the problem is reduced to selecting the hypothesis H0 or H1 with the largest posterior probability P Hi y From the Bayes rule L y H1 P H0 py H1 y y H1 y H0 py H0 y H0 P H1 1 In the LRT it is assumed that the number of observations is fixed and represented by a vector y The performance of the decision procedure can be improved by incorporating more observations to the statistical test When N measurements y 1 y 2 y N are available in a two class classification problem a multiple observation likelihood ratio test MO LRT can be defined by 1 y 2 y N LN y py1 y2 yN H1 y 1 y 2 y N H1 py1 y2 yN H0 y 1 y 2 y N H0 2 This test involves the evaluation of an N th order LRT which enables a computationally efficient evaluation when the individual measurements y k are independent However they are not since the 
windows used in the computation of the observation vectors yk are usually overlapped In order to evaluate the proposed MCO LRT VAD on an incoming signal an adequate statistical model for the feature vectors in presence and absence of speech needs to be selected The joint probability distributions under both hypotheses are assumed to be jointly gaussian independently distributed in frequency and in each part real and imaginary of vector with correlation components between each pair of frequency observations 248 O LN y 1 y 2 y N p R I 1 1 2 2 py y y H1 y1 y 2 y N H1 N N y y H p py y y H0 y1 2 N 0 3 This is a more realistic approach that the one presented in 9 taking into account the overlap between adjacent observations We use following joint gaussian probability density function jGpdf for each part 1 T N y C py Hs y Hs KHs N mean frequency observation vector Cy Hs is the N order covariance matrix of the observation vector under hypothesis Hs and denotes determinant of a matrix The model selected for the 
observation vector is similar to that used by Sohn et al 6 that assumes the discrete Fourier transform DFT coefficients of the clean speech Sj and the noise Nj to be asymptotically independent Gaussian random variables In our case the observation vector consist of the real and imaginary parts of frequency DFT coefficient at frequency of the set of m observations y Hs 3 Evaluation of the LRT In order to evaluate the MCO LRT the computation of the inverse matrices and determinants are required Since the covariances matrices under H0 H1 are assumed to be tridiagonal symmetric matrices1 the inverses matrices can be computed as the following qk qN 1 Cy pm pk N 1 m k 0 6 mk pk pN where N is the order of the model and the set of real numbers qn pn n 1 satisfies the three term recursion for k 1 0 rk qk 1 pk 1 k 1 qk pk rk 1 qk 1 pk 1 with initial values 1 7 The covariance matrix will be modeled as a tridiagonal matrix that is we only consider the correlation function between adjacent observations according to the 
number of samples 200 and window shift 80 that is usually selected to build the observation vector This approach reduces the computational effort achieved by the algorithm with additional benefits from the symmetric tridiagonal matrix properties N Cy mk 2 2 y E ym m rmk E ym yk 0 other case if if m k k m 1 5 2 where 1 i j N and y rij are the variance and correlation frequency i components of the observation vector y denoted for clarity i ri which must be estimated using instantaneous values An Efficient VAD Based on a Generalized Gaussian PDF 1 p0 1 and p1 r1 1 q0 0 and q1 r1 249 8 In general this set of coefficients are defined in terms of orthogonal complex polynomials which satisfy a Wronskian like relation 11 and have the continuedfraction representation 12 qn z 1 pn z z 1 2 r1 z 2 2 rn 1 z n 9 where denotes the continuos fraction This representation is used to compute the coefficients of the inverse matrices evaluated on z 0 In the next section we show a new VAD based on this methodology for N 2 and 3 
that is this robust speech detector is intended for real time applications such us mobile communications The decision function will be described in terms of the correlation and variance coefficients which constitute a correction to the previous LRT method 9 that assumed uncorrelated observation vectors in the MO 4 Application to Voice Activity Detection The use of the MCO LRT for voice activity detection is mainly motivated by two factors i the optimal behaviour of the so defined decision rule and ii a multiple observation vector for classification defines a reduced variance LRT reporting clear improvements in robustness against the acoustic noise present in the environment The proposed MCO LRT VAD is described as follows The l 1 y l y l 1 MCO LRT is defined over the observation vectors y l m y y l m as follows l N 1 2 y T N y ln N Cy H0 N Cy H1 10 N 1 N Cy 1 N 2m 1 is the order of the model l where N Cy H0 H1 denotes the frame being classified as speech H 1 or non speech H 0 and y is the previously defined 
frequency observation vector on the sliding window 4 1 Analysis of jGpdf Voice Activity Detector for N 2 In this section the improvement provided by the proposed methodology is evaluated by studying the most simple case for N 2 In this case assuming that squared correlations 2 1 under H0 H1 and the correlation coefficients are negligible under H0 noise correlation coefficients n 1 0 vanish the LRT can be evaluated according to 1 2 L1 L2 2 1 2 l 2 s 1 1 1 1 2 11 250 O x 10 4 3 2 1 0 1 2 Noisy Speech jGpdf VAD decision N 3 7 5 8 8 5 9 9 5 x 10 4 10 0 10 20 30 40 50 7 5 8 8 5 9 9 5 jGpdf LRT N 3 jGpdf LRT N 2 MO LRT x 10 4 a 100 PAUSE HIT RATE HR0 80 60 Sohn s VAD 40 jGpdf VAD N 2 20 0 0 5 FALSE ALARM RATE FAR0 10 15 20 25 30 b Fig 1 a jGpdf VAD vs MO LRT decision for N 2 and 3 b ROC curve for jGpdf VAD with lh 8 and Sohn s VAD 6 using a similar hang over mechanism s s s where s 1 r1 1 2 is the correlation coefficient of the observations s n 2 n under H1 i i i and i yi i are the SNRs a priori and a 1 2 1 2 
posteriori of the DFT coefficients L 1 2 1 1 2 ln 1 1 2 are the independent LRT of the observations y 1 y 2 connection with the previous MOLRT 9 which are corrected with the term depending on s 1 the new parameter to be modeled and l indexes to the second observation At this point frequency ergodicity of the process must be assumed to estimate the new model parameter s 1 This means that the correlation coefficients are constant in frequency thus an ensemble average can be estimated using the sample mean correlation of the 2 included in the sliding window observations y 1 and y 4 2 Analysis of jGpdf Voice Activity Detector for N 3 In the case for N 3 the properties of a symmetric and tridiagonal matrix come out The likelihood ratio can be expressed as An Efficient VAD Based on a Generalized Gaussian PDF 251 l 3 ln KH1 3 1 T y y KH0 3 2 3 3 12 1 3 1 2 where ln KH ln 1 2 H1 i 1 1 i and 3 is computed 2 ln 1 2 H0 3 1 2 using the following expression under hypotheses H0 H1 T 3 y Cy 1 y Hs 2 1 2 y 2 1 2 2 2 1 y1 2 
1 2 1 2 y y2 y y y y 2 3 1 3 21 1 1 2 22 2 3 21 2 1 3 K 1 2 2 H0 1 2 2 1 3 y3 13 Assuming that squared correlations under H0 H1 and the correlations under H0 vanish the log LRT can be evaluated as the following l 3 3 1 i 1 2 2 2 3 s 2 2 Li 1 2 s 1 1 2 1 3 1 1 1 2 s 2 1 3 s 1 2 14 1 1 1 2 2 1 3 5 Experimental Framework The ROC curves are frequently used to completely describe the VAD error rate The AURORA 3 subset of the original Spanish SpeechDat Car SDC database 10 was used in this analysis The files are categorized into three noisy conditions quiet low noisy and highly noisy conditions which represent different driving conditions with average SNR values between 25dB and 5dB The non speech hit rate HR0 and the false alarm rate FAR0 100 HR1 were determined in each noise condition Using the proposed decision functions equations 14 and 11 we obtain an almost binary decision rule as it is shown in figure 1 a which accurately detects the beginnings of the voice periods In this figure we have used the same level 
of information in both methods m 1 The detection of voice endings is improved using a hang over scheme based on the decision of previous frames Observe how this strategy cannot be applied to the independent LRT 6 because of its hard decision rule and changing bias as it is shown in the same figure We implement a very simple hang over mechanism based on contextual information of the previous frames thus no delay obstacle is added to the algorithm h l N l N l lh N 15 where the parameter lh is selected experimentally The ROC curve analysis for this hang over parameter is shown in figure 2 a for N 3 where the influence of hang over in the zero hit rate is studied with variable detection threshold Finally the benefits of contextual information 9 can be incorporated just averaging the decision rule over a set of multiple observations windows two observations for each window A typical value for m 8 produces increasing levels of detection 252 O 100 PAUSE HIT RATE HR0 80 60 jGpdf LRT VAD N 3 ho 0 m 1 jGpdf LRT VAD N 
3 ho 4 m 1 40 jGpdf LRT VAD N 3 ho 6 m 1 jGpdf LRT VAD N 3 ho 8 m 1 jGpdf LRT VAD N 3 ho 10 m 1 20 0 0 5 10 FALSE ALARM RATE FAR0 15 20 25 30 35 a 100 PAUSE HIT RATE HR0 80 G 729 AMR1 AMR2 AFE Noise Est AFE frame dropping Li Li TU 3 db TL 3dB Marzinzik Sohn Woo jGpdf VAD ho 8 N 2 m 8 jGpdf VAD ho 8 N 2 m 1 jGpdf VAD ho 8 N 3 m 1 60 40 20 0 0 5 10 FALSE ALARM RATE FAR0 15 20 25 30 35 40 b Fig 2 a ROC curve analysis of the jGpdf VAD N 3 for the selection of the hangover parameter lh b ROC curves of the jGpdf VAD using contextual information eight MO windows for N 2 and standards and recently reported VADs accuracy as it is shown in the ROC curve in figure 2 b Of course these results are not the optimum ones since only pair wise dependence is considered here However for a small number of observations the proposed VAD presents the best trade off between detection accuracy and computational delay 6 Conclusion This paper showed a new VAD for improving speech detection robustness in noisy environments The proposed 
method is developed on the basis of previous proposals that incorporate long term speech information to the decision rule 9 However it is not based on the assumption of independence between observations since this hypothesis is not realistic at all It defines a statistically optimum likelihood ratio test based on multiple and correlated observation vectors which avoids the need of smoothing the VAD decision thus reporting significant benefits for speech pause detection in noisy environments The algorithm has an optional inherent delay that for several applications including robust speech An Efficient VAD Based on a Generalized Gaussian PDF 253 recognition does not represent a serious implementation obstacle An analysis based on the ROC curves unveiled a clear reduction of the classification error for second and third order model In this way the proposed VAD outperformed at the same conditions the Sohn s VAD as well as the standardized G 729 AMR and AFE VADs and other recently reported VAD methods in both 
speech nonspeech detection performance 6 1 Computation of the LRT for N 2 KH1 2 1 T y y KH0 2 2 2 H0 H0 H0 2 2 r1 1 1 H H H 1 1 1 2 1 2 r1 2 From equation 4 for N 2 we have that the MCO LRT can be expressed as l 2 ln 16 where ln 1 KH1 2 ln KH0 2 2 N Cy H0 N Cy H1 17 and Cy is defined as in equation 5 If we assume that the voice signal is observed in additive independent noise that is for i 1 2 H1 H0 H1 n s i i i H0 n i i 18 and the a priori SNR s and define the correlation coefficient H 1 i s i n i H1 1 H H 1 1 2 1 we have that ln KH1 2 1 ln KH0 2 2 0 2 1 H 1 H1 2 1 1 2 ln 1 i i 1 19 On the other hand the inverse matrix is expressed in terms of the orthogonal complex polynomials qk z pk z as q0 q2 q1 q2 p p p p 0 0 0 1 p p2 p1 p2 2 1 q 0 20 Cy Hs q2 q1 q2 1 p p 0 1 p1 p2 p1 p2 p1 p1 Hs 2 where p0 1 q0 0 p1 1 r1 and q2 p2 2 r1 1 2 under hypothesis Hs Thus the second term of equation 16 can be expressed as T 2 2 y1 y 2 y 2 00 y2 2 11 2y1 y2 2 01 21 where 2 00 H 1 1 H H H 2 1 1 1 r1 1 2 H 2 0 H0 H0 H 2 1 r1 0 2 
and 2 01 and neglect the squared correlation the a posteriori SNR i functions under both hypotheses we have equation 11 2 n yi i H 2 1 H1 H1 H 2 11 2 1 r1 1 2 H0 H1 r1 r1 H H H H H H r1 0 2 2 0 1 0 r1 0 2 2 0 1 0 H 1 0 H0 H0 H 2 1 r1 0 2 Finally if we define 254 O References 1 Benyassine A Shlomot E Su H Massaloux D Lamblin C Petit J ITUT Recommendation G 729 Annex B A silence compression scheme for use with G 729 optimized for V 70 digital simultaneous voice and data applications IEEE Communications Magazine 35 9 Estimating the Dispersion of the Biometric Glottal Signature in Continuous Speech Pedro Facultad de Abstract The biometric voice signature may be derived from voice as a whole or from the separate vocal tract and glottal source after inverse filtering extraction This last approach has been used by the authors in early work where it has been shown that the biometric signature obtained from the glottal source provides a good description of speaker s characteristics as gender or age In the present 
work more accurate estimations of the singularities in the power spectral density of the glottal source are obtained using an adaptive version of the inverse filtering to carefully follow the spectral changes in continuous speech Therefore the resulting biometric signature gives a better description of intra speaker variability Typical male and female samples chosen from a database of 100 normal speakers are used to determine certain gender specific patterns useful in pathology treatment availing The low intra speaker variability present in the biometric signature makes it suitable for speaker identification applications as well as for pathology detection and other fields of speech characterization Keywords Speaker s biometry glottal signature glottal source estimation 1 Introduction The biometric signature obtained from the glottal source after careful removal of the vocal tract function by inverse filtering gives good descriptions of the speaker s identity and characteristics as gender age or pathology 1 2 
3 Earlier implementations 4 required frame based pitch synchronous processing of the glottal source by phonation cycles for the estimation of the signature parameters This requirement is difficult to be met with sounds of dynamic nature consonants and glides To solve this problem a new methodology is proposed using the accurate estimation of the glottal source by the adaptive removal of the vocal tract transfer function and the robust detection of the glottal spectral singularities In what follows an overview of the adaptive estimation of the glottal source and the vocal tract is briefly summarized followed by a description of the glottal biometric signature estimation from the glottal source power spectral density The normalized singularities detected on the envelope of the spectral distributions maxima and minima are used as biometric descriptors as these are strongly related to vocal fold M Chetouani et al Eds NOLISP 2007 LNAI 4885 pp 256 P biomechanics 5 In section 4 signatures from male and female voice 
show that these singularities present gender specificities Intra speaker variability is explored in section 5 where a study case is shown on the use of intra speaker variability in pathology treatment assessment Conclusions and future lines are given in section 6 2 Glottal Source Adaptive Estimation The key for the accurate estimation of the glottal source is to obtain a good representation of the vocal tract transfer function and vice versa 6 7 Traditionally the profile of the glottal source power spectral density has not been considered of relevance for biometric purposes Nevertheless this profile is strongly influenced by vocal fold biomechanics and can be used in applications such as the speaker s biometrical description 8 or in pathology detection 3 Input voice s n Radiation Compensated Voice sl n Glottal Pulse Inverse Model Hg z Deglottalized voice sv n Inverse Radiation Model H z Vocal Tract Model Fv z a Vocal Tract Inverse Model Hv z Glottal pulse sg n Glottal Pulse Model Fg z k 1 fn f nk Adaptive 
LMS Estimation of ck k 1 gn z 1 k gn k pn k 1 pn b k 1 qn z 1 k qn Fig 1 a Iterative estimation of the vocal tract transfer function Fv z and the glottal pulse residual sg n b Paired adaptive fixed lattice section to implement parallel function estimation and removal The extraction method is based on the iteration of the following loop as shown in Figure 1 a 1 Estimate the inverse glottal source model Fg z from input voice using an order 2 gradient adaptive lattice upper lattice in Figure 1 b Estimating the Dispersion of the Biometric Glottal Signature in Continuous Speech 257 2 3 4 Remove the glottal source model from input voice by a paired fixed lattice lower lattice in Figure 1 b using the parameters obtained in step 1 The resulting trace sv n will be an estimate of the vocal tract impulse response Estimate the vocal tract transfer function Fv z from this last trace using another adaptive lattice typically of order 20 30 Remove the vocal tract transfer function from input voice using a fixed lattice 
using the filter parameters from step 3 The resulting trace sg n will be an estimate of the glottal residual This iteration is repeated till estimates of the vocal tract transfer function are almost free from glottal source information and vice versa The glottal signals from utterances of the vowel a by typical male and female speakers are shown in Figure 2 Fig 2 Typical male left and female right utterances of vowel a and derived glottal traces From top to bottom input voice glottal residual after adaptive inverse removal of the vocal tract glottal source and glottal flow An initialization lap and two more iterations were enough for good vocal tract removal A gradient adaptive lattice was implemented whose details may be found in 9 Both male and female glottal source traces show clear L F 10 patterns as an avail of the extraction method accuracy 3 Estimating the Biometric Signature of Voiced Speech The FFT power spectral density of the glottal source after normalization is used to obtain the positions of 
envelope singularities as follows 258 P The envelope maxima and minima in amplitude and frequency are estimated as ordered pairs with order index k TMk fMk and Tmk fmk The largest of all maxima TMm fMm is used as a normalization reference both in amplitude and in frequency as given by Mk TMk TMm 1 k K mk Tmk TMm Mk mk f Mk f Mm 1 k K f mk f Mm 1 2 Fig 3 Short term Power Spectral Density of Glottal Signals Top Glottal Source from a typical male speaker vowel a showing superimposed singularities Bottom idem for a typical female speaker vowel a Horizontal axes given in Hz The slenderness factor is a parameter derived from each V trough profile formed by each minimum and the two neighbour maxima as mk f Mm 2Tmk TMk 1 TMk 1 k K 2 f Mk 1 f Mk 3 The set of normalized ordered pairs and the derived slenderness parameters constitute the proposed biometric signature The normalized singularity profiles for both reference male and female traces are plotted in Figure 4 Estimating the Dispersion of the Biometric Glottal 
Signature in Continuous Speech 259 4 Materials and Methods To study the properties of the biometric signature a set of 100 normal speakers equally distributed by gender was used Subject ages ranged from 19 to 39 with an average of 26 77 years and a standard deviation of 5 75 years The normal phonation condition of speakers was determined by electroglottographic video endoscopic and GRBAS 11 evaluations The recordings consisted in three utterances of the vowel a produced in different sessions of about 3 sec per record at a sampling rate of 44 100 Hz a 0 2 sec segment derived from the central part for use in the experiments For presentation purposes the traces were re sampled at 11 025 Hz This database was fully parameterized to obtain the singularity biometric signature described in section 3 The most representative male and female speakers in this database were selected for the study their biometric signatures being plotted in Figure 4 Fig 4 Normalized singularity profiles for the male left and female right 
records A first inspection shows that the male speaker s signature exhibits more and deeper V troughs than the female case This is consistent with the biomechanical explanation of the nature of peaks and troughs as these are based on the mechanical resonances and anti resonances of the systems of masses and springs describing vocal fold vibration In general female vocal folds show more stiff links among body and cover masses and this would explain why they show lower amount of less sharp antiresonances see 4 and 12 for a wider explanation 5 Intra speaker Variability The robustness of the estimates depends in large extent on intra speaker variability The processing of the 0 2 sec segments in 512 sample windows sliding in 2 msec steps produce around 76 estimates per segment The positions of the 8 minima and maxima for the typical male and female speakers plus the origin value vs time are given in Figure 5 It may be observed that lower order singularities show more stable values and positions than higher order 
ones This finding is consistent under biomechanical considerations Lower frequency troughs and peaks are due to larger vocal fold masses which for a given articulation and vocal tract load do not change substantially during the phonation frame observed whereas higher order singularities are due to irregular small mass distributions on the cord which may suffer important alterations during phonation and are more sensitive to vocal tract coupling effects In 260 P Fig 5 Statistical distribution of the first 8 singularity points and the origin for a 0 2 sec segment of the male left and female right samples referenced From top to bottom Amplitudes and Singularity Orders Fig 6 Glottal Source Power Spectral Signature for a pathological case Top pre treatment Bottom post treatment Horizontal axes given in Hz general it may be said that singularities in the male case are deeper and mainly appear at lower frequencies than in the female case Low frequency singularities are less spread over than high frequency ones 
which show stronger skewness this fact being more evident in the female case Estimating the Dispersion of the Biometric Glottal Signature in Continuous Speech 261 To explore the applicability of the glottal signature proposed a study has been conducted on a specific pathologic case as the one shown in Figure 6 corresponding to the pre and post treatment glottal source signatures from a non smoking 34 year old female after suffering a four year lasting vocal production limitation The patient reported chronic disphonia vocal fatigue changes in loudness and soaring during speaking or singing After medical examination a gelatine type small polyp affecting the free lip of the medial third of the left vocal fold substrate attached and mildly edematous was diagnosed This resulted in incomplete glottal closure during phonation and in a reduction and asymmetry on the mucosal wave appearing on the vocal cord affected The pre treatment signature shows that the harmonic structure between 1 6 kHz and 3 3 kHz was 
completely altered or distorted and that natural phonation was tenser and at a higher pitch Fig 7 Normalized singularity profiles for the pre treatment left and post treatment right records The post treatment signature 3 months after the surgical removal of the polyp shows a restoration of the distorted harmonic band and the production of a glottal signature closer to normal female voicing conditions as confirmed by the plots in Figure 7 Comparing these plots against the one in Figure 4 right it may be seen that the glottal signature between normalized frequencies of 5 and 20 in the pre treatment case shows an average decay slope of 12 9 dB oct vs an equivalent slope of 8 0 dB oct in the post treatment case this last one being more in correspondence with the average 5 4 dB oct for the normal female case This behavior avails the usefulness of the glottal signature as a biometrical description of pathologic voice 6 Conclusions The work presented is a generalization of prior studies using non adaptive 
estimations of the vocal tract on short segments of vowels where it was shown that estimates from the glottal source could be used in the determination of the biomechanical parameters of the vocal fold The use of adaptive estimations allow a higher accuracy in the estimates of the vocal tract and consequently on the glottal signals and the underlying biomechanics The extension of the glottal spectra singularities to time varying conditions allow a better description of the non stationary processes appearing in vocal fold vibration even in the production of sustained sounds As an example a study case from pre and post treatment of a specific mild pathological case has been 262 P exposed Both the harmonic structure and the biometric signature of the glottal source confirmed the success of the treatment and the recovery of normal phonation conditions This has to be attributed to the tracking accuracy of the adaptive methods used This methodology will help in conducting more careful studies about interspeaker 
and intra speaker variability to extend the use of the glottal source spectral signature to speaker identification and characterization applications as well as in voice quality measurements pathology detection and treatment assessment Acknowledgments This work is funded by grants TIC2003 08756 TEC2006 12887C02 00 from Plan Nacional de I D i Ministry of Education and Science CCG06UPM TIC 0028 from the Plan Regional de References 1 Trajectory Mixture Density Networks with Multiple Mixtures for Acoustic Articulatory Inversion Korin Richmond Centre for Speech Technology Research Edinburgh University Edinburgh United Kingdom korin cstr ed ac uk Abstract We have previously proposed a trajectory model which is based on a mixture density network MDN trained with target variables augmented with dynamic features together with an algorithm for estimating maximum likelihood trajectories which respects the constraints between those features In this paper we have extended that model to allow diagonal covariance matrices 
and multiple mixture components in the trajectory MDN output probability density functions We have evaluated this extended model on an inversion mapping task and found the trajectory model works well outperforming smoothing of equivalent trajectories using low pass filtering Increasing the number of mixture components in the TMDN improves results further 1 Introduction Mainstream speech technology such as automatic speech recognition and concatenative speech synthesis is strongly focused on the acoustic speech signal This is natural considering the acoustic domain is where the speech signal exists in transmission between humans and we can conveniently measure and manipulate an acoustic representation of speech However an articulatory representation of speech has certain properties which are attractive and which may be exploited in modelling Speech articulators move relatively slowly and smoothly and their movements are continuous the mouth cannot jump from one position to the next Using knowledge of the 
speech production system could improve speech processing methods by providing useful constraints Accordingly there is growing interest in exploiting articulatory information and representations in speech processing with many suggested applications for example low bit rate speech coding 1 speech analysis and synthesis 2 automatic speech recognition 3 4 animating talking heads and so on For an articulatory approach to be practical we need convenient access to an articulatory representation Recent work on incorporating articulation into speech technology has used data provided by X ray microbeam cinematography and electromagnetic articulography EMA These methods particularly the latter mean we are now able to gather reasonably large quantities of articulatory data However they are still invasive techniques and require bulky M Chetouani et al Eds NOLISP 2007 LNAI 4885 pp 264 K Richmond and expensive experimental setups Therefore there is interest in developing a way to recover an articulatory representation from 
the acoustic speech signal In other words for a given acoustic speech signal we aim to estimate the underlying sequence of articulatory configurations which produced it This is termed acoustic articulatory inversion or the inversion mapping The inversion mapping problem has been the subject of research for several decades One approach has been to attempt analysis of acoustic signals based on mathematical models of speech production 5 Another popular approach has been to use articulatory synthesis models either as part of an analysisby synthesis algorithm 6 or to generate acoustic articulatory corpora which may be used with a code book mapping 7 or to train other models 8 Much of the more recent work reported has applied machine learning models to human measured articulatory data including artificial neural networks ANNs 9 codebook methods 10 and GMMs 11 The inversion mapping is widely regarded as difficult because it may be an illposed problem multiple evidence exists to suggest the articulatory to acoustic 
mapping is many to one which means that instantaneous inversion of this mapping results in a one to many mapping If this is the case an inversion mapping method must take account of the alternative articulatory configurations possible in response to an acoustic vector In previous work 12 9 we have successfully employed the mixture density network MDN 13 to address this problem The MDN provides a probability density function pdf of arbitrary complexity over the target articulatory domain which is conditioned on the acoustic input In 14 we began to extend this work to provide a statistical trajectory model termed the Trajectory MDN along similar lines as the HMM based speech production model of 15 and the GMM based inversion mapping of 11 This was achieved by augmenting the static articulatory target data with dynamic delta and deltadelta features and incorporating the maximum likelihood parameter generation MLPG algorithm 16 This allows to calculate the maximum likelihood estimate of articulatory trajectories 
which respect the constraints between the static and derived dynamic features This paper seeks to further the work in 14 with three specific aims 1 to evaluate an extension to the TMDNs in 14 which were limited to using spherical covariance matrices that allows mixture models with diagonal covariance matrices 2 to evaluate the new implementation of TMDN on the full set of articulator channels and in comparison with a low pass filtering approach previously reported 3 to evaluate TMDNs with multiple mixture components 2 The Trajectory Mixture Density Network Model We give here a very brief introduction to the MDN and describe how it may be extended with the MLPG algorithm to give a trajectory model For full details of the MDN and MLPG the reader is referred to 13 and 16 respectively To avoid introducing unnecessary confusion we have attempted to retain the original notation as far as possible Trajectory Mixture Density Networks with Multiple Mixtures conditional probability density 265 p t x 2 2 2 mixture 
model neural network input vector x Fig 1 The mixture density network combines a mixture model and a neural network 2 1 Mixture Density Networks The MDN combines a mixture model with an ANN Here we will consider a multilayer perceptron and Gaussian mixture components The ANN maps from the input vector x to the control parameters of the mixture model priors means and variances 2 which in turn gives a pdf over the target domain conditioned on the input vector p t x The toy example MDN in Figure 1 takes an input vector x dimensionality 5 and gives the conditional probability density of a vector t dimensionality 1 in the target domain This pdf takes the form of a GMM with 3 components so it is given as M p t x j 1 j x j t x 1 where M is the number of mixture components in this example 3 j t x is the probability density given by the j th kernel and j x is the prior for the j th kernel In order to constrain the GMM priors to within the range 0 j x 1 and to sum to unity the softmax function is used j exp zj M l 1 
exp zl 2 where zj is the output of the ANN corresponding to the prior for the j th mixture component The variances are similarly related to the outputs of the ANN as j exp zj 3 is the output of the ANN corresponding to the variance for the j th where zj mixture component This avoids the variance becoming 0 Finally the means are represented directly 266 K Richmond jk zjk zjk 4 is the value of the output unit corresponding to the k th dimension of where the mean vector for the j th mixture component Training the MDN aims to minimise the negative log likelihood of the observed target data points M ln j xn j tn xn 5 E n j 1 given the mixture model parameters Since the ANN part of the MDN provides the parameters for the mixture model this error function must be minimised with respect to the network weights The derivatives of the error at the network output units corresponding separately to the priors means and variances of the mixture model are calculated see 13 and then propagated back through the network to 
find the derivatives of the error with respect to the network weights Thus standard non linear optimisation algorithms can be applied to MDN training 2 2 Maximum Likelihood Parameter Generation The first step to an MDN based trajectory model is to train an MDN with target feature vectors augmented with dynamic features i e deltas and deltadeltas derived from linear combinations of a window of static features For the sake of simplicity we will first consider MDNs with a single Gaussian distribution and a single target static feature ct at each time step Given the output of this MDN in response to a sequence of input vectors in order to generate the maximum likelihood trajectory we aim to maximize P O Q with respect to O where T T T O oT 1 o2 oT ot ct ct ct and Q is the sequence of Gaussians output by our MDN The relationship between the static features and those augmented with derived dynamic features can be arranged in matrix form O WC 6 where C is a sequence of static features and W is a transformation 
matrix composed of the coefficients of the delta and deltadelta calculation window and 0 Under the condition expressed in Eq 6 maximising P O Q is equivalent to maximising P WC Q with respect to C By setting log P WC Q 0 C a set of linear equations is obtained see 16 for the details WT U 1 WC WT U 1 MT T 1 1 1 1 diag U q1 Uq2 UqT 7 8 where M q1 q2 qT and U qT and 1 are the Trajectory Mixture Density Networks with Multiple Mixtures 267 UL LL LI TB TT TD V label UL LL LI V articulator Upper lip Lower lip Lower incisor Velum label TT TB TD articulator Tongue tip Tongue body Tongue Dorsum Fig 2 Placement of EMA receiver coils in the MOCHA database for speaker fsew0 Coil placement abbreviations may be suffixed with x and y to designate the xand y coordinate for a given coil in the midsagittal plane respectively To extend the MLPG algorithm to the case of a sequence of pdfs with multiple mixture components we make use of an iterative EM method described in 16 Essentially for each time frame at each iteration 
instead of using the means and covariances of a single Gaussian in 8 we use a weighted sum of these parameters of the multiple mixture components weighed by their occupancy probabilities i e posterior probability of each component given the augmented observation sequence O Hence we first choose an initial static feature trajectory C and use this to calculate the occupancy probabilities using the forward backward algorithm We can then solve 8 using the weighted means and covariances to obtain an updated feature trajectory C which is then used to calculate updated occupancy probabilities This two stage iteration is repeated until convergence 3 3 1 Inversion Mapping Experiment MOCHA Articulatory Data The multichannel articulatory MOCHA dataset 17 used for the experiments in this paper gives the acoustic waveform recorded at the same time as electromagnetic articulograph 2D EMA data The sensors shown in Figure 2 provide x and y coordinates in the midsagittal plane at 500Hz sample rate Speakers were recorded 
reading a set of 460 short phonetically balanced British TIMIT sentences Female speaker fsew0 was used for the experiments here This is the same data set as used previously 9 14 and so enables comparison with those and similar results reported in the literature e g 11 Data Processing The acoustic data was converted to frames of 20 melscale filterbank coefficients using a Hamming window of 20ms with a shift of 10ms These were z score normalised and scaled to the range 0 0 1 0 The EMA trajectories were downsampled to match the 10ms shift rate then z score normalised and scaled to the range 0 1 0 9 using the normalisation method described in 12 Frames of silence at the beginning and end of the files were discarded using the labelling provided with MOCHA 268 K Richmond 368 utterances were used for the training set and the validation and test sets contained 46 utterances each the same subsets as 9 14 A context window of 20 consecutive acoustic frames was used as input to the TMDN which increased the order of the 
acoustic vector paired with each articulatory vector to 400 3 2 Method We trained TMDNs with 1 2 and 4 mixture components for each of the 14 EMA channels making a total of 42 models trained In 14 we trained separate MDNs for the static delta and deltadelta features for each articulatory channel because the implementation limited output pdfs to spherical covariance Here in contrast our implementation has been extended and now allows diagonal covariance matrices and so the three feature streams for each articulator channel were trained in a single network All networks contained a hidden layer of 80 units The scaled conjugate gradients non linear optimisation algorithm was run for a maximum of 4000 epochs and the separate validation set was used to identify the point at which an optimum appeared to have been reached The validation error was calculated in terms of RMS error between the target trajectories and those resulting from the Trajectory MDN This differs from using simply the likelihood of the target data 
given the pdfs output by the MDN To generate output trajectories from the TMDN we simply ran the input data for an utterance through the TMDNs for each articulatory channel and then ran the MLPG algorithm on the resulting sequences of pdfs over the static and dynamic feature spaces To evaluate the Trajectory MDN we compared the resulting trajectories with those of the output units corresponding to the mean of the static feature alone This output is in theory approximately equivalent to that of an MLP with linear output activation function trained with a standard least squares error function1 In this way we can directly observe the effect of using the augmented features without considering the effects of two systems having been trained differently Finally we also low pass filtered the static mean trajectories as a smoothing step which has been shown in the past to improve inversion results 12 11 and compared those smoothed trajectories with the TMDN output 4 Results Table 1 lists the results of 14 TMDNs 
trained on each articulatory channel separately using an output pdf containing a single Gaussian Two error metrics have been used correlation between the target and output trajectories and root mean square error RMSE expressed in millimetres The table also lists the results previously reported in 9 which used an MLP with exactly the same dataset for comparison It can be seen that the improvement is substantial By way of further comparison with other studies 11 reported an average RMS error of 1 45mm for MOCHA speaker fsew0 1 Although the MLP component of the TMDN here has been trained with augmented target features which from comparison with previous results e g 9 seems beneficial Trajectory Mixture Density Networks with Multiple Mixtures 269 Table 1 Comparison of results for Trajectory MDNs TMDN with a single Gaussian with the MLP described in 9 Exactly the same training validation and testing datasets have been used Average RMSE mm in 9 was 1 62mm compared with 1 4mm here Channel ul x ul y ll x ll y li x 
li y tt x tt y tb x tb y td x td y vx vy Correlation MLP TMDN 0 58 0 68 0 72 0 79 0 60 0 69 0 75 0 83 0 56 0 63 0 80 0 85 0 79 0 85 0 84 0 90 0 81 0 85 0 83 0 89 0 79 0 84 0 71 0 82 0 79 0 86 0 77 0 83 RMSE mm RMSE mm MLP TMDN reduction 0 99 0 90 9 5 1 16 1 05 9 9 1 21 1 10 9 2 2 73 2 27 16 8 0 89 0 82 8 1 1 19 1 03 13 3 2 43 2 12 12 9 2 56 2 08 18 7 2 19 1 96 10 4 2 14 1 76 17 6 2 04 1 85 9 5 2 31 1 89 18 2 0 42 0 35 15 6 0 41 0 37 10 2 Table 2 Channel specific cutoff frequencies used for low pass filtering ul ll li tt tb td v x 3 Hz 3 Hz 3 Hz 6 Hz 6 Hz 7 Hz 5 Hz y 5 Hz 8 Hz 7 Hz 9 Hz 7 Hz 6 Hz 5 Hz Table 3 Comparison of correlation and RMS error in millimetres for Trajectory MDN model TMDN with the static mean MDN output only static only and low pass filtered static mean static lpfilt The TMDN here has a single Gaussian Correlation RMSE mm RMSE mm Channel static only static lpfilt TMDN static only static lpfilt TMDN reduction ul x 0 63 0 67 0 68 0 93 0 90 0 90 0 6 ul y 0 74 0 77 0 79 1 13 1 06 1 05 1 5 ll 
x 0 64 0 69 0 69 1 17 1 11 1 10 1 0 ll y 0 81 0 83 0 83 2 40 2 31 2 27 1 6 li x 0 57 0 62 0 63 0 88 0 84 0 82 2 4 li y 0 83 0 84 0 85 1 07 1 05 1 03 1 5 tt x 0 82 0 84 0 85 2 26 2 14 2 12 1 0 tt y 0 88 0 89 0 90 2 19 2 12 2 08 1 8 tb x 0 83 0 85 0 85 2 05 1 99 1 96 1 2 tb y 0 87 0 89 0 89 1 88 1 80 1 76 1 8 td x 0 81 0 83 0 84 1 95 1 88 1 85 2 1 td y 0 78 0 81 0 82 2 04 1 92 1 89 1 7 vx 0 84 0 85 0 86 0 37 0 36 0 35 1 2 vy 0 80 0 82 0 83 0 39 0 37 0 37 1 6 270 K Richmond In order to investigate the effect of using dynamic features and the MLPG algorithm within the Trajectory MDN we have compared these results for TMDNs with a single Gaussian with those obtained using low pass filtering as described in 12 11 Table 3 compares three conditions TMDN static only and static lpfilt For the static only condition we have used the TMDN s output corresponding to the mean for the static target feature as the output trajectory For the static lpfilt condition we have further low pass filtered the static mean above using 
the cutoff frequencies listed in Table 2 These channel specific cutoff frequencies were determined empirically in 12 and are very similar to those given in 11 As expected it can be seen that low pass filtering improves results for all channels However using the dynamic features and the MLPG algorithm in the Trajectory MDN results in the best performance with improvements varying between 0 6 and 2 4 over low pass filtering The improvements over using low pass filtering shown in Table 3 although consistent are not huge However in contrast to low pass filtering the TMDN is able to make use of multiple mixture components which can potentially increase performance further Table 4 performs this comparison by the addition of results for TMDNs with 2 and 4 mixture components We see that in the majority of cases increasing the number of mixture components improves results e g by up to 8 3 in the case of the tt y channel using 4 mixture components Table 4 Comparison of RMS error in millimetres between using the low 
pass filtered static feature mean static lpfilt and Trajectory MDNs with 1 2 or 4 mixture components Average min RMSE 1 37mm static TMDN opt best Channel lpfilt 1 mix 2 mix 4 mix mixes reduction upper lip x 0 90 0 90 0 90 0 91 1 0 6 upper lip y 1 06 1 05 1 03 1 06 2 3 3 lower lip x 1 11 1 10 1 10 1 12 1 1 0 lower lip y 2 31 2 27 2 20 2 22 2 4 7 lower incisor x 0 84 0 82 0 80 0 81 2 4 2 lower incisor y 1 05 1 03 1 04 1 03 1 1 5 tongue tip x 2 14 2 12 2 09 2 10 2 2 1 tongue tip y 2 12 2 08 1 98 1 94 4 8 3 tongue body x 1 99 1 96 1 97 1 98 1 1 2 tongue body y 1 80 1 76 1 73 1 78 2 3 5 tongue dorsum x 1 88 1 85 1 81 1 83 2 4 1 tongue dorsum y 1 92 1 89 1 85 1 88 2 3 6 velum x 0 36 0 35 0 35 0 35 2 3 3 velum y 0 37 0 37 0 36 0 37 2 2 8 Finally Figure 3 gives a qualitative demonstration of the nature of this improvement In these plots we compare the tt y trajectory estimated by the TMDN with 4 mixture components bottom plot with the low pass filtered static mean top plot It can be seen in several places that the 
TMDN with 4 mixtures is substantially closer to the real target trajectory Trajectory Mixture Density Networks with Multiple Mixtures fsew0_036 1 271 0 8 real static lpfilt tt_y norm 0 6 0 4 0 2 0 0 50 100 150 Frame number fsew0_036 200 250 300 1 real TMDN 0 8 tt_y norm 0 6 0 4 0 2 0 0 50 100 150 Frame number 200 250 300 Fig 3 Only the most accomplished artists obtain popularity fsew0 036 Comparison of TMDN output trajectory with low pass filtered static mean output trajectory 5 Conclusion The results of this paper show we have successfully extended the Trajectory MDN first described in 14 to allow diagonal covariance matrices For all 14 articulator channels tested the TMDN with a single Gaussian output pdf performed better than low pass filter smoothing Increasing the number of mixture components improved results further This is a unique advantage of the TMDN model over smoothing single trajectories using for example low pass filters Acknowledgments Many thanks to Junichi Yamagishi for useful discussion 
pertaining to implementation This work was supported by EPSRC grant EP E027741 1 References 1 Schroeter J Sondhi M M Speech coding based on physiological models of speech production In Furui S Sondhi M M eds Advances in Speech Signal Processing pp 272 K Richmond 2 Toda T Black A Tokuda K Mapping from articulatory movements to vocal tract spectrum with Gaussian mixture model for articulatory speech synthesis In Proc 5th ISCA Workshop on Speech Synthesis 2004 3 King S Frankel J Livescu K McDermott E Richmond K Wester M Speech production knowledge in automatic speech recognition Journal of the Acoustical Society of America 121 2 Application of Feature Subset Selection Based on Evolutionary Algorithms for Automatic Emotion Recognition in Speech Computer Science Faculty University of the Basque Country Manuel Lardizabal 1 E 20018 Donostia Gipuzkoa Spain aitor alvarez ehu es Abstract The study of emotions in human computer interaction is a growing research area Focusing on automatic emotion recognition work is 
being performed in order to achieve good results particularly in speech and facial gesture recognition In this paper we present a study performed to analyze different machine learning techniques validity in automatic speech emotion recognition area Using a bilingual affective database different speech parameters have been calculated for each audio recording Then several machine learning techniques have been applied to evaluate their usefulness in speech emotion recognition including techniques based on evolutive algorithms EDA to select speech feature subsets that optimize automatic emotion recognition success rate Achieved experimental results show a representative increase in the success rate Keywords Affective computing Machine Learning speech features extraction emotion recognition in speech 1 Introduction Human beings are eminently emotional as their social interaction is based on the ability to communicate their emotions and perceive the emotional states of others 1 Affective computing a discipline 
that develops devices for detecting and responding to users emotions 2 is a growing research area 3 The main objective of affective computation is to capture and process affective information with the aim of enhancing the communication between the human and the computer Within the scope of affective computing the development of affective applications is a challenge that involves analyzing different multimodal data sources In order to develop such applications a large amount of data is needed in order to include a wide range of emotionally significant material Affective databases are a good chance for developing affective recognizers or affective synthesizers In this paper different speech paralinguistic parameters have been calculated for the analysis of the human emotional voice using several audio recordings These recordings are stored in a bilingual and multimodal affective database Several works have already been done in which the use of Machine Learning paradigms takes a principal role M Chetouani et al 
Eds NOLISP 2007 LNAI 4885 pp 274 2 Related Work As previously mentioned affective databases provide a good opportunity for training affective applications This type of databases usually record information such as images sounds psychophysiological values etc There are some references in the literature that present affective databases and their characteristics 4 5 6 Many studies have been focused on the different features used in human emotional speech analysis 7 8 The number of voice features analysed varies among the studies but basically most of these are based in fundamental frequency energy and timing parameters such as speech rate or mean phone duration Works where the use of Machine Learning paradigms take a principal role can also be found in the literature 9 10 The work by 4 is related with this paper in the sense of using a Feature Selection method in order to apply a Neural Network to emotion recognition in speech although both the methods to perform the FSS and the paradigms used are different In 
this line it has to be pointed out the work by 11 which uses a reduced number of emotions and a greedy approach to select the features 3 3 1 Study of Automatic Emotion Recognition Relevant Parameters Using Machine Learning Paradigms RekEmozio Database The RekEmozio bilingual database was created with the aim of serving as an information repository for performing research on user emotion adding descriptive information about the performed recordings so that processes such as extracting speech parameters and video features could be carried out on them Members of different work groups involved in research projects related to RekEmozio have performed several processes for extracting speech and video features this information was subsequently added to the database The emotions used were chosen based on 12 and the neutral emotion was added The characteristics of the RekEmozio database are described in 13 The languages that are considered in RekEmozio database are Spanish and Basque 3 2 Emotional Feature Extraction 
For emotion recognition in speech one of the most important questions is which features should be extracted from the voice signal Previous studies show that it is difficult to find specific voice features that could be used as reliable indicators of the emotion present in the speech 14 In this work RekEmozio database audio recordings stereo wave files sampled at 44100 Hz have been processed using standard signal processing techniques windowing Fast Fourier Transform auto correlation to extract a wide group of 32 features which are described below Supposing that each recording in the database corresponds to one single emotion only one global vector of features has been obtained for each recording Application of Feature Subset Selection Based on Evolutionary Algorithms 275 by using some statistical operations Parameters used are calculated over entire recordings Selected features are detailed next in italics Fundamental Frequency F0 The most common feature analyzed in several studies 7 8 For F0 estimation we 
used Sun algorithm 15 and statistics are computed Maximum Minimum Mean Range Variance Standard deviation and Maximum positive slope in F0 contour RMS Energy The mean energy of speech quantified by calculating root mean square RMS value and 6 statistics Maximum Minimum Mean Range Variance and Standard Deviation Loudness Absolute loudness based on Zwicker s model 16 Spectral distribution of energy Each emotion requires a different effort in the speech and it is known that the spectral distribution of energy varies with speech effort 7 We have computed energy in Low band between 0 and 1300 Hz Medium band between 1300 and 2600 Hz and High band from 2600 to 4000 Hz 17 Mean Formants and Bandwidth Energy from the sound source vocal folds is modified by the resonance characteristics of the vocal tract formants Acoustic variations dues to emotion are reflected in formants 18 The first three mean Formants and their corresponding mean Bandwidths Jitter Perturbation in vibration of vocal chords It is estimated based on 
the model presented by 19 Shimmer Perturbation cycle to cycle of the energy Its estimation is based on the previously calculated absolute loudness Speaking Rate Rhythm is known to be an important aspect in recognition of emotion in speech Progress has been made on a simple aspect of rhythm the alternation between speech and silence 7 The speaking rate estimation has been divided in 6 values based on their duration with respect to the whole elocution Duration of voice part Silence part Maximum voice part Minimum voice part Maximum silence part and Minimum silence part 3 3 Machine Learning Standard Paradigms Used In the supervised learning task a classification problem has been defined where the main goal is to construct a model or a classifier able to manage the classification itself with acceptable accuracy With this aim some variables are to be used in order to identify different elements the so called predictor variables For the current problem each sample is composed by the set of 32 speech related values 
while the label value is one of the seven emotions identified The single paradigms used in our experiments that come from the family of Machine Learning ML are briefly introduced Decision trees A decision tree consists of nodes and branches to partition a set of samples into a set of covering decision rules In each node a single test or decision is made to obtain a partition The starting node is usually referred as the root node In each node the goal is to select an attribute that 276 makes the best partition between the classes of the samples in the training set 20 21 In our experiments two well known decision tree induction algorithms are used ID3 22 and C4 5 23 Instance Based Learning Instance Based Learning IBL has its root in the study of nearest neighbor algorithm 24 in the field of machine learning The simplest form of nearest neighbor NN or k nearest neighbor k NN algorithms simply stores the training instances and classifies a new instance by predicting the same class its nearest stored instance has 
or the majority class of its k nearest stored instances have respectively according to some distance measure as described in 25 The core of this non parametric paradigm is the form of the similarity function that computes the distances from the new instance to the training instances to find the nearest or k nearest training instances to the new case In our experiments the IB paradigm is used an inducer developed in the MLC project 26 and based on the works of 27 and 28 Naive Bayes classifiers The Naive Bayes NB rule 29 uses the Bayes theorem to predict the class for each case assuming that the predictive genes are independent given the category To classify a new sample characterized by d genes X X1 X2 Xd the NB classifier applies the following rule d cN B arg max p cj i 1 p xi cj 1 where cN B denotes the class label predicted by the Naive Bayes classifier and the possible classes of the problem are grouped in C c1 cl A normal distribution is assumed to estimate the class conditional densities for predictive 
genes Despite its simplicity the NB rule obtains better results than more complex algorithms in many domains Naive Bayesian Tree learner The naive Bayesian tree learner NBTree 30 combines naive Bayesian classification and decision tree learning It uses a tree structure to split the instance space into sub spaces defined by the paths of the tree and generates one naive Bayesian classifier in each sub space Feature Subset Selection by Estimation of Distribution Algorithms The basic problem of ML is concerned with the induction of a model that classifies a given object into one of several known classes In order to induce the classification model each object is described by a pattern of d features Here the ML community has formulated the following question are all of these d descriptive features useful for learning the classification rule On trying to respond to this question we come up with the Feature Subset Selection FSS 31 approach which can be reformulated as follows given a set of candidate features select 
the best subset in a classification problem In our case the best subset will be the one with the best predictive accuracy Most of the supervised learning algorithms perform rather poorly when faced with many irrelevant or redundant depending on the specific characteristics of the classifier features In this way the FSS proposes additional methods to reduce the number of features so as to improve the performance of the supervised classification algorithm FSS Application of Feature Subset Selection Based on Evolutionary Algorithms 277 can be viewed as a search problem 32 with each state in the search space specifying a subset of the possible features of the task Exhaustive evaluation of possible feature subsets is usually unfeasible in practice due to the large amount of computational effort required In this way any feature selection method must determine the nature of the search process In the experiments performed an Estimation of Distribution Algorithm EDA has been used which has the model accuracy as 
fitness function To assess the goodness of each proposed gene subset for a specific classifier a wrapper approach is applied In the same way as supervised classifiers when no gene selection is applied this wrapper approach estimates by using the 10 fold crossvalidation 33 procedure the goodness of the classifier using only the variable subset found by the search algorithm Other Feature Subset Selection Approaches Several approaches have been developed to search a good attribute selection On Filter Based approach the attribute selection takes a statistical measure of each variable and sorts them according to the obtained value Among them Principal Component analysis takes a measure of the correlation and covariance among the predictor variables and the class Transformation based Feature Selection is a second approach which transforms the representation space to obtain a reduced one For instance Singular Value Decomposition transforms the classification problem in another one after projecting the variables 
through a Matrix while the number of singular selected values determines the dimension size of the vectors Another Feature Subset Selection technique the one used in this paper is the so called Wrapper approach in which a subset of variables is selected based on the accuracy of the classifier itself Two simple ways to perform this selection are the following Forward starts with an empty set of variables and adds to the set at each step the variable which most increases the obtained accuracy until no increase is obtained Backward starts with all the variables and deselects variables while no decrease is produced These approaches are outperformed by a more powerfull method based on Evolutionary Algorithms such as EDA As it can be seen the rest of the approaches are local guided while the used one is a more sophisticated search engine which is supposed to outperform the rest of the approaches In fact this happens with any greedy search when compared to an evolutionary one when applied to the same search problem 
4 Experimental Results The above mentioned methods have been applied over the crossvalidated data sets using the MLC library 26 Each dataset corresponds to a single actor Experiments were carried out with and without FSS in order to extract the accuracy improvement introduced by the feature selection process Tables 1 and 2 278 show the classification results obtained using the whole set of variables for Basque and Spanish languages respectively Each column represents a female Fi of male Mi actor and mean values corresponding to each classifier gender are also included Last column presents the total average for each classifier Results don t seem very impressive ID3 best classifies the emotions for female actresses for both Basque and Spanish languages while C4 5 outstands for Basque male actors and IB for Spanish male actors Results obtained after applying FSS are more appealing as can be seen in Tables 3 and 4 There classifier IB appears as the best paradigm for all the categories female and male and Basque 
and Spanish languages Moreover the accuracies outperform the previous ones in more than 15 It must also be highlighted that FSS improves the well classified rate for all the ML paradigms as it can be seen in Figure 1 Table 1 10 fold crossvalidation accuracy for Basque language using the whole variable set F1 35 4 38 7 41 5 42 9 42 3 Female F2 F3 mean 48 8 35 2 39 8 45 5 44 7 43 0 52 2 35 0 42 9 45 8 37 7 42 1 39 8 35 2 39 1 M1 44 2 46 7 60 4 52 2 53 1 M2 49 3 46 9 53 3 44 1 46 2 Male M3 M4 36 9 40 9 43 3 51 1 45 1 49 5 36 2 41 4 45 2 43 3 Total mean 42 8 47 0 52 0 43 5 46 9 41 5 45 3 48 1 42 9 43 6 IB ID3 C4 5 NB NBT Table 2 10 fold crossvalidation accuracy for Spanish language using the whole variable set Female F3 F4 F5 54 6 54 6 38 2 49 1 47 3 42 7 46 4 43 6 42 7 49 1 40 0 42 7 49 1 50 0 39 1 Male M3 M4 51 8 47 7 40 9 47 3 46 4 42 7 49 1 45 5 40 9 48 2 Total M5 33 6 40 0 35 5 34 6 42 7 mean 38 4 36 0 37 1 36 9 35 5 41 8 40 8 39 9 39 7 40 2 IB ID3 C4 5 NB NBT F1 34 6 36 4 30 9 38 2 42 7 F2 43 6 52 7 50 0 
42 7 43 6 mean 45 1 45 6 42 7 42 5 44 9 M1 25 5 20 9 29 1 24 6 18 2 M2 33 6 30 9 31 8 30 9 27 3 Table 3 10 fold crossvalidation accuracy for Basque language using FSS F1 63 0 62 7 60 2 64 5 58 6 Female F2 F3 mean 68 0 59 3 63 5 60 5 65 5 62 9 66 0 60 0 62 1 64 6 48 9 59 3 61 1 54 8 58 1 M1 72 7 72 7 71 8 74 6 74 4 M2 67 4 62 0 62 8 62 5 59 9 Male M3 M4 61 0 62 8 56 5 62 7 60 1 63 6 62 7 60 0 62 7 59 4 Total mean 65 9 63 4 64 6 64 9 64 1 64 9 63 2 63 5 62 5 61 6 IB ID3 C4 5 NB NBT Application of Feature Subset Selection Based on Evolutionary Algorithms Table 4 10 fold crossvalidation accuracy for Spanish language using FSS Female F3 F4 F5 75 5 71 8 68 2 66 4 60 0 61 8 64 6 65 5 63 6 68 2 65 5 60 0 63 6 58 2 60 0 Male M3 M4 69 1 63 6 66 4 61 8 65 5 64 6 64 6 59 1 60 0 63 6 Total M5 60 9 60 0 56 4 51 8 59 1 mean 58 7 56 5 57 3 52 9 53 6 63 7 59 6 60 0 57 2 57 0 279 IB ID3 C4 5 NB NBT F1 61 8 59 1 57 3 54 6 53 6 F2 66 4 66 4 62 7 59 1 66 4 mean 68 7 62 7 62 7 61 5 60 4 M1 42 7 42 7 43 6 40 9 38 2 M2 57 3 51 8 56 
4 48 2 47 3 Fig 1 Improvement in Basque and Spanish languages using FSS in all classifiers 5 Conclusions and Future Work RekEmozio database has been used to training some automatic recognition systems In this paper we have shown that applying FSS enhances classification rates for the ML paradigms that we have used IB ID3 C4 5 NB and NBTree An analysis of the selected features by FSS is required Moreover the speech data should be combined with visual information This combination could be performed by means of a multiclassifier model 34 References 1 Casacuberta D La mente humana Diez Enigmas y 100 preguntas 280 6 Application of Feature Subset Selection Based on Evolutionary Algorithms 281 25 Ting K M Common issues in Instance Based and Naive Bayesian classifiers PhD thesis Baser Department of Computer Science The University of Sidney Australia 1995 26 Kohavi R Sommerfield D Dougherty J Data mining using MLC A machine learning library in C In Tools with Artificial Intelligence pp Author Index Henrich Nathalie 
Hueber T 28 Iriondo Ignasi 1 78 86 Jelfs Beth 57 Joublin Frank 142 Kaufmann Tobias Kirkpatrick Barry 124 132 Landais R 28 Lazkano Elena 273 179 Paraskevas Ioannis 204 284 Author Index Wersing Heiko 142 Stoll Lara 114 Sturmel Nicolas Turias Ignacio Vayanos Phebe Vu Ngoc Tuan 1 246 57 1 Zamora 179 
41	a	Large scale Topic Detection And Language Model Adaptation Kristie Seymore Ronald Rosenfeld June 1997 CMU CS 97 152 School of Computer Science Carnegie Mellon University Pittsburgh PA 15213 This research was sponsored by the Department of the Navy Naval Research Laboratory under Grant No N00014 93 1 2005 the National Security Agency under Grant numbers MDA904 961 0113 and MDA904 97 1 0006 and under a National Science Foundation Graduate Research Fellowship The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies either expressed or implied of the U S Government or the National Science Foundation Keywords speech recognition statistical language modeling topic detection topic adaptation document clustering Abstract The subject matter of any conversation or document can typically be described as some combination of elemental topics We have developed a language model adaptation scheme that takes a piece of text chooses the 
most similar topic clusters from a set of over 5000 elemental topics and uses topic specific language models built from the topic clusters to rescore N best lists We are able to achieve a 15 reduction in perplexity and a small improvement in word error rate by using this adaptation We also investigate the use of a topic tree where the amount of training data for a specific topic can be judiciously increased in cases where the elemental topic cluster has too few word tokens to build a reliably smoothed and representative language model Our system is able to fine tune topic adaptation by interpolating models chosen from thousands of topics allowing for adaptation to unique previously unseen combinations of subjects 1 Introduction In this paper we explore large scale fine tunable topic adaptation for statistical language modeling We are interested in taking the initial transcription of a story supplied by a speech recognizer identifying a set of topics that describe the content of the story by choosing topic 
specific subsets of the language model training text building a language model from each of the selected subsets interpolating these models at the word level and using the new language model score to reevaluate speech recognition hypotheses The goal of the adaptation is to lower the word error rate WER of the story transcription output by the speech recognizer by providing language model scores that reflect a higher expectation of words and word sequences that are characteristic of the identified topics of the story This adaptation can be described as large scale because the most similar topics to a new piece of text are chosen from a set of over 5000 topic candidates One strength of this approach is the ability for diverse typically unrelated topics to be selected and interpolated together to match the unique events present in a new story Previously unseen combinations of topics occur frequently in domains such as Broadcast News where current events dictate the contents of each article 2 Topic Adaptation 
The topic adaptation scheme we are using consists of the following steps 1 Stories from an annotated corpus that share similar topics are gathered together into a set of clusters based on manually assigned keywords 2 A classifier is used to find the clusters that are most similar in topic to a story transcription output by a speech recognizer 3 Language models are built from each of the clusters of data found to be the most similar to the new story 4 The language models are interpolated at the word level and the interpolated score is used to rescore the speech recognizer s hypotheses in an N best framework Each of these steps will be reviewed in detail in the following sections 1 Keyword 1 Keyword 2 Keyword 3 Keyword 4 Keyword n 1 Keyword n Figure 1 Keyword based topic clusters 2 1 Clustering Given a corpus with story boundaries marked and manually chosen keywords assigned to each story topic clusters are created by defining each unique keyword as a label for a cluster as in Figure 1 Each keyword represents 
an elemental topic and all stories that have that keyword are assigned to its particular cluster Each cluster is then a candidate to be used in topic adaptation Topic trees can be built from the topic clusters by treating the clusters as leaves and iteratively merging the topics together to form a tree as in Figure 2 When two clusters are merged together the resulting node in the tree has the benefit of more training data with which to estimate language model parameters but is more general in topic than the children clusters We can use the topic tree structure to combine the advantages of having larger clusters for parameter estimation and smaller clusters for topic focus Each path from leaf to root specifies a set of nodes that start out in a very distinct topic and then gradually become more general as the clusters become larger At runtime automatic topic identification is performed on a decoded document and results in a small number of active leaf topics Language models built at various nodes along the 
active paths can be combined to best model the current document The construction of topic trees has been explored in the Switchboard domain by Carlson 1 Agglomerative clustering has been used successfully for topic adaptation in a mixture modeling framework 2 3 In these cases training data was partitioned into a relatively small set of topic clusters less than one hundred One advantage of retaining thousands of individual topic clusters is the ability to make fine distinctions between different subjects and mix unusual topics together that may occur in a future story An important feature of creating topic clusters based on keywords is the presence of data overlap between clusters If one story contains five different keywords describing its content then the text for the story will appear in five different clusters When using agglomerative clustering to create a topic tree the effects of data overlap on the measure of cluster similarity need to be considered In this 2 Root General Language Model Keyword 1 
Keyword 2 Keyword 3 Keyword 4 Keyword n 1 Keyword n Figure 2 A topic tree built from keyword clusters work no corrective action was taken to account for the similarity measure bias due to data overlap Possible solutions include excluding the overlapping data from all similarity calculations assigning half of each duplicated story to each leaf or using supervised clustering to make reasonable decisions 2 2 Topic Detection Once we have a set of topic clusters we can use topic detection to determine the most topic similar clusters to a new piece of text We consider two topic detection methods the TFIDF classifier and the naive Bayes classifier Both methods input a story and output the list of topic clusters ranked in order of decreasing similarity Even when the text given to the classifiers contains word errors as is the case when we use speech recognition hypotheses for detection topic detection will still perform reasonably well as we will show below As long as the word errors in the hypothesis are not 
significantly topic correlated the correct content words in the hypothesis will provide enough evidence for the selection of appropriate clusters 2 2 1 The TFIDF Classifier The TFIDF measure 4 assigns a weight to each unique word in a document representing how topic specific that word is to its document or cluster If a cluster 3 contains distinct words the cluster text can be represented as a dimensional vector of weights where each weight is given by 1 2 3 weighting function assigns high values to topic specific words which are those words that appear with high frequency within one cluster but appear in relatively few other clusters Words that occur in many clusters or that occur with low frequency are deemed more general and are assigned low weights Given some new text represented by weight vector the topic similarity between cluster and the new text can be computed with the following cosine measure 2 5 1 A 1 2 1 2 2 Equation 2 computes the cosine of the angle between the two vectors representing the two 
sets of text It is normalized for vector length so that large clusters are not favored This similarity measure produces a high value when the two texts being compared are similar with a value of 1 when they are identical A similarity value of zero means that the topics of the texts are unrelated 2 2 2 Naive Bayes Classifier A naive Bayes classifier calculates the probability of a topic given the words in a new document We make the traditional simplifying assumption that words in the document occur independently of one another in Equation 3 Q 3 The topic priors are computed from the topic document frequencies and the probability of a word given a topic is computed by smoothing the unigram distribution within the topic cluster with the general unigram distribution obtained from 4 the entire training corpus as shown in Equation 4 The smoothing parameter empirically chosen x is Pyiq g7g tsvu 4 Many other topic detection techniques exist Imai et al have developed a Hidden Markov Model system for topic detection 
which identifies multiple topics per story and considers that each word in the story need not be related to all of the story s topics 5 Joachims analyzes several topic detection algorithms including TFIDF and the naive Bayes classifier in 6 2 3 Language Models In the speech recognition paradigm each time a new story is decoded an initial hypothesis transcription is produced We then feed the hypothesis transcription to the classifier which chooses the most similar topic clusters Language models are built from the text in each of the selected clusters Here Good Turing discounted trigram backoff models 7 using all bigrams and trigrams no cutoffs were built with the CMU Statistical Language Modeling toolkit 8 2 4 Model Interpolation The individual language models built from the chosen clusters or from nodes farther up in the tree when a topic tree is being used are interpolated together at the word level to produce a new language score as in Equation 5 P u 2 1 2 1 1 1 3 Experiments The training data used in 
these experiments is the Broadcast News corpus obtained from Primary Source Media 9 The data used here covers the period from 1992 5 1995 and consists of 130 million words of news reports and interviews from ABC News CNN PBS and National Public Radio Story boundaries are marked and each story is accompanied by a set of keywords 4 to 5 on average that describe the story s content The corpus was split into topic clusters by collecting the keywords from all stories and assigning each keyword to a cluster The text for each story was assigned to the clusters of the story s keywords Many of the keywords have sub categories in which case the sub categories were separated from the main keyword and treated as keywords themselves Summary stories keywords with only one story and certain geographic keywords were excluded resulting in 5883 topic clusters A sample list of keywords is shown in Table 1 Gems General Agreement on Tariffs and Trade General Dynamics Corp General Electric Co General Mills General Motors Corp 
General Motors automobiles Generals Generation gap Generic drugs Generic products Genetic counseling Genetic engineering Genetics Genital mutilization Genocide Genovese Kitty Geography Geology George Periodical George Washington University Table 1 Sample of topic cluster keywords Each keyword represents a topic cluster 6 The most frequent 63k words from the four years of Broadcast News text defined the vocabulary for calculating cluster similarity Twenty Broadcast News articles obtained from the Linguistic Data Consortium s LDC release of the Broadcast News corpus were randomly selected from the period covering January 1996 through April 1996 as a test set to compare the TFIDF and naive Bayes classifiers Each of these twenty articles contained a minimum of 500 word tokens and at least one manually assigned keyword from the list of 5883 topic clusters The development and evaluation sets from the 1996 ARPA Hub4 continuous speech recognition evaluation were used as speech recognition test sets These sets 
contain story boundaries where each boundary indicates a change in topic The development set contains 57 stories and the evaluation set contains 74 stories The number of word tokens in each story from the development and evaluation sets ranges from 6 to 2131 3 1 Topic Detection Experiments The TFIDF and naive Bayes classifiers were used for topic detection on the twenty0 25 Each classifier story test set from 1996 The Bayes classifier used compared each test story to the 5883 topic leaf clusters and generated a ranked list of topic clusters in order of decreasing similarity to the test story The correct topics for each test story were the manually assigned keywords that accompanied each story that were also found among the 5883 leaf clusters Precision and recall results at 5 10 and 20 were calculated as in 10 and are shown in Table 2 For this task the naive Bayes classifier outperforms the TFIDF classifier across all three levels of precision and recall The largest story from the Hub4 development set 
consists of 2131 words and discusses suspicions of drug use by Chinese swimmers during the 1996 Olympics The correct story transcript and the errorful first pass Sphinx III 11 recognition hypotheses for this story 45 WER were classified using both the TFIDF measure and the naive Bayes classifier The 10 most similar clusters chosen by the TFIDF measure for the correct and errorful transcripts are shown in Table 3 The 10 most similar clusters chosen by the naive Bayes classifier for both transcripts are shown in Table 4 Both classification methods choose reasonable topics when using either the correct or errorful story transcripts For both classifiers six of the clusters chosen when using the correct transcript are also chosen when using the errorful transcript It is interesting to note that the two methods seem to choose slightly different 7 Precision at 5 Precision at 10 Precision at 20 Recall at 5 Recall at 10 Recall at 20 TFIDF 49 0 32 5 22 5 48 5 62 8 82 0 Bayes 59 0 42 0 24 3 56 1 79 1 89 2 Table 2 
Precision and Recall values at 5 10 and 20 for the TFIDF and Naive Bayes classifiers types of clusters In this case the TFIDF classifier chooses many clusters about China whereas the naive Bayes classifier chooses more sports related clusters Most importantly we see that the clusters chosen by either method when using a transcript with a high word error rate are related to the topic of the story 3 2 Perplexity Reduction In order to determine the best way to interpolate topic specific language models we varied the number of topic specific models chosen per story for adaptation and measured development set perplexity First topic detection was run using the TFIDF and naive Bayes classifiers on errorful first pass Sphinx III recognition hypotheses from each of the 57 stories from the development set The word error rate WER of the development set was 40 A 51k vocabulary general trigram backoff language model was built from LDC s release of the Broadcast News corpus Good Turing discounted trigram backoff language 
models were built from each of the 20 most similar topic clusters chosen by the classifiers for each development set story The perplexity for each story was computed by interpolating the most similar 5 10 or 20 topic models for each story with the 51k general language model at the word level Model interpolation weights were obtained with the EM algorithm and perplexity was computed using two way cross validation All of the story perplexities were combined at the entropy level to adjust for different numbers of word tokens to give a final development set perplexity Results are shown in Table 5 Using twenty topic models chosen by the naive Bayes classifier yields the greatest reduction in perplexity over the 8 TFIDF Classifier Correct Transcript Errorful Transcript 45 WER China China Olympic Games Favored nation clause Olympic Games Barcelona 1992 Chinese Americans Favored nation clause Olympic Games Chinese Americans Intellectual property rights Drug testing Chinese in the United States Olympic Games Atlanta 
1996 Olympic Games Barcelona 1992 Intellectual property rights Wu Harry Swimming Civil rights Athletes Zemin Jiang Table 3 Ten most similar clusters chosen with TFIDF correct and errorful transcripts general Broadcast News model from 222 to 188 a 15 reduction Next we built two topic trees The first tree was built automatically by merging the 5883 topic leaf clusters iteratively to the root At each iteration the node with the fewest words was chosen to be merged with its most similar node which was chosen by the TFIDF classifier The second tree was built in the same way as the first except that if the similarity value between the smallest cluster and its most similar cluster was below a threshold of 0 3 the smallest cluster was orphaned or linked directly to the root The orphan tree did not force a merge if no good match existed whereas the automatic tree forced a merge at each iteration The 5883 leaf clusters range in token size from 393 to 6 234 183 Two hundred thirty of the 5883 leaf clusters contain less 
than one thousand word tokens In cases where so few tokens are available adaptation may benefit from using more data In an effort to verify this hypothesis three development set stories and one of the most similar leaves for each story were selected For each of the three story leaf pairs language models were built at various nodes along the path from leaf to root for both the automatic tree and the orphan tree Each model was interpolated with the 51k general model and the perplexity of the story was computed using two way cross validation In all cases the perplexity decreased or stayed the same when a model built from a node with more data than the leaf cluster was used as 9 Naive Bayes Classifier Correct Transcript Errorful Transcript 45 WER Olympic Games Barcelona 1992 Olympic Games Olympic Games Olympic Games Barcelona 1992 Drug testing China Athletes Athletics Sports Drug testing Gymnastics Olympic Games Sydney 2000 Louganis Greg Gymnastics Athletics Running races Diving Athletes Olympic Games Seoul 1988 
Wu Harry Table 4 Ten most similar clusters chosen with naive Bayes classifier correct and errorful transcripts shown in Tables 6 and 7 For example interpolating a leaf cluster language model built from 35 680 tokens with the general language model results in a perplexity of 219 whereas interpolating a language model built from a node located higher up the path with 100 500 tokens with the general language model results in a perplexity of 210 This limited example demonstrates that at least in some cases when interpolating only one leaf with the general language model per story adding additional relevant text is helpful Topic tree adaptation was tested on the development set stories by setting token cutoffs In all cases twenty leaf clusters were considered per story For both trees General model 222 Leaves TFIDF Bayes 5 193 193 10 191 189 20 189 188 Table 5 Development set perplexity leaves only 10 Tokens 13445 25353 64820 100500 574818 Root Paths in automatic tree PP Tokens PP Tokens 233 266125 201 35680 229 
300170 200 100500 225 451893 202 574818 227 1002910 223 264 Root 220 Root PP 219 210 226 233 272 Table 6 Perplexity variation moving up automatic tree paths from leaf to root Paths in orphan tree Tokens PP Tokens PP Tokens PP 13445 233 266125 201 35680 219 25353 229 305562 201 96495 210 60815 225 333591 202 96495 226 Root 264 Root 220 Root 272 Table 7 Perplexity variation moving up orphan tree paths from leaf to root automatic and orphan whenever a leaf cluster was chosen for interpolation the topic model was built from the lowest node in the path from leaf to root that had at least as many word tokens as the predetermined threshold These nodes are referred to as active nodes in the discussion below Thresholds of 50k and 200k were set Occasionally the paths for similar leaves merge and in these cases less than twenty models were interpolated for those stories The general broadcast news model i e the model at the root of the tree was always interpolated with the topic models In the case of the orphan tree 
sometimes the node just below the root in an active path had fewer tokens than the threshold leaving only the root node with enough tokens for interpolation Therefore two orphan tree scenarios were evaluated in the first all paths that assigned the root as the active node because all other 11 nodes in the path had fewer tokens than the threshold were left out completely meaning that the selected leaf did not contribute a model for interpolation In the second scenario designated by leaves all paths that assigned the root as the active node built the topic model from the leaf of the path even though there were fewer tokens in the leaf node than the threshold Perplexity results for these cases are shown in Tables 8 and 9 In all cases interpolating topic models results in a decrease in perplexity over using only the general trigram model Generally none of the tree scenarios works as well as interpolating only the leaves except for the Bayes orphan tree leaves cases which perform as well as the twenty Bayes 
leaves General model 222 Token thresh TFIDF Bayes Leaves only 189 188 50k 191 189 200k 192 191 Table 8 Development set perplexity automatic tree General model 222 Token thresh TFIDF Bayes Leaves only 189 188 50k 191 189 50k leaves 190 188 200k 196 192 200k leaves 191 188 Table 9 Development set perplexity orphan tree 3 3 N best Rescoring Next we wanted to see if using these models to rescore N best lists would lead to a reduction in recognition WER Two interpolation weighting schemes were 12 tested In the first indicated by min PP the cluster language models and the 51k general language model were interpolated with weights obtained by minimizing the perplexity of the errorful first pass decoder hypothesis The second interpolation scheme uniform assigned a weight of 0 55 to the general 51k language model and uniform interpolation weights to the remaining topic models Rescoring consisted of using the original acoustic score the new language model score and a word insertion penalty For the development set N 500 
and the for the evaluation set N 200 Filled pauses were predicted from manually set unigram probabilities 12 For the development set the first pass WER with no rescoring was 40 2 The lowest N best WER found by using the reference transcripts to choose the N best hypotheses with the lowest error was 34 6 The lowest N best WER represents an upper bound on the performance of N best rescoring Using just the 51k general language model to rescore results in a WER of 40 1 Language model score and insertion penalty weights were chosen by two way cross validation and the average weight values were used for evaluation set rescoring The evaluation N best lists were generated after two passes of the Sphinx III decoder Topic adaptation scenarios tested with rescoring include twenty TFIDF chosen leaves twenty Bayes chosen leaves and the Bayes orphan leaves topic tree with a token threshold of 200k Rescoring results are shown in Tables 10 and 11 Condition No topic adaptation Lowest N best WER General trigram TFIDF leaves 
min PP TFIDF leaves uniform Bayes leaves min PP Bayes leaves uniform Bayes orphan tree 200k leaves min PP Bayes orphan tree 200k leaves uniform WER 40 2 34 6 40 1 39 6 39 7 39 5 39 5 39 6 39 6 Table 10 Development set word error rate using different language scores For both the evaluation and development sets there is no large WER difference between using uniform model interpolation weights or choosing weights by 13 Condition 2nd pass decoder output TFIDF leaves min PP TFIDF leaves uniform Bayes leaves min PP Bayes leaves uniform Bayes orphan tree 200k leaves min PP Bayes orphan tree 200k leaves uniform WER 35 5 35 3 35 5 35 4 35 4 35 3 35 5 Table 11 Evaluation set word error rate minimizing perplexity Rescoring the N best lists with the topic score from the interpolation of Bayes chosen leaves results in the greatest decrease in WER over the original 1st pass transcription no adaptation on the development set In this case the error rate drops from 40 2 to 39 5 Adaptation with either the TFIDFchosen leaves 
or the orphan tree lowers the WER to 39 6 However none of the topic scores results in a significant improvement in WER on the evaluation set Adaptation on the evaluation set with Bayes chosen leaves results in only a 0 1 decrease in WER For both the development and evaluation sets rescoring with a Kneser Ney smoothed general trigram model as opposed to our Good Turing smoothed general model results in a lower WER than the topic models 12 The Kneser Ney model results in a WER of 39 4 on the development set and 34 9 on the evaluation set Therefore while topic adaptation does result in slightly better WERs than no adaptation future work in topic adaptation must include better smoothing techniques for models built from small amounts of training data 4 Conclusion Large scale finely tuned topic adaptation is possible and does result in a decrease in perplexity and a slight decrease in WER in the Broadcast News domain Choosing the 20 most topic similar clusters for an individual story from among 5883 candidates and 
interpolating models built from these clusters results in a 15 decrease in perplexity over a general Broadcast News model even when the word 14 error rate of the story hypothesis used for topic detection is quite high Having many candidate clusters permits fine topic distinction and the possibility of mixing of topics in a way that might not have been previously seen in the training data Furthermore the semantic landscape of Broadcast News has been mapped out in two different topic trees Future work may find these structures helpful in more complex topic detection and adaptation systems 5 Acknowledgements We would like to thank Richard Schwartz Yiming Yang Stanley Chen and Bin Zhou for their contributions to this work References 1 B Carlson Unsupervised topic clustering of switchboard speech messages In Proceedings of ICASSP 96 pages 15 7 Slava M Katz Estimation of probabilities from sparse data for the language model component of a speech recognizer IEEE Transactions on Acoustics Speech and Signal 
Processing ASSP 35 3 16 
42	a	Carnegie Mellon University Research Showcase Computer Science Department School of Computer Science 1 1 1997 Using Story Topics For Language Model Adaptation Kristie Seymore Carnegie Mellon University Roni Rosenfield Carnegie Mellon University roni cs cmu edu Follow this and additional works at http repository cmu edu compsci Recommended Citation Seymore Kristie and Rosenfield Roni Using Story Topics For Language Model Adaptation 1997 Computer Science Department Paper 1334 http repository cmu edu compsci 1334 This Technical Report is brought to you for free and open access by the School of Computer Science at Research Showcase It has been accepted for inclusion in Computer Science Department by an authorized administrator of Research Showcase For more information please contact researchshowcase andrew cmu edu USING STORY TOPICS FOR LANGUAGE MODEL ADAPTATION Kristie Seymore and Ronald Rosenfeld School of Computer Science Carnegie Mellon University Pittsburgh Pennsylvania 15213 kseymore cs cmu edu roni cs cmu 
edu 3 Language models are built from the clusters of data found to be the most similar to the test data The models are interpolated at the word level and the interpolated score is used to rescore the speech recognizer s hypotheses in an N best framework 2 1 Clustering Given a corpus with story boundaries marked and keywords manually assigned to each story topic clusters are created by defining each unique keyword as a label for a cluster For each keyword all stories that have that keyword are assigned to its particular cluster Each cluster is then a candidate to be used in future adaptation Topic trees can be built by treating the topic clusters as leaves and iteratively merging the topics together to form a tree Agglomerative clustering has been used successfully for topic adaptation in a mixture modeling framework 2 3 In these cases training data was partitioned into a relatively small set of topic clusters which was used for adaptation However one advantage of retaining a high number of individual topic 
clusters is the ability to make fine distinctions between different subjects and mix unusual topics together that may occur in a future story As similar clusters are merged together they lose their topic focus but they acquire the advantage of having additional data to build more statistically sound language models A topic tree is one way to combine the data advantages of larger clusters and the topic focus of many of smaller clusters Each path from leaf to root specifies a set of nodes that start out in a very distinct topic and then gradually become more general as the clusters become larger At runtime automatic topic identification is performed on a decoded document and results in a small number of active leaf topics Language models built at various nodes along the active paths can be combined to best model the current document The use of topic trees has also been explored in the Switchboard domain by Carlson 4 Automatic topic clustering does not always result in optimal clustering decisions We are 
investigating semiautomatic methods where the system asks for cues whenever its confidence in its clustering decision is weak We have developed a web interface that allows the user to make clustering decisions when building a topic tree drawing from all the text keyword and tree information available ABSTRACT The subject matter of any conversation or document can typically be described as some combination of elemental topics We have developed a language model adaptation scheme that takes a piece of text chooses the most similar topic clusters from a set of over 5000 elemental topics and uses topic specific language models built from the topic clusters to rescore N best lists We are able to achieve a 15 reduction in perplexity and a small improvement in WER by using this adaptation We also investigate the use of a topic tree where the amount of training data for a specific topic can be judiciously increased in cases where the elemental topic cluster has too few word tokens to build a reliably smoothed and 
representative language model Our system is able to fine tune topic adaptation by interpolating models chosen from thousands of topics allowing for adaptation to unique previously unseen combinations of subjects 1 INTRODUCTION In this paper we explore large scale fine tunable topic adaptation Specifically we examine the reduction in perplexity and word error rate made possible by detecting a story s topic and then using a series of interpolated language models trained on topic specific data to reevaluate speech recognition hypotheses The most similar topics to a new piece of text are chosen from over 5000 topic candidates One strength of this approach is the ability for diverse typically unrelated topics to be selected and interpolated together to match the unique events present in a new story Previously unseen combinations of topics occur frequently in domains such as Broadcast News where current events dictate the contents of each article For details of our earlier work in topic adaptation see 1 2 TOPIC 
ADAPTATION The topic adaptation scheme we are using consists of the following three steps 1 Stories from an annotated corpus that share similar topics are gathered together into a set of clusters based on manually assigned keywords 2 A classifier is used to find the clusters that are most similar in topic to the text that is being decoded An important feature of creating topic clusters based on keywords is the presence of data overlap between clusters If one story contains five different keywords describing its content then the text for the story will appear in five different clusters When using agglomerative clustering to create a topic tree the effects of data overlap on the measure of cluster similarity need to be considered In this work no corrective action was taken to account for the similarity measure bias due to data overlap 2 2 Topic Detection Once we have a set of topic clusters we can use topic detection to determine the most topic similar clusters to a new piece of text We consider two topic 
detection methods the TFIDF classifier and the marked and each story is accompanied by a set of keywords that describe the story s content The corpus was split into topic clusters by collecting the keywords from all stories and assigning each keyword to a cluster The text for each story was assigned to the clusters of the story s keywords Many of the keywords have subcategories in which case the sub categories were separated from the main keyword and treated as keywords themselves Summary stories keywords with only one story and certain geographic keywords were excluded resulting in 5883 topic clusters The most frequent 63k words from the four years of Broadcast News text defined the vocabulary for calculating cluster similarity The development and evaluation sets from the 1996 ARPA Hub4 continuous speech recognition evaluation were used as speech recognition test sets These sets contain story boundaries where each boundary indicates a change in topic The development set contains 57 stories and the 
evaluation set contains 74 stories The number of word tokens in each story ranges from 6 to 2131 3 1 Perplexity Reduction In order to determine the best way to interpolate topic specific language models we varied the number of topic specific models and measured development set perplexity First topic detection was run using the TFIDF and Next we built two topic trees The first automatic tree merged the 5883 topic leaf clusters iteratively to the root At each iteration the node with the fewest words was chosen to be merged with its most similar node which was chosen by the TFIDF classifier The second tree was built in the same way as the first except that if the similarity value between the smallest cluster and its most similar cluster was below a threshold of 0 3 the smallest cluster was orphaned or linked directly to the root The orphan tree did not force a merge if no good match existed whereas the automatic tree forced a merge at each iteration Of the 5883 leaf clusters 230 contain less than one thousand 
word tokens In cases where so few tokens are available adaptation may benefit from using more data In an effort to verify this hypothesis three development set stories and one of the most similar leaves for each story were selected For each of the three story leaf pairs language models were built at various nodes along the path from leaf to root for both the automatic tree and the orphan tree Each model was interpolated with the 51k general model and the perplexity of the story was computed using two way cross validation In all six cases the perplexity decreased or stayed the same when a model built from a node with more data than the leaf cluster was used This limited example indicates that using more data by traveling up the tree from the leaf nodes may improve adaptation Topic tree adaptation was tested on the development set stories by setting token cutoffs In all cases twenty leaf clusters were considered per story For both trees automatic and orphan whenever a leaf cluster was chosen for interpolation 
the topic model was built from the lowest node in the path that had at least as many word tokens as the pre determined threshold Thresholds of 50k and 200k were set Occasionally the paths for similar leaves merge and in these cases less than twenty models were interpolated for those stories In the case of the orphan tree sometimes a leaf cluster would lead straight to the root Therefore two orphan tree scenarios were evaluated in the first the leaf clusters that had fewer tokens than the threshold but were connected directly to the root were left out completely and in the second designated by leaves the leaf clusters were left in even if they contained fewer tokens than the threshold if a larger node with more tokens than the threshold was not available Perplexity results for these cases are shown in Tables 2 and 3 In all cases there is a perplexity reduction over the general trigram model and the orphan leaves trees do as well as using leaves only when the leaves are chosen by the General model 222 Token 
thresh TFIDF Bayes Leaves only 189 188 50k 191 189 200k 192 191 Table 2 Development set perplexity automatic tree General model 222 Token thresh TFIDF Bayes Leaves only 189 188 50k 191 189 50k leaves 190 188 200k 196 192 200k leaves 191 188 Table 3 Development set perplexity orphan tree 3 2 N best Rescoring Next we wanted to see if using these models to rescore N best lists would lead to a reduction in recognition WER Two interpolation weighting schemes were tested In the first the cluster language models and the 51k general language model were interpolated with weights obtained by minimizing the perplexity of the errorful first pass decoder hypothesis The second interpolation scheme assigned a weight of 0 55 to the general 51k language model and uniform interpolation weights to the remaining topic models In all cases twenty leaf clusters were chosen per story Rescoring consisted of using the original acoustic score the new language model score and a word insertion penalty Filled pauses were predicted from 
manually set unigram probabilities 1 For the development set the first pass WER with no rescoring was 40 2 The lowest N best WER found by using the reference transcripts to choose the N best hypotheses with the lowest error was 34 6 The lowest N best WER represents an upper bound on the performance of N best rescoring Using just the 51k general language model to rescore a WER of 40 1 was obtained Language model score and insertion penalty weights were chosen by two way cross validation and the average weight values were used for evaluation set rescoring The evaluation N best lists were generated after two passes of the Sphinx 3 decoder Rescoring was tried using TFIDF chosen leaves Bayes chosen leaves and the 200k leaves orphan tree with Bayes chosen leaves The two interpolation weighting schemes minimized perplexity and uniform weights were tested for each condition Results are shown in Tables 4 and 5 Condition WER No topic adaptation 40 2 Lowest N best WER 34 6 General trigram 40 1 TFIDF leaves min PP 39 6 
TFIDF leaves uniform 39 7 Bayes leaves min PP 39 5 Bayes leaves uniform 39 5 Bayes 200k orphan tree min PP 39 6 Bayes 200k orphan tree uniform 39 6 Table 4 Development set word error rate using different language scores Condition WER 2nd pass decoder output 35 5 TFIDF leaves min PP 35 3 TFIDF leaves uniform 35 5 Bayes leaves min PP 35 4 Bayes leaves uniform 35 4 Bayes 200k orphan tree min PP 35 3 Bayes 200k orphan tree uniform 35 5 Table 5 Evaluation set word error rate All of the topic adaptation methods lead to an improved WER on the development set with the Bayes chosen leaves providing the greatest WER reduction of 0 7 over 1st pass development hypothesis Improvement on the evaluation set is less significant with most methods providing a very slight decrease in WER The choice of model interpolation weights does not seem to significantly affect WER results with the minimized perplexity weights performing slightly better than the uniform weights On both the development and evaluation sets using a Kneser 
Ney smoothed general trigram model to rescore results in a lower WER than the topic models 1 A Kneser Ney model results in a WER of 39 4 on the development set and 34 9 on the evaluation set Future work in topic adaptation must include better smoothing techniques for models built from small amounts of training data 4 CONCLUSION Large scale finely tuned topic adaptation is possible and does result in a decrease in perplexity and a slight decrease in WER in the Broadcast News domain Choosing the 20 most topic similar clusters for an individual story from among 5883 candidates and interpolating models built from these clusters results in a 15 decrease in perplexity over a general Broadcast News model even when the word error rate of the story hypothesis used for topic detection is quite high Having many candidate clusters permits fine topic distinction and the possibility of mixing topics in a way that might not have been previously seen in the training data Furthermore the semantic landscape of Broadcast News 
has been mapped out in two different topic trees Future work may find these structures helpful in more complex topic detection and adaptation systems For a more detailed presentation of this work see 6 5 ACKNOWLEDGEMENTS We would like to thank Richard Schwartz Yiming Yang Stanley Chen and Bin Zhou for their contributions and help with this work This research was sponsored by the Department of the Navy Naval Research Laboratory under Grant No N00014 93 1 2005 and by the National Security Agency under Grant numbers MDA904 96 10113 and MDA904 97 1 0006 The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies either expressed or implied of the U S Government The first author is additionally supported under a National Science Foundation Graduate Research Fellowship 6 REFERENCES 1 K Seymore S Chen M Eskenazi and R Rosenfeld Language and Pronunciation Modeling in the CMU 1996 Hub 4 Evaluation Proc of the 1997 ARPA Speech 
Recognition Workshop 1997 2 R Iyer M Ostendorf Modeling Long Distance Dependence in Language Topic Mixtures vs Dynamic Cache Models Proc ICSLP vol 1 1996 pp 236 239 3 P Clarkson A Robinson Language Model Adaptation Using Mixtures and an Exponentially Decaying Cache Proc ICASSP vol 2 1997 pp 799802 4 B Carlson Unsupervised Topic Clustering of Switchboard Speech Messages Proc ICASSP 1996 pp 315 318 5 G Salton Developments in Automatic Text Retrieval Science Vol 253 1991 pp 974 980 6 K Seymore R Rosenfeld Large scale Topic Detection and Language Model Adaptation Carnegie Mellon University Technical Report June 1997 7 P Placeway et al The 1996 Hub 4 Sphinx 3 System Proc of the 1997 ARPA Speech Recognition Workshop 1997 8 S Katz Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer IEEE Transactions on Acoustics Speech and Signal Processing vol ASSP 35 no 3 March 1987 pp 400 401 
43	a	Carnegie Mellon University Research Showcase Computer Science Department School of Computer Science 6 1 1997 Large Scale Topic Detection and Language Model Adaptation Kristie Seymore Carnegie Mellon University Roni Rosenfield Carnegie Mellon University roni cs cmu edu Follow this and additional works at http repository cmu edu compsci Recommended Citation Seymore Kristie and Rosenfield Roni Large Scale Topic Detection and Language Model Adaptation 1997 Computer Science Department Paper 1335 http repository cmu edu compsci 1335 This Technical Report is brought to you for free and open access by the School of Computer Science at Research Showcase It has been accepted for inclusion in Computer Science Department by an authorized administrator of Research Showcase For more information please contact researchshowcase andrew cmu edu Large scale Topic Detection And Language Model Adaptation Kristie Seymore Ronald Rosenfeld June 1997 CMU CS 97 152 School of Computer Science Carnegie Mellon University Pittsburgh PA 
15213 This research was sponsored by the Department of the Navy Naval Research Laboratory under Grant No N00014 93 1 2005 the National Security Agency under Grant numbers MDA904 961 0113 and MDA904 97 1 0006 and under a National Science Foundation Graduate Research Fellowship The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies either expressed or implied of the U S Government or the National Science Foundation Keywords speech recognition statistical language modeling topic detection topic adaptation document clustering Abstract The subject matter of any conversation or document can typically be described as some combination of elemental topics We have developed a language model adaptation scheme that takes a piece of text chooses the most similar topic clusters from a set of over 5000 elemental topics and uses topic specific language models built from the topic clusters to rescore N best lists We are able to achieve 
a 15 reduction in perplexity and a small improvement in word error rate by using this adaptation We also investigate the use of a topic tree where the amount of training data for a specific topic can be judiciously increased in cases where the elemental topic cluster has too few word tokens to build a reliably smoothed and representative language model Our system is able to fine tune topic adaptation by interpolating models chosen from thousands of topics allowing for adaptation to unique previously unseen combinations of subjects 1 Introduction In this paper we explore large scale fine tunable topic adaptation for statistical language modeling We are interested in taking the initial transcription of a story supplied by a speech recognizer identifying a set of topics that describe the content of the story by choosing topic specific subsets of the language model training text building a language model from each of the selected subsets interpolating these models at the word level and using the new language 
model score to reevaluate speech recognition hypotheses The goal of the adaptation is to lower the word error rate WER of the story transcription output by the speech recognizer by providing language model scores that reflect a higher expectation of words and word sequences that are characteristic of the identified topics of the story This adaptation can be described as large scale because the most similar topics to a new piece of text are chosen from a set of over 5000 topic candidates One strength of this approach is the ability for diverse typically unrelated topics to be selected and interpolated together to match the unique events present in a new story Previously unseen combinations of topics occur frequently in domains such as Broadcast News where current events dictate the contents of each article 2 Topic Adaptation The topic adaptation scheme we are using consists of the following steps 1 Stories from an annotated corpus that share similar topics are gathered together into a set of clusters based on 
manually assigned keywords 2 A classifier is used to find the clusters that are most similar in topic to a story transcription output by a speech recognizer 3 Language models are built from each of the clusters of data found to be the most similar to the new story 4 The language models are interpolated at the word level and the interpolated score is used to rescore the speech recognizer s hypotheses in an N best framework Each of these steps will be reviewed in detail in the following sections 1 Keyword 1 Keyword 2 Keyword 3 Keyword 4 Keyword n 1 Keyword n Figure 1 Keyword based topic clusters 2 1 Clustering Given a corpus with story boundaries marked and manually chosen keywords assigned to each story topic clusters are created by defining each unique keyword as a label for a cluster as in Figure 1 Each keyword represents an elemental topic and all stories that have that keyword are assigned to its particular cluster Each cluster is then a candidate to be used in topic adaptation Topic trees can be built 
from the topic clusters by treating the clusters as leaves and iteratively merging the topics together to form a tree as in Figure 2 When two clusters are merged together the resulting node in the tree has the benefit of more training data with which to estimate language model parameters but is more general in topic than the children clusters We can use the topic tree structure to combine the advantages of having larger clusters for parameter estimation and smaller clusters for topic focus Each path from leaf to root specifies a set of nodes that start out in a very distinct topic and then gradually become more general as the clusters become larger At runtime automatic topic identification is performed on a decoded document and results in a small number of active leaf topics Language models built at various nodes along the active paths can be combined to best model the current document The construction of topic trees has been explored in the Switchboard domain by Carlson 1 Agglomerative clustering has been 
used successfully for topic adaptation in a mixture modeling framework 2 3 In these cases training data was partitioned into a relatively small set of topic clusters less than one hundred One advantage of retaining thousands of individual topic clusters is the ability to make fine distinctions between different subjects and mix unusual topics together that may occur in a future story An important feature of creating topic clusters based on keywords is the presence of data overlap between clusters If one story contains five different keywords describing its content then the text for the story will appear in five different clusters When using agglomerative clustering to create a topic tree the effects of data overlap on the measure of cluster similarity need to be considered In this 2 Root General Language Model Keyword 1 Keyword 2 Keyword 3 Keyword 4 Keyword n 1 Keyword n Figure 2 A topic tree built from keyword clusters work no corrective action was taken to account for the similarity measure bias due to 
data overlap Possible solutions include excluding the overlapping data from all similarity calculations assigning half of each duplicated story to each leaf or using supervised clustering to make reasonable decisions 2 2 Topic Detection Once we have a set of topic clusters we can use topic detection to determine the most topic similar clusters to a new piece of text We consider two topic detection methods the TFIDF classifier and the naive Bayes classifier Both methods input a story and output the list of topic clusters ranked in order of decreasing similarity Even when the text given to the classifiers contains word errors as is the case when we use speech recognition hypotheses for detection topic detection will still perform reasonably well as we will show below As long as the word errors in the hypothesis are not significantly topic correlated the correct content words in the hypothesis will provide enough evidence for the selection of appropriate clusters 2 2 1 The TFIDF Classifier The TFIDF measure 4 
assigns a weight to each unique word in a document representing how topic specific that word is to its document or cluster If a cluster 3 contains distinct words the cluster text can be represented as a dimensional vector of weights where each weight is given by 1 2 3 weighting function assigns high values to topic specific words which are those words that appear with high frequency within one cluster but appear in relatively few other clusters Words that occur in many clusters or that occur with low frequency are deemed more general and are assigned low weights Given some new text represented by weight vector the topic similarity between cluster and the new text can be computed with the following cosine measure 2 5 1 A 1 2 1 2 2 Equation 2 computes the cosine of the angle between the two vectors representing the two sets of text It is normalized for vector length so that large clusters are not favored This similarity measure produces a high value when the two texts being compared are similar with a value of 
1 when they are identical A similarity value of zero means that the topics of the texts are unrelated 2 2 2 Naive Bayes Classifier A naive Bayes classifier calculates the probability of a topic given the words in a new document We make the traditional simplifying assumption that words in the document occur independently of one another in Equation 3 Q 3 The topic priors are computed from the topic document frequencies and the probability of a word given a topic is computed by smoothing the unigram distribution within the topic cluster with the general unigram distribution obtained from 4 the entire training corpus as shown in Equation 4 The smoothing parameter empirically chosen x is Pyiq g7g tsvu 4 Many other topic detection techniques exist Imai et al have developed a Hidden Markov Model system for topic detection which identifies multiple topics per story and considers that each word in the story need not be related to all of the story s topics 5 Joachims analyzes several topic detection algorithms 
including TFIDF and the naive Bayes classifier in 6 2 3 Language Models In the speech recognition paradigm each time a new story is decoded an initial hypothesis transcription is produced We then feed the hypothesis transcription to the classifier which chooses the most similar topic clusters Language models are built from the text in each of the selected clusters Here Good Turing discounted trigram backoff models 7 using all bigrams and trigrams no cutoffs were built with the CMU Statistical Language Modeling toolkit 8 2 4 Model Interpolation The individual language models built from the chosen clusters or from nodes farther up in the tree when a topic tree is being used are interpolated together at the word level to produce a new language score as in Equation 5 P u 2 1 2 1 1 1 3 Experiments The training data used in these experiments is the Broadcast News corpus obtained from Primary Source Media 9 The data used here covers the period from 1992 5 1995 and consists of 130 million words of news reports and 
interviews from ABC News CNN PBS and National Public Radio Story boundaries are marked and each story is accompanied by a set of keywords 4 to 5 on average that describe the story s content The corpus was split into topic clusters by collecting the keywords from all stories and assigning each keyword to a cluster The text for each story was assigned to the clusters of the story s keywords Many of the keywords have sub categories in which case the sub categories were separated from the main keyword and treated as keywords themselves Summary stories keywords with only one story and certain geographic keywords were excluded resulting in 5883 topic clusters A sample list of keywords is shown in Table 1 Gems General Agreement on Tariffs and Trade General Dynamics Corp General Electric Co General Mills General Motors Corp General Motors automobiles Generals Generation gap Generic drugs Generic products Genetic counseling Genetic engineering Genetics Genital mutilization Genocide Genovese Kitty Geography Geology 
George Periodical George Washington University Table 1 Sample of topic cluster keywords Each keyword represents a topic cluster 6 The most frequent 63k words from the four years of Broadcast News text defined the vocabulary for calculating cluster similarity Twenty Broadcast News articles obtained from the Linguistic Data Consortium s LDC release of the Broadcast News corpus were randomly selected from the period covering January 1996 through April 1996 as a test set to compare the TFIDF and naive Bayes classifiers Each of these twenty articles contained a minimum of 500 word tokens and at least one manually assigned keyword from the list of 5883 topic clusters The development and evaluation sets from the 1996 ARPA Hub4 continuous speech recognition evaluation were used as speech recognition test sets These sets contain story boundaries where each boundary indicates a change in topic The development set contains 57 stories and the evaluation set contains 74 stories The number of word tokens in each story 
from the development and evaluation sets ranges from 6 to 2131 3 1 Topic Detection Experiments The TFIDF and naive Bayes classifiers were used for topic detection on the twenty0 25 Each classifier story test set from 1996 The Bayes classifier used compared each test story to the 5883 topic leaf clusters and generated a ranked list of topic clusters in order of decreasing similarity to the test story The correct topics for each test story were the manually assigned keywords that accompanied each story that were also found among the 5883 leaf clusters Precision and recall results at 5 10 and 20 were calculated as in 10 and are shown in Table 2 For this task the naive Bayes classifier outperforms the TFIDF classifier across all three levels of precision and recall The largest story from the Hub4 development set consists of 2131 words and discusses suspicions of drug use by Chinese swimmers during the 1996 Olympics The correct story transcript and the errorful first pass Sphinx III 11 recognition hypotheses for 
this story 45 WER were classified using both the TFIDF measure and the naive Bayes classifier The 10 most similar clusters chosen by the TFIDF measure for the correct and errorful transcripts are shown in Table 3 The 10 most similar clusters chosen by the naive Bayes classifier for both transcripts are shown in Table 4 Both classification methods choose reasonable topics when using either the correct or errorful story transcripts For both classifiers six of the clusters chosen when using the correct transcript are also chosen when using the errorful transcript It is interesting to note that the two methods seem to choose slightly different 7 Precision at 5 Precision at 10 Precision at 20 Recall at 5 Recall at 10 Recall at 20 TFIDF 49 0 32 5 22 5 48 5 62 8 82 0 Bayes 59 0 42 0 24 3 56 1 79 1 89 2 Table 2 Precision and Recall values at 5 10 and 20 for the TFIDF and Naive Bayes classifiers types of clusters In this case the TFIDF classifier chooses many clusters about China whereas the naive Bayes classifier 
chooses more sports related clusters Most importantly we see that the clusters chosen by either method when using a transcript with a high word error rate are related to the topic of the story 3 2 Perplexity Reduction In order to determine the best way to interpolate topic specific language models we varied the number of topic specific models chosen per story for adaptation and measured development set perplexity First topic detection was run using the TFIDF and naive Bayes classifiers on errorful first pass Sphinx III recognition hypotheses from each of the 57 stories from the development set The word error rate WER of the development set was 40 A 51k vocabulary general trigram backoff language model was built from LDC s release of the Broadcast News corpus Good Turing discounted trigram backoff language models were built from each of the 20 most similar topic clusters chosen by the classifiers for each development set story The perplexity for each story was computed by interpolating the most similar 5 10 
or 20 topic models for each story with the 51k general language model at the word level Model interpolation weights were obtained with the EM algorithm and perplexity was computed using two way cross validation All of the story perplexities were combined at the entropy level to adjust for different numbers of word tokens to give a final development set perplexity Results are shown in Table 5 Using twenty topic models chosen by the naive Bayes classifier yields the greatest reduction in perplexity over the 8 TFIDF Classifier Correct Transcript Errorful Transcript 45 WER China China Olympic Games Favored nation clause Olympic Games Barcelona 1992 Chinese Americans Favored nation clause Olympic Games Chinese Americans Intellectual property rights Drug testing Chinese in the United States Olympic Games Atlanta 1996 Olympic Games Barcelona 1992 Intellectual property rights Wu Harry Swimming Civil rights Athletes Zemin Jiang Table 3 Ten most similar clusters chosen with TFIDF correct and errorful transcripts 
general Broadcast News model from 222 to 188 a 15 reduction Next we built two topic trees The first tree was built automatically by merging the 5883 topic leaf clusters iteratively to the root At each iteration the node with the fewest words was chosen to be merged with its most similar node which was chosen by the TFIDF classifier The second tree was built in the same way as the first except that if the similarity value between the smallest cluster and its most similar cluster was below a threshold of 0 3 the smallest cluster was orphaned or linked directly to the root The orphan tree did not force a merge if no good match existed whereas the automatic tree forced a merge at each iteration The 5883 leaf clusters range in token size from 393 to 6 234 183 Two hundred thirty of the 5883 leaf clusters contain less than one thousand word tokens In cases where so few tokens are available adaptation may benefit from using more data In an effort to verify this hypothesis three development set stories and one of the 
most similar leaves for each story were selected For each of the three story leaf pairs language models were built at various nodes along the path from leaf to root for both the automatic tree and the orphan tree Each model was interpolated with the 51k general model and the perplexity of the story was computed using two way cross validation In all cases the perplexity decreased or stayed the same when a model built from a node with more data than the leaf cluster was used as 9 Naive Bayes Classifier Correct Transcript Errorful Transcript 45 WER Olympic Games Barcelona 1992 Olympic Games Olympic Games Olympic Games Barcelona 1992 Drug testing China Athletes Athletics Sports Drug testing Gymnastics Olympic Games Sydney 2000 Louganis Greg Gymnastics Athletics Running races Diving Athletes Olympic Games Seoul 1988 Wu Harry Table 4 Ten most similar clusters chosen with naive Bayes classifier correct and errorful transcripts shown in Tables 6 and 7 For example interpolating a leaf cluster language model built 
from 35 680 tokens with the general language model results in a perplexity of 219 whereas interpolating a language model built from a node located higher up the path with 100 500 tokens with the general language model results in a perplexity of 210 This limited example demonstrates that at least in some cases when interpolating only one leaf with the general language model per story adding additional relevant text is helpful Topic tree adaptation was tested on the development set stories by setting token cutoffs In all cases twenty leaf clusters were considered per story For both trees General model 222 Leaves TFIDF Bayes 5 193 193 10 191 189 20 189 188 Table 5 Development set perplexity leaves only 10 Tokens 13445 25353 64820 100500 574818 Root Paths in automatic tree PP Tokens PP Tokens 233 266125 201 35680 229 300170 200 100500 225 451893 202 574818 227 1002910 223 264 Root 220 Root PP 219 210 226 233 272 Table 6 Perplexity variation moving up automatic tree paths from leaf to root Paths in orphan tree 
Tokens PP Tokens PP Tokens PP 13445 233 266125 201 35680 219 25353 229 305562 201 96495 210 60815 225 333591 202 96495 226 Root 264 Root 220 Root 272 Table 7 Perplexity variation moving up orphan tree paths from leaf to root automatic and orphan whenever a leaf cluster was chosen for interpolation the topic model was built from the lowest node in the path from leaf to root that had at least as many word tokens as the predetermined threshold These nodes are referred to as active nodes in the discussion below Thresholds of 50k and 200k were set Occasionally the paths for similar leaves merge and in these cases less than twenty models were interpolated for those stories The general broadcast news model i e the model at the root of the tree was always interpolated with the topic models In the case of the orphan tree sometimes the node just below the root in an active path had fewer tokens than the threshold leaving only the root node with enough tokens for interpolation Therefore two orphan tree scenarios were 
evaluated in the first all paths that assigned the root as the active node because all other 11 nodes in the path had fewer tokens than the threshold were left out completely meaning that the selected leaf did not contribute a model for interpolation In the second scenario designated by leaves all paths that assigned the root as the active node built the topic model from the leaf of the path even though there were fewer tokens in the leaf node than the threshold Perplexity results for these cases are shown in Tables 8 and 9 In all cases interpolating topic models results in a decrease in perplexity over using only the general trigram model Generally none of the tree scenarios works as well as interpolating only the leaves except for the Bayes orphan tree leaves cases which perform as well as the twenty Bayes leaves General model 222 Token thresh TFIDF Bayes Leaves only 189 188 50k 191 189 200k 192 191 Table 8 Development set perplexity automatic tree General model 222 Token thresh TFIDF Bayes Leaves only 189 
188 50k 191 189 50k leaves 190 188 200k 196 192 200k leaves 191 188 Table 9 Development set perplexity orphan tree 3 3 N best Rescoring Next we wanted to see if using these models to rescore N best lists would lead to a reduction in recognition WER Two interpolation weighting schemes were 12 tested In the first indicated by min PP the cluster language models and the 51k general language model were interpolated with weights obtained by minimizing the perplexity of the errorful first pass decoder hypothesis The second interpolation scheme uniform assigned a weight of 0 55 to the general 51k language model and uniform interpolation weights to the remaining topic models Rescoring consisted of using the original acoustic score the new language model score and a word insertion penalty For the development set N 500 and the for the evaluation set N 200 Filled pauses were predicted from manually set unigram probabilities 12 For the development set the first pass WER with no rescoring was 40 2 The lowest N best WER 
found by using the reference transcripts to choose the N best hypotheses with the lowest error was 34 6 The lowest N best WER represents an upper bound on the performance of N best rescoring Using just the 51k general language model to rescore results in a WER of 40 1 Language model score and insertion penalty weights were chosen by two way cross validation and the average weight values were used for evaluation set rescoring The evaluation N best lists were generated after two passes of the Sphinx III decoder Topic adaptation scenarios tested with rescoring include twenty TFIDF chosen leaves twenty Bayes chosen leaves and the Bayes orphan leaves topic tree with a token threshold of 200k Rescoring results are shown in Tables 10 and 11 Condition No topic adaptation Lowest N best WER General trigram TFIDF leaves min PP TFIDF leaves uniform Bayes leaves min PP Bayes leaves uniform Bayes orphan tree 200k leaves min PP Bayes orphan tree 200k leaves uniform WER 40 2 34 6 40 1 39 6 39 7 39 5 39 5 39 6 39 6 Table 10 
Development set word error rate using different language scores For both the evaluation and development sets there is no large WER difference between using uniform model interpolation weights or choosing weights by 13 Condition 2nd pass decoder output TFIDF leaves min PP TFIDF leaves uniform Bayes leaves min PP Bayes leaves uniform Bayes orphan tree 200k leaves min PP Bayes orphan tree 200k leaves uniform WER 35 5 35 3 35 5 35 4 35 4 35 3 35 5 Table 11 Evaluation set word error rate minimizing perplexity Rescoring the N best lists with the topic score from the interpolation of Bayes chosen leaves results in the greatest decrease in WER over the original 1st pass transcription no adaptation on the development set In this case the error rate drops from 40 2 to 39 5 Adaptation with either the TFIDFchosen leaves or the orphan tree lowers the WER to 39 6 However none of the topic scores results in a significant improvement in WER on the evaluation set Adaptation on the evaluation set with Bayes chosen leaves 
results in only a 0 1 decrease in WER For both the development and evaluation sets rescoring with a Kneser Ney smoothed general trigram model as opposed to our Good Turing smoothed general model results in a lower WER than the topic models 12 The Kneser Ney model results in a WER of 39 4 on the development set and 34 9 on the evaluation set Therefore while topic adaptation does result in slightly better WERs than no adaptation future work in topic adaptation must include better smoothing techniques for models built from small amounts of training data 4 Conclusion Large scale finely tuned topic adaptation is possible and does result in a decrease in perplexity and a slight decrease in WER in the Broadcast News domain Choosing the 20 most topic similar clusters for an individual story from among 5883 candidates and interpolating models built from these clusters results in a 15 decrease in perplexity over a general Broadcast News model even when the word 14 error rate of the story hypothesis used for topic 
detection is quite high Having many candidate clusters permits fine topic distinction and the possibility of mixing of topics in a way that might not have been previously seen in the training data Furthermore the semantic landscape of Broadcast News has been mapped out in two different topic trees Future work may find these structures helpful in more complex topic detection and adaptation systems 5 Acknowledgements We would like to thank Richard Schwartz Yiming Yang Stanley Chen and Bin Zhou for their contributions to this work References 1 B Carlson Unsupervised topic clustering of switchboard speech messages In Proceedings of ICASSP 96 pages 15 7 Slava M Katz Estimation of probabilities from sparse data for the language model component of a speech recognizer IEEE Transactions on Acoustics Speech and Signal Processing ASSP 35 3 16 
44	a	Natural Language Engineering 1 3 289 307 289 A hierarchical Dirichlet language model DAVID J C MACKAY Cavendish Laboratory Cambridge CB3 OHE UK email mackaySmrao cam a c uk LINDA C BAUMAN PETOf Department of Computer Science University of Toronto Canada email petoacs toronto edu Received 16 February 1995 Abstract We discuss a hierarchical probabilistic model whose predictions are similar to those of the popular language modelling procedure known as smoothing A number of interesting differences from smoothing emerge The insights gained from a probabilistic view of this problem point towards new directions for language modelling The ideas of this paper are also applicable to other problems such as the modelling of triphomes in speech and DNA and protein sequences in molecular biology The new algorithm is compared with smoothing on a two million word corpus The methods prove to be about equally accurate with the hierarchical model using fewer computational resources 1 Introduction Speech recognition and 
automatic translation both depend upon a language model that assigns probabilities to word sequences The automatic translation system implemented at IBM used a crude trigram model of language with impressive results Brown DellaPietra DellaPietra and Mercer 1993 Similar language models are also used in speech recognition systems Bahl Jelinek and Mercer 1983 Jelinek and Mercer 1980 Trigram models are often implemented using a particular kludge involving smoothing their predictions with the predictions of better determined bigram and monogram models the smoothing coefficients being determined by deleted interpolation Jelinek and Mercer 1980 Bahl Brown de Souza Mercer and Nahamoo 1991 Another generally used language model employs a similar procedure known as backing off Katz 1987 Text compression is a similar prediction task in which character sequences are to be predicted adaptively or otherwise In text compression the smoothing technique t Present address c o P O Box 3398 Cambridge Ontario N3H 4T3 Canada 290 D 
J C MacKay and L C Bauman Peto is known as blending and is used to combine the predictions obtained using contexts of different orders Bell Cleary and Witten 1990 This paper s aim is to reverse engineer the underlying model which gives a probabilistic meaning to smoothing allowing it to be better understood objectively tuned and sensibly modified The objective is not to create a rival language model but rather to demonstrate the Bayesian approach to language modelling and show that it is feasible For simplicity this paper will pretend that the language model is simply a bigram model since the key issues can be addressed by studying the smoothing of bigram statistics This paper assumes throughout that a bigram model i e a Markov process is an appropriate language model and discusses optimal inference subject to that assumption 1 1 The bigram language model with smoothing To develop a predictive model for language a string of T words D wi w2 wT is observed and the marginal and conditional frequencies are 
observed We define the marginal count F to be the number of times that word i occurs and the conditional count Fy to be the number of times that word j is immediately followed by word f We are ignoring the option of grouping words by a common root and other complications not central to the concept of smoothing Given these counts estimators of the marginal probability of word i and of the conditional probability of word i following word j are Thus the noisy bigram statistics are smoothed by the better determined monogram model s predictions A cross validation procedure called deleted interpolation is used to set X Jelinek and Mercer 1980 Bahl et al 1991 This involves dividing the data into a number of blocks computing predictions for each block using the other blocks as training data and adjusting k to optimize predictive performance It has been found that better predictions can be obtained if contexts W with similar values of W are grouped together with a separate k for each group determined by deleted 
interpolation Hierarchical Dirichlet language model 291 In text compression blending combines together the predictions of different models in a manner similar to equation 1 The parameters equivalent to X are not adapted but are fixed by the a priori choice of an escape mechanism According to Bell et al 1990 there can be no theoretical justification for choosing any particular escape mechanism We would agree that it is not possible to make language models without making a priori assumptions but we argue that it is possible within a hierarchical model effectively to determine the smoothing parameters a posteriori from the data 1 2 Any rational predictive procedure can be made Bayesian The smoothing procedure sounds sensible but slightly ad hoc Since rational inference can always be mapped onto probabilities Cox 1946 the aim of this paper is to discover what implicit probabilistic model the above procedure can be related to The smoothing formula and deleted interpolation were originally conceived as a way of 
combining together the predictions of different models But in this paper we will define a single hierarchical model with a non trivial Dirichlet prior which gives predictive distributions similar to 1 including adaptive expressions for the weighting coefficients equivalent to X However various interesting differences will emerge highlighting problems with equation 1 2 An explicit model using Dirichlet priors The heart of a bigram model is a conditional distribution P w i w _i j described by W W independent parameters where W is the number of words in the language W possible conditioning terms on the right hand side for each of which a probability distribution with W 1 independent parameters is specified These parameters will be denoted by Q with P w i w _i j q y Q is a W x W transition probability matrix A single row of Q the probability vector for transitions from state j is denoted by q Alternative ways of parameterizing the model might be defined using for example the marginal word probabilities P w and 
the joint probabilities P n w _i However the conditional probability parameterization Q is chosen because it is the natural representation of a Markov process the marginal distribution P w is not independent but is a deterministic function of the conditional probability matrix Q namely P w is the principal eigenvector of Q The parameters Q are never perfectly known and our uncertainty about their values can be represented by a probability distribution over possible Qs 2 1 The inferences we will make A model jV is a specification of the model parameters the way that the probability of the data depends on those parameters and a prior probability distribution on those parameters Given a model 3f there are two inferences we will be interested in making Both these inferences can be made mechanically using the rules of probability theory 292 D J C MacKay and L C Bauman Peto A Infer the parameters given the data We do this by Bayes theorem which gives the probability of the parameters Q given the data D in terms of 
the likelihood function P D Q J4 and the prior distribution P Q JP The normalizing constant is given by integrating the numerator over Q 3 P D tf J P D Q 3f P Q je dkQ where k is the dimensionality of Q B Predict the next word in a given context To obtain the probability of wt given wt and the data we use the sum rule of probability P A C P A B C P B C dB to marginalize over the unknown parameters Q 4 P wt wt UD Jfr J P Wt W UQ D jr P Q D Jf dkQ 5 JqWtlWl P Q D je dkQ The distribution inside the integral P Q D JF depends upon the likelihood function and the prior as shown in equation 2 2 2 The likelihood function The likelihood function P D Q JF can be written down immediately independent of the assumptions which define the rest of the model We make the simplifying assumption that the first word of the data set is given a priori and is not to be predicted by the model The probability of the string of words is then the probability of the second word given the first times the probability of the third word 
given the second and so forth 6 We can rewrite this product by counting how often each variable q appears in the product This is given by the conditional count F y Thus 7 j i So given the assumed bigram model the conditional counts F contain all the relevant information that the data convey about Q 2 3 What prior Thus having defined the parameterization of the model Q the only question that remains before the two inferences above are fully defined is what is the prior over Hierarchical Dirichlet language model 293 QT In particular this paper examines the question what prior P Q 3 F would give us predictive distributions of the smoothed form I 2 4 A convenient family of priors Dirichlet distributions The Dirichlet distribution Antoniak 1974 for a probability vector p with components is parameterized by a measure u a vector with all coefficients u 0 which we will write here as u am where m is a normalized measure over the components 52m 1 a n d a is a positive scalar 8 P p am _ _ _ Dirichlet p am The function 
5 x is the Dirac delta function which simply restricts the distribution to the simplex such that p is normalized i e J2 p 1 The normalizing constant of the Dirichlet distribution is 9 The vector m is the mean of the probability distribution 10 I Dirichlet p am p d p m The role of a can be characterized in two ways First the parameter a measures the sharpness of the distribution it measures how different we expect typical samples p from the distribution to be from the mean m A large value of a produces a distribution over p which is sharply peaked around m The effect of a can be visualized by drawing a typical sample from the distribution Dirichlet p am with m set to the uniform vector m I I and making a Zipf plot that is a ranked plot of the values of the components p It is traditional to plot both p vertical axis and the rank horizontal axis on logarithmic scales so that power law relationships appear as straight lines Figure 1 shows these plots for a single sample from ensembles with 100 and 1000 and with 
a from 0 1 to 1000 For large a the plot is shallow with many components having similar values For small a typically one component p receives an overwhelming share of the probability and of the probability that remains to be shared among the other components another component pi receives a similarly large share In the limit as a goes to zero the plot tends to an increasingly steep power law Second we can characterize the role of a in terms of the predictive distribution that results when we observe samples from p and obtain counts F F if I 1 P F am 294 D J C MacKay and L C Bauman Peto I 100 0 0001 100 0 0001 100 1000 Fig 1 Zipf plots for random samples from Dirichlet distributions with various values of a 0 1 1000 For each given and a samples from a standard gamma distribution were generated with shape parameter a I and normalized to give a sample p from the Dirichlet distribution The Zipf plot shows the probabilities p ranked by magnitude versus their rank 13 14 P F ccm Z am Dirichlet p F am The predictive 
distribution given the data F is then 15 P i F am Dirichlet p F am p d p F am Hierarchical Dirichlet language model 295 Notice that the term ami appears as an effective initial count in bin i The value of a defines the number of samples from p that are required in order that the data dominate over the prior in subsequent predictions If a Yl Fitnen P W am mil if a Y Fi then P i F am F f 16 1M p F am pm Z F am r n nF T a nE F a n The important role of the evidence also known as the marginalized likelihood will become clear shortly Additional useful formulae and approximations are found in Appendix A 2 5 Definition of the hierarchical model 34 D We now define the prior of a hierarchical model that we denote J D D for Dirichlet It is called a hierarchical model because as well as containing unknown parameters Q which place a probability distribution on data it contains unknown hyperparameters which define a probability distribution over the parameters Q To obtain predictions similar to those of the smoothing 
equation 1 we must assign a coupled prior to the parameters Q that is a prior under which learning the probability vector for one context q7 gives us information about what the probability vectors q in other contexts might be We introduce an unknown measure on the words u am and define a separable prior given am on the vectors q that make up Q 17 F Q am jfD J Dirichlet q m j We produce a dependence between the vectors q by putting an uninformative prior P am on the measure am to be precise a flat prior on m and a broad gamma prior over a The prior on Q defined by this hierarchical model is then 18 P Q 3tfD j Dirichlet q m F am d am J j When we use this hierarchical model we can effectively find out from the data what the measure should be as we will show in section 3 If we have additional prior knowledge about the language such that we expect specific structure in the measure then we could define a more informative prior P am which should further improve the model s predictive performance In this paper we 
aim simply to demonstrate a minimal data driven Bayesian model where the emphasis is on getting information from the data rather than adding detailed human knowledge The hierarchical model that we have described puts a qualitative prior over the parameters Q qualitative in that the form of a Dirichlet distribution is specified but without specifying quantitative values for the hyperparameters am this is effectively turned into a quantitative prior by consulting the data This general approach is sometimes called empirical Bayes Our method is distinguished from many empirical Bayes prescriptions in that a we use Bayesian inference to control the hyperparameters 296 D J C MacKay and L C Bauman Peto b we motivate this procedure as an approximation to the ideal predictive Bayesian approach 2 6 Inference and prediction using the hierarchical Dirichlet model It is convenient to distinguish two levels of inference We are interested in the plausible values of at level 1 the parameters Q qiy and at level 2 the 
hyperparameters am We use the results of section 2 4 2 6 1 Level 1 inference At level 1 we assume we know m and a It is then easy to infer a posterior distribution for Q and get a predictive distribution By Bayes theorem the posterior distribution is This distribution is separable into a product over contexts j because both the prior P Q am 3 i D and the likelihood P D Q 3fD are separable 20 P Q D am J D J P D am j o The posterior distribution of each conditional probability vector is simply another Dirichlet distribution 21 P D am JfD oc Jq j i This posterior can be used for prediction P i j where fiy F y Fj and 24 X FTT Note that in contrast to X in equation 1 this quantity kj is not constant It varies inversely with the frequency of the given context j Practitioners of deleted interpolation have as mentioned in the introduction found it useful to divide the contexts j into different groups according to their frequency Fj with a separate X for each group Each X has to be optimized using deleted 
interpolation Here simply by turning the handle of Bayesian inference we have produced a smoothing prescription which we anticipate eliminates this need to group contexts by their frequency The appropriate variation of X with F is automatically present in 24 Not that this is a new idea the blending method in text compression Bell et al 1990 uses the same variation with Fj Hierarchical Dirichlet language model 2 6 2 Level 2 inference 297 At the second level of inference we infer the hyperparameters given the data The posterior distribution of am is by Bayes theorem 25 The data dependent term P D xm Jfo is the normalizing constant from the first level of inference 19 We call it the evidence for am We will proceed by finding the maximum am MP of the posterior distribution P am D J D The ideal Bayesian method would put a proper prior on the hyperparameters and marginalize over them when making predictions 26 P i j D JtrD jP am D J D P i j D am jtTD dw am However if as we expect the posterior distribution P am D 
D is sharply peaked in am so that it is effectively a delta function in 26 relative to P i j D am 3 PD then we may approximate 27 P i j D JfD P i j D am MP So instead of marginalizing over the hyperparameters we optimize them the optimization is computationally more convenient and often gives predictive distributions that are indistinguishable from the true predictive distribution MacKay 1995c We are assuming a noninformative prior P am fD so the posterior probability maximum am MP is found by maximizing the evidence F D am Jfo If the accuracy of this approximation is doubted in any specific case then the correct marginalization over the hyperparameters can be performed by for example Monte Carlo methods see for example Neal 1992 1993 and West 1992 We note in passing that the mode of a posterior probability distribution does not have any fundamental status in Bayesian inference and its location can be changed arbitrarily by a non linear reparameterization The maximum of the evidence on the other hand is 
invariant under reparameterization Now the question is will mMP turn out equal to the marginal statistics If it did then this Bayesian procedure would reproduce the predictions of smoothing The answer is no the optimal measure is different This will be discussed first using a toy example to persuade the reader that equation 1 is unsatisfactory The mathematics of the Bayesian optimization of am will then be worked out in detail Example A data set for which equation 1 is evidently unsatisfactory Imagine you see that the language you see has you see a frequently occurring couplet you see you see in which the second word of the couplet see follows the first word you with very high probability you see Then the marginal statistics you see are going to become hugely dominated you see by the words you and see with equal frequency you see Now given this data set what is the conditional probability of each word if and when a novel context occurs In particular what are the probabilities of the words you and see Where 
the Dirichlet model 23 would assign probabilities mTM the 298 D J C MacKay and L C Bauman Peto smoothing formula 1 would assign probabilities proportional to So using the smoothing formula the predictions P you novel and P see novel would come out equal since you and see have both occurred equally often 11 times so far But is this intuitively reasonable You evidently has a relatively high probability in any context whereas see only has a high frequency because it has a high probability of following you Thus intuitively P you novel should be greater than P see novel We would like the probability of a word to relate not to its raw frequency but rather to the number of contexts in which it has occurred We will see shortly that mMP does exactly this It should be emphasized that this failure of the smoothing formula is not because of any inadequacy of the bigram model a Markov process can easily capture the couplet in the data set above In text compression the method known as update exclusion Bell et al 1990 
avoids the problem described above 3 Inferring Dirichlet hyperparameters 3 1 The dice factory An analogy may be useful to describe the inferences we will now make Imagine that a factory produces biased sided dice We might model the probability vector q of a single die as coming from a Dirichlet prior with unknown hyperparameters u am that characterize the factory The data are the outcomes of rolls of J dice labelled by j Each die j is rolled a number of times Fj and we are told the counts of the outcomes F y which give us imperfect information about the parameters Q jf Given these measurements our task is to infer the hyperparameters u am of the factory in order to make better predictions about future rolls of individual dice This problem is identical to the language modelling problem where the number of dice J and the number of classes are both equal to the number of words W We can imagine the language being generated by a dice rolling procedure in which the outcome of roll t determines which die is rolled 
at time t 1 Other inference problems in genome modelling for example can also be related to the inference of models for dice factories 3 2 The evidence for am The posterior probability of am is proportional to P D am JJj f F y am which we obtain from equation 16 28 We now work in terms of w am To find the most probable u am we differentiate using digamma functions defined by F x d log T x dx The motivation for evaluating the gradient is that the optimization of a continuous function of many Hierarchical Dirichlet language model variables u is easiest if the gradient of the function is calculated 29 log P D a y V F j 299 This gradient may be fed into any optimization program to find uMP A conjugate gradients algorithm Press Flannery Teukolsky and Vetterling 1988 for example easily finds the optimum However we can obtain further insight and derive an explicit optimization algorithm by making some approximations 3 3 Inferring u am approximations for w 1 and a 1 We now assume that a 1 and u 1 to derive an 
algorithm specialized for the parameter regimes expected in language modelling We expect a to be greater than 1 because a corresponds to the rough number of data points needed to overwhelm the Dirichlet prior How many times Fj do we expect we need to see context j for us to have learnt the principal properties of qy If we have only seen a context one or two times then we intuitively expect our prior knowledge of the high frequency of the word the for example to still be important But once we have seen a context a few tens or hundreds of times we expect that the observed counts will differ significantly from the default distribution And in preliminary experiments we did find that the most probable a ranged from about 1 4 to about 60 Now since the m s sum to one a typical m will be l size of vocabulary therefore u am can be expected to be less than 1 We use the relationship vP x 1 tiU 1 Ui l F T Fj j 2 u 2 u T 1 Ui u The number of terms in this sum is F Assuming M is smaller than 1 we can approximate this sum 
for Ft j 1 by 31 1 u 53 l 1 2 Ui 2 2 l I 2 Approximating the other terms a jrmax 32 33 G 2 pmAX H J2Nfi f l 2 2 300 and define D J C MacKay and L C Bauman Peto then the optimal hyperparameters u satisfy the implicit equation 35 p K a d K MP TV r This defines a one dimensional problem to find the a such that the u given by 35 satisfy M a This optimal a can be found by a bracketing procedure or by a reestimation procedure in which we alternately use 35 to set u given a and then set a a We use this algorithm in section 4 3 4 Comments thus 4 Application to a small corpus We conducted an experiment to compare deleted interpolation with the new method empirically Peto 1994 We used each algorithm to construct an alternative model from the training corpus We then compared the predictive accuracy of the algorithms by evaluating the perplexity of the test data under each of the competing models the better the model the smaller the perplexity Perplexity is defined as 2 H G where H Q P is the cross entropy between the 
unknown true model Q and the assumed model P For the case of two distributions Hierarchical Dirichlet language model 301 over alternatives i H Q P 51 Qi log2 A For the bigram models we use and a large enough test corpus the perplexity of the test corpus can be approximated by 36 i r rr Perplexity m P w t w _ where T is the number of words in the corpus see Brown et al 1992 The training and test corpora were taken from the English portion of Gale and Church s 1991 sentence aligned version of the Canadian Hansard the proceedings of the Canadian Parliament This text had already been separated into sentences and stripped of titles formatting codes and speaker identifiers We removed sentence numbers and added sentence begin and sentence end markers In keeping with common practice for experiments of this type we split off punctuation and suffixes beginning with apostrophes from the words they followed making them separate tokens In order to reduce the total number of types in the vocabulary we also replaced each 
number by the special token The resulting sentences were distributed into nine blocks of about 1 7 Mbytes each with consecutive sentences going to different blocks This interleaving of the sentences performs the important function of homogenizing the data otherwise significant differences in token frequencies could result from different portions of the corpus as different topics were being discussed The first six blocks were used for training data about 2 million words and the test data were extracted from the remaining three blocks We prepared three different test samples from the test data Because the algorithms being compared only assign probabilities to bigrams composed of tokens that appear in the training data they have no way of dealing with previously unseen tokens we chose not to address the important zero frequency problem in this study Therefore we removed all sentences that contained a token that did not occur in the training data This left 14 393 sentences about 260 000 tokens in Sample 1 Next 
recognizing that the Hansard contains many conventional phrases and sentences that might skew the results of the experiment we removed from Sample 1 all sentences that were duplicated in either the test data or the training data This left 12 000 sentences about 243 000 tokens in Sample 2 Finally to test whether the sample was large enough for the approximation of perplexity in equation 36 to hold we pseudorandomly chose half the sentences in Sample 2 to become Sample 3 6000 sentences about 116 000 tokens The two algorithms have different numbers of parameters to be optimized For the deleted interpolation method the number of AS is chosen subjectively We ran the deleted interpolation method with 3 15 and 150 As to judge the effect of this choice In the hierarchical model presented in this paper there is one hyperparameter u for each type in the training data vocabulary The experiment was conducted as follows First raw frequencies and relative frequencies of tokens and bigrams were obtained from the training 
data as a whole Next the most probable values for the parameters of each model were solved for iteratively For the Dirichlet model this meant solving the simultaneous equations 302 D J C MacKay and L C Bauman Peto Table 1 Perplexities of the three test data samples under the different models T number of tokens in sample Algorithm Deleted interpolation Sample 1 2 3 T 1000 260 243 116 3 s 89 57 15 As 79 60 88 47 91 82 150 As 88 91 79 90 89 06 92 28 Dirichlet given by equation 35 to obtain uMP For the smoothing method separate frequencies were first calculated for each block and then the A s were obtained using deleted interpolation Jelinek and Mercer 1980 The optimization was halted when on average each parameter of the model had converged to eight decimal places The optimized parameter values for each model were then used to compute predictive probabilities P i j for each bigram in the test data Finally the perplexity of each of the three test data samples was evaluated using each of the models and the 
results were compared The perplexity of each test sample under each model is given in Table 1 For all three samples the perplexities under the deleted interpolation model and under the Dirichlet model are nearly the same For Sample 2 three deleted interpolation models having different numbers of As were tested The effect of altering the number of As was very small When 150 As were used we found that the values of A decreased with the frequency Fj roughly as expected from equation 24 Finally the perplexity results for the smaller Sample 3 are close to the corresponding results for Sample 2 This suggests that Sample 2 is large enough to provide a meaningful comparison of models The fact that the perplexity results for Sample 1 are lower than those of Sample 2 probably reflects the high degree of regularity of the extra conventional data more than the small increase in test data size With regard to resource use the new algorithm has an advantage The number of iterations required for each algorithm to converge 
was comparable However a single iteration of our Dirichlet model requires time linear in the size of the vocabulary while an iteration of deleted interpolation requires time linear in the size of the training corpus The larger the training corpus the more significant would be this advantage of the Dirichlet model Also deleted interpolation requires more memory because it keeps separate count and frequency data for each block of the training corpus In our implementation there were six such blocks We have not made a direct comparison with the backing off algorithm because Katz s 1987 results indicate that backing off is indistinguishable in performance from deleted interpolation on a similar bigram modelling task Hierarchical Dirichlet language model 5 Discussion 303 The exercise of creating a Bayesian version of the smoothing procedure has given several benefits 1 The Dirichlet model is not identical to smoothing the differences are intuitively reasonable 2 The Dirichlet model does away with cross validation 
and therefore makes full use of the data while requiring fewer computational resources We would like to distinguish the general Bayesian method from the particular hierarchical Bayesian model discussed in this paper and the computational approximations used to implement it We emphatically do not view the presented algorithm as the Bayesian answer to language modelling nor do we claim that this particular algorithm will necessarily be superior to deleted interpolation in any given application There are many possible Bayesian language models and the one we have studied is virtually the simplest possible We now discuss other possible models 5 1 Generalizations Language modelling has here been viewed as the modelling of a set of probability vectors q drawn from a coupled density over the simplex the simplex is the space of probability vectors q satisfying q 0 and J2 a 1 w i t n o n e probability vector qi for each context j In this paper s model the context j is simply the previous word and the density over the 
simplex is a single Dirichlet distribution parameterized by am The two simplest modifications to this model are to change the functional form of the density over the simplex and to change the definition of a context An alternative density over probabilities to the Dirichlet distribution is the entropic prior Skilling 1989 Gull 1989 37 The entropic prior like the Dirichlet prior characterizes a language by a single mean and spread of a distribution of conditional probabilities q y for all contexts j Recent work at IBM on maximum entropy language modelling S V Delia Pietra personal communication might be interpreted in terms of an entropic prior This interpretation could then be used to obtain a Bayesian prescription for a and m as this paper has done for the Dirichlet model A more interesting model might assert that there are different types of context such that for all contexts of the same type the conditional probabilities q are similar If we do not know a priori what the type of each context is then this 
model is a mixture model A mixture model Jf M defines a density over q as a weighted combination of C independently parameterized simple distributions where each mixture component c 1 C might be a Dirichlet or entropic distribution Various algorithms can be used to implement mixture models both Monte Carlo methods Neal 1992 and Gaussian approximations Hanson Stutz and Cheeseman 1991 304 D J C MacKay and L C Bauman Peto Mixture models are applied to the modelling of amino acid probabilities in Mackay 1995d Alternatively a model might define the context to be the last two words with the type of the context being defined by the most recent word With a coupled prior for the context hyperparameters this model would give predictions similar to those of the smoothed trigram language model The mixture model is also able to capture the same clustered structure as the hierarchical trigram model but has the potential advantage that it can discover other relationships between the contexts for example if it happens to be 
the case that the last word but one is sometimes more important than the last word in characterizing q then the more flexible mixture model can capture this structure in the data Finally we might believe that the type of a context is more naturally described with a componential structure G Hinton personal communication see also Williams and Hinton 1991 Imagine for example that any context is either legalistic or not and that in any context either a verb is likely or is unlikely A traditional mixture model would have to use four mixture components to capture these two sources of variation and in general we would need a number of mixture components exponential in the dimensionality of the context space whereas we might believe that the number of parameters needed to describe the probability distribution ought only to be linear in that number of dimensions This motivates the development of componential models a type of latent variable model in which the type of a context is represented with several continuous 
or discrete dimensions A componential model is described and applied to the modelling of amino acid probabilities in Mackay 1995d It has been generalized to the modelling of joint distributions of multiple amino acids in MacKay 1995a 1995b 5 2 Relationship to previous empirical Bayes approaches An approach similar in spirit to the one advocated in this paper has been described by Nadas 1984 His empirical Bayes approach also interprets the smoothing formula 1 in terms of a prior whose hyperparameters are determined from the data In contrast to the present paper however Nadas at several points chooses estimators in an arbitrary way in the fully Bayesian approach there are no choices only mechanistic inferences Another weakness of Nadas paper is that the prior that is considered is a technically inappropriate prior that neglects normalization of the probability vectors qi The technique of smoothing is also used in modelling with classification trees and this literature contains a similar paper in which an 
empirical Bayes approach is used Buntine 1992 As above this approach is compromised by the invocation of ad hoc estimators instead of the derivation of inferences An estimator for m is given that is not in fact the most probable m No objective procedure for setting a is given Hierarchical Dirichlet language model 305 A fully Bayesian approach to the hyperparameter a has been given by West 1992 along with a Monte Carlo algorithm for Gibbs sampling of this hyperparameter The advantages of a fully Bayesian attitude to data modelling are firstly that one is forced to make all one s assumptions explicit and secondly that once the model is defined all inferences and predictions are mechanically defined by the rules of probability theory Acknowledgements DJCM thanks Peter Brown Radford Neal Geoff Hinton Phil Woodland David Robinson Martin Oldfield Steve Gull John Bridle and Graeme Mitchison for helpful discussions and the Isaac Newton Institute for hospitality LCBP thanks Bill Gale Radford Neal and Peter Brown for 
helpful discussions Appendix A The Gamma function and Digamma function The Gamma function is defined by T x J 0 du M X e u for x 0 In general T x 1 xr x and for integer arguments T x 1 x The digamma function is defined x logr x log x And for small x for practical purposes 0 x 0 5 40 41 log T x log i yex O x2 where ye is Euler s constant The digamma function satisfies the following recurrence relation exactly 42 Formula for a more general algorithm The algorithm presented in this paper is based on series expansions of u and is not valid for all u The following formula although it is not part of a series expansion gives an approximation to the difference This approximation is useful for gradient based optimization of Dirichlet distributions MacKay 1995d 306 D J C MacKay and L C Bauman Peto References Antoniak C E 1974 Mixtures of Dirichlet processes with applications to nonparametric problems Annals of Statistics 2 1152 1174 Bahl L R Brown P de Souza P Mercer R L and Nahamoo D 1991 A fast algorithm for deleted 
interpolation Proceedings of Eurospeech 91 Genoa pp 1209 1212 Bahl L R Jelinek F and Mercer R L 1983 A maximum likelihood approach to continuous speech recognition IEEE Trans PAMI 5 2 179 190 Bell T C Cleary J G and Witten I H 1990 Text compression Englewood Cliffs NJ Prentice Hall Brown P F Delia Pietra S A Delia Pietra V J Lai J C and Mercer R L 1992 An estimate of an upper bound for the entropy of English Computational Linguistics 18 1 31 40 Brown P F Delia Pietra S A Delia Pietra V J and Mercer R L 1993 The mathematics of statistical machine translation Parameter estimation Computational Linguistics 19 2 263 311 Buntine W 1992 Learning classification trees Statistics and Computing 2 63 73 Cox R 1946 Probability frequency and reasonable expectation Am J Physics 14 1 13 Gale W and Church K 1991 A program for aligning sentences in bilingual corpora Proceedings of 29th Annual Meeting of the ACL pp 177 184 Gull S F 1989 Developments in maximum entropy data analysis In Maximum Entropy and Bayesian Methods 
Cambridge 1988 J Skilling ed pp 53 71 Dordrecht Kluwer Hanson R Stutz J and Cheeseman P 1991 Bayesian classification with correlation and inheritance Proceedings of the 12th International Joint Conference on Artificial Intelligence Sydney Australia volume 2 pp 692 698 San Mateo CA Morgan Kaufmann Jelinek F and Mercer R L 1980 Interpolated estimation of Markov source parameters from sparse data In Pattern Recognition in Practice E S Gelsema and L N Kanal eds pp 381 402 Amsterdam North Holland Katz S M 1987 Estimation of probabilities from sparse data for the language model component of a speech recognizer IEEE Transactions on Acoustics Speech and Signal Processing 35 3 400 401 MacKay D J C 1995a Bayesian neural networks and density networks Nuclear Instruments and Methods in Physics Research Section A 354 1 73 80 MacKay D J C 1995b Density networks and protein modelling In Maximum Entropy and Bayesian Methods Cambridge 1994 J Skilling and S Sibisi eds Dordrecht Kluwer MacKay D J C 1995c Hyperparameters 
Optimize or integrate out In Maximum Entropy and Bayesian Methods Santa Barbara 1993 G Heidbreder ed Dordrecht Kluwer MacKay D J C 1995d Models for dice factories and amino acid probability vectors In preparation Nadas A 1984 Estimation of probabilities in the language model of the IBM speech recognition system IEEE Trans ASSP 32 4 859 861 Neal R M 1992 Bayesian mixture modelling In Maximum Entropy and Bayesian Methods Seattle 1991 C Smith G Erickson and P Neudorfer eds pp 197 211 Dordrecht Kluwer Neal R M 1993 Probabilistic inference using Markov chain Monte Carlo methods Technical Report CRG TR 93 1 Dept of Computer Science University of Toronto Peto L B 1994 A comparison of two smoothing methods for word bigram models Technical Report CSRI 304 Computer Systems Research Institute University of Toronto Press W Flannery B Teukolsky S A and Vetterhng W T 1988 Numerical Recipes in C Cambridge Cambridge University Press Skilling J 1989 Classic maximum entropy In Maximum Entropy and Bayesian Methods Cambridge 
1988 J Skilling ed Dordrecht Kluwer West M 1992 Hyperparameter estimation in Dirichlet process mixture models Working paper 92 A03 Duke Inst of Stats and Decision Sciences Hierarchical Dirichlet language model 307 Williams C K I and Hinton G E 1991 Mean field networks that learn to discriminate temporally distorted strings In Connectionist Models Proceedings of the 1990 Summer School D S Touretzky J L Elman and T J Sejnowski eds San Mateo CA Morgan Kaufmann 
45	a	Modeling Long Distance Dependence in Language Topic Mixtures vs Dynamic Cache Models R Iyer M Ostendorf rukmini mo bu edu Electrical and Computer Engineering Department Boston University Boston MA 02215 USA ABSTRACT In this paper we investigate a new statistical language model which captures topic related dependencies of words within and across sentences First we develop a sentence level mixture language model that takes advantage of the topic constraints in a sentence or article Second we introduce topic dependent dynamic cache adaptation techniques in the framework of the mixture model Experiments with the static or unadapted mixture model on the 1994 WSJ task indicated a 21 reduction in perplexity and a 3 4 improvement in recognition accuracy over a general n gram model The static mixture model also improved recognition performance over an adapted n gram model Mixture adaptation techniques contributed a further 14 reduction in perplexity and a small improvement in recognition accuracy Our approach to 
representing long term dependence attempts to address both task level and sentence level dependencies while using a simple model We investigate a sentence level mixture of m component language models 7 each of which can be identified with the n gram statistics of a specific topic or a broad class of sentences Though this approach is similar to those that use mixtures at the ngram level 8 9 it differs in the use of mixtures at the sentence level and automatic clustering to define the mixtures Specifically the probability of a word sequence w1 wT is given as P w1 wT m X k 1 k T 1 Y i 1 Pk wijwi 1 2 1 Introduction Statistical language models which model the probability of different word sequences are an integral part of state of the art speech recognizers The most commonly used statistical language modeling technique also referred to as n gram language modeling considers the word sequence w1 w2 wT to be a Markov process with probability where k are the mixture weights and Pk is the n gram model for the k th 
class The sentence level mixture can be used either as a static or dynamic model easily leveraging existing techniques for adaptive language modeling The recognition search cost of a sentence level mixture model is potentially high but in an N best rescoring framework the additional cost of the language model is minimal The general framework and mechanism for designing the mixture language model will be described in Section 2 including descriptions of automatic topic clustering and robust parameter estimation techniques Next Section 3 presents an extension of the mixture model to incorporate dynamic adaptation techniques Section 4 describes the experimental paradigm and results and finally Section 5 concludes with a discussion of possible extensions of mixture language models P w1 w2 wT T 1 Y i 1 P wi jwi 1 wi n 1 1 where w0 and wT 1 are sentence boundary markers and n refers to the order of the Markov process typically 2 or 3 a bigram or trigram language model respectively For notational simplicity in the 
discussion we use the bigram representation however all experiments in this paper use trigram models Although quite powerful given their simplicity n gram models are constrained in their inability to take advantage of dependencies longer than n One approach to overcome this limitation is to use dynamic cache language models 1 2 3 which model tasklevel dependencies by increasing the likelihood of a word given that it has been observed previously However cache models do not account for dependencies within a sentence Context free grammars 4 5 could account for grammatical dependencies within a sentence however it is costly to build task specific grammars Trigger language models 6 capture dependencies within a sentence but are computationally very expensive while training the models 2 Mixture Model Framework Training the model given in Equation 2 requires that we address two main issues automatic clustering to handle data not explicitly marked for topic dependence and robust parameter estimation Our solution to 
these problems is described below 2 1 Clustering Topics or natural groupings of data can be specified by hand if text labels are available or determined automatically which is the approach taken here since we used a corpus that does not have topic labels associated with the data Agglomerative clustering is used to partition the training data into the desired number of clusters topics 7 Clustering is performed at the article level to reduce computation relying on the assumption that an entire article comes from a single topic Starting with single article clusters clusters are progressively grouped by computing the similarity and grouping the most similar two clusters The similarity measure is based on the combination of inverse document frequencies 10 specifically to the j th topic in the pth iteration is p z ij P Pm yi jzj P p zj p p j 1 P yi jzj P zj p 5 Sij Nij 1 j Aw j jAi jjAj j w2Ai Aj X 3 where zj is the j th class and yi is the ith sentence The bigram probability for the j th topic model is then re 
computed as where jAi j is the number of unique words in article i jAw j is the number of articles containing the word w and ML j Pj wcjwb 1 j b Pj wcjwb b Pj wc where j b is the unigram back off mass 6 Nij r Ni Nj Ni Nj 4 j b is a normalization factor with Ni being the number of articles in cluster i The normalization factor is used to avoid the tendency for small clusters to group with one large cluster rather than with other small clusters An advantage of the inverse document frequency measure is that high frequency words such as function words are automatically discounted Pn i p n z i 1 bq ij q Pn ni bq i 1 Pn i p P Pn i p P P n z i 1 bq ij ij n i q i 1 nbq z q P i 1 7 nbq n represents the number of training sentences nibc represents the number of occurrences of the bigram wb wc in the ith senML wc jwb is given as tence and Pj Pn p i ij nbc ML i 1 z Pj wcjwb P P n ni z p q i 1 bq ij 8 2 2 Robust Parameter Estimation In parameter estimation two important issues are addressed First the initial topic 
dependent clusters may not be optimal for estimating the n gram parameters of the m component models in which case better results can be obtained by iterative re estimation Second we need to account for articles or sentences that do not belong to any of the topics as well as consider the sparse data problems caused by the fragmentation of the training data which we address by using double mixtures one at the sentence level and the other at the n gram level Iterative Topic Model Re Estimation Initial estimates for each of the component n gram models are based on the partitions of the training data obtained by the clustering procedure described earlier It is essential that some back off smoothing technique be used in the component models to account for n grams not observed in training here the Witten Bell technique 11 is used We iteratively re estimate these parameters since some sentences belonging to one topic according to the clustering metric might be more likely in another topic according to the n gram 
models Initially we developed a Viterbi style training technique that iteratively partitions the data by topic and re estimates the topicdependent parameters 7 Here we instead use the ExpectationMaximization EM algorithm where we assign the same sentence to different topics weighted by the likelihood of that particular topic The EM algorithm is complicated for language model training because the parameter estimates must incorporate some sort of backoff to avoid zero probabilities The simplicity of the Witten Bell back off scheme facilitates the integration of the back off directly into the EM algorithm as a constraint in re estimation In each EM iteration we first compute the likelihood of every sentence in the training corpus given each of the m topics For a bigram mixture model the likelihood that the ith training sentence belongs Equations 6 8 give the model parameter estimates for the next iteration The derivation of this solution is given in 12 Topic Model Smoothing For smoothing the n gram estimates of 
the component models to handle fragmentation of the training data we interpolate with the general model at the n gram level We deal with the problem of non topic sentences by including a general model in addition to the m component models at the sentence level Specifically the model is now given by P w1 wT m G X k 1 k T 1 Y i 1 1 k Pk wijwi 1 1 k PG wijwi 9 where PG refers to a general model trained on all the available training data k are the sentence level mixture weights and k are the n gram level mixture weights The two sets of weights k and k are estimated separately using a held out data set The sentences in the held out data set are first labeled according to their most likely topic and thus the data is split into m clusters A simple maximum likelihood criterion is used to estimate the s iteratively beginning with uniform weights for each After the component model smoothing factors have been estimated the s are estimated at the sentence level using an analogous algorithm over the entire data 3 Dynamic 
Adaptation The sentence level mixture model defined in the previous section has static parameters i e the n gram estimates are not updated as we observe new sentences However it is easy to extend the dynamic modeling techniques of cache language models to our mixture language model as shown next 3 1 Dynamic n gram Cache Adaptation In dynamic cache adaptation the entire partially observed document is cached and used to build an n gram model The cache probabilities are linearly interpolated with the static n gram probabilities as follows data from the NAB corpus to estimate the sentence level and the n gram level mixture weights The lexicon for all the experiments was a 46K vocabulary provided by BBN The results are based on the BU Stochastic Segment Model SSM recognition system 13 using the N best rescoring formalism with N best hypotheses generated by the BBN Byblos System a speaker independent HMM system In this formalism the top N sentence hypotheses in our case N 100 are rescored by the SSM and the 
mixture LM and a weighted combination of scores from different knowledge sources is used to re rank the hypotheses The top ranking hypothesis is used as the recognized output The weights for re combination are estimated on the development test data 1994 H1 development test and held fixed for the evaluation test set We report recognition word error rates and perplexity numbers on the 1994 ARPA development and evaluation test sets referred to as the Hub 1 H1 condition 13 We test on both the H1 P0 and the H1 C2 conditions The N best used in both these conditions are the same the difference lies in using unadapted language models vs adapted language models The adaptation is supervised i e the true transcriptions are available The n gram level and sentencelevel mixture weights as well as the cache are reset at the end of a session typically 15 sentences Unless stated no BBN knowledge sources are used in these experiments P w1 wT T 1 Y i 1 sP s wijwi 1 c P c wijwi 1 10 where P s represents the smoothed static 
model parameters P c represents the cache model parameters and s and c represent the weights for the static and cache models respectively The interpolation weights s and c for the cache language models Equation 10 were estimated on the development test data to minimize recognition word error rate in a series of experiments The important issues to be resolved in dynamic language modeling with a cache are the definition of the cache and the mechanism for choosing the interpolation weights We experimented with two cache models Since the function words are well estimated due to their high frequency in the training data we considered a content word unigram cache This is similar to the approach suggested by Rosenfeld 3 where only rare words observed in a document are cached rare being defined by the frequency of the word in the training corpus We observed that defining rare as a content word alleviated the problem of deciding a threshold frequency below which a word is considered rare as well as gave us small but 
consistent improvements in performance over the frequency based rare word cache We also worked with a conditional bigram trigram cache 3 which is used in addition to the content word unigram cache 4 2 Results 3 2 Adaptation in Mixture Framework To extend cache based n gram adaptation to the mixture model we maintain separate counts for all the component models including the general model A sentence is weighted according to its likelihood given each of the component models For the kth model the likelihood for sentence w1 wT is given by Equation 5 Fractional counts are then assigned to each topic according to their relative likelihoods which allows all the topic models to take advantage of the partially observed document The equation for the adapted mixture model incorporating component caches is a combination of Equation 2 and Equation 10 We did not use dynamic mixture weight adaptation 8 though we developed a recursive solution to this problem in 12 since initial experiments with this algorithm indicated no 
significant improvement in terms of perplexity or word error rate In all experiments described here a 5 component sentence mixture is used Details of our exploratory work regarding different similarity measures and training techniques are in 12 7 We assessed the usefulness of the mixture language model using two criteria recognition word error rate and perplexity Table 1 compares four different language models in terms of perplexity on both the development and evaluation test sets The unadapted mixture Table 1 Perplexity 1994 NAB 46K development and evaluation test sets Test Dev Dev Eval Eval Adaptation No Yes No Yes Trigram Model 211 171 210 175 5 component mixture model 165 141 175 145 4 Experiments 4 1 Paradigm model reduces perplexity by 22 over the unadapted trigram language model The mixture model also provides a small perplexity improvement over the adapted trigram language model on the development test set Incorporating dynamic cache modeling techniques in the mixture framework leads to a further 
improvement in the mixture model of 14 5 Table 2 gives the recognition performance of the four different language models namely the adapted and unadapted general and mixture trigram models The recognition results are consistent with the The language model training corpus referred to as the North American Business NAB corpus encompasses the period from 19881994 and consists of approximately 230 million words of articles in the SGML format We also used 1 million words of set aside test Table 2 Recognition word error rate on the 1994 NAB development and evaluation test sets Test Dev Dev Eval Eval Adaptation No Yes No Yes Trigram model 10 5 10 3 11 5 11 1 5 component mixture model 10 2 10 2 11 0 10 8 of n gram techniques and hence other language modeling advances can be easily incorporated in this framework Acknowledgments This work was supported jointly by ARPA and ONR on grant ONR number ONR N00014 92 J 1778 We gratefully acknowledge the cooperation of several researchers at BBN who provided the N best 
hypotheses used in our recognition experiments improvements in perplexity The mixture model gives a 3 4 improvement in recognition accuracy over the general model Adaptation in the mixture framework gives a small but significant gain on the evaluation test set although no gain was observed on the development test set In addition to working with the BU acoustic models confirming that the mixture model is more powerful than supervised adaptation for short sessions We also performed recognition experiments combining the mixture model with the BBN baseline acoustic model We obtained a best case performance of 8 9 word error rate on the development test set and 10 6 on the evaluation test set as compared to 9 3 and 11 1 respectively using an adapted general model confirming that the mixture model is more powerful than supervised adaptation for short sessions 6 REFERENCES 1 F Jelinek B Merialdo S Roukos and M Strauss A Dynamic LM for Speech Recognition Proc ARPA Workshop on Speech and Natural Language pp 293 295 
1991 2 R Kuhn and R de Mori A Cache Based Natural Language Model for Speech Recognition IEEE Transactions PAMI Vol 14 pp 570 583 1992 3 R Rosenfeld A Hybrid Approach To Adaptive Statistical Language Modeling Proc ARPA Workshop on Human Language Technology pp 76 87 1994 4 J H Wright G J F Jones and E N Wrigley Hybrid Grammar Bigram Speech Recognizer System with First Order Dependence Model Proc ICASSP Vol I pp 169 171 1992 5 M Meteer and J R Rohlicek Statistical Language Modeling Combining N gram and Context Free Grammars Proc ICASSP Vol II pp 37 40 1993 6 R Lau R Rosenfeld and S Roukos Trigger Based Language Models a Maximum Entropy Approach Proc ICASSP Vol II pp 45 48 April 1993 7 R Iyer M Ostendorf and R Rohlicek An Improved Language Model Using a Mixture of Markov Components Proc ARPA Workshop on Human Language Technology pp 82 87 1994 8 R Kneser and V Steinbiss On the Dynamic Adaptation of Stochastic LM Proc ICASSP Vol 2 pp 586 589 1993 9 L Bahl et al The IBM Large Vocabulary Continuous Speech 
Recognition System for the ARPA NAB News Task Proc ARPA Workshop on Spoken Language Technology pp 121 126 1995 10 S Sekine Automatic Sublanguage Identification for a New Text Second Annual Workshop on Very Large Corpora Kyoto Japan pp 109 120 August 1994 11 P Placeway R Schwartz P Fung and L Nguyen Estimation of Powerful LM from Small and Large Corpora Proc ICASSP pp 33 36 Vol 2 April 1993 12 R Iyer Language Modeling with Sentence Level Mixtures Boston University M S Thesis 1994 anonymous ftp to raven bu edu in the pub reports directory 13 M Ostendorf F Richardson R Iyer A Kannan O Ronen and R Bates The 1994 BU NAB News Benchmark System Proc ARPA Workshop on Spoken Language Technology pp 139 142 1995 5 Conclusions The sentence level mixture model has the potential to capture long range within sentence effects and topic dependent effects across sentences using a simple variation of the n gram approach To implement the mixture model we developed an automatic clustering algorithm to classify text introduced two 
levels of mixture models for smoothing and developed an EM algorithm for n gram probability estimation with a Witten Bell back off constraint Our experiments show that the static unadapted topic dependent mixture model outperforms the standard n gram model and that the static mixture model improves recognition performance over a dynamically adapted n gram model Incorporating cache models in the mixture framework provides us with an additional reduction in both perplexity and word error rate This work can be extended in several ways One approach to overcome fragmentation problems in very sparse data domains could include using an n gram part of speech sequence model as the base for all component models and topic dependent word likelihoods given the part of speech labels a natural extension of 2 assuming that the part of speech sequence probabilities are not topicdependent and can be based on the entire training data The simple static mixture language model can also be useful in applications other than 
continuous speech transcription For example topicdependent models could be used for topic spotting The mixture framework could also be extended to use speaker gender style or goal related dependencies In summary we present a new language model that gives significant gains in recognition performance over unadapted as well as adapted n gram models At the same time the model is a simple variation
46	a	570 IEEE TRANSACTIONS ON PATTERN ANALYSIS A N D MACHINE INTELLIGENCE VOL 12 NO 6 JUNE 1990 A Cache Based Natural Language Model for Speech Recognition ROLAND KUHN AND RENATO DE MORI Abstract Speech recognition systems must often decide between competing ways of breaking up the acoustic input into strings of words Since the possible strings may he acoustically similar a language model is required given a word string the model returns its linguistic probability This paper discusses several Markov language models Subsequently we present a new kind of language model which reflects shortterm patterns of word use by means of a cache component analogous to cache memory in hardware terminology The model also contains a 3g gram component of the traditional type The combined model and a pure 3g gram model were tested on samples drawn from the LOB Lancaster OslolBergen corpus of English text We disCUSS the relative performance of the two models and make suggestions for future improvements Index Terms Cache based 
language model language models for speech recognition Markov language models natural language perplexity probabilistic language model 3g gram language model I INTRODUCTION TYPE of system popular today for automatic speech recognition ASR consists of two components An acoustic component matches the acoustic input to words in its vocabulary producing a set of the most plausible word candidates together with a probability for each The second component which incorporates a language model estimates for each word in the vocabulary the probability that it will occur given a list of previously hypothesized words Each word candidate originally selected by the acoustic component is thus associated with two probabilities the first based on its resemblance to the observed signal and the second based on the linguistic plausibility of that word occurring immediately after the previously recognized words Multiplication of these two probabilities produces an overall probability for each word candidate Our work focuses on 
the language model incorporated ir the second component The language model we use is based on a class of Markov models identified by Jelinek the n gram and Mg gram models These models whose parameters are calculated from a large training text produce a reasonable nonzero probability for every word in the vocabulary during the speech recognition task Our A Manuscript received October 10 1988 revised November 7 1989 This work was supported by the National Science and Engineering Research Council of Canada The authors are with the School of Computer Science McGill University Montreal P Q H 3 A 2A7 Canada IEEE Log Number 9034826 combined model incorporates both a Markov 3g gram component and an added cache component which tracks short term fluctuations in word frequency The addition of the cache component and the evaluation of its effects are the original contributions of this paper In addition we provide an overview of several probabilistic language models currently used in the field of speech recognition We 
adopted the hypothesis that a word used in the recent past is much more likely to be used soon than either its overall frequency in the language or a 3g gram model would suggest The cache component of our combined model estimates the probability of a word from its recent frequency of use The model uses a weighted average of the 3g gram and cache components in calculating word probabilities where the relative weights assigned to each component depend on the part of speech POS The 153 POS s used are defined in Johansson et al 161 For purposes of comparison we also created a pure 3g gram model consisting of only the 3g gram component of the combined model For each POS the combined model may therefore place more reliance on the cache component than on the 3ggram component or vice versa the relative weights were obtained experimentally for each POS from a training text using the deleted interpolation method 5 The cachebased probabilities Cj W i were calculated as follows For each POS a cache just a buffer with 
room for 200 words was maintained Each new word was assigned to a single POS gj and pushed into the corresponding buffer As soon as there were five words in a cache it began to output probabilities which corresponded to the relative proportions of words it contained The lower limit of 5 on the size of the cache before it starts producing probabilities and the upper size limit of 200 are arbitrary there are many possible heuristics for producing cache based probabilities The dependence on POS in the combined model arose from the hypothesis that a content word such as a particular noun or verb will occur in bursts Function words on the other hand would be spread more evenly across a text or conversation their short term frequencies of use would vary less dramatically from their long term frequencies One of the aims of our research was to assess this hypothesis experimentally We believed that if it was correct the relative weight calculated from the training text for the cache component for most content POS s 
0162 8828 90 0600 0570 01 OO O 1990 IEEE I KUHN AND D E MORI LANGUAGE MODEL FOR SPEECH RECOGNITION 57 1 would be higher than the cache weighting for most function POS s Our research was greatly facilitated by the availability of a large and varied collection of modem texts in which each word is labeled with an appropriate POS This is the Lancaster Oslo Bergen LOB Corpus of modem English consisting of 500 samples drawn from 15 different categories of texts published in the United Kingdom in 1961 This corpus is described by Johansson and others in 6 8 it is available to academic researchers but not unfortunately to their colleagues in industry from the Norwegian Computing Centre for the Humanities We chose to employ the same 153 POS s found in the LOB Corpus in our model in the belief that it was more rational to rely on the syntactical judgments of a large team of trained grammarians and lexicographers than to devise our own idiosyncratic POS s Part of this corpus 391 658 words was utilized as a training text 
which determined the parameters of both models the standard 3g gram model and our combined model consisting of the same 3g gram model along with a cache component We required a yardstick with which to compare the performance of the two models The measure chosen is called perplexity it was devised by F Jelinek R L Mercer and L R Bahl 4 The perplexity of a model can be estimated by the success with which it predicts a sample text which should NOT be the one used to train the model The better the model the higher the probability it will assign to the sequence of words that actually occurs in the sample text To compare two models one employs each to calculate the word by word probability of the same sample text One can then calculate the average probability per word of sample text given by each of the two models the model for which this average probability is higher is better than the other The perplexity is simply the reciprocal of this average probability In principle low perplexity implies good performance 
although it does not take into account the varying degrees of acoustic difficulty among words It is possible that a language model that accurately distinguished between easily confused words such as short words might be more useful in practice than another model of slightly lower perplexity On the other hand perplexity provides a straightforward way of comparing language models independently of the other components of an ASR system Once the parameters of the two models the pure 3ggram and the combined had been calculated from part of the LOB Corpus we could have used any sample text from any source whatsoever to compare the perplexity of the models We chose to use part of the remaining portion of the LOB Corpus because of the wide range of different types of text represented therein The sample text we constructed like the training text includes such diverse types of written English as press reports religious literature love stories and government documents This sample text posed a difficult challenge to the 
two models if a model performs well on such a variety of written material it is likely to perform well on most types of written English The results of the comparison between the two models exceeded our expectations The pure 3g gram model as expected had a high estimated perplexity 332 The estimated perplexity of the combined model on the other hand was 107 This more than threefold improvement indicates that addition of a cache component to a 3g gram language model can lead to dramatic improvement in the performance of the model as measured by its perplexity The cache component reflects short term fluctuations in the frequency of word use on the premise that different writers or speakers have idiosyncratic word frequencies Furthermore a given subject or context may demand a particular set of word frequencies The cache component of our combined model represents a cheap easily implemented technique for permitting automatic speech recognition systems to track these short term fluctuations in frequency of word 
use whatever their cause It is interesting to speculate whether the addition of a cache component to other probabilistic language models in the literature such as Jelinek s trigram model would also improve the performance of these models One of the hypotheses we tested in the course of this research was disproved We thought it likely that the usefulness of the cache component would depend on the POS with content words such as nouns and verbs being more affected by context than function words such as articles and prepositions which would not vary much from their overall frequency in the English language Hence we expected higher best fit weights for the cache component of content POS s than for the cache component of function POS s This turned out to be false When the best fit values for the weightings assigned to the cache component for each POS were determined experimentally by means of the deleted interpolation method they did vary considerably from POS to POS But there was no consistent trend of high 
values for content POS s and lower ones for function POS s If anything the pattern was the reverse This and other aspects of the results are discussed in the conclusion 11 MARKOV MODELS FOR NATURAL LANGUAGE A Mathematical Background An automatic speech recognition ASR system takes an acoustic input A and derives from it a string of words w W taken from the system s vocabulary I In the course of this process the system considers a set of plausible word strings assigns each a probability and outputs the candidate with the highest probability If I is large the candidates may all be equally plausible on acoustic grounds Thus the system requires a purely linguistic component which assigns probabilities to word strings Formally let WS W W W denote one of these possible word strings and P WS I A the probability that it was uttered given the acoustic evidence 4 Then the speech recognizer will pick the word string WS w 572 IEEE TRANSACTIONS ON PATTERN ANALYSIS A N D MACHINE INTELLIGENCE VOL 12 NO 6 JUNE 1990 
satisfying P I s A maxP WS A ws 1 Any language model for speech recognition to be used for maximum likelihood estimation will consist of such a mapping M of word strings into equivalence classes Whenever we design a system intended to achieve human performance levels in the accomplishment of a certain task there are two strategies we could follow These might be termed the anthropomorphic strategy and the it follows that abstract strategy The first requires that we learn as much as possible about how human beings perform the given task and then incorporate this knowledge in the system s model The second demands that we consider the task in the abstract as a problem to be solved and find Since A is fixed it follows that the algorithm for solving it that will run most efficiently on our machines Thus the details of the procedure we W S WS such that P W S come up with might converge on human strategies in the P A ws is a maximum 4 task domain as we learn more about these strategies or diverge from them as our 
approach to the task becomes In this paper we will not discuss P A I W S the prob increasingly abstract and as we exploit our hardware more ability of the actual acoustic input being observed if the effectively Language models held by humans undoubtedly incorstring WS W Wn is uttered The calculation of P A I WS is the responsibility of the acoustic model porate knowledge about syntax semantics and the praging part of the speech recognition system We are con matics of discourse as well as knowledge about the world cerned instead with the model that estimates P WS the and often about the psychology of an individual speaker probability of a given word string independent o f the Of these knowledge sources only syntax can claim to have been successfully formalized or so linguists would acoustic input From elementary probability theory we decompose have us believe There is as yet no complete formal grammar for the English language Furthermore few of the P W S as innumerable parsers in the literature are equipped 
to make n probability estimates most would assign a probability of P W S P W rI P W W W 0 to ungrammatical sentences though these occur with i 2 high frequency in spoken English 5 Thus the case for an abstract strategy in natural language modeling for speech recognition is very strong The Thus the probability that a word W is spoken depends exceptions occur in specialized domains where the vocabon the past history of the dictation As Jelinek et al from ulary syntax or semantics are so constrained that the whom the above account is derived 3 5 point out the mechanisms underlying speech recognition by human W are in practice probabilities P W I W beings within the domain can be guessed at and incorpoimpossible to estimate since each history W rated into a parser Where unconstrained human speech is W I has occurred at most only a few times in the hisconcerned Jelinek and his colleagues believe that parsers tory of the English language For a vocabulary of size V should act only as final filters for word strings 
chosen by there are V I different possible histories since P W means of a more appropriate language model 3 5 W l W W I must be found for each possible The Markov models employed by Jelinek and his group W that is for each word in I there are V different probtherefore are not intended to reflect natural language abilities to be estimated V is an astronomically large models possessed by humans Instead they are designed number for reasonable values of V and i thus another to produce a mapping M of word strings to equivalence approach must be found Whatever solution we adopt will consist of mapping the classes that facilitate estimation of P W j WI M W W This involves a compromise between the W into a more set of possible histories W I need for a refined classification that loses little relevant manageable number of equivalence classes Let us denote this many to one mapping by M Thus M W 1 information about the history W I W i I and the need for a small number of classes so that enough data W I denotes the 
equivalence class of the string W can be gathered for each one W l The novelty of Jelinek s approach is that it considers it The probability P W W is approximated by more important to keep the information contained by the last few words than to concentrate on syntax which by P W W P Y W l M W W I P 1 definition involves the entire sentence A high percentage 6 of English speech and writing consists of stock phrases i e the most likely word string given the evidence Since by the Bayes formula we have B Justification of Jelinek s Markov Approach I KUHN AND DE MORI LANGUAGE MODEL FOR SPEECH RECOGNITION 513 that reappear again and again if someone is halfway through one of them we know with near certainty what his next few words will be Jelinek s trigram model automatically picks up this kind of information from a training text a parser does not The 3g gram model addresses one of the tasks parsers are designed to achieve the prediction of the part of speech of the next word but its structure owes everything to 
Jelinek s approach and nothing to traditional parsers Like the trigram model the 3ggram model uses only the context provided by the two preceding words The advantages of the Jelinek approach are the assignment of a probability to every possible word string and the automatic calculation of parameters from a training text permitting the model to incorporate valuable information that is not described by any existing linguistic theory An important disadvantage is the loss of information that goes more than a few words back In the next section we will discuss our combined model which tries to overcome this disadvantage by using information about the lexical preferences of the speaker gathered during the recognition task and extending hundreds of words into the past The Markov component of this model is based on the 3g gram model which is an adaptation by Derouault and Merialdo of Jelinek s original trigram model and f W W N W W NT where NT total number of words in training text If qo 0 this smoothed trigram model 
guarantees that any word W that occurs at least once in the training text is assigned a nonzero probability so it avoids the problem with the pure trigram model mentioned above The values for qo q l and q2 are chosen in order to meet the maximum likehood criterion that is the probability of a new text calculated by means of the smoothed trigram formula is maximized Note that these 4 s depend on the size of the training text since as it gets larger more of the possible trigrams are encountered f W WI W 2 W I becomes a more reliable estimator and the value of q2 can be increased There are other ways of using bigram and singlet frequencies to smooth trigram estimates Katz s method backs off from a trigram to a bigram to a singlet estimate 191 described in 131 P W w Iv w i f N W wfp1 W o then r2f W W W _ W f P l else if N W W 0 thenr f W W l i V l else rof W C The Trigram Model The trigram model is based on the mapping of a history WiI onto the state formed by the two most WI recent words W 10 M W1 Wj 1 W j 2 W 
j 1 7 Thus it is a Markov model approximating P Wi WI W1 kV I b y P W WI W j 2 W j l The latter in tum is estimated from the training text as the ratio of the number of times the word sequences W W occurred to the number of times the sequence W 2 W I occurred 2 The weightings r2 r I and ro ensure that the probability summed over all words W adds up to 1 as with the 4 s in the previous model they are chosen to maximize the probability of a new text and depend on the size of the training text D The 3g gram Model The 3g gram model terminology of A Martelli and of Derouault and Merialdo l 121 is analogous to the trigram model this model is also Markov but not completely divorced from grammatical theory It employs abbreviated grammatical parts of speech henceforth POS Let g W g denote the POS of the word that appears at time i Note that we might have W W W for i k but g W g W This is because a word Win the vocabulary can belong to different POS s at different times for instance light can be a noun verb or 
adjective By definition each occurrence of a word only has one POS in practice it may be difficult to single out that POS among the set of POS s associated with the word The 3g gram model has two levels At time i it assigns a probability to each POS on the basis of the information provided by g I and g I p 2 This part of the model functions exactly like the trigram model except that the vocabulary consists of POS s and not words Thus the model gives a nonzero probability that g is a noun or a verb or an article etc Next probabilities of individual words are calculated on the basis of their frequency within POS s Suppose that the model gave a probability of 0 99 to the occurrence of a noun at time i Then the estimated P W W W 2 W In practice many trigrams that do not occur in the training text show up during the recognition task and should therefore not have the zero probability assigned them by this formula One way of dealing with this problem is to use a weighted average of trigram bigram and individual 
word frequencies q2f w WI W 2 I W 1W l f Y w 9 where qo q 1 q2 1 and f Wl Wl W 2 W1 1 wIWl 1 N W W I W N W 2 w I N W l W N W I f W 5 14 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE VOL 12 NO 6 JUNE 1990 probability that W desk would be almost exactly equal to the frequency of the word desk among the nouns in the training text Let G be the set of POS s recognized by our model and let g be a particular POS whose probability of occurring w e wish to predict The model will give us an estimate P g g I g 2 g of the probability based on the identity of the two preceding POS s For a word W that only has one possible POS g W the probability P W W is estimated by the product of the estimated probability that g W will occur at time i by the estimated probability that if g W occurs the word will be W using individual POS frequencies as the third component of a weighted average these researchers chose to add an arbitrary small value e lop4to the overall probability estimate of each word in order to 
prevent zero estimates for the probability of occurrence of any given word Thus they approximated the probability of occurrence of a word W at time i given that W has part of speech g j the two preceding parts of speech are gi and gi I and vocabulary size is n as P Y W g W gi 2 gi 1 gjlgi 2 1 ne f Wlgj x llf gi gi 1 P W wIg 2 2 f g i s gi l e e I P w g w P s g w g g l l 1 l2 1 13 f W g W P g g W l g S l 11 where the frequencies f are calculated from the training text as before Generally things are not as simple as this since many words belong to more than one POS category The probability that light will occur is the probability that it will occur as a noun plus the probability that it will occur as a verb plus the probability that it will occur as an adjective Thus the general 3g gram formula is P W W W 2 W I gr 2 g1 1 8IEG P W g P g g Given a sufficiently large training text P g g I g 2 g could be estimated for every POS gJ in G as f g g g 2 g In practice existing training texts are I too small many POS 
triplets will never appear in the training text but will appear during a recognition task If we do not modify the procedure to prevent zero probabilities a particular g that actually occurs may have zero estimated probability Recall that an analogous problem occurred with the trigram model The two solutions we described were the weighted average approach and the back off approach both using bigram and singlet frequencies to smooth out the trigram frequencies These two solutions are also applicable to the 3g gram model Derouault and Merialdo I 2 employed a variant of the weighted average 3g gram approach Their work will be described in some detail as the 3g gram component of our model was based on it It must be emphasized that not all of their conclusions are relevant to our work as they were dealing with French rather than English However their methods are applicable to English Their corpus consisted of 1 2 million words of French text tagged with 92 POS s Only 5 of the possible triplets occurred Thus the 
doublets were tabulated as well this time half of the possible pairs occurred Instead of They experimented with two different ways of calculating l1 and 1 Intuitively it makes sense that if there are many triplets beginning g 2 g i p the frequencyf gi gj 1 gi gi gives reliable information 1 should therefore be high If there are few such triplets l2 should be given more weight Following this reasoning Derouault and Merialdo first let 1 and l2 be a function of the count of occurrences of g j 2 gi I Each possible history gi 2 g was assigned to one of ten groups depending on how often it had occurred in the training text Each of the groups had different values of I and 12 with the highest value of l2 occurring in the group for histories g i 2 g i that never occurred in the training text Another way of looking at the problem is to argue that Z1 and l2 should depend on gi I the POS of the last word recognized If it is an article for instance we can be almost certain that the next word W is a noun or an adjective 
In other cases we may have to look at g i p as well Thus the other way in which these researchers calculated 1 and Z2 was to allow them to depend on g i p I Let h gi 2 gi I denote the parameter on which l I and Z2 depend For Derouault and Merialdo s first approach h N g 2 g i the number of occurrences of gi 2 gi I in the training text for the second approach h gi I the POS of the preceding word They calculated l I h and 12 h by the same algorithm in both cases called the deleted interpolation method SI Having split the training text into two portions in the ratio 3 1 they used the larger portion to calculate f gi 1 gi 2 g i l a n d f g i g They then set l l h and 12 h to arbitrary values such that I h 12 h 1 and iteratively reset them from the remaining portion of the corpus Summing over all triplets g i _ 2 g i I g i in this portion they defined KUHN AND D E MORI LANGUAGE MODEL FOR SPEECH RECOGNITION 575 probability to it since they are better at prediction than the others Thus the better the model the 
higher the avl l h S l h S l h W erage probability per word How could one estimate this average for a text of n words The logical answer is to 16 2 h S h S I h S2 h take the nth root of the sample s overall probability as Then the first two formulas were calculated again on the estimated by a given model since the individual probasame portion of the corpus Iteration continued until l l h bilities are multiplicative But this nth root is simply the and 12 h converged to fixed values This procedure is reciprocal of Jelinek s perplexity measure Thus low perguaranteed to produce the l I and l2 that maximize the es plexity corresponds to high probability per word of samtimated probability of the smaller portion of the corpus ple text both are signs that the model in question is a based on the frequencies obtained from the larger portion good predictor for the sample Derouault and Merialdo found only a small difference between thDpp formance of the model in which Z I l2 de111 THE COMBINED MODEL pend on the count N 
g 2 g l and that in which they depend on the POS g Both models were superior A Argument for the Cache Component to one in which the coefficients were arbitrarily set to l I The central idea underlying the work presented in this 0 99 l2 0 01 for all POS As expected when trainpaper concerns a crucial limitation of all the Markov ing text size was varied the algorithm described above models described earlier Fortunately this limitation can gave larger values of I for larger text size easily be overcome by means of a mechanism which does The first level of both our combined model and our 3 g not compromise the robust simplicity of the Markov apgram model the level that predicts the POS works in proach almost exactly the way we have described for Derouault The main limitation of the Markov models is their inand Merialdo s 3g gram model The other level to be con ability to reflect short term patterns in word use Suppose sidered is the lexical level which estimates the probabil the word sequence the old has just 
been recogity of a word given its POS At this level our 3g gram nized and that the word man followed these two words model is again almost identical to Derouault and Merial 10 of the time in the training text while the word do s model In both cases the probability of a word given band followed them 1 of the time The trigram model its POS is estimated by its frequency among the words will assign man a probability of 0 1 and band a found in that POS category in the training text Thus the probability of 0 01 If the acoustic component assigns only substantial difference between our 3g gram model these words roughly equal probability man will be and Derouault and Merialdo s model is the choice of chosen For an isolated sentence this would be the reaPOS s they define 92 POS s we use the 153 POS s in sonable choice to make But now suppose that several the LOB Corpus In the next chapter we will see how the previous sentences contained the word band while combined model differs from 3g gram model at the level none 
contained the word man We contend that a huof lexical prediction man would then assign overwhelmingly higher probability to the word band A word used in the immediate E Perplexity A Measure of the Performance of a past say the last 2000 words or so is much more likely Language Model to be used soon than either its overall frequency in the We can view a language as a source of information English language or any of the popular Markov models whose output symbols are words w Unfortunately we would predict cannot know the probabilities P w l w2 wn for There is strong empirical evidence for this Studies on strings of a language AHowever each language model three corpora of English and American texts 7 8 by provides an estimate P w l w2 w for such S Johansson show that word frequencies vary greatly strings depending upon the type of text both among content The difficulty of recognition of a sample text using a words and function words 8 p 341 The idea undergiven language model is given by lying our research was 
that a language model that exL P l n log P W I w2 w 17 ploited short term shifts in word use frequencies might Jelinek et al 4 suggest that it is intuitively more satis perform significantly better than the pure Markov models fying to measure the difficulty of the speech recognition described in the previous chapter A similar problem was faced by computer hardware designers some years ago task by the value of the perplexity given by 121 It was known that computers often accessed a par 1 ll p p 2LP B w w 18 ticular memory location with high frequency within a seRoughly speaking if the perplexity is PP the speech rec quence of accesses The designers took advantage of these ognition task is as difficult as it would be if the language short term patterns in memory reference by building a had PP equiprobable words small high speed expensive cache memory next to There is another way of looking at the perplexity When the CPU When a memory access is made the contents we employ language models to calculate the 
probability of the accessed location plus its neighbors is copied to of a sample text the better models will assign a higher the cache 12 p 2301 When space must be made in the They then redefined 9 3 516 IEEE TRANSACTIONS ON PATTERN ANALYSIS A N D MACHINE INTELLIGENCE VOL 12 NO 6 JUNE 1990 cache to insert new information the least recently used LRU data is overwritten Following this analogy we decided to design a language model that had both a cache and a Markov component Our linguistic intuition suggested that these shortterm word frequency fluctuations depend on the POS For example a given noun will appear in bursts whenever a topic that evokes it is being discussed a given preposition would be likely to appear at a steady rate throughout This consideration led us to employ a model with a component that predicts the POS so that the model would be able to weight the short term cache component heavily when for example a noun was expected while virtually ignoring the cache component when a preposition was 
expected Any Markov model that predicts the POS would have suited as we chose the 3g gram model because it has been thoroughly studied and well described in the literature Just as was described for the l values in Section 11 D the relative weights assigned to the cache and 3g gram components within each POS category were determined experimentally by means of the deleted interpolation method Thus the combined model assigns a probability to each POS in the same way as the 3g gram model For a fixed POS the probability of any word which belongs to it is a weighted average of the word s frequency in the POS category in the training text the 3g gram component and its frequency in the cache belonging to the POS category the cache component At a given time during the speech recognition task the cache for a POS will contain the last N words which were guessed to have that POS we arbitrarily set N to 200 If a word has occurred often in the recent past it will occur many times in the cache for its POS supposing for the 
purposes of argument that the word only has one possible POS Thus the word will be assigned a higher probability than when its recent frequency of occurrence is low In this way the inclusion of a cache component satisfies our goal of dynamically tracking changing patterns of word use We believe that the recent past is a good guide to the direction of the variance Thus let C W i denote the cache based probability estimate for word W at time i for POS gj This is calculated from the frequency of Wamong the N most recent words belonging to POS g in our implementation N 200 Our combined model estimates P W WI g g by 5 IkiM j x f Y 1 gi gj kc x Cj W i 20 where khf J kc 3 instead of by f Wi WI g g j alone This should allow the estimate of P W WI g g to deviate from its average value to reflect temporary high or low values The relative weights of kM and k c are found by the deleted interpolation method mentioned in Section 11 D the values thus obtained maximize the probability of the training text Note that the 3s 
gram model is simply the special case of the combined model obtained by setting all k M to 1 O and all kc to 0 0 We must also specify how we estimated the POS component P g g I g i P 2 g i p I of both the 3g gram and the combined models This was done in almost the same way as was done by Derouault and Merialdo as described in Section 11 D We chose to use the variant of their model in which the 1 values giving the relative weights of the POS triplet and POS doublet probability estimates depend on the previous POS g l To ensure that no POS g is ever assigned a probability of zero we added an arbitrary small number 0 0001 We thus made the approximation P g i gjl gi 23 gi 1 I l g i l f g i g j gi 2 gi 1 B Mathematical Treatment of the Combined Model The combined model is now introduced mathematically Recall that the pure 3g gram model is 1 2 g i I f g i gj1gi l 0 0001 21 where 11 g T 12 g 0 9847 for all x where 0 9847 1 no of POS s x 0 0001 The above description ignores the possibility that a word will be 
encountered in the sample text that is not in the system s vocabulary V There are different ways of calculating the probability that such a word will occur at time i 9 we estimated this probability by Turing s formula which uses the frequency of unique words among all words in the training text There were 13 610 unique words among 391 658 words in the training text so the probability of encountering a word not in the vocabulary was estimated as about 0 035 All such unknown words are treated as if they were a single word whose probability of occurrence is always 0 035 no matter what the preceding POS s were They are discarded instead of being placed in a cache As is described in Section IV D the system guesses the POS g of the discarded word on the 19 The combined model leaves the POS component P g g I g 2 g I of the 3g gram model unchanged Our modification affects only the component that predicts the probability of a word given the POS P W WI g g In the 3g gram model this is estimated by f W W1 g g J 
calculated from the training text This is certainly a good estimate of the mean around which the value P W WI g g fluctuates however it does not take account of the variance around that mean KUHN AND DE MORI LANGUAGE MODEL FOR SPEECH RECOGNITION 511 basis of the two preceding POS s by assigning it the value of gJ that maximizes the value of the previous equation We can now give the overall formula that we used POS to POS there is no reason not to merge our combined model with the trigram model The resulting cachetrigram model would estimate P W W I W W j as follows P W W l W P W if Win V l WIg1 2 g1 1 d l R I E G kh f J f V cj w3 W I W g l gJ al x P w r w Fv kc else d where d i lf gl gJIgl 2 g 1 l 1 12f gr s Ig 1 O O l 0 035 kM k 1 II l2 0 9847 22 This model would incorporate features from each of the three main approaches we have discussed so far and might lead to substantially improved predictive power over any single one of them IV IMPLEMENTATION A N D TESTING OF MODEL THE Only one major modification to 
this model proved to be necessary in practice We were faced with severe memory limitations which required that we economize on the amount of data stored For this reason we decided to restrict the number of POS s for which 200 word caches were maintained To be given a cache a POS had to meet two criteria It had to 1 comprise more than 1 of the total LOB Corpus 2 consist of more than one word for instance the LOB category BEDZ was excluded because it consists of the single word was Only 19 POS s met these two criteria however these 19 together make up roughly 65 of the LOB Corpus They are introduced in Section IV Thus for POS s other than these 19 there is no cache component in the combined model the estimated probability is identical to that of the pure 3g gram model Another problem was what to do when the recognition containing the pretask is beginning and the cache for g vious words that belong to POS g is nearly empty i e the number of words on which our estimate is based is far less than N One could argue 
that kc should not be fixed but should increase with the number of words in the cache corresponding to POS gJ attaining its maximum when that cache is full However we decided to keep things simple Arbitrarily we set kc 0 until the corresponding cache has five words in it at that moment kc attains its maximum value In future work we may permit kc l to increase with the number of elements in the corresponding cache In order to test our hypothesis that each POS should be given a different best fit pair of weights for its cache and 3g gram components we experimented briefly with a model in which all POS s had the same pair of weights Recall that the two weights must add up to 1 0 We experimented with kc k 0 0 1 0 0 1 0 9 0 2 0 8 0 9 0 1 We did not try to find a best fit pair of relative weights for this simpler version of the combined model A final point must be made Although the use of a cache component implies the existence of a POS predictor since short term word frequencies differ so dramatically from 
COMBINED A The LOB Corpus and Texts Extracted from It The Lancaster Oslo Bergen Corpus of British English consists of 500 samples of about 2000 words each The average length per sample is slightly over 2000 as each sample is extended past the 2000 word mark in order to complete the final sentence Each word in the corpus is tagged with exactly one of 153 POS s The samples were extracted from texts published in Britain in 1961 and have been grouped by the LOB researchers into 15 categories spanning a wide range of English prose 6 8 These categories are shown in Table I The table shows the 15 text categories The column labeled Corpus gives the number of samples in each category in the original LOB Corpus We extracted three different nonoverlapping collections of samples from the tagged LOB Corpus and used each for a different purpose All three were designed to reflect the overall composition of the LOB Corpus as closely as possible The column labeled Training Text shows the number of samples in each category 
for the first of these collections the last column applies to both remaining collections The training text for our models was used to obtain counts for triplets doublets and singlets of POS s It also gave rise to the vocabulary for the models and to the counts for the number of occurrences of a word within each POS It contained 169 samples altogether for a total of 39 1 658 words The second collection of samples was used for further parameter setting including calculation of the I values in the Derouault Merialdo formula Section 11 D which give the relative weights to be placed on triplet and doublet probability estimates for the POS prediction portion of both models It was also used to calculate the k values which give the relative weights to be placed on the cache 578 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE VOL I NO 6 JUNE 1990 TABLE 1 DISTRIBUTION OF LOB CATEGORIES Training Text Para Setting Testing Texts Symbol A B C Description Press reportage Editorials Press reviews Religion 
Skills and hobbies Popular lore Biography and essays Miscellaneous Learned writings General fiction Mystery fiction Science fiction Adventures and westerns Love stories Humor corpus D E F G H J K L M N P R 44 27 17 17 38 44 77 30 80 29 24 6 29 29 9 15 9 6 6 13 15 25 10 27 10 8 2 10 10 3 9 5 3 3 8 9 15 6 16 6 5 1 6 6 2 component and the 3g gram component in the combined model It contained 100 samples altogether The third collection of samples formed the testing text It was used to compare the combined model with the 3g gram model It contained 100 samples distributed among the LOB categories in exactly the same way as in the parameter setting text Note however that only the categories and not the samples themselves are the same We required labeled texts for training and parameter setting By contrast as pointed out in the Introduction any text from any source whatsoever could have been used as the testing text The diversity of the testing text poses a difficult challenge to both models we tested It is true that 
the composition of the two texts used for modelbuilding resembles that of the testing text but that has always been the case in this type of research It seems to us that the difficulty of prediction here when all three texts are derived from a variety of sources is greater than when all three are derived from business correspondence alone as in Jelinek s work In one way only could we be accused of making the task of the combined model easier We kept samples of the same LOB category contiguous in the testing text following the order given above Thus the testing text consists of the 9 A samples followed by the 5 B samples and so on Note that the cache component of our combined model will contain many words from samples previous to the current one If as we hypothesize discourse of a certain type has a characteristic vocabulary and pattern of word frequencies our combined model will work much better on our testing text than on one constructed from the same samples in random order In other words the final 
perplexity result for the combined model gives an idea of its performance when the domain of discourse changes slowly This seems a reasonable restriction The comprehensiveness of the LOB Corpus made it an ideal training text and a tough test of the robustness of the language model Furthermore the fact that it has been tagged by an expert team of grammarians and lexicog raphers freed us from having to devise our own tagging procedure B Parameter Calculation All parameters for both the 3g gram model and the combined model were calculated from the training text and the parameter setting text The two models share a POS prediction component which is estimated by the Derouault Merialdo method Triplet and doublet POS frequencies were obtained from the 169 sample training text this text also supplied the vocabulary and the count for each word subdivided by POS The vocabulary size can be given in two different ways If we ignore the POS of a word there were 24 279 different words in the training text and hence in the 
vocabulary of our models On the other hand if words with the same spelling but different POS s are counted separately the vocabulary size is 30 718 The 100 sample parameter setting text gave the weights 11 g j and 12 g j I needed for smoothing between the triplet and doublet POS frequencies These were computed iteratively using interpolated deletion as described in Section 11 D above Now the portion of both models that calculates POS probabilities is complete it remains to find k M and kc for the combined model This was calculated by means of interpolated deletion from the parameter setting text in exactly the same way C Implementing the Combined Model Because of memory limitations it proved impossible to implement a cache for every one of the 153 POS s in the LOB Corpus As was mentioned in Section 111 B two criteria were used to select the POS s which would be assigned a cache 1 the POS had to constitute more than 1 of the LOB corpus 2 the POS had to contain more than one word or symbol The second criterion 
is obvious if only one vocabulary KUHN AND D E MORI LANGUAGE MODEL FOR SPEECH RECOGNITION 579 TABLE I1 PARTS OF SPEECH FOR WHICH CACHES AREDEFINED Cache Number 1 POS Name AT AT1 BEZ Description singular article a an every singular or plural article the no is s coordinating conjunction and and or but nor only or yet cardinal 2 3 etc hundred thousand etc dozen zero subordinating conjunction after although etc preposition about above etc adjective modal auxiliary 11 can could etc singular common noun plural common noun singular proper noun possessive determiner personal pronoun 3rd pers plur nom he she adverb base form of verb uninflected present tense imperative infinitive past tense of verb present participle gerund past participle 2 3 4 5 6 7 8 9 10 11 cc 12 13 14 15 16 17 18 19 CD CS IN JJ MD NN NNS NP PPS PP3A RB VB VBD VBG VBN item has a given POS the cache component yields no extra information The first criterion is based on the premise that rare POS s will be more spread out in time so that the 
predictive power of the cache component will be weakened The 19 POS s that survived this selection process are listed in Table 11 D Testing the Combined Model As described in Section IV B two parts of the LOB Corpus were used to find the best fit parameters for the pure 3g gram model and the combined model made up of the 3g gram model plus a cache component These two models were then tested on 20 of the LOB Corpus 100 samples as follows Each was given this portion of the LOB Corpus word by word calculating the probability of each word as it went along The probability of this sequence of 230 598 words as estimated by either model is simply the product of the individual word probabilities as estimated by that model The higher this overall probability the better the model However it is more convenient to calculate the logarithm of this estimated overall probability equal to the sum of the logs of the individual word estimated probabilities LO log P w w2 w log2 B W l log2 P w 2 log I The perplexity is then 
calculated as pp 2 w where n is the number of words 230 598 in this case Recall that we also tested a simpler version of the combined model in which the cache component has the same weight for all POS s The weights tried were 0 0 0 1 0 2 0 9 the 3g gram component is always 1 O mi nus the cache component The perplexity was also estimated from the testing text for these 10 variants of the simpler model Note that in order to calculate word probabilities both models must have guessed the POS s of the two preceding words Thus every word encountered must be assigned a POS There are three cases 1 the word did not occur in the tagged training text and therefore is not in the vocabulary 2 the word was in the training text and had the same tag whenever it occurred 3 the word was in the training text and had more than one tag e g the word light might have been tagged as a noun verb and adjective The heuristics employed to assign tags were as follows 1 in this case the two previous POS s are substituted in the Derouault 
Merialdo weighted average formula and the program tries all 153 possible tags to find the one that maximizes the probability given by the formula 2 in this case there is no choice the tag chosen is the unique tag associated with the word in the training text 3 when the word has two or more possible tags the tag chosen from them is the one which makes the largest contribution to the word s probability Thus although the portion of the LOB Corpus used for testing is tagged these tags were not employed in the implementation of either model in both cases the heuristics given above guessed POS s A separate part of the program compared actual tags with guessed ones in order to collect statistics on the performance of these heuristics As one of the referees of this paper pointed out the assignment of a tag to an unknown word would benefit from the use of information about one or more of the words that succeed it In fact an even simpler improvement could have been made We did not take into account the fact that rare 
words are more likely to occur in some r 580 IEEE TRANSACTIONS ON PATTERN ANALYSIS A N D MACHINE INTELLIGENCE VOL I NO 6 JUNE I Y Y O POS classes than in others For instance a word we do not recognize is unlikely to be a connective or a preposition Recall that we counted the unique words in the training text it would have been possible to record their distribution among POS categories and modify heuristic 1 above so as to take this information into account Either approach would probably have reduced the number of misassigned unknown words V RESULTS A Calculation of the L Values The first results of our calculations are the values 11 g I and 12 g I obtained iteratively to optimize the weighting between the POS triplet frequency f g 1 g 2 g and the POS doublet frequency f g 1 g in the estimation of P g gJ 1 g 2 g I As one might expect 11 g I tends to be high relative to 12 g I when g occurs often because the triplet frequency is quite reliable in this case For instance the most frequent tag in the LOB Corpus 
is N N singular common noun we have I 0 57 The tag HVG attached only to the word having is fairly rare we have l I H V G 0 17 However there are other factors to consider Derouault and Merialdo l state that when g I was an article 1 was relatively low because we need not know the POS g to predict that g is a noun or adjective Thus doublet frequencies alone were quite reliable in this case On the other hand when g I is a negation knowing g was very important in making a prediction of g because of French phrases like il ne veut and je ne veux so II was high Our results from English texts show somewhat similar patterns The tag AT for singular articles had an lI that was neither high nor low 0 46 The tag XNOT including only not and n t had a high l1 value 0 84 Adjectives J J and adverbs R B had l1 values even higher than one would expect on the basis of their high frequencies of occurrence 0 85 and 0 80 respectively TABLE I11 O P T I M A L WEIGHTS BY POs Cache Component 0 999 0 998 0 999 0 997 0 783 0 973 0 919 0 
402 0 989 0 403 0 498 0 592 0 997 1 000 0 660 0 456 0 519 0 518 0 326 POS AT AT1 BEZ Description singular article sing or pl art is s coord conjunction cardinal subord conjunction preposition adjective modal auxiliary sing noun pl noun sing proper noun possessive det pers pron 3rd pers nom adverb verb base form verb past tense present part gerund past part 3g gram Component 0 001 0 002 0 001 0 003 0 217 0 027 0 081 0 598 0 01 I 0 597 0 502 0 408 0 003 0 000 0 340 0 544 0 481 0 482 0 673 cc cs CD IN JJ MD NN NNS NP PPS PP3A RB VB VBD VBG VBN B Calculation of the K Values For each part of speech g J we calculated the weight kc given to the cache component of the combined model and the weight kM given to its 3g gram component Recall that we originally created a different cache for each POS because we had hypothesized that the cache component would be more useful for prediction of content words than for function words The optimal weights calculated by means of the deleted interpolation method in Table 111 
decisively refute this hypothesis The pattern in Table 111 is just the opposite of what we had expected with function POS s having significantly higher optimal weights for the cache component of the combined model than content POS s This intriguing result is discussed in the conclusion C Performance of Both Models on the Testing Text We calculated the performance of the various models on the testing text of 100 samples from the LOB Corpus 230 598 words the most important results will be given first The pure 3g gram model gives perplexity equal to 332 average probability per word is 0 003008 On the other hand the combined model gives perplexity equal to 107 average probability per word is 0 009341 This dramatic more than threefold improvement can only be attributed to the inclusion of a cache component in the combined model Would such a dramatic improvement have been obtained if all caches had had the same weight Recall that we experimented with a simpler version of the combined version in which all 19 caches 
had the same weight The results are shown in Table IV Thus the lowest perplexity 118 was obtained when the cache component weight was 0 7 and 3g gram component weight was 0 3 It is difficult to be sure without using deleted interpolation to obtain the optimal weights but these figures seem to indicate a minimum for the perplexity of this simpler version of the combined model of about 116 still a vast improvement over the 3g gram model We collected statistics on the success rate of the POS component of both models in guessing the POS of the latest word using the tag actually assigned the word in the LOB Corpus as the criterion This rate has a powerful impact on the performance of both models especially the combined model each incorrectly guessed POS leads to looking in the wrong cache and thus to a cache based probability of zero unless the same incorrect guess has been made in the recent past We are particularly interested in forming an idea of how fast this success rate will increase as we increase the size 
of the training text There were 230 598 words in the testing text Of these 14 436 6 2 had never been encountered in the training text and were thus assumed not to be in the vocabulary not recognized Of the remaining 216 162 words that had occurred at least once in the training text 202 882 KUHN A N D D E MORI LANGUAGE MODEL FOR SPEECH RECOGNITION 58 1 TABLE IV RESULTS FOR MODEL WITH EQUALLY WEIGHTED CACHES Cache Weight 0 I 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 3g gram Weight 0 9 0 8 0 7 0 6 0 5 0 4 0 3 0 2 Perplexity I80 152 137 128 122 119 118 121 131 Average Probability 0 00555 I 0 006570 0 007277 0 007790 0 008 150 0 008363 0 00841 1 0 008232 0 007620 0 1 93 8 had tags that were guessed correctly 6 2 incorrectly The 14 436 words that never occurred in the training text were assigned the correct tag only 3676 times 25 4 correct 74 6 incorrect Recall that a word that was encountered in the training text is always assigned one of the POS tags that it had there Apparently the information contained in the counts of 
POS triplets doublets and singlets is a good POS predictor when combined with some knowledge of the possible tags a word may have but not nearly as good on its own Overall of the 230 598 words in the training text 206 558 89 5 were assigned the correct POS Among the 216 162 words that appeared at least once in the training text a surprisingly high number 1 11 319 5 1 4 had more than one possible POS Of these 99 242 89 1 had POS s that were guessed correctly Of the 12 077 faulty guesses that occurred for words with more than one possible POS only 294 2 4 occurred because the POS for the word in the testing text had not been encountered in the training text VI CONCLUSIONS The results listed in the previous chapter seem to strongly confirm our hypothesis that recently used words have a higher probability of occurrence than the 3g gram model would predict When a 3g gram model and a combined model resembling it but containing in addition a cache component whose effect is to assign a higher probability to recently 
encountered words were used to calculate the perplexity of a testing text the perplexity of the combined model was lower by a factor of more than 3 How representative are our results We suspect that if we had employed a training text from one source for instance business correspondence and a testing text drawn from another source for instance sports journalism the advantages of including a cache component in the language model would have been even more apparent since the 3g gram component would not be as good a predictor in this case By the same logic if both the training text and the testing text came from the same narrowly defined area of discourse the cache component would probably not bring about such a dramatic improvement as was observed by us because the 3g gram component would be a better predictor than it was with the highly diverse training and testing texts we employed Similarly an increase in the size of the training text would make the 3g gram component of the model more reliable and hence 
decrease the relative amount of improvement in perplexity contributed by the cache component Thus the importance of our results is in the trend they show not in the precise values we obtained these depend on the size and origin of both the training text and the testing text Nevertheless the sheer magnitude of the improvement in perplexity over the pure 3g gram model we achieved by incorporating a cache component a factor of 3 indicates that we are not being misled by some random fluctuation in word frequencies A time locality effect exists and it is important Would the performance of the trigram model also be improved if a cache component were incorporated with it Strictly speaking the data presented here do not answer the question one way or the other It is probably true that the two words preceding a given word often indicate the nature of the topic being discussed or the verbal habits of the speaker so that the trigram model would perform better than the 3g gram model on a diverse testing text such as the 
one we used On the other hand we think that a great deal of information about the lexical choices likely to be made by the speaker depends on what happened hundreds of words ago and will thus be lost by the trigram model yet is easily obtained by means of a cache component The only way to find out is to test a pure trigram model against a trigram plus cache model we hope that those of our readers with sufficient resources to try the experiment will do so It came as a surprise to us that in general the best fit weight of the cache component for function POS s was higher than the best fit weight for the cache component for content POS s We had naively assumed that the bestfit weight for the cache component would reflect the burstiness of the POS in question and hence be larger for content POS s Actually another factor seems to be more powerful in determining the weighting for the cache component of a given POS the less diverse a category is the larger its best fit cache component weight For instance the POS 
category PP3A contains only the words he and she the corresponding cache of 200 words containing these two words in different proportions will contain a precise estimate of their probability distribution within the category In this case the best fit cache weight was 1 000 On the other hand consider a POS category like N N containing singular nouns By definition all the singular nouns that were not among the last 200 singular nouns will be assigned a probability of zero which is unrealistic furthermore even probability estimates for nouns within the cache are based on a very small sample Thus diverse categories like this one need the information in the 3g gram component to smooth out the probability estimates for N N the best fit cache component weight is only 0 403 If there is a moral to this it is a point often made by Jelinek whenever possible do not make intuitive judg 5 82 IEEE TRAhSACTIONS ON PATIPRN ANALYSIS A N D MACHllLE INTELLIGENCE VOL 12 NO 6 J U N E 1990 ments about the parameters of your 
language model but train them from actual data We might easily have decided in advance that building cache components for function POS s was pointless thus impairing the performance of the combined model An interesting point is that the way we implemented our model was actually rather unfavorable to the cache component for content POS s Recall that for training and testing purposes we created two texts which were concatenations of 2000 word actual texts taken from the LOB Corpus As there are 19 caches containing 200 words each the caches at any time during training and testing except the beginning contain many words from previous actual texts Even though actual texts drawn from the same general category were placed together content words may differ so much in frequency from actual text to actual text that this implementation reduces the efficiency of the cache component Perhaps all caches should be emptied at the end of each actual text even though this would mean that there are often less than 200 words in 
each cache In this case we almost certainly would want the weighting of the cache component to depend on the number of words in the cache in a more sophisticated way than our current step function heuristic which sets the cache contribution to 0 until the cache has 5 words in it then leaves it constant Several other ideas for improvements have occurred to us One might explore the possibility of building a morphological component so that the occurrence of a word would increase the estimated probability of related words Thus the occurrence of the singular form of a noun would raise the probability of its plural and vice versa Different tenses and persons of a verb could be related in the same way Another promising idea would be to extend the idea of a model that dynamically tracks the linguistic behavior of the speaker or writer from the lexical to the syntactic component of the model In other words perhaps the relatively recent past before the preceding two POS s is a good guide to the POS s that will be 
employed as well as to the words that will be uttered Recently employed POS s would be assigned higher probabilities One might also consider combining the model considered here with the trigram model There are several different ways of doing this one of which was presented at the end of Section I11 a weighted average of the trigram model and our combined model Alternatively one could use our combined model only when a bigram is not found or rarely found in the training text Trigram purists who dislike the use of POS s might prefer to construct a more dynamic version of the original trigram model in which trigrams and bigrams encountered during the recognition task would be assigned higher probabilities than if they only occurred in the training text It seems obvious that this model would at a minimum handle noun phrases better than any existing model one has only to glance at a newspaper story to see the same often very idiosyncratic noun phrases appearing again and again The line of research described in 
this paper has more general implications The results above suggest that at a given time a human being works with only a small fraction of his vocabulary Perhaps if we followed an individual s written or spoken use of language through the course of a day it would consist largely of time spent in language islands or sublanguages with brief periods of time during which he is in transition between islands One might attempt to chart these islands by identifying groups of words which often occur together in the language If this work is ever carried out on a large scale it could lead to pseudosemantic language models for speech recognition since the occurrence of several words characteristic of an island makes the appearance of all words in that island more probable ACKNOWLEDGMENT We would like to thank the two anonymous referees whose comments helped to make this a better paper REFERENCES A M Derouault and B Mirialdo Natural language modeling for phoneme to text transcription IEEE Trans Pattern Anal Machine Inrell 
vol PAMI 8 pp 742 749 Nov 1986 Language modeling at the syntactic level in Proc 7th Int Con Parrern Recognirion vol 11 Montreal Aug 1984 pp 13731375 F Jelinek The development of an experimental discrete dictation recognizer Proc IEEE vol 73 no 1 1 pp 1616 1624 Nov 1985 F Jelinek R L Mercer and L R Bahl A maximum likehood approach to continuous speech recognition IEEE Trans Parrern Anal Machine Inreelf vol PAMI 5 pp 179 90 Mar 1983 F Jelinek and R L Mercer Interpolated estimation of Markov source parameters from sparse data in Parrern Recognition in Pracrice E S Gelsema and L H Kanal Eds 1981 pp 381 397 S Johansson E Atwell R Garside and G Leech The TaggedLOE Corpus User s Manual Norwegian Computing Centre for the Humanities Bergen Norway 1986 S Johansson Some observations on word frequencies in three corpora of present day English texts ITL Rev Appl Linguisrics vol 67 68 pp 117 126 1985 Word frequency and text type Some observations based on the LOB corpus of British English texts Compur Humanities vol 19 PP 
23 36 1985 9 S Katz Recursive M gram modeling via a smoothing of Turing s formula forthcoming paper E M Muckstein A natural language parser with statistical applications IBM Res Rep RC7516 38450 Mar 1981 A Nadas Estimation of probabilities in the language model of the IBM speech recognition system IEEE Trans Acousr Speech Signal Processing vol 32 pp 859 861 Aug 1984 I Peterson and A Silberschatz Operaring Sysrem Concepts Reading MA Addison Wesley 1983 L R Rabiner and B H Juang An introduction to hidden Markov models IEEE ASSP Mag pp 4 16 Jan 1986 Roland Kuhn received the B Sc Honours degree in mathematics and biology from Trinity College University of Toronto Toronto Ont Canada the S M degree in theoretical biology from the University of Chicago Chicago IL and the M Sc degree in computer science from McGill University Montreal P Q Canada He is now a doctoral candidate in the McGill University School of Computer Science His research interests include natural language speech recognition and robotics KUHN A N D 
DE MORI LANGUAGE MODEL FOR SPEECH RECOGNITION 583 pers published mostly in international journals and in the proceedings of international conferences devoted to computer systems pattern recognition and artificial intelligence Dr De Mori is Associate Editor of the IEEE TRANSACTIONS O N PATTERN AND MACHINE INTELLIGENCE and of the journals Computer Speech ANALYSIS and Languuge New York Academic Signal Processing Speech Communication and Pattern Recognition Letters Amsterdam The Netherlands North Holland He is the Vice President of the Canadian Society for Computational Studies of Intelligence Chairman of the Artificial Intelligence Associate Committee of the National Research Council of Canada member of the Scientific Council of the Centre National d Etudes de Telecommunications CNET Lannion A France and of the Information Technology Research Centre of the Province of Ontario Renato De Mori was born in Milan Italy in 1941 He received the Doctorate degree in electronic engineering from Politecnico di Torino 
Torino Italy in 1967 Since January 1986 he hds been Professor dnd the Director of the School of Computer Science at McGill University Montreal P Q Canada Since March 1987 he has been Vice President and Director of research at the Centre de Recherche en Infomatique de Montreal CRIM a research center involving seven universities and more than twenty companies He has been a member of various committees in the U S Europe and Canada He is the author of 3 books and over 100 pa 1
47	b	The Breast xxx 2013 1e6 Contents lists available at ScienceDirect The Breast journal homepage www elsevier com brst Original article Factors predictive of immediate breast reconstruction following mastectomy for invasive breast cancer in Australia D Roder a b H Zorbas a J Kollias c d e C Pyke c D Walters c I Campbell f c C Taylor c F Webster a a Cancer Australia Sydney New South Wales Australia School of Population Health University of South Australia Adelaide Australia Quality Audit Steering Committee Breast Surgeons of Australia and New Zealand Sydney New South Wales Australia d Breast Endocrine and Surgical Oncology Unit Royal Adelaide Hospital Adelaide Australia e University of Adelaide Adelaide Australia f Waikato Clinical School University of Auckland Faculty of Medical and Health Sciences Hamilton New Zealand b c a r t i c l e i n f o a b s t r a c t Article history Received 3 December 2012 Received in revised form 17 September 2013 Accepted 23 September 2013 Purpose To investigate person cancer and 
treatment determinants of immediate breast reconstruction IBR in Australia Methods Bi variable and multi variable analyses of the Quality Audit database Results Of 12 707 invasive cancers treated by mastectomy circa 1998e2010 8 had IBR This proportion increased over time and reduced from 29 in women below 30 years to approximately 1 in those aged 70 years or more Multiple regression indicated that other IBR predictors included high socio economic status private health insurance being asymptomatic a metropolitan rather than inner regional treatment centre higher surgeon case load small tumour size negative nodal status positive progesterone receptor status more cancer foci multiple affected breast quadrants synchronous bilateral cancer not having neo adjuvant chemotherapy adjuvant radiotherapy or adjuvant hormone therapy and receiving ovarian ablation Conclusions Variations in access to specialty services and other possible causes of variations in IBR rates need further investigation Ó 2013 Elsevier Ltd All 
rights reserved Keywords Invasive breast cancer Breast reconstruction Mastectomy Introduction Improving cosmetic outcomes of breast cancer surgery has long been a priority in clinical practice 1 2 This is reﬂected in the increase in breast conserving surgery following the 1990 Consensus Statement from the U S National Institutes of Health that equivalent survivals occur from early breast cancer irrespective of whether treated by mastectomy or breast conserving surgery and radiotherapy 3 Australian clinical practice guidelines for the management of early breast cancer released in 1995 and 2001 were consistent with this Statement and indicated the importance Corresponding author 4 Stonyfell Road Stonyfell South Australia 5066 Australia Tel þ61 8 8431 6240 fax þ61 2 9357 9477 E mail addresses roder internode on net D Roder Helen Zorbas canceraustralia gov au H Zorbas Jim Kollias health sa gov au J Kollias Christophermpyke hotmail com C Pyke djw walterssurgery com au D Walters Ian Campbell waikatodhb health nz I 
Campbell Corey Taylor surgeons org C Taylor Fleur Webster canceraustralia gov au F Webster of women having a choice between breast conserving surgery and mastectomy 4 The U S Statement and associated trial evidence were followed by increases in breast conserving surgery rates in the U S Canada and some European countries 5e10 Australian data also showed a trend away from mastectomy towards breast conserving surgery both nationally and in some jurisdictions 11e17 Today around 60 of early breast cancers are treated by breast conserving surgery in Australia and the U S to improve cosmetic outcomes 7 17 Mastectomy is still the preferred surgical option for many early breast cancers including large cancers relative to breast size and cancers in difﬁcult locations where cosmetic outcomes would be difﬁcult to achieve with breast conservation 18 On other occasions patient choice is a determining factor because of fear of incomplete excision increased likelihood of multiple operations and concerns of an increase in 
risk of recurrence following breast conserving surgery 19 20 A number of studies have indicated that clinician choice may also be important 19e22 In a previous 0960 9776 e see front matter Ó 2013 Elsevier Ltd All rights reserved http dx doi org 10 1016 j breast 2013 09 011 Please cite this article in press as Roder D et al Factors predictive of immediate breast reconstruction following mastectomy for invasive breast cancer in Australia The Breast 2013 http dx doi org 10 1016 j breast 2013 09 011 2 D Roder et al The Breast xxx 2013 1e6 analysis of the Quality Audit database of the Society of Breast Surgeons of Australia and New Zealand mastectomy was found to be more common in women with large cancers those living in locations remote from major city centres and those treated by surgeons with low case volumes 23 North American data indicate that U S surgeons trained prior to the 1980s and male surgeons in particular were more likely to use mastectomy than breast conserving surgery 19 22 In addition access to 
specialists in breast reconstruction may be another important factor when women consider their surgical options 24 For women treated by mastectomy immediate or delayedimmediate breast reconstruction is often used to improve cosmetic outcomes 25 Innovations in surgical technique in the 1990s have led to skin sparing surgery that facilitated IBR 26 and reportedly had psychological beneﬁts for many women with a heightened fear of breast loss and disﬁgurement 27 28 In addition IBR with autogenous tissue transplantation rather than silicone implants became more common leading to more natural breast texture and potential psychological beneﬁts 27 28 Despite these advances IBR is still used for only a minority of mastectomy patients 17 24e29 One reason may be women s preferences to avoid major surgical procedures and concern that complications of IBR might delay adjuvant therapy Not being given the option of IBR and or a lack of available surgical expertise in smaller centres may be other reasons There may also be 
differences in demand by age and other sociodemographic characteristics and perhaps barriers from concerns that implants might compromise detection of local cancer recurrences and lead to poorer survival outcomes despite research evidence to the contrary 30e34 There is also concern that where post mastectomy radiotherapy is given for improved cancer control and survival this may detract from IBR healing and cosmetic outcomes 35e39 or that the IBR may impede radiotherapy effectiveness 36e38 Studies in some populations have found breast reconstruction rates to vary by geographic area of residence age of patient race income status and private health insurance status availability of breast reconstruction specialists general health status and tumour characteristics such as size nodal status number of tumour foci and whether the cancer is bilateral or unilateral 24 31 40 Availability of multidisciplinary team treatment planning is another factor that may affect reconstruction rates and coordination of 
reconstruction services In this study we investigate IBR rates following mastectomy for early invasive breast cancer in women treated by Australian breast surgeons participating in the Quality Audit 41 Although these cancers were not selected to be representative of all early breast cancers in Australia they comprise the majority and appear to be broadly representative in their survival outcomes 41e44 Also differences in survival from these cancers by conventional risk factors such as tumour size grade nodal status and oestrogen receptor status have accorded with differences observed in population based studies indicating that these data may be a credible basis for population inference 41 Research and health service implications of differences in IBR rates by sociodemographic and tumour characteristic are explored Methods Patients We analysed data for early invasive breast cancers treated by mastectomy by surgeons participating in the Quality Audit circa 1998e2010 DCIS cases were excluded The number of 
patients covered by the Audit has increased progressively and represents about 60 of women with early breast cancer in Australia 41e44 The Audit did not record residential postcode as part of its minimum data set throughout the study period We analyse data for 12 707 early invasive breast cancers diagnosed in Australian women and treated by mastectomy where residential postcodes were recorded This enabled analyses of IBR by remoteness and socio economic status of residential area which were factors of central interest 42 45 IBR was recorded for 958 invasive cases Statistical analysis Variables analysed as candidate predictors of IBR included all person provider cancer and treatment characteristics recorded on the database This breadth of analysis was undertaken because other studies indicated associations of a wide range of characteristics with breast reconstruction rates 24 31 40 Variables analysed included Age at diagnosis 30 30e39 40e49 50e59 60e69 70e79 80þ years Place of residence major city inner 
regional and more remote 42 Private health insurance status Table 1 Relative rates 95 conﬁdence limits of immediate breast reconstruction IBR following mastectomy Australia Breast Cancer Audit circa 1998e2010a By sociodemographic and provider characteristics Characteristic Numbers IBR No IBR Age at diagnosis yrs Under 30 9 29 0 22 71 0 30e39 82 18 7 356 81 3 40e49 320 16 8 1583 83 2 50e59 324 11 0 2630 89 0 60e69 186 5 8 2998 94 2 70e79 29 1 3 2288 98 7 8 0 4 1872 99 6 80þ Residential location Major city 695 7 9 8157 92 1 Other 263 6 8 3592 93 2 Socio economic quintile 1 low 158 6 3 2343 93 7 2 169 6 9 2263 93 1 3 174 7 0 2322 93 0 4 175 6 7 2425 93 3 5 high 282 10 5 2396 89 5 Private health insurance No 234 4 8 4644 95 2 Yes 691 11 2 5487 88 8 Diagnostic epoch 2000 1 0 2 484 99 8 2000e2002 21 2 0 1016 98 0 2003e2005 103 4 7 2066 95 3 2006þ 833 9 2 8183 90 8 Referral source Symptomatic 568 7 2 7327 92 8 BreastScreen 180 7 6 2191 92 4 Other 127 10 9 1043 89 1 Treatment centre location Major city 774 8 9 7885 
91 1 Inner regional 120 3 9 2942 96 1 More remote 64 6 5 922 93 5 Surgeon annual case load 10 29 2 2 1265 97 8 11e30 155 5 3 2760 94 7 31e100 523 7 7 6235 92 3 101þ 251 14 4 1489 85 6 a b Relative rates p Valuesb 1 00 0 64 0 58 0 38 0 20 0 04 0 01 c2 6 p 0 001 MW p 0 001 0 36 0 33 0 22 0 11 0 02 0 01 1 16 1 01 0 66 0 36 0 08 0 04 1 00 0 87 0 76 1 00 c2 1 p 0 043 1 00 1 10 1 10 1 07 1 67 c2 4 p 0 001 MW p 0 001 0 89 0 90 0 87 1 38 1 36 1 36 1 31 2 01 1 00 2 33 2 02 2 69 c2 1 p 0 001 1 00 9 82 1 32 72 81 23 03 3 22 164 66 44 81 6 32 317 82 c2 3 p 0 001 MW p 0 001 1 00 1 06 0 90 1 24 1 51 1 26 1 81 c2 2 p 0 001 1 00 0 44 0 36 0 53 0 73 0 57 0 93 c2 2 p 0 001 MW p 0 001 1 00 2 37 1 60 3 51 3 45 2 39 5 00 6 44 4 41 9 39 c2 3 p 0 001 MW p 0 001 Invasive cancers treated by Australian breast surgeons see text c2 df Pearson chi square degrees of freedom MW Mann Whitney U test Please cite this article in press as Roder D et al Factors predictive of immediate breast reconstruction following mastectomy for invasive 
breast cancer in Australia The Breast 2013 http dx doi org 10 1016 j breast 2013 09 011 D Roder et al The Breast xxx 2013 1e6 Socio economic quintile SEIFA Relative Index of Socio economic Disadvantage inferred from residential postcode 42 45 Location of treatment centre major city inner regional more remote 42 Surgeon mean annual case load 10 11e30 31e100 101þ Year of diagnosis Referral source symptomatic non symptomatic BreastScreen non symptomatic other Breast cancer size 10 10e14 15e19 20e29 30e39 40þ mm histology type ductal lobular other grade low intermediate high lymphatic vascular invasion positive negative nodal involvement positive negative oestrogen and progesterone receptor status positive negative respectively HER 2 receptor status positive negative breast location including whether multiple quadrants were affected number of tumour foci 1 2 3þ and whether the cancer was unilateral or bilateral 23 Treatment by neo adjuvant radiotherapy chemotherapy hormone therapy aromatase inhibitor and anti 
HER 2 immunotherapy adjuvant radiotherapy chemotherapy hormone therapy and aromatase inhibitor and ovarian ablation and history of prior ipsilateral or contralateral breast cancers treated by surgery Initially bi variable associations with IBR were explored using the Pearson chi square test for binary and nominal variables and the ManneWhitney U test for ordinal variables 46 47 Relative rates i e rate ratios for IBR were analysed by variable category Then multiple logistic regression analyses were undertaken to Table 2 Relative rates 95 conﬁdence limits of immediate breast reconstruction IBR following mastectomy Australian Breast Cancer Audit circa 1998e2010a By cancer characteristics Characteristic Numbers IBR Tumour size mm 10 190 13 3 10e14 131 8 3 15e19 127 7 2 20e29 189 6 6 30e39 126 7 2 40þ 179 6 1 Grade Low 161 8 3 Intermediate 424 7 8 High 350 7 2 Nodal status Negative 519 9 2 Positive 382 6 3 Oestrogen receptor Negative 186 6 7 Positive 748 7 8 Progesterone receptor Negative 257 6 3 Positive 675 8 3 
Number of cancer foci 1 361 5 4 2 90 8 0 3þ 165 9 7 Tumour location Other 484 5 9 Axillary tail 0 0 0 1 quadrant 75 1 7 Medial 29 8 3 Synchronous bilateral No 880 8 2 Yes 73 13 2 a b No IBR Relative rates p Valuesb 1241 1451 1649 2674 1623 2751 1 00 0 62 0 54 0 50 0 54 0 46 c2 5 p 0 001 MW p 0 001 86 7 91 7 92 8 93 4 92 8 93 9 0 51 0 43 0 41 0 44 0 38 0 77 0 67 0 60 0 67 0 56 determine key predictors of IBR checking that model assumptions such as lack of co linearity were met 46 47 Results The proportion of mastectomy cases having IBR was 7 5 Results of bi variable and multi variable analyses were as follows Bi variable IBR proportions reduced with age at diagnosis from 29 0 for women under 30 years to 0 4 for those aged 80 years or more p 0 001 i e relative rate 95 conﬁdence limits of 0 01 0 01 0 04 Table 1 Socio demographic and provider characteristics predictive of IBR included residence in a major city p 0 043 higher socio economic quintile p 0 001 private health insurance p 0 001 a more recent diagnosis 
p 0 001 asymptomatic referral other than BreastScreen e g in response to surveillance of high risk women or de facto private screening p 0 001 major city treatment centre p 0 001 and higher surgeon case load p 0 001 Table 1 IBR proportions were lower for larger tumour size p 0 001 relative rate of 0 46 0 38 0 56 for 40 mm compared with 10 mm Table 2 Cancer characteristics predictive of IBR included negative nodal status p 0 001 positive oestrogen receptor status p 0 041 positive progesterone receptor status p 0 001 multiple cancer foci p 0 001 multiple breast quadrant involvement p 0 001 and presence of synchronous bilateral cancer p 0 001 Table 2 No associations were found with histology type grade lymphatic vascular invasion HER 2 receptor status or triple negative status i e oestrogen progesterone and HER 2 receptor negative p 0 050 The IBR proportion was higher for cases who had previously experienced a surgically treated ipsilateral breast cancer p 0 001 relative rate of 1 63 1 30 2 04 Table 3 whereas 
no Table 3 Relative rates 95 conﬁdence limits of immediate breast reconstruction IBR following mastectomy Australian Breast Cancer Audit circa 1998e2010a By treatment characteristics Characteristic 1 00 0 94 0 79 1 12 0 86 0 72 1 03 5094 90 8 5721 93 7 1 00 0 68 0 60 0 77 c2 1 p 0 001 2600 93 3 8791 92 2 1 00 1 18 1 01 1 37 c2 1 p 0 041 3854 93 7 7464 91 7 1 00 1 33 1 15 1 52 c2 1 p 0 001 6308 94 6 1029 92 0 1528 90 3 1 00 1 49 1 19 1 86 1 80 1 51 2 15 c2 2 p 0 001 7705 94 1 41 100 0 434 98 3 320 91 7 1 00 0 00 0 00 1 91 2 49 1 99 3 13 1 41 0 98 2 01 9840 91 8 480 86 8 1 00 1 61 1 29 2 01 MW p 0 086 MW p 0 001 c2 3 p 0 001 c2 1 p 0 001 Invasive cancers treated by Australian breast surgeons see text c2 df Pearson chi square degrees of freedom MW Mann Whitney U test Numbers IBR c2 2 p 0 230 1784 91 7 5020 92 2 4544 92 8 3 No IBR Prior ipsilateral breast cancer treated by surgery No 547 7 2 7026 92 8 Yes 77 11 8 578 88 2 Neo adjuvant chemotherapy No 839 8 6 8913 91 4 Yes 30 5 4 524 94 6 Neo adjuvant hormone 
therapy No 869 8 5 9318 91 5 Yes 0 0 56 100 Adjuvant radiotherapy No 594 9 2 5876 90 8 Yes 293 6 4 4258 93 6 Adjuvant chemotherapy No 290 6 8 3972 93 2 Yes 586 8 5 6278 91 5 Adjuvant hormone therapy No 485 8 6 5130 91 4 Yes 353 7 5 4349 92 5 Ovarian ablation No 748 8 0 8622 92 0 Yes 78 20 2 308 79 8 Relative rates p Valuesb 1 00 1 63 1 30 2 04 c2 1 p 0 001 1 00 0 63 0 44 0 90 c2 1 p 0 009 1 00 0 00 0 00 0 93 FET p 0 013 1 00 0 70 0 61 0 80 c2 1 p 0 001 1 00 1 25 1 10 1 44 c2 1 p 0 001 1 00 0 87 0 76 0 99 c2 1 p 0 036 1 00 2 53 2 05 3 12 c2 1 p 0 001 a Invasive cancers treated by Australian breast surgeons see text c2 df Pearson chi square degrees of freedom FET Fisher Exact test 2tailed b Please cite this article in press as Roder D et al Factors predictive of immediate breast reconstruction following mastectomy for invasive breast cancer in Australia The Breast 2013 http dx doi org 10 1016 j breast 2013 09 011 4 D Roder et al The Breast xxx 2013 1e6 Table 4 Relative odds 95 conﬁdence limits of immediate 
breast reconstruction IBR following mastectomy Australian Breast Cancer Audit circa 1998e2010a Multiple logistic regression Predictor Age at diagnosis yrs 30 n 31 30e39 n 438 40e49 n 1903 50e59 n 2954 60e69 n 3184 70e79 n 2317 80þ n 1880 Socio economic quintile 1 low n 2501 2e4 mid range n 7528 5 high n 2678 Private health insurance No n 4878 Yes n 6178 Unknown n 1651 Diagnostic year 2000 n 485 2000e02 n 1037 2003e05 n 2169 2006þ n 9016 Referral source Symptomatic n 7895 BreastScreen n 2371 Other n 1170 Unknown n 1271 Treatment centre location Major city n 8659 Inner regional n 3062 More remote n 986 Surgeon annual case load 10 n 1294 11e30 n 2915 31e100 n 6758 101þ n 1740 Tumour size 10 n 1431 10e14 n 1582 15e19 n 1776 20e29 2863 30e39 n 1749 40þ n 2930 Unknown n 376 Nodal status Negative n 5613 Positive n 6103 Unknown n 991 Progesterone receptor Negative n 4111 Positive n 8139 Unknown n 457 Number of cancer foci 1 n 6669 2 n 1119 3þ 1693 Unknown n 3226 Number of quadrants affected 1 n 8579 1 n 509 Unknown 
n 3619 Synchronous bilateral No n 10 720 Yes n 553 Unknown n 1434 Prior ipsilateral breast cancer treated by surgery No n 7573 Yes n 655 Unknown 4479 Neo adjuvant chemotherapy No n 9752 Yes n 554 Unknown n 2401 Relative odds 1 00 0 52 0 46 0 26 0 12 0 03 0 01 0 21 0 19 0 11 0 05 0 01 0 00 1 30 1 11 0 63 0 29 0 07 0 03 1 00 1 13 0 93 1 38 1 67 1 34 2 09 1 00 2 08 1 75 2 44 1 40 0 88 2 24 1 00 6 77 0 89 51 33 10 02 1 37 73 08 12 22 1 69 88 61 1 00 1 21 1 00 1 48 1 32 1 04 1 67 1 27 0 93 1 73 1 00 0 60 0 48 0 74 1 00 0 74 1 35 1 00 1 93 1 27 2 95 2 29 1 53 3 41 4 74 3 07 7 31 1 00 0 69 0 53 0 90 0 65 0 50 0 85 0 61 0 48 0 78 0 66 0 50 0 87 0 59 0 46 0 77 0 17 0 09 0 33 1 00 0 71 0 59 0 84 0 80 0 56 1 15 1 00 1 41 1 01 1 37 0 78 0 49 1 25 1 00 1 18 0 90 1 54 1 34 1 08 1 67 1 79 1 28 2 50 1 00 1 64 1 22 2 22 1 18 0 85 1 64 1 00 1 45 1 09 1 92 0 04 0 01 0 12 1 00 1 33 0 99 1 80 0 75 0 50 1 12 1 00 0 65 0 43 0 98 0 86 0 59 1 24 Table 4 continued Predictor Adjuvant radiotherapy No n 6470 Yes n 4551 Unknown n 1686 
Adjuvant hormone therapy No n 5615 Yes n 4702 Unknown n 2390 Ovarian ablation No n 9370 Yes n 386 Unknown n 2951 a Relative odds 1 00 0 62 0 51 0 75 0 86 0 63 1 18 1 00 0 80 0 67 0 95 0 85 0 62 1 17 1 00 1 41 1 05 1 89 1 11 0 79 1 56 Invasive cancers treated by Australian breast surgeons see text association was found with history of a prior contralateral breast cancer p 0 129 Treatment characteristics predictive of IBR included adjuvant chemotherapy p 0 001 ovarian ablation p 0 001 and not having neo adjuvant chemotherapy p 0 009 not having neo adjuvant hormone therapy p 0 009 not having adjuvant radiotherapy p 0 001 and not having adjuvant hormone therapy p 0 013 Table 3 No associations were found with neo adjuvant radiotherapy anti HER 2 immunotherapy or treatment with aromatase inhibitors ovarian ablation or with adjuvant anti HER 2 immunotherapy or treatment with aromatase inhibitors p 0 100 Multiple logistic regression Results of multiple regression analysis indicated that predictors of IBR included 
lower age at diagnosis high socio economic quintile having private health insurance a later diagnostic epoch asymptomatic referral from BreastScreen or another source treatment at a major city as opposed to inner regional centre higher surgeon case load small tumour size 10 mm negative nodal status positive progesterone status three or more cancer foci more than one breast quadrant affected presence of synchronous bilateral breast cancer not having neo adjuvant chemotherapy not having adjuvant radiotherapy or adjuvant hormone therapy and having adjuvant ovarian ablation Table 4 There was also the indication that women treated surgically for prior ipsilateral breast cancer were more likely to have IBR but statistical signiﬁcance was not achieved p 0 060 No other cancer or treatment characteristic approached statistical signiﬁcance as a predictor of IBR p 0 100 Discussion Data from the Quality Audit indicate an 8 IBR rate for women undergoing mastectomy for early invasive breast cancers diagnosed circa 
1998e2010 An upward trend in IBR rate was evident with a peak of 9 applying for cases diagnosed in 2006e2010 Early preliminary data for 2011e2012 indicate a further rise in IBR rate to 12 This upward trend may reﬂect an increase in number of breast surgeons trained in both the oncological and breast reconstructive aspects of breast cancer management The IBR rate for the 1998e2010 study years ranged from 29 for women under 30 years to less than one per cent for those aged 80 years or more and showed an increase over time The overall 8 ﬁgure lies within the range that would apply in the U S U S SEER data aggregated for IBR plus reconstructions within the ﬁrst four months from mastectomy indicate rates ranging by state from 8 to 34 24 31 40 Other data for combined IBR and later Please cite this article in press as Roder D et al Factors predictive of immediate breast reconstruction following mastectomy for invasive breast cancer in Australia The Breast 2013 http dx doi org 10 1016 j breast 2013 09 011 D Roder et 
al The Breast xxx 2013 1e6 reconstructions indicate rates of 8 for Ontario in 1994e1995 14 for Denmark in 1999 16 for New Zealand in 2008 and 17 for England in 2006 26 48 49 International comparisons are complicated by variable inclusion of delayed reconstructions The American Society of Plastic Surgeons reported in 2003 that 60 of breast reconstructions performed by its members were not IBR with many occurring after the 4 month period covered by the SEER data 26 Another potential source of variation is differential inclusion of ductal carcinoma insitu DCIS lesions 31 These lesions were excluded from the present study which would have lowered the IBR rate note the Audit database indicates a more than 2 fold IBR rate for DCIS compared with early invasive cancer i e 19 Vs 8 Consistent with the SEER data IBR rates in this study were lower in older women those from lower socio economic areas and those without private insurance 26 29 31 38 Older women may be more accepting of loss of body form through mastectomy 
than younger women and less willing to have more major surgery which may lead to a lower demand for IBR Also they may have a higher prevalence of co morbidity of a magnitude that would contra indicate the longer and more complex operations for combined mastectomy and IBR Meanwhile women from lower socioeconomic backgrounds and those without private insurance may have fewer IBR service options leading to a lower IBR Results from a previous study identiﬁed poorer access to breast reconstruction in the public sector and non metropolitan treatment centres which suggested a need to develop effective models for the rural sector address a lack of plastic surgeons reduce waiting times for reconstruction surgery in the public sector improve training in breast reconstruction and increase access to multidisciplinary team treatment planning 50 Asymptomatic women referred from BreastScreen and other sources had higher IBR rates after adjusting for tumour size nodal status and other clinical patient and service provider 
characteristics This may reﬂect differences in referral pathways with more asymptomatic women being referred to surgeons who perform IBR Meanwhile women attending surgeons in inner regional rather than major city centres were less likely to receive IBR This may be due to differences in service availability IBR was more common when surgeons had an annual case load of 11 rather than a lower case load This may relate to the availability of plastic and reconstructive surgeons to participate in a multidisciplinary team with breast surgeons Alternatively high volume breast surgeons may have had more training and experience in breast reconstructive and oncoplastic techniques simplifying the logistics of performing IBR As observed with SEER data lower IBR rates applied for more advanced disease as indicated by tumour size and nodal status 31 39 Adjuvant radiotherapy is recommended for large tumours 50 mm and or in the presence of metastatic involvement of over three axillary lymph nodes 51 and has been shown to 
offer a survival advantage for invasive breast cancers with intermediate risk features for loco regional recurrence 52 53 Post mastectomy radiotherapy has been associated with signiﬁcant short and longterm complication rates for patients undergoing IBR using tissue expander implant methods 54 and with autogenous tissue methods including TRAM and DIEP ﬂaps 55 The results from a recent Australian study have indicated that the most common reason for not offering IBR is the anticipated need for postmastectomy radiotherapy for larger tumours with clinical axillary involvement 56 Other factors such as signiﬁcant medical comorbidities and smoking have also been associated with detrimental outcomes for IBR 57 It is likely that many surgeons take these issues into consideration when offering IBR for women undergoing mastectomy for invasive breast cancer 5 By comparison higher IBR rates applied for multifocal tumours those affecting more than one breast quadrant and synchronous bilateral breast cancers Although these 
cancers may be less amenable to breast conserving therapy they may still exhibit features that may not require post mastectomy radiotherapy e g small invasive cancer with an extensive DCIS component where mastectomy and IBR may be regarded as a reasonable option 23 The reason for the statistical association of IBR with ovarian ablation is not known and requires further investigation It is hypothesized that this may reﬂect concerns of many younger premenopausal women having IBR about body image and a preference to have ovarian ablation rather than adjuvant chemotherapy to avoid treatment side effects such as alopecia Meanwhile the suggested elevation in IBR rate for women who had a prior ipsilateral breast cancer treated by surgery although not quite statistically signiﬁcant in the multiple regression analysis relative odds 1 33 0 99 1 80 would be due to a history of earlier radiotherapy with their breast conserving surgery such that radiotherapy was not recommended again with their mastectomy for local 
recurrence Such cases without further radiotherapy would be good candidates for IBR Again this warrants further investigation together with the association of IBR with PR positive cancers which we suspect may have occurred by chance Data from the Quality Audit are important for describing the practices of specialist breast surgeons in Australia In addition other population based data should be collected to gain a more complete picture with provision for linkage of these data to cancer registries Conclusions IBR rates vary by socio demographic and provider characteristics Reasons for these differences including potential effects of variations in service access require further investigation In particular the reasons for low IBR rates for low case load surgeons and inner regional centres and for women from lower socioeconomic areas and those without private insurance raises questions about equity of access Additional population based data on breast reconstruction are needed to obtain a more complete picture 
with provision for linkage with cancer registry data Ethical approval Ethics approval for this study was obtained from the research ethics committee of the Royal Australasian College of Surgeons Conﬂicts of interest statement The authors have no conﬂicts of interest References 1 Dean C Chetty U Forrest AP Effects of immediate breast reconstruction on psychosocial morbidity after mastectomy Lancet 1983 1 459e62 2 Schain WS Breast reconstruction update of psychosocial and prognostic concerns Cancer 1991 68 1170e5 3 U S Dept Health Human Services National Institutes of Health NIH Consensus Development Program Treatment of early stage breast cancer NIH consensus development conference statement June 18e21 1990 8 1e19 Online 4 National Breast Cancer Centre Clinical practice guidelines for the management of early breast cancer 2nd ed Canberra National Health Medical Research Council 2001 5 Abrams JS Phillips PH Friedman MA Meeting highlights a reappraisal of research results for the local treatment of early stage 
breast cancer J Natl Cancer Inst 1995 87 1837e45 6 Fisher B Anderson S Redmond CK Wolmark N Wickerham DL Cronin WM Reanalysis and results after 12 years of follow up in a randomized clinical trial Please cite this article in press as Roder D et al Factors predictive of immediate breast reconstruction following mastectomy for invasive breast cancer in Australia The Breast 2013 http dx doi org 10 1016 j breast 2013 09 011 6 D Roder et al The Breast xxx 2013 1e6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 composing total mastectomy with lumpectomy with or without irradiation in the treatment of breast cancer N Engl J Med 1995 333 1456e61 Habermann EB Abbott A Parsons HM Virnig BA Al Refaie WB Tuttle TM Are mastectomy rates really increasing in the United States J Clin Oncol 2010 28 3437e41 Lazovich D White E Thomas DB Moe RE Taplin S Change in use of breastconserving surgery in western Washington after the 1990 NIH Consensus Development Conference Arch Surg 1997 132 418e23 Gaudettee 
LA Gao RN Spence A Shi F Johansen H Olivotto IA Declining use of mastectomy for invasive breast cancer in Canada 1981e2000 Can J Public Health 2004 95 336e40 Zorzi M Puliti D Vettorazzi M De Lisi V Falcini F Federico M et al Mastectomy rates are decreasing in the era of service screening a populationbased study in Italy 1997e2001 Br J Cancer 2006 95 1265e8 Hill D Jamrozik K White V Collins J Boyages J Shugg D et al Surgical management of breast cancer in Australia in 1995 Sydney NHMRC National Breast Cancer Centre 1999 Hill DJ Giles GG Russell IS Collins JP Mapperson KJ Management of primary operable breast cancer in Victoria Med J Aust 1990 152 67e72 Hill DJ White VM Giles GG Collins JP Kitchen PR Changes in the investigation and management of primary operable breast cancer in Victoria Med J Aust 1994 161 110e1 Byrne MJ Jamrozik K Parsons RW Fitzgerald CJ Dewar JM Harvey JM et al Breast cancer in Western Australia in 1989 II Diagnosis and primary management Aust N Z J Surg 1993 63 624e9 Kricker A Using 
linked data to explore quality of care for breast cancer N S W Public Health Bull 2001 12 110e3 Department of Human Services South Australian Cancer Registry Epidemiology of cancer in South Australia Incidence mortality and survival 1977 to 1999 Incidence and mortality 1999 Adelaide Openbook Publ 2000 p 138 Cuncins Hearn AV Boult M Babidge W National breast cancer audit overview of invasive breast cancer management ANZ J Surg 2006 76 745e50 El Nemr M Rimareix F Karsenti G Acevedo Henao CM El Husseiny G Marsiglia H et al Breast reconstruction state of the arts Cancer Radiother 2012 16 302e8 Kotwall C Covington D Churchill P Brinker C Weintritt D Maxwell JG Breast conservation surgery for breast cancer at a regional medical center Am J Surg 1998 176 510e4 Caldon LJ Collins KA Wilde DJ Ahmedzai SH Noble TW Stotter A et al Why do hospital mastectomy rates vary Differences in the decision making experiences of women with breast cancer Br J Cancer 2011 104 1551e7 Dixon JM Mak C Predictors of mastectomy in a 
certiﬁed breast center e the surgeon is an independent risk factor Breast J 2008 14 321e3 Mandelblatt JS Berg CD Meropol NJ Edge SB Gold K Hwang YT et al Measuring and predicting surgeons practice styles for breast cancer treatment in older women Med Care 2001 39 228e42 Roder D Zorbas H Kollias J Pyke C Walters D Campbell I et al Factors predictive of treatment by Australian breast surgeons of invasive breast cancer by mastectomy rather than breast conserving surgery Asian Pac J Cancer Prev 2013 14 539e45 Jahkola T Asko Seljavaara S Von Smitten K Immediate breast reconstruction Scand J Surg 2003 92 249e56 Kronowitz SJ Hunt KK Kuerer HM Babiera G McNeese MD Buchholz TA et al Delayed immediate breast reconstruction Plast Reconstr Surg 2004 113 1617e28 Wilkins EG Alderman AK Breast reconstruction practices in North America current trends and future priorities Semin Plast Surg 2004 18 149e55 Cederna PS Yates WR Chang P Cram AE Ricciardelli EJ Post mastectomy reconstruction comparative analysis of psychosocial 
functional and cosmetic effects of transverse rectus abdominis musculocutaneous ﬂap versus breast implant reconstruction Ann Plast Surg 1995 85 458e68 Robb GL Immediate versus delayed breast reconstruction Breast Cancer Res 2007 9 Suppl 1 S9 Epub ahead of print Alderman AK McMahon L Wilkins EG The national utilization of immediate and early delayed breast reconstruction and the effect of sociodemographic factors Plast Reconstr Surg 2003 111 695e703 Heneghan HM Prichard RS Lyons R Regan PJ Kelly JL Malone C et al Quality of life after immediate breast reconstruction and skin sparing mastectomy e a comparison with patients undergoing breast surgery Eur J Surg Oncol 2011 37 937e43 Agarwal S Pappas L Neumayer L Agarwal J An analysis of immediate postmastectomy breast reconstruction frequency using the surveillance epidemiology and end results database Breast J 2011 17 352e8 32 Nedumpara T Jonker L Williams MR Impact of immediate breast reconstruction on breast cancer recurrence and survival Breast 2011 20 437e43 
33 Agarwal S Liu JH Crisera CA Buys S Agarwal JP Survival in breast cancer patients undergoing immediate breast reconstruction Breast J 2010 16 503e9 34 Kronowitz SJ Lam C Terefe W Hunt KK Kuerer HM Valero V et al A multidisciplinary protocol for planned skin preserving delayed breast reconstruction for patients with locally advanced breast cancer requiring postmastectomy radiation therapy 3 year follow up Plast Reconstr Surg 2011 127 2154e66 35 Kronowitz SJ Robb GL Radiation therapy and breast reconstruction a critical review of the literature Plast Reconstr Surg 2009 124 395e408 36 Robb GL Reconstructive surgery In Hunt KK Strom EA Ueno NT editors Breast cancer MD Anderson cancer series New York Springer Verlag Inc 2011 p 223e53 37 Tran NV Evans GR Kroll SS Baldwin BJ Miller MJ Reece GP et al Postoperative adjuvant irradiation effects on transverse rectus abdominis muscle ﬂap breast reconstruction Plast Reconstr Surg 2000 106 313e7 38 Kronowitz SJ Robb GL Breast reconstruction and radiation therapy In 
Singletary SA Robb GL Hortobagy GN editors Advanced therapy of breast disease 2nd ed London BC Decker Inc 2003 p 427e38 39 Sharma R Rourke LL Kronowitz SJ Oh JL Lucci A Litton JK et al Management of local regional recurrence following immediate breast reconstruction in patients with early breast cancer treated without postmastectomy radiotherapy Plast Reconstr Surg 2011 127 1763e72 40 Polednak AP Geographic variation in postmastectomy breast reconstruction rates Plast Reconstr Surg 2000 106 298e301 41 Roder D Wang JX Zorbas H Kollias J Maddern G Survival from breast cancers managed by surgeons participating in the National Breast Cancer Audit of the Royal Australasian College of Surgeons ANZ J Surg 2010 80 776e80 42 Australian Institute of Health and Welfare Cancer Australia Australasian Association of Cancer Registries Cancer survival and prevalence in Australia cancers diagnosed from 1982 to 2004 Cancer Series no 42 Cat No CAN 38 Canberra Australian Institute of Health and Welfare 2008 43 Australian 
Institute of Health and Welfare Australian cancer incidence and mortality ACIM books Canberra Australian Institute of Health and Welfare 2012 44 Cancer Institute NSW New cancer registry statistical reporting Sydney Cancer Institute NSW 2012 45 Australian Bureau of Statistics 1996 census of population and housing Socioeconomic index for areas Canberra Australian Bureau of Statistics 1998 46 Armitage P Berry G Statistical methods in medical research Oxford Blackwell Scientiﬁc Publication 1987 47 StataCorp Stata statistical software Release 9 2 College Station Texas StataCorp LP 2005 48 Platt J Baxter N Zhong T Breast reconstruction after mastectomy for breast cancer CMAJ 2011 183 2109e16 49 Ooi CW Campbell ID Kollias J De Silva P National Breast Cancer Audit overview of invasive breast cancer in New Zealand N Z Med J 2012 125 7e16 50 Sandelin K King E Redman S Breast reconstruction following mastectomy current status in Australia ANZ J Surg 2003 73 701e6 51 National Breast Cancer Centre Clinical practice 
guidelines for the management of early breast cancer 2nd ed Canberra Commonwealth of Australia 2001 52 Overgaard M Hansen PS Overgaard J Rose C Andersson M Bach F et al Postoperative radiotherapy in high risk premenopausal women with breast cancer who receive adjuvant chemotherapy Danish Breast Cancer Cooperative Group 82b Trial N Engl J Med 1997 337 949e55 53 Overgaard M Jensen MB Overgaard J Hansen PS Rose C Andersson M et al Postoperative radiotherapy in high risk postmenopausal breast cancer patients given adjuvant tamoxifen Danish Breast Cancer Cooperative Group DBCG 82c randomised trial Lancet 1999 353 1641e8 54 Whitﬁeld GA Horan G Irwin MS Malata CM Wishart GC Wilson CB Incidence of severe capsular contracture following implant based immediate breast reconstruction with or without chest wall radiotherapy using 40 Gray in 15 fractions Radiother Oncol 2009 90 141e7 55 Tran NV Chang DW Gupta A Kroll SS Robb GL Comparison of immediate and delayed free TRAM ﬂap breast reconstruction in patients receiving 
postmastectomy radiation therapy Plast Reconstr Surg 2001 108 78e82 56 Musgrave KJ Bochner M Kollias J Surgical decision making in immediate breast reconstruction World J Surg 2010 34 3029e35 57 Pinsolle V Grinfeder C Mathoulin Pelisser S Faucher A Complications analysis of 266 immediate breast reconstructions J Plast Reconstr Aesthet Surg 2006 59 1017e24 Please cite this article in press as Roder D et al Factors predictive of immediate breast reconstruction following mastectomy for invasive breast cancer in Australia The Breast 2013 http dx doi org 10 1016 j breast 2013 09 011 
48	b	The Breast xxx 2013 1e5 Contents lists available at ScienceDirect The Breast journal homepage www elsevier com brst Original article Subsequent axillary surgery after sentinel lymph node biopsy Results from the BreastSurgANZ Quality Audit 2006e2010 Chilton Chong a David Walters a b Primali de Silva b Corey Taylor b Andrew Spillane c James Kollias d Chris Pyke e Ian Campbell f Guy Maddern a g a Department of Surgery The Queen Elizabeth Hospital South Australia Australia BreastSurgANZ Quality Audit Australian Safety and Efﬁcacy Register of New Interventional Procedures e Surgical ASERNIP S Royal Australasian College of Surgeons North Adelaide Australia c Melanoma Institute of Australia Breast Oncology Centre North Sydney New South Wales Australia d Department of Surgery Royal Adelaide Hospital South Australia Australia e Mater Hospital Department of Surgery South Brisbane Queensland Australia f Waikato Clinical School University of Auckland Faculty of Medical and Health Sciences Auckland New Zealand g National 
Quality Audit Australian Safety and Efﬁcacy Register of New Interventional Procedures e Surgical ASERNIP S Royal Australasian College of Surgeons North Adelaide Australia b a r t i c l e i n f o a b s t r a c t Article history Received 4 July 2013 Received in revised form 19 August 2013 Accepted 22 September 2013 Objectives To use data from the BreastSurgANZ Quality Audit BQA to examine the patterns of completion axillary lymph node dissection cALND after sentinel lymph node SLN biopsy in women treated for early breast cancer in Australia and New Zealand and to compare it to the Australian and New Zealand guidelines in cases of both positive and negative SLN results Materials and methods Patients were sub grouped as having primary tumours 3 cm and 3 cm and further analysed according to year of surgery SLN status and ﬁnal nodal status where cALND was recorded Multivariate analysis was performed examining tumour size grade presence of lymphovascular invasion LVI HER2 and oestrogen receptor status patient age 
and number of positive sentinel nodes as predictors for subsequent axillary surgery Results 14879 patients were identiﬁed from 2006 to 2010 79 8 of patients with a positive SLN result underwent cALND Age 70 years and a greater number of involved SLN predicted no cALND among SLN positive patients 10 3 of patients who had a negative SLN result underwent cALND Younger age higher grade lymphovascular invasion and tumour size 3 cm predicted cALND among SLN negative patients Conclusions According to the BQA from 2006 to 2010 the Australian and New Zealand guideline recommendations for SLN positive patients to have cALND and SLN negative patients not to have cALND were adhered to in 79 8 and 89 7 of cases respectively Ó 2013 Elsevier Ltd All rights reserved Keywords Sentinel lymph node biopsy Axillary lymph node dissection Introduction Early breast cancer refers to cases in which the primary breast tumour is ﬁve or less centimetres in diameter with either impalpable or palpable but not ﬁxed lymph nodes and no 
evidence of distant metastases 1 Sentinel lymph node SLN biopsy is commonly used to stage the axilla in women with early breast cancer by allowing identiﬁcation for pathological examination of the nodes at highest risk of harbouring metastases Corresponding author Department of Surgery Royal Perth Hospital Perth Western Australia 6000 Australia Tel þ61 401570879 E mail address chiltonchong hotmail com C Chong SLN biopsy has been shown to provide highly sensitive and speciﬁc staging information with lower morbidity than axillary lymph node dissection in women with unifocal breast cancer 3 cm 2 As a result of overwhelming single and multiple institution data and randomised trials 3e5 in 2008 Australia s National Breast and Ovarian Cancer Centre now Cancer Australia recommended SLN biopsy as a valid alternative to ALND for cases of unifocal early breast cancer 3 cm 6 The guidelines recommended a completion axillary lymph node dissection cALND when a sentinel node is found to have a metastatic deposit and also 
recommended that if the SLN is negative then the axilla should be observed only New Zealand s current National Guidelines on the Management of Early Breast Cancer published in 2009 have made the same recommendations 7 0960 9776 e see front matter Ó 2013 Elsevier Ltd All rights reserved http dx doi org 10 1016 j breast 2013 09 005 Please cite this article in press as Chong C et al Subsequent axillary surgery after sentinel lymph node biopsy Results from the BreastSurgANZ Quality Audit 2006e2010 The Breast 2013 http dx doi org 10 1016 j breast 2013 09 005 2 C Chong et al The Breast xxx 2013 1e5 The aim of this study was to use data from the BreastSurgANZ Quality Audit BQA to examine the patterns of cALND performed on women in Australia and New Zealand and compare them to the Australian and New Zealand clinical guidelines on axillary node management in cases of both positive and negative SLN results Methods The BQA is a contemporary clinical database developed in 1998 as a joint initiative of the Breast Section 
of the Royal Australasian College of Surgeons RACS in conjunction with the Australian Safety and Efﬁcacy Register of New Interventional Procedures e Surgical ASERNIP S operating out of the Research Audit and Academic Surgery Division of the RACS Contribution to the audit was initially voluntary although participation is now mandatory for member surgeons of the Breast Surgeon s Society of Australia and New Zealand BreastSurgANZ The BQA is the largest database recording breast cancer surgery in Australia and New Zealand and today contains over 120 000 episodes of breast cancer with more than 330 surgeons contributing data each year from both countries The data set includes variables relating to demography method of diagnosis surgeries performed pathology data and adjuvant therapies Between January 1999 and May 2011 70 688 episodes of early breast cancer with axillary surgery data were reported to the BQA Data entered prior to 2006 were less complete and therefore excluded from study as were episodes with 
missing data and patients who had neoadjuvant chemotherapy The data recorded in the BQA for locoregional recurrence and survival events is incomplete and unreliable and therefore those endpoints were not available for analysis The BQA deﬁnes tumour size as the measurement of the invasive component of tumour in the pathology report for multiple tumours only the maximum diameter of the largest primary lesion is recorded Patients were sub grouped as having primary tumours 3 cm and 3 cm since amongst the randomised trial populations validating SLN biopsy tumours 3 cm in diameter were not well represented 2 4 5 Patients were subsequently analysed according to year of surgery SLN status and ﬁnal nodal status where a second axillary surgery was recorded The data includes both those subsequent axillary surgeries performed on the same day for example after frozen section or on a separate day of surgery A positive sentinel node result is deﬁned by the BQA as one in which the affected node s has a tumour deposit 0 2 mm 
in size Conversely cases of isolated tumour cells in sentinel nodes tumour deposits 0 2 mm are categorised by the BQA as being node negative Unfortunately at that time the BQA did not categorise size of SLN metastases so data on isolated tumour cells versus micrometastases versus macrometastases were not available Multivariate analyses were performed on prognostic factors that may predict locoregional failure including pathologic tumour size modiﬁed BloomeRichardson grade lymphovascular invasion LVI oestrogen receptor ER and HER2 Human Epidermal Growth Factor Receptor 2 status total number of involved nodes and patient age Two sample t tests for continuous variables and chisquare testing for categorical variables were applied with pvalues of 0 05 considered signiﬁcant Results 14 879 patients with SLN biopsy results were identiﬁed in the study period comprising 13 386 patients with tumours 3 cm and 1403 patients with tumours 3 cm Tumours 3 cm Fig 1 24 1 of patients with tumours 3 cm had at least one positive 
SLN Despite the guideline recommendation of cALND for these women only 78 7 of these SLN positive patients were recorded as having undergone subsequent axillary surgery leaving 21 3 with no documented second surgery This pattern of subsequent axillary surgery remained constant over the ﬁve year study period Of the remaining 75 9 of patients with tumours 3 cm and a negative SLN 9 6 963 proceeded to second surgery in spite of the guideline recommendations to only observe the axilla The proportion of SLN negative patients proceeding to second axillary surgery fell from 14 1 in 2006 to 6 9 by 2010 Among this cohort of SLN negative patients undergoing second axillary surgery 176 18 3 were subsequently found to then have node positive disease Fig 1 Results of second axillary surgery for tumours 3 cm Please cite this article in press as Chong C et al Subsequent axillary surgery after sentinel lymph node biopsy Results from the BreastSurgANZ Quality Audit 2006e2010 The Breast 2013 http dx doi org 10 1016 j breast 
2013 09 005 C Chong et al The Breast xxx 2013 1e5 Tumours 3 cm Fig 2 Half the patients with tumours 3 cm had a positive SLN approximately twice the proportion of the 3 cm tumour cohort 108 of 708 15 3 patients with a positive SLN did not record a second axillary procedure in spite of clinical guidelines By comparison 21 of patients with a negative SLN went onto a second axillary staging operation of whom an average of 29 5 were subsequently shown to be node positive As occurred with the 3 cm cohort the proportion of SLN negative patients in the 3 cm cohort proceeding to second axillary surgery declined over time from 25 9 in 2006 to 15 by 2010 Multivariate analysis in SLN positive patients Table 1 Of the patient and tumour characteristics examined only patient age 70 yo attained statistical signiﬁcance favouring SLN positive patients not proceeding to second axillary surgery Odds Ratio OR 2 3 p 0 001 95 Conﬁdence Interval CI 1 6e3 3 SLN biopsy patients were less likely to forego subsequent axillary surgery 
as more positive sentinel nodes were found p 0 001 ER and HER2 status were both non signiﬁcant upon univariate analysis and therefore omitted from the ﬁnal predictor model Multivariate analysis in SLN negative patients Table 2 Among patients with an initially negative SLN result those with tumour size 3 cm p 0 001 higher tumour grade p 0 006 diagnosis of LVI p 0 008 and age 40 yo p 0 01 were more likely to proceed to subsequent axillary surgery ER and HER2 status were both non signiﬁcant upon univariate analysis and therefore omitted from the ﬁnal predictor model Tumour characteristics of SLN negative patients who either did not or did have subsequent axillary surgery Figs 3 and 4 Among women with a negative SLN and no second axillary operation the average rate of LVI was 13 0 and 23 4 for patients with tumours 3 cm and 3 cm respectively By comparison the LVI rate among patients undergoing a second axillary operation despite their negative SLN result was 20 4 and 40 8 respectively representing a 
statistically signiﬁcant difference for both tumour 3 Table 1 Multivariate analysis for the likelihood of SLN positive patients NOT proceeding to subsequent axillary surgery Predictor Odds ratio OR p value 95 Conﬁdence interval CI Lower Tumour size 30 mm ref 1 30 mm 0 872 BloomeRichardson grade I ref 1 II or III 0 841 Lymphovascular invasion No ref 1 Yes 0 888 Age 40 yo ref 1 41e70 yo 1 002 70 yo 2 275 Number of positive sentinel nodes 0 ref 1 1 0 214 2 0 098 3 or more 0 046 Upper 0 259 0 688 1 106 0 099 0 685 1 033 0 187 0 744 1 06 0 989 0 001 0 720 1 577 1 396 3 282 0 001 0 001 0 001 0 145 0 064 0 029 0 316 0 152 0 074 size groups p 0 001 SLN negative patients in whom positive nodes were then subsequently diagnosed after ALND had an even higher average rate of LVI of 41 5 and 64 3 compared to those who were conﬁrmed as still having no nodal metastases at 15 9 and 29 5 The ER status did not differ signiﬁcantly amongst both tumour groups whether they proceeded to second surgery or not and this was in keeping 
with the results of univariate analyses The mean tumour size was marginally greater for the group receiving second axillary surgery following a negative SLN result compared to the group that did not but this was only statistically signiﬁcant for tumours 3 cm 1 6 cm versus 1 4 cm N 10 156 p 0 001 Discussion cALND for SLN positive disease Despite the Australian and New Zealand guideline recommendations for cALND after a positive SLN biopsy result 20 2 of these Fig 2 Results of second axillary surgery for tumours 3 cm Please cite this article in press as Chong C et al Subsequent axillary surgery after sentinel lymph node biopsy Results from the BreastSurgANZ Quality Audit 2006e2010 The Breast 2013 http dx doi org 10 1016 j breast 2013 09 005 4 C Chong et al The Breast xxx 2013 1e5 Table 2 Multivariate analysis for the likelihood of SLN negative patients proceeding to subsequent axillary surgery Predictor OR p value 95 CI Lower Tumour size 30 mm ref 1 30 mm 2 093 BloomeRichardson grade I ref 1 II or III 1 239 
Lymphovascular invasion No ref 1 Yes 1 277 Age 40 yo ref 1 41e70 yo 0 704 70 yo 0 656 Upper 1 686 2 599 0 006 1 062e1 446 1 066 0 008 1 066 1 530 0 010 0 007 0 539 0 484 0 921 0 890 0 001 patients did not record a subsequent axillary surgery regardless of tumour size This rate of failure to proceed to cALND was consistent over the observation period and echoes a recent paper by Morris et al 8 who found that in the ﬁrst six months following the 2008 release of the NBOCC guidelines on the use of SLN biopsy only 77 of women with positive SLN in the BQA went on to receive ALND Variance from guidelines for SLN positive patients arguably represents under treatment of the axilla Multivariate analysis showed that patient age 70 years was a signiﬁcant predictor for SLN positive patients foregoing cALND when compared to younger aged cohorts Table 1 Indeed elderly patients with breast cancer are commonly noted in the literature to undergo differing management pathways from younger women with equivalent tumours 9 due to 
both the perceived diminishing beneﬁt of adjuvant systemic therapy on long term survival as well as concerns about morbidity measures used to achieve local control at an older age Multivariate analysis also showed that patients with lower volume axillary disease fewer positive sentinel nodes relative to negative sentinel nodes were less likely to undergo axillary dissection This is consistent with known data on the reduced likelihood of additional positive nodes on axillary dissection for women with increasing numbers of negative sentinel nodes as reﬂected in nomograms used for predicting nonsentinel lymph node metastases 10 We did not have the data to assess the inﬂuence of size of nodal metastasis eg distinguishing between micrometastases and macrometastases on the decision to forego axillary dissection A similar pattern of foregoing cALND for SLN positive disease in this study has previously been observed overseas In a study of 97 314 patients from the National Cancer Data Base in the United States from 
1998 to 2005 who underwent SLN biopsy and were then found to have nodal involvement 20 8 underwent SLN biopsy alone without a cALND 11 In that study multivariate analysis for the years 2004e2005 found that patients were signiﬁcantly more likely to undergo SLN biopsy alone if they were older had smaller tumours or were treated at noneNational Cancer Institutee designated cancer centres Similarly analysis of 239 661 women with operable breast cancer from the Surveillance Epidemiology and End Results SEER cancer registry found that in pathologic node positive women who had SLN biopsy failure to undergo cALND increased from 20 in 1998 to 32 in 2004 and that omission of cALND was independently associated with smaller lowergrade tumours smaller size nodal metastasis and age 70 years 12 cALND for SLN negative patients An SLN negative result at the ﬁrst axillary operation is intended to preclude the need for cALND given its high negative predictive value Notwithstanding 10 3 of patients in this study proceeded to 
second axillary surgery despite a diagnosis of a negative SLN biopsy a pattern consistent throughout the study period Some of these women had isolated tumour cells ITC but the BQA does not collect data on ITC to further assess this The paper by Morris et al 8 similarly reported that 6 of patients with negative SLN went onto a second axillary operation which varies from the Australian and New Zealand guidelines to merely observe the axilla In our study multivariate analysis showed that patients with tumours sized 3 cm were twice as likely to proceed to subsequent axillary surgery following a negative SLN result as those with tumours 3 cm Other factors found to predict for subsequent axillary surgery among SLN negative patients included the presence of LVI higher tumour grade and younger age 40 years old all being commonly cited independent predictors for poorer prognosis Table 2 Surgeons would have been aware of these high risk factors either at the time or soon after the SLN biopsy The pattern of subsequent 
axillary surgery from 2006 to 2010 may help explain the decision making role of further axillary surgery after a negative SLN There may have for example been a requirement for cALND for surgeons performing SLN biopsy technique during their learning phase this explanation is supported by the observation in this series of a trend towards fewer second axillary surgeries for SLN negative patients across both tumour size cohorts during the study period Figs 1 and 2 Also what is not recorded in our data base is intra operative uncertainty about the robustness of the SLN procedure performed if the SLN is not convincing for technical reasons then the surgeon would be more likely to perform a cALND The high rates of positivity in the completion dissection when the SLN was negative is suggestive this may be a signiﬁcant factor The RACS Sentinel Node or Axillary Clearance SNAC trial 2 reported a false negative rates for SLN biopsy of 5 5 and negative predictive value 98 for sentinel node staging of the axilla It is 
noteworthy that the SNAC trial and others like it recruited only women with tumours 3 cm since in our study 9 of the women in the BQA underwent SLN based management despite their breast cancers measuring 3 cm SLN biopsy is currently not recommended by the Australian and New Zealand guidelines for tumours 3 cm given the lack of cancer outcomes from randomised trials in this group yet many surgeons internationally have nevertheless Fig 3 Tumour characteristics negative SLN patients T 3 cm Please cite this article in press as Chong C et al Subsequent axillary surgery after sentinel lymph node biopsy Results from the BreastSurgANZ Quality Audit 2006e2010 The Breast 2013 http dx doi org 10 1016 j breast 2013 09 005 C Chong et al The Breast xxx 2013 1e5 5 Fig 4 Tumour characteristics negative SLN patients T 3 cm adopted this practice based on the extrapolation of results of randomised trials among women with tumours 3 cm and on the strength of limited evidence from published series 13 More evidence will become 
available on cancer outcomes in Australia and New Zealand for tumours 3 cm and multifocal cancers from the currently accruing SNAC II trial 14 Conﬂict of interest statement The authors have no conﬂicts of interest or relevant funding source to declare Ethics approval Ethics approval was not required for this study Tumour characteristics of SLN negative patients before and after subsequent axillary surgery In this study 19 7 of all the SLN negative patients who were sub selected to undergo a second axillary operation were eventually upstaged to be node positive A focussed analysis of the tumour characteristics among all SLN negative patients was performed to explore the potential for a selection bias underlying both the decision to offer second axillary surgery for certain patient subsets despite a negative SLN result and subsequently ﬁnding positive nodes among them LVI is an established poor prognostic factor in breast cancer for locoregional recurrence and survival 15 Across both tumour size cohorts 
patients undergoing second axillary surgery after a negative SLN were found to have a signiﬁcantly higher average prevalence of LVI than those who did not proceed to a second operation 20 4 and 40 8 versus 13 0 and 23 4 respectively This is in keeping with the observation from the multivariate analysis that LVI was an independent predictor for second axillary surgery among SLN negative patients Furthermore the utility of interpreting LVI as an independent biological marker for increased likelihood of lymph node involvement seems to be borne out in the results of the cALND where those SLN negative patients in whom positive nodes were subsequently diagnosed had a higher prevalence of LVI than those who were conﬁrmed as still having no nodal metastases Conclusions According to the BQA data for 2006e2010 the Australian and New Zealand best practice recommendations for the treatment of the axilla for both positive and negative SLN biopsy results were adhered to in 79 8 and 89 7 of cases respectively Older age and 
lower sentinel node burden predicted no subsequent axillary surgery among initially SLN positive patients Young age higher grade LVI and larger tumour size predicted subsequent axillary surgery among initially SLN negative patients Patients who were initially SLN negative but subsequently found to have positive nodes after undergoing second axillary surgery had tumours that were associated with a higher prevalence of LVI This study forms an important part of the audit feedback loop in line with the aims of a quality audit and has identiﬁed areas where collection of some additional data will help future research Acknowledgements The authors acknowledge the data reported here has been supplied by the Breast Surgeons of Australia and New Zealand Inc BreastSurgANZ and the Royal Australasian College of Surgeons from the BreastSurgANZ Quality Audit The interpretation and reporting of these data are the responsibility of the authors and should not be seen as an ofﬁcial interpretation by the BreastSurgANZ Quality 
Audit References 1 Clinical practice guidelines for the management of early breast cancer 2nd ed National Medical Health and Research Council August 2001 2 Gill G Gillett D Molland G Harbour C Ross B Simon R et al Sentinel lymphnode based management or routine axillary clearance One year outcomes of sentinel node biopsy versus axillary clearance SNAC a randomized controlled surgical trial Ann Surg Oncol 2009 Feb 16 2 266e75 3 Krag D Weaver D Ashikaga T Moffat F Klimberg VS Shriver C et al The sentinel node in breast cancer e a multicenter validation study N Engl J Med 1998 Oct 1 339 14 941e6 4 Veronesi U Paganelli G Viale G Luini A Zurrida S Galimberti V et al A randomized comparison of sentinel node biopsy with routine ALND in breast cancer N Engl J Med 2003 349 6 546e53 5 Mansel RE Fallowﬁeld L Kissin M Goyal A Newcombe RG Dixon JM et al Randomized multicenter trial of sentinel node biopsy versus standard axillary treatment in operable breast cancer the ALMANAC trial J Natl Cancer Inst 2006 98 9 599e609 6 
Recommendations for use of sentinel node biopsy in early breast cancer Cancer Australia June 2008 7 New Zealand Guidelines Group Management of early breast cancer e evidence best practice guidelines Wellington New Zealand Guidelines Group 2009 8 Morris T Wetzig N Sinclair S Kollias J Zorbas H Evaluation of implementation of sentinel node biopsy in Australia ANZ J Surg 2012 82 541e7 9 Wang J Kollias J Boult M Babidge W Zorbas HN Roder D et al Patterns of surgical treatment for women with breast cancer in relation to age Breast J 2010 16 60e5 10 Memorial Sloan Kettering cancer Center breast cancer Nomogram Breast Additional Non SLN Metastases Available at http nomograms mskcc org Breast BreastAdditionalNonSLNMetastasesPage aspx 11 Bilimoria KY Bentrem DJ Hansen NM Bethke KP Rademaker AW Ko CY et al Comparison of sentinel lymph node biopsy alone and completion axillary lymph node dissection for node positive breast cancer J Clin Oncol 2009 Jun 20 27 18 2946e53 12 Rescigno J Zampell JC Axelrod D Patterns of 
axillary surgical care for breast cancer in the era of sentinel lymph node biopsy Ann Surg Oncol 2009 Mar 16 3 687e96 13 Spillane AJ Brennan ME Accuracy of sentinel lymph node biopsy in large and multifocal multicentric breast carcinoma a systematic review EJSO 2011 37 371e85 14 Australian New Zealand Clinical Trials Registry SNAC II Available at http www anzctr org au trial_view aspx ID 484 15 Truong PT Yong CM Abnousi F Lee J Kader HA Hayashi A et al Lymphovascular invasion is associated with reduced locoregional control and survival in women with node negative breast cancer treated with mastectomy and systemic therapy J Am Coll Surg 2005 Jun 200 6 912e21 Please cite this article in press as Chong C et al Subsequent axillary surgery after sentinel lymph node biopsy Results from the BreastSurgANZ Quality Audit 2006e2010 The Breast 2013 http dx doi org 10 1016 j breast 2013 09 005 
49	b	ORIGINAL ARTICLE Surgical Treatment for Women with Breast Cancer in Relation to Socioeconomic and Insurance Status Jonathan Azzopardi MBBS David Walsh FRACS Chilton Chong FRACS and Corey Taylor BSc Grad Dip Psych Lyell McEwin Hospital Division of Surgery Adelaide South Australia Australia The Queen Elizabeth Hospital Royal Australasian College of Surgeons Adelaide South Australia Australia Audit and Academic Surgery Division Royal Australasian College of Surgeons Adelaide South Australia Australia n Abstract Based on the National Breast Cancer Audit of the Royal Australasian College of Surgeons an association between patient age and type of breast cancer surgery received has already been demonstrated The aim of this study is to assess the patterns of surgical treatment for women with early breast cancer in relation to socioeconomic and insurance status Data on patient demographics diagnostic and surgical procedures and cancer characteristics in 115 872 episodes of early breast cancer reported to the National 
Breast Cancer Audit between 1998 and 2012 is used for this study Tumor size histologic grade number of tumors lymph node positivity and lymphovascular invasion are the major prognostic factors adjusted for Reconstruction following mastectomy is the most likely surgical procedure for the higher socioeconomic and privately insured patients Mastectomy alone is the most likely surgical procedure for the lower socioeconomic and for public patients No surgery is the most likely surgical outcome for the lower socioeconomic and the least likely for the higher socioeconomic population Open biopsy is the most likely diagnostic procedure for the lower socioeconomic and ﬁne needle aspiration for the higher socioeconomic population Socioeconomic and insurance status are both independently associated with the types of treatment and diagnostic procedure for women with breast cancer Opportunities present to investigate an association of these factors with morbidity and survival outcomes n Key Words breast cancer insurance 
socioeconomic status surgery B ased on the National Breast Cancer Audit NBCA of the Royal Australasian College of Surgeons RACS an association between patient age and type of breast cancer surgery received has already been demonstrated 1 2 The National Health and Medical Research Council NHMRC has issued a set of clinical practice guidelines for the Management of Early Breast Cancer 3 with treatment recommendations outlined regardless of age Wang et al 2 have however demonstrated an independent association between patient age and type of breast cancer surgery received Some reports suggest that other factors may inﬂuence the treatment for women with breast cancer One overseas study suggests that factors such as level of education socioeconomic and income status as Address correspondence and reprint requests to Jonathan Azzopardi Lyell McEwin Hospital 107 211 Haydown Road Elizabeth Vale SA 5112 Australia or e mail jonathan azzopardi health sa gov au DOI 10 1111 tbj 12203 2013 Wiley Periodicals Inc 1075 122X 13 
The Breast Journal 2013 1 6 well as insurance status may contribute 4 Hall et al 5 examined the effects of socioeconomic status and access to private health care on surgical patterns of care in people with colorectal cancer To our best knowledge there are however no studies that speciﬁcally evaluate such factors in addition to age for the Australian female population with breast cancer This study aims to assess the patterns of surgical treatment for women with early breast cancer in relation to socioeconomic and insurance status MATERIALS AND METHODS Data for this study was obtained from the NBCA This is a clinical audit that contains data on patients with breast cancer collected by surgeons throughout Australia and New Zealand with the aim of improving quality of care 1 Data on Australian patient demographics diagnostic and surgical procedures and cancer characteristics in 115 872 episodes of early breast cancer reported to 2 azzopardi et al 25 Percentage per procedure 20 FNA 15 Core biopsy 10 Open biopsy 5 
0 1 N 2190 2 N 2117 3 N 2102 4 N 2216 5 N 2554 SEIFA Higher socioeconomic status with larger number the NBCA between 1998 and 2012 is used for this study As stated in the NHMRC Clinical Practice Guidelines for the Management of Early Breast Cancer early breast cancer is deﬁned as invasive tumors of not more than 5 cm in diameter with either impalpable or palpable but not ﬁxed lymph nodes and with no evidence of metastases This corresponds to tumors that are classiﬁed as T1 2 N0 1 and M0 1 Socioeconomic status quintiles 1 5 and insurance private public are the two main category groups analysed with respect to surgical treatment breast conserving surgery BCS mastectomy reconstruction following mastectomy no surgery and diagnostic procedure ﬁne needle aspiration FNA core biopsy open biopsy received Socioeconomic status is divided into quintiles with a lower number reﬂecting a lower status The following are the major prognostic factors accounted for in the analysis tumor size 1 10 11 20 21 30 31 40 and 41 50 mm 
histologic grade I II and III number of tumors one two and multicentric lymph node status positive negative and lymphovascular invasion absent and present The concept of socioeconomic status is based on the Socioeconomic Indexes for Areas SEIFA produced by the Australian Bureau of Statistics ABS The SEIFA index of relative disadvantage uses a number of variables such as income education and unemployment to quantify quintiles representing the areas of greatest relative disadvantage 6 In this study the most disadvantaged socioeconomic group SEIFA 1 is referred to as the lower socioeconomic population Figure 1 Socioeconomic status and diagnostic procedure p 0 001 and the least disadvantaged socioeconomic group SEIFA 5 is referred to as the higher socioeconomic population Chi squared testing was used to analyse the relationships between the two category groups socioeconomic status and insurance and the diagnostic and surgical treatment A p 0 001 was used for statistical signiﬁcance Generalized ordinal logistic 
regression was used to further illustrate the effect of socioeconomic status and insurance on the diagnostic procedure RESULTS Socioeconomic Status and Diagnostic Procedure Of a total of 11 179 patients with early breast cancer each quintile in this group falls within a range of 18 23 Fig 1 The most likely diagnostic procedure for the lower socioeconomic population is open biopsy and for the higher socioeconomic population is FNA The generalized ordinal logistic regression in Table 1 demonstrates further that the lower socioeconomic population is more than twice as likely to be part of the open biopsy group as opposed to FNA core biopsy when compared to the higher socioeconomic population Socioeconomic Status and Surgical Procedure The most likely surgical procedure for the higher socioeconomic population is reconstruction following Social and Insurance Effect on Breast Cancer 3 Table 1 Generalized Ordinal Logistic Regression for Likelihood of Open Biopsy OR SEIFA 5 SEIFA 4 SEIFA 3 SEIFA 2 SEIFA 1 Private 
Public Histologic grade 3 Histologic grade 2 Histologic grade 1 Number of tumors 1 Number of tumors 2 Number of tumors 2 LVI absent LVI present 1 2 064 2 554 2 763 2 088 1 0 749 1 0 55 0 568 1 1 35 1 24 1 1 212 95 CI for OR p value 1 239 1 536 1 639 1 216 3 44 4 245 4 657 3 584 0 005 0 001 0 001 0 05 0 573 0 979 0 05 0 399 0 393 0 758 0 82 2 044 1 975 NS NS 0 844 1 741 NS Histologic grade number of tumors and nodal status are some of the major prognostic factors accounted for Table 2 As Table 2 illustrates high grade tumors are more likely to be managed with a mastectomy than reconstruction as opposed to lower grades which are more likely to have reconstructive breast surgery than a mastectomy The latter was also more likely than a mastectomy in the absence of nodal involvement 0 001 0 005 0 891 0 776 Pathological Characteristics in Relation to Surgery SEIFA 1 5 higher socioeconomic status with larger number NS not signiﬁcant LVI lymphovascular invasion mastectomy as opposed to a mastectomy alone for 
patients from the lower socioeconomic population that do have a surgical procedure Fig 2 The most likely surgical outcome for the lower socioeconomic population is no surgery On the other hand this is the least likely surgical outcome for the higher socioeconomic population Insurance Status and Diagnostic Procedure Patients with private insurance make up 49 5 of a total of 25 025 patients analysed Fig 3 Public patients make up 42 7 with the rest being unknown When comparing a private to a public patient there is no particular trend in the type of diagnostic procedure received Core biopsy is the most likely procedure among public patients and the least likely among private patients In the generalized ordinal logistic regression Table 1 a public patient is less likely to be part of the open biopsy group as opposed to FNA core biopsy when compared with a private patient Insurance Status and Surgical Procedure No surgery is the most likely surgical outcome for public patients and the least likely for private 
patients Fig 4 If they do have surgery mastectomy alone is the most likely procedure for public patients as opposed to reconstructive breast surgery for private patients DISCUSSION This study demonstrates differences in both the diagnostic procedure as well as the surgical treatment received between the two demographic category groups that encompass socioeconomic status and insurance This is on a background of age already known to be independently associated with the surgical treatment for women with breast cancer with cancer pathologic characteristics adjusted for Since the beginning of the NBCA data set in 1998 and its ﬁrst publication in 2000 no studies have evaluated any discrepancies in the surgical treatment received among women with breast cancer apart from Wang et al analysing different age groups To our best knowledge this is the ﬁrst study that analyses the patterns of surgical treatment after taking socioeconomic status and insurance into consideration based on an Australian population of women 
with early breast cancer As illustrated in Figures 2 and 4 reconstruction following a mastectomy is the most likely surgical procedure for the higher socioeconomic population and patients with private insurance On the other hand a mastectomy is the most likely surgical procedure for the lower socioeconomic population and patients who do not have private insurance As outlined in the NHMRC Clinical Practice Guidelines for the Management of Early Breast Cancer a signiﬁcant proportion of women who have had a mastectomy report major psychological distress relating to their body image including being unable to look at themselves naked and a low self esteem 7 Furthermore women who have a breast reconstruction report a number of beneﬁts including psychologic and social adjustment to their breast cancer and surgical treatment received 8 No woman suffering from breast cancer should be denied the opportunity of a complete surgical 4 azzopardi et al 35 Breast conserving surgery 30 Percentage per procedure 25 Mastectomy 
alone 20 15 10 Reconstruc on 5 0 1 N 4076 2 N 4005 3 N 4342 4 N 4435 5 N 4604 None Figure 2 Socioeconomic status and surgical procedure p 0 001 SEIFA Higher socioeconomic status with larger number 60 50 Percentage per procedure 40 Private 30 Public Unknown 20 10 0 FNA N 7142 Core biopsy N 15970 Open biopsy N 1913 Diagnos c procedure management of their disease Despite the two category groups socioeconomic status and insurance being independently assessed our results demonstrate consistent discrepancies in the surgical treatment received among the different patient groups within each demographic category We propose ﬁnancial well being to be a likely common factor between patients with private insurance and those classiﬁed as being part of the higher socioeconomic population responsible for such differences Our results may therefore suggest that the option of reconstructive breast surgery is a less realistic outcome for a patient from a less afﬂuent background In the assessment of a patient suspected of having 
breast cancer the combination of mammography an Figure 3 Insurance status and diagnostic procedure p 0 001 ultrasound and FNA cytology in addition to the clinical examination provides the highest diagnostic accuracy and lowest risk of diagnostic error 3 As illustrated in Figure 1 patients that are most likely to beneﬁt from FNA in adherence to this guideline are patients from the higher socioeconomic population Once an absolute malignant diagnosis is made based on FNA cytology deﬁnite surgery may be sought without the need of further work up such as an open biopsy 9 A more invasive open biopsy may be obtained prior to surgery in a patient who has not had a histologic diagnosis despite a high suspicion of cancer 3 The lower socioeconomic population fails to beneﬁt from Social and Insurance Effect on Breast Cancer 5 70 60 Percentage per procedure 50 40 Private Public 30 Unknown 20 10 0 BCS N 22956 Figure 4 Insurance status procedure p 0 001 and surgical Recon N 1512 None N 619 Surgical procedure BCS breast 
conserving surgery Mx mastectomy Recon reconstruc on None no surgery Table 2 Pathologic Characteristics in Relation to Surgery BCS Histologic grade 1 Histologic grade 2 Histologic grade 3 Number of tumors 1 Number of tumors 2 Number of tumors 2 Nodes negative Nodes positive Mx alone N 13007 Mastectomy Reconstruction No Surgery 27 88 44 54 27 58 88 09 6 27 5 64 68 65 31 35 16 34 45 23 38 43 70 82 10 82 18 36 48 27 51 73 20 62 46 63 32 75 62 22 13 04 24 74 59 4 40 6 22 93 38 81 38 26 84 25 7 31 8 44 18 23 81 77 FNA as its most likely diagnostic test and patients in this group are in fact most likely to have an open biopsy With the NHMRC guidelines in mind this may indicate that the most disadvantaged patient group or the lower socioeconomic population is not adequately screened and presents with clinically obvious disease and thus unable to beneﬁt from a less invasive diagnostic test in an otherwise only suspicious lesion This may reﬂect poor conveyance of public health awareness campaigns to the lower 
socioeconomic population Adjusting public health messages speciﬁcally to different patient groups is probably necessary to improve compliance In comparison with other countries the overall health and well being of Australians is relatively high However there are signiﬁcant disparities in health outcomes amongst different populations 6 Disadvantaged Australians have higher levels of disease risk factors and lower use of preventative health services than those who experience socioeconomic advantage 10 This study continues to illustrate the disparity in the surgical treatment received between a higher and lower socioeconomic status As discussed above a mastectomy is the most likely surgical procedure for a patient from the lower socioeconomic population However the actual surgical outcome for these women with early breast cancer is no surgery at all in contrast to the higher socioeconomic population for whom this is the least likely outcome Some mechanisms leading to the lower socioeconomic population receiving 
suboptimal care have been proposed Another important consideration is the extent of the presence or absence of patient decision making throughout the management of disease Furthermore a similar analyses to this study based on data including a range of stages of disease beyond just early breast cancer would be of great interest to attempt to ﬁnd out whether patients from a lower socioeconomic population present with higher stage disease and whether this may be responsible for some of the outcomes discussed The SEIFA index of relative disadvantage has limitations that need to be kept in mind Socioeconomic classiﬁcation is only a framework which may be used to classify and compare data No demographic classiﬁcation can be safely used to substitute another variable without appropriate testing of assumptions 6 azzopardi et al CONCLUSIONS Socioeconomic status and insurance have both been demonstrated to be associated with treatment for women with early breast cancer with the disparities outlined An opportunity 
exists to evaluate whether these patterns are consistent with any morbidity and survival differences among the two category groups as well as to attempt to discern further the processes responsible for suboptimal care Acknowledgments The Authors acknowledge that the data reported here has been supplied by the Breast Surgeons of Australia and New Zealand Inc and the Royal Australasian College of Surgeons from the National Breast Cancer Audit The interpretation and reporting of these data are the responsibility of the authors and should not be seen as an ofﬁcial interpretation by the National Breast Cancer Audit The Authors would like to further acknowledge Mr David Walters for providing authority and access to data from the RACS Breast Cancer Audit REFERENCES 1 National Breast Cancer Audit Internet 2011 Available at http www surgeons org for health professionals audits and surgical research morbidity audits nbca July 31 2012 cited September 5 2012 2 Wang J Kollias J Boult M et al Patterns of surgical 
treatment for women with breast cancer in relation to age Breast J 2010 16 60 5 3 Clinical Practice Guidelines for the Management of Early Breast Cancer 2nd edn PDF on the internet National Health and Medical Research Council 2001 Available at http www nhmrc gov au _ﬁles_nhmrc publications attachments cp74 pdf cited June 19 2012 4 Naeim A Hurria A Leake B Maly RC Do age and ethnicity predict breast cancer treatment received A cross sectional urban population based study Breast cancer treatment age and ethnicity Crit Rev Oncol Hematol 2006 59 234 42 5 Hall SE Holman CDJ Platell C et al Colorectal cancer surgical care and survival Do private health insurance socioeconomic and locational status make a difference ANZ J Surg 2005 75 929 35 6 Health and Socioeconomic Disadvantage PDF on the internet Australian Bureau of Statistics 2010 Available at http www ausstats abs gov au Ausstats subscriber nsf 0 5703A93771AE2E4ECA2 576E70016C8D3 File 41020_ 20healthandseifa pdf updated June 29 2010 cited September 30 2012 7 
Kissane DW Clarke DM Ikin J et al Psychological morbidity and quality of life in Australian women with early stage breast cancer a cross sectional survey Med J Aust 1998 169 192 6 8 Burcham J Breast Reconstruction A Review of the Research and Patient and Professional Resources Woolloomooloo NSW NHMRC National Breast Cancer Centre 1998 9 Sterrett G Harvey J Parsons RW et al Breast cancer in Western Australia in 1989 III Accuracy in FNA cytology in diagnosis ANZ J Surg 1994 64 745 9 10 Health status Health socioeconomic disadvantage area Internet Australian Bureau of Statistics 1999 Available at http www abs gov au ausstats abs nsf 2f762f95845417aeca25706c0083 4efa 21a26c94c69f98daca2570ec00112610 OpenDocument updated May 2 2006 cited July 8 2012 
50	b	ANNALS OF SURGERY Vol 220 No 3 391 401 Cc 1994 J B Lippincott Company Lymphatic Mapping and Sentinel Lymphadenectomy for Breast Cancer Armando E Giuliano M D Daniel M Kirgan M D J Michael Guenther M D and Donald L Morton M D From the Joyce Eisenberg Keefer Breast Center John Wayne Cancer Institute at Saint John s Hospital and Health Center Santa Monica Califomia Objective The authors report the feasibility and accuracy of intraoperative lymphatic mapping with sentinel lymphadenectomy in patients with breast cancer Summary Background Data Axillary lymph node dissection ALND for breast cancer generally is accepted for its staging and prognostic value but the extent of dissection remains controversial Blind lymph node sampling or level dissection may miss some nodal metastases but ALND may result in lymphedema In melanoma intraoperative lymph node mapping with sentinel lymphadenectomy is an effective and minimally invasive alternative to ALND for identifying nodes containing metastases Methods One hundred 
seventy four mapping procedures were performed using a vital dye injected at the primary breast cancer site Axillary lymphatics were identified and followed to the first sentinel node which was selectively excised before ALND Results Sentinel nodes were identified in 1 14 of 174 65 5 procedures and accurately predicted axillary nodal status in 109 of 1 14 95 6 cases There was a definite learning curve and all falsenegative sentinel nodes occurred in the first part of the study sentinel nodes identified in the last 87 procedures were 100 predictive In 16 of 42 38 0 clinically negative pathologically positive axillae the sentinel node was the only tumor involved lymph node identified The anatomic location of the sentinel node was examined in the 54 most recent procedures ten cases had only level 11 nodal metastases that could have been missed by sampling or low level 1 axillary dissection Conclusions This experience indicates that intraoperative lymphatic mapping can accurately identify the sentinel node i e 
the axillary lymph node most likely to contain breast cancer metastases in some patients The technique could enhance staging accuracy and with further refinements and experience might alter the role of ALND The presence or absence of axillary lymph node metastases remains the most important prognostic factor in patients with potentially curable carcinoma of the breast and the development of effective adjuvant systemic therapies has made recognition of these metastases critical for patient management Historically nodal involvement was determined by radical axillary lymph node dissection usually as part of a radical mastectomy Recent data suggest that less radical axillary procedures may re sult in adequate axillary staging and regional control but 391 392 Giuliano and Others the extent of such limited operations is a point of controversy Underlying this controversy are questions concerning the accuracy of limited surgical staging and the role of axillary lymphadenectomy ALND Noninvasive staging of the axilla 
is inadequate Physical examination cannot accurately predict axillary lymph node metastasis Furthermore lymphangiography has not reliably demonstrated nodal disease 2 A recent study used positron emission tomography PET with intravenous 18 fluoro 2 deoxyglucose FDG to demonstrate primary breast carcinoma and regional metastases 3 however results are preliminary and the limits of detection and size of detectable lesions or metastases are unknown Definitive diagnosis of axillary metastasis in patients with breast cancer requires excision and histologic examination of axillary lymph nodes How many nodes should be removed to ensure accurate staging Although ALND remains the gold standard for sensitivity and accuracy of detection it carries a higher morbidity than sampling techniques Morton and others4 have demonstrated the accuracy of intraoperative lymphatic mapping and selective sentinel lymphadenectomy to identify lymph node metastasis in patients with primary cutaneous malignant melanoma reporting a false 
negative rate of less than 1 in more than 500 cases This high degree of accuracy has been substantiated at other institutions where patients with melanoma are treated 5 We developed and used a modification of lymphatic mapping and sentinel lymphadenectomy to detect axillary lymph node metastasis in patients with breast carcinoma PATIENTS AND METHODS Patients with potentially curable breast carcinoma who were undergoing ALND as part of their standard treatment were evaluated Patients with prior axillary operations dissection excisional biopsy were excluded Each patient underwent intraoperative lymphatic mapping and sentinel lymphadenectomy during modified radical mastectomy or segmental mastectomy with ALND All operations were performed by the same surgeon who had no prior experience with lymphatic mapping and sentinel lymphadenectomy Informed consent was obtained in all cases The technique of lymphatic mapping and sentinel lymphadenectomy for melanoma described in detail Ann Surg September 1994 Table 1 
PATIENT DEMOGRAPHIC INFORMATION Total no of cases Total no of patients Mean age Age range Premenopausal Postmenopausal Mode of tumor detection Physical examination Mammography Operative procedure Breast conserving surgery Modified radical mastectomy 174 172 56 yrs 29 84 yrs 74 43 0 98 57 0 109 63 3 65 37 7 142 81 7 32 18 3 Segmental mastectomy and axillary lymphadenectomy elsewhere 6 was modified for patients with breast cancer After induction ofgeneral anesthesia isosulfan blue vital dye Lymphazurin Hirsch Industries Inc Richmond VA was injected with a 25 gauge needle into the breast mass and surrounding breast parenchyma The first 20 patients received 0 5 to 10 mL of dye in subsequent cases the dose was standardized at 3 to 5 mL If the primary tumor had been excised previously the dye was injected into the wall of the biopsy cavity and surrounding breast parenchyma through several points along the incision During the first 20 cases the interval between dye injection and axillary incision was varied from 1 
to 20 minutes to determine the time required for dye to reach the axillary drainage basin A standard interval of approximately 5 minutes was used in the remaining cases A transverse incision was made just below the hair bearing region ofthe axilla Blunt dissection was performed until a lymphatic tract or blue stained node was identified The dye filled tract was dissected to the first blue lymph node If possible the tract was followed proximally to the tail of the breast to ensure that the identified lymph node was the most proximal lymph node and thus the sentinel node This lymph node was excised with a rim of sur Table 2 SIZE OF PRIMARY TUMOR AND HISTOLOGIC STATUS OF AXILLARY NODES Size of tumor Supported by the Joyce and Ben Eisenberg Foundation Presented at the I 14th Annual Meeting of the American Surgical Association April 7 9 1994 San Antonio Texas Address reprint requests to Armando E Giuliano M D John Wayne Cancer Institute at Saint John s Hospital 2200 Santa Monica Blvd Santa Monica California 90404 
Accepted for publication April I 1 1994 Tis Ti T2 T3 Histologic status of axillary nodes Tumor positive Tumor negative 15 8 6 104 59 8 37 21 3 18 10 3 62 35 6 112 64 4 Vol 220 No 3 rounding tissue and submitted as a separate specimen for histologic examination using hematoxylin and eosin H E staining 6 For patients undergoing segmental mastectomy ALND was completed and the primary breast tumor was excised through a separate incision For patients undergoing modified radical mastectomy the mastectomy with en bloc axillary dissection was completed All ALND procedures included level I level II and at least some level III nodes the pectoralis minor muscle was left intact If gross lymph node metastases were evident in sentinel or nonsentinel nodes complete level III dissection was performed All nodes in the ALND specimen were processed for histologic examination using H E Lymph node clearance techniques to identify additional nodes were not performed Histologic Examination All axillary specimens were examined by 
pathologists at Saint John s Hospital and Health Center Fixed sections of false negative sentinel nodes were examined retrospectively with immunohistochemical techniques using antibodies to cytokeratin Statistical Analysis Data were analyzed by the members of the Biostatistical Unit at the John Wayne Cancer Institute and the University of California at Los Angeles A likelihood ratio chi square test was used to compare the difference between two proportions The statistical package of SAS procedures FREQ was used in the analyses RESULTS Between October 1 1991 and February 1 1994 172 women underwent intraoperative lymphatic mapping and sentinel lymphadenectomy immediately before modified radical mastectomy or breast conserving surgical treatment of primary breast carcinoma Their mean age was 56 years range 29 84 years Menopausal status mode of tumor detection and operative procedures are listed in Table 1 Because two patients presented with synchronous bilateral breast primaries there were 174 surgical 
procedures 142 81 7 segmental mastectomies with ALND and 32 18 3 modified radical mas tectomies Tumor size and histologic axillary nodal status are listed in Table 2 Of the 23 patients whose preoperative clinical assessment indicated axillary involvement 6 26 0 had histologically negative nodes of 151 patients with clinically negative axillary nodes 45 29 8 had histologic evidence of axillary metastasis The sensi Lymphatic Mapping for Breast Cancer 393 Figure 1 Two side by side blue staining sentinel lymph nodes and lymphatic tracts in situ tivity of clinical examination was 27 4 with a specificity of 94 9 The blue staining sentinel lymph node was identified in 114 of 174 65 5 procedures More than one lymph node often was found in the sentinel lymphadenectomy specimen for a total of 207 sentinel nodes Figure 1 shows blue staining sentinel lymph nodes and lymphatic tracts in situ There was a clear learning curve the surgeon s rate of sentinel node detection increased with experience Fig 2 Sentinel nodes were 
detected in 51 58 6 of the first 87 mapping procedures and in 63 72 4 of the last 87 procedures In the last 50 cases the rate of detection was 78 0 The accuracy of lymphatic mapping was examined by comparing the histopathology of sentinel node and nonsentinel node ALND specimens The sentinel node accurately identified axillary nodal status in 109 of 114 cases 95 6 Table 3 In 5 of 114 cases 4 3 the sentinel node was falsely negative i e no tumor was identified in the sentinel node but at least one nonsentinel node harbored metastasis All false negative sentinel nodes occurred in the first 87 cases most in the first 50 cases Fig 3 The overall sensitivity of the sentinel node technique was 88 0 with a specificity of 100 The overall positive and negative predictive values were 100 and 93 5 respectively In the last half of the study the positive and negative predictive values were 100 and 100 respectively The five false negative sentinel node specimens were retrospectively re examined and then studied with an 
immunohistochemical technique using antibodies to cytokeratin Three 60 0 specimens contained 2 mm foci of lymphoid tissue in several centimeters of fat these specimens had been misinterpreted as lymph nodes One 20 0 sentinel lymph node stained positive for metastatic breast cancer by immunohistochemistry the fifth sentinel node 20 0 remained negative 394 Giuliano and Others Ann Surg September 1994 B3lue 1Nodes Identitifd Yd 70 S 0 5 0 40 2 0 10 w 1 1 i1 i11 2i lI i t 1 1 14 7 I 4 Experience cases Figure 2 The incidence of blue node detection according to the surgeon s experience with lymphatic mapping and sentinel lymphadenectomy To determine whether the uptake of dye by lymph nodes containing metastases was random we analyzed the 14 cases in which only one tumor positive lymph node and at least one sentinel node were identified Ta Table 3 DISTRIBUTION OF METASTASES IN SENTINEL AND NONSENTINEL LYMPH NODES Total no of mapping procedures Total no of positive axillary basins No of successful mapping procedures 
No of positive sentinel node specimens No of falsely negative sentinel node specimens Sentinel and nonsentinel nodal histology in agreement Sentinel node s identified 174 62 35 6 114 65 5 37 32 4 5 4 3 109 95 6 ble 4 Of 285 total lymph nodes examined in these 14 cases 18 were sentinel tumors were found in 13 of 18 72 2 sentinel nodes and only 1 of 267 0 37 nonsentinel nodes p 0 00001 likelihood ratio chi square analysis Thus it was highly unlikely that uptake of dye by an involved node was a result of chance alone We also compared the sensitivity of lymphatic mapping and sentinel lymphadenectomy with that of random blind biopsy We calculated the probability of detecting nodal metastases using each of these techniques in 34 clinically negative histologically positive axillae in which a sentinel node was identified The 34 dissections yielded a total of 751 lymph nodes 132 were tumor positive In these axillae there were 63 sentinel nodes 39 were tumor positive Therefore in patients with subclinical axillary 
metastases the probability ofexcising a positive node using random sampling was 132 751 17 5 versus 39 63 61 9 using lymphatic mapping p Lymphatic Mapping for Breast Cancer Vol 220 No 3 395 False Ne gative O I C 6 4 2 O S 09 1st 1 2 2 n d 1 2 5 87 0 87 T t o l 5 1 74 Experience cases Figure 3 The incidence of false negative sentinel nodes according to the surgeon s experience with lymphatic mapping and sentinel lymphadenectomy 0 0001 likelihood ratio chi square analysis Table 5 In 16 of 42 total cases 38 0 with clinically negative pathologically positive axillae sentinel nodes were the only positive lymph nodes identified At the time of operation we recorded the anatomic level of the sentinel node in the 54 most recent cases Table 6 Of 43 cases in which the sentinel node was identified 27 62 7 had sentinel nodes only in level I whereas 10 23 7 exhibited skip drainage of isosulfan blue dye to sentinel nodes in level II but not level I The remaining six cases had blue staining sentinel nodes in levels I and II 
DISCUSSION Determination of axillary nodal status is essential for the staging of breast cancer However the extent of axillary dissection required for accurate staging is controversial Total dissection of the axilla has the highest morbid ity 7 but offers the greatest staging accuracy The accuracy of limited dissections or sampling procedures is unclear partially because these procedures often are ill defined and partially because of the methods used to evaluate their accuracy Differences in staging techniques are best described by Kinne 8 Sampling is the removal of an axillary node or nodes from the lower axilla without defining precise anatomic boundaries Low axillary dissection is an en bloc excision of level I lymph nodes defined anatomically as lymph nodes medial to the latissimus dorsi muscle and extending to the lateral border of the pectoralis minor muscle and the axillary vein cephalad Level I and II dissection is an en bloc excision of the low and middle portions of the axilla dissection extends 
from the latissimus dorsi to the medial edge of the pectoralis minor and to the axillary vein Total axillary dissection levels I II and III removes the entire contents of the axilla from the latissimus dorsi laterally to the subclavius muscle Halsted s ligament medially 396 Giuliano and Others Ann Surg September 1994 Table 4 ANALYSIS OF AXILLAE WITH ONE INVOLVED LYMPH NODE AND ONE SENTINEL LYMPH NODE No of axillae No of lymph nodes No of sentinel lymph nodes Tumor positive No of nonsentinel nodes Tumor positive 14 285 18 13 72 2 267 1 0 37 Tumor positive sentinel nodes significantly more common than tumor positive nonsentinel nodes p 0 00001 likelihood ratio chi square analysis Overall recurrence rates and staging accuracy should be considered when making decisions regarding the extent of axillary dissection in breast cancer Fisher et al have suggested that ALND offers no survival advantage and that level I and II dissection would accurately stage most patients with breast cancer in this study the majority 
of breast cancer patients with axillary disease had metastases limited to level I nodes Because the survival benefit of ALND is unclear the necessary extent of axillary dissection may be determined best by staging accuracy and local control Axillary staging operations range from sampling with blind biopsy alone 9 13 to complete or total dissection 14 20 Intraoperative assessment of axillary nodal status using immediate imprint cytology of blind or randomly selected enlarged lymph nodes has a reported accuracy of 85 however it has a false negative rate of 14 13 Blind sampling of axillary lymph nodes misses metastases in 15 to 42 of cases 3 8 In a study of patients undergoing radical mastectomy Davies et al 5 removed fibroadipose tissue from approximately the level of the third rib immediately before ALND Forty two per cent of these tissue specimens were tumor negative whereas higher level lymph nodes in the corresponding ALND specimen were tumor positive The reported incidence Table 5 ANALYSIS OF CLINICALLY 
NEGATIVE HISTOLOGICALLY POSITIVE AXILLAE CONTAINING IDENTIFIABLE SENTINEL NODES No of clinically negative histologically positive axillae No of nodes excised No of sentinel nodes Tumor positive No of nonsentinel nodes Tumor positive 34 751 63 8 3 39 61 9 688 91 7 93 13 5 Sentinel node significantly more likely to contain tumor than nodes selected by random biopsy p 0 0001 likelihood ratio chi square analysis Table 6 ANATOMIC DISTRIBUTION OF SENTINEL NODES No of mapping procedures Level sentinel nodes only Level II sentinel nodes only Level and 11 sentinel nodes Level IlIl sentinel nodes 43 27 62 8 10 23 2 6 14 0 0 of such skip metastases ranges from 1 3 to 42 1 o1 114 16 18 21 23 This variation in incidence may be a result of different techniques of axillary dissection difficulties in reconstructing normal axillary anatomy in isolated surgical specimens and variations in lymphatic anatomy and drainage patterns Sarce et al 8 divided postoperative specimens into the following five separate levels low axillary 
midaxillary high axillary interpectoral and apical They found that 15 of the involved lymph nodes were identified only in the higher axillary levels Finally Kissen et al 7 showed that sampling failed to identify 8 of patients with lymph node metastases and failed to obtain a specimen with identifiable lymph nodes in another 10 of patients The accuracy of sampling is related to the number and location of lymph nodes examined Forrest et al 9 removed pectoral nodes near the tail ofthe breast probably level I nodes and reported metastasis detection rates similar to those in radical mastectomy specimens However 25 of the sampling procedures did not yield lymph nodes and 10 5 of the patients whose pectoral nodes were not identified or were identified as uninvolved with tumors developed regionally recurrent disease Forrest et al demonstrated an 8 false negative rate in specimens with three to four lymph nodes and Steele et al 12 suggested that removal of at least four nodes from the lower axillary fat pad near the 
tail of the breast was as accurate as ALND Kjaergaard et al 24 showed that axillary recurrence in breast cancer patients with low risk primary lesions decreases as the number of excised lymph nodes increases Their rate of axillary recurrence resulting from missed or untreated disease was 12 when no nodes were removed 7 when two lymph nodes were removed and only 2 when more than three lymph nodes were removed In a similar study Graversen et al 25 reported a 3 rate of axillary recurrence after removal of five to ten axillary lymph nodes Mathiesen et al 26 suggested ten as the minimum number of excised nodes for an adequate sampling procedure Axillary recurrence is associated with the number of metastatic nodes 27 especially when four or more nodes are involved or fewer than ten nodes have been sampled 28 Complete axillary dissection markedly decreases the incidence of axillary recurrence 29 30 Benson et al 3 Lymphatic Mapping for Breast Cancer Vol 220 No 3 demonstrated a statistically significant decrease in 
axillary recurrence rate among patients receiving complete nodal dissection described as levels I and II when compared to a sampling procedure similar to the pectoral node procedure This difference in regional recurrence was seen in patients with positive nodes and patients with negative nodes Complete axillary dissection results in recurrence rates approaching zero at 50 months and provides accurate staging 20 The accuracy of sampling only can be equivalent to complete node dissection if sampling procedures included enough lymph nodes to detect skip metastases to any level of the axilla The number of nodes also must be large enough to detect metastatic involvement by chance alone The importance of accurate axillary staging to select candidates for adjuvant therapy has led most authorities to recommend level I and II axillary dissection for patients with breast cancer despite the apparently low rate of skip metastases This is summarized in a 1992 consensus statement from the National Cancer Institute 32 We 
have demonstrated a technique that appears to identify specific lymph nodes draining specific primary breast cancer sites Excision of these sentinel nodes alone should have an extremely low morbidity and a high degree of staging accuracy In our study sentinel lymph nodes were significantly more likely to contain metastases than nonsentinel lymph nodes removed during ALND In the second half of this study 87 patients the sentinel node accurately predicted tumor involvement of the axilla in every patient Although most of the sentinel nodes were in anatomic level I of the axilla 23 3 of our most recent dissections yielded a sentinel node in level II alone Because the sentinel node was the only positive lymph node in 38 of tumor positive axillary basins a blind sampling procedure or a level I dissection could miss involved sentinel nodes in level II Although Veronesi et al 21 23 proposed an orderly progression of tumor cells from level I to level II and then to level III our data suggest that metastatic spread to 
the axilla is determined by the specific lymphatic drainage of the primary tumor which in turn depends on each patient s lymphatic anatomy The ability to identify a tumor free sentinel lymph node could enable the surgeon to accurately stage nodenegative breast cancer patients without subjecting them to the morbidity of a formal dissection Total ALND could be reserved for those patients proven to have regional axillary nodal metastases Because we believe that the accuracy of sentinel lymphadenectomy in breast cancer should be equivalent to its accuracy in malignant melanoma we are investigating methods to increase the rate of identifying the sentinel node and detecting nodal metastases The findings for lymphatic mapping and sentinel lymphadenectomy if confirmed could have sig 397 nificant implications Axillary mapping with sentinel lymphadenectomy enhances the accuracy of surgical staging and also may improve histologic staging by enabling the pathologist to focus on fewer lymph nodes Further metastases in 
the axilla may occur in an orderly fashion by appearing first in the sentinel lymph node Nodal metastases that appear to skip an axillary level may result from variations in regional lymphatic drainage rather than nonsequential progression oftumor cells Lymphatic mapping and sentinel lymphadenectomy should diminish staging morbidity and could alter the surgical management of the axilla in women with breast cancer Addendum Since this manuscript was submitted the authors have experienced one false negative sentinel node Thus the false negative rate since the first 87 cases is 1 of 137 0 73 equivalent to the false negative rate in melanoma References 1 Fisher B Wolmark N Bauer M et al The accuracy of clinical nodal staging and of limited axillary dissection as a determinant of histologic nodal status in carcinoma of the breast Surg Gynecol Obstet 1981 152 765 772 2 Musumeci R Tess T Costa A Veronesi U Indirect lymphography of the breast with lotasul a vanishing hope Lymphology 1984 17 118 123 3 Nieweg 0 Kim E 
Wong W Broussard W Positron emission with flourine 1 8 deoxyglucose in the detection and staging of breast cancer Cancer 1993 71 3920 3925 4 Morton D Wen D Cochran A Management of early stage melanoma by intraoperative lymphatic mapping and selective lymphadenectomy an alternative to routine elective lymphadenectomy or watch and wait Surg Oncol Clin North Am 1992 1 247 259 5 Thompson J McCarthy W Robinson E et al Sentinel lymph node biopsy in 102 patients with clinical stage 1 melanoma undergoing elective lymph node dissection Presented at the 47th Annual Cancer Symposium Society of Surgical Oncology March 1720 1994 Houston Texas 6 Morton D Wen D R Wong J et al Technical details of intraoperative lymphatic mapping for early stage melanoma Arch Surg 1992 127 392 399 7 Shaw J Rumball E Complications and local recurrence following lymphadenectomy Br J Surg 1990 77 760 764 8 Kinne D Controversies in primary breast cancer management Am J Surg 1993 166 502 508 9 Forrest A Roberts M Cant E Shivas A Simple 
mastectomy and pectoral node biopsy Br J Surg 1976 63 569 575 10 Forrest A Stewart H Roberts M Steele R Simple mastectomy and axillary node sampling pectoral node biopsy in the management of primary breast cancer Ann Surg 1982 196 371 378 11 Boova R Bonanni R Rosato F Patterns of axillary nodal involvement in breast cancer predictability of level one dissection Ann Surg 1982 196 642 644 12 Steele R Forrest A Gibson T Chetty U The efficacy of lower ax 398 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 Giuliano and Others illary sampling in obtaining lymph node status in breast cancer a controlled randomized trial Br J Surg 1985 72 368 369 Fisher C Boyle S Burke M Price A Intraoperative assessment of nodal status in the selection of patients with breast cancer for axillary clearance Br J Surg 1993 80 457 458 Kitchen P McLennan R Mursell A Node positive breast cancer a comparison of clinical and pathological findings and assessment of axillary clearance Aust N Z J Surg 1980 50 580 583 Davies G 
Millis R Hayward J Assessment ofaxillary lymph node status Ann Surg 1980 192 148 151 Pigott J Nichols R Maddox W Balch C Metastases to the upper levels of the axillary nodes in carcinoma of the breast and its implications for nodal sampling procedures Surg Gynecol Obstet 1984 158 255 259 Kissin MW Thompson EM Price AB et al The inadequacy of axillary sampling in breast cancer Lancet 1982 1 1210 1212 Sacre R Clinical evaluation of axillar lymph nodes to surgical and pathological findings Eur J Surg Oncol 1986 12 169 173 Moffat F Senofsky G Davis K et al Axillary node dissection for early breast cancer some is good but all is better J Surg Oncol 1992 51 8 13 Senofsky G Moffat F Davis K et al Total axillary lymphadenectomy in the management of breast cancer Arch Surg 1991 126 1336 1342 Veronesi U Rilke F Luini A et al Distribution of axillary node metastases by level of invasion an analysis of 539 cases Cancer 1986 59 682 687 Rosen P Lesser M Kinne D Beattie E Discontinuous or skip metastases in breast 
carcinoma analysis of 1228 axillary dissections Ann Surg 1983 197 276 283 Veronesi U Luini A Galimberti V et al Extent of metastatic axillary involvement in 1446 cases of breast cancer Eur J Surg Oncol 1990 16 127 133 Kjaergaard J Blichert Toft M Andersen J et al Probability of false negative nodal staging in conjunction with partial axillary dissection in breast cancer Br J Surg 1985 72 365 367 Graversen H Blichert Toft M Andersen J Zedeler K Breast cancer risk of axillary recurrence in node negative patients following partial dissection of the axilla Eur J Surg Oncol 1988 14 407 412 Mathiesen 0 Bonderup 0 Panduro J Axillary sampling and the risk of erroneous staging of breast cancer an analysis of 960 consecutive patients Acta Oncologica 1990 29 721 725 Smith J Gamez Araujo J Gallager H et al Carcinoma of the breast analysis of total lymph node involvement versus level of metastasis Cancer 1977 39 527 532 Wilking N Rutqvist L Carstensen J et al Prognostic significance of axillary nodal status in primary 
breast cancer in relation to the number of resected nodes Acta Oncologica 1992 31 29 35 Ball A Waters R Fish S Thomas J Radical axillary dissection in the staging and treatment of breast cancer Ann Royal Coll Surg 1992 74 126 129 Cabanes P Salmon R Vilcoq J et al Value of axillary dissection in addition to lumpectomy and radiotherapy in early breast cancer Lancet 1992 339 8804 1245 1248 Benson E Thorogood J The effect of surgical technique on local recurrence rates following mastectomy Eur J Surg Oncol 1986 12 267 271 NIH Consensus Development Conference NIH Consensus Statement 1992 8 1 19 Discussion DR KIRBY I BLAND Providence Rhode Island First let me begin by congratulating the authors Dr Giuliano Dr Mor Ann Surg September 1994 ton and associates for bringing this important new technique to the attention ofthe Association As the authors have properly emphasized axillary lymph node dissection is essential to address the proper therapy both in the adjuvant as well as the therapeutic setting for invasive 
breast carcinoma The difficulty however with blind sampling of Level 1 and 2 axillary nodes is the fact that the surgeon would frequently miss pathologically positive nodal metastases Most clinics in North America and Europe currently recommend the sampling of a minimal ten nodes to accurately predict the extent of regional disease Numerous studies have correlated the number of nodes to disease free survival and overall survival therapy schedules are often designed for the number of pathologically positive nodes As an example as you know young women with ten or more nodes today very commonly are relegated to autologous bone marrow transplantation The authors should be acknowledged for their important contribution to intraoperative lymph node mapping with sentinel lymphadenectomy for intermediate thickness melanoma And now we see if for breast carcinoma staging Although I was originally skeptical about the application of these techniques for melanoma with increasing usage I have now become comfortable with 
the technique for preoperative lymphoscintigraphy to assist identification of sentinel nodes and the vital blue dye Azozurin TM to actually provide you with visual identification of the first echelon of these potential metastases This technique is efficacious cost effective and provides a very high predictive value as Dr Giuliano just showed us for metastasis The authors now have confirmed similar value for breast staging The breast however often has variable and multidirectional distribution of lymphatic flow this is especially true of medial quadrant and central lesions This study concluded that overall two thirds of the sentinel nodes were identified however in the latter phases you have shown us that all the lymphatics could be identified in sentinel sites Importantly the accurate pathological node status was determined in 96 of these cases Armando my first question to you would be how might we increase the yield to identify all sentinel nodes Is this simply a function of familiarity with the technique 
Are there other issues that you perhaps should ad dress Secondly lymphoscintigraphy using subdermal injections with technetium sulfur colloid or albumin is an extremely advantageous technique for mapping of cutaneous melanoma lymphatic distribution Will your future approaches perhaps for mapping be inclusive of this technique either prior to or synchronous with the injection of isosulfan blue Your study has suggested statistically significant value to predict positive nodal disease when only one sentinel node is identified You identified tumor positive nodes in 72 of these patients in the sentinel node sites That s compared with only 1 or less if the nodes were nonsentinel in location You found essentially the same thing however in the clinically negative node positive axilla Your numbers were 62 but on the other hand you ve had an increase of over 30 fold to 14 for finding node positive disease in nonsentinel sites So I d ask you again would lymphoscintigraphy perhaps have enhanced the probability of 
selective biopsy of these vital dye stained nodes This is an important consideration because 
51	b	Strong Candidate for the Breast and Ovarian Cancer Susceptibility Gene BRCA1 A Yoshio Miki Jeff Swensen Donna Shattuck Eidens P Andrew Futreal Keith Harshman Sean Tavtigian Qingyun Liu Charles Cochran L Michelle Bennett Wei Ding Russell Bell Judith Rosenthal Charles Hussey Thanh Tran Melody McClure Cheryl Frye Tom Hattier Robert Phelps Astrid Haugen Strano Harold Katcher Kazuko Yakumo Zahra Gholami Daniel Shaffer Steven Stone Steven Bayer Christian Wray Robert Bogden Priya Dayananth John Ward Patricia Tonin Steven Narod Pam K Bristow Frank H Norris Leah Helvering Paul Morrison Paul Rosteck Mei Lai J Carl Barrett Cathryn Lewis Susan Neuhausen Lisa Cannon Albright David Goldgar Roger Wiseman Alexander Kamb Mark H Skolnick A strong candidate for the 17q linked BRCA1 gene which influences susceptibility to breast and ovarian cancer has been identified by positional cloning methods Probable predisposing mutations have been detected in five of eight kindreds presumed to segregate BRCA1 susceptibility alleles The 
mutations include an 11 base pair deletion a 1 base pair insertion a stop codon a missense substitution and an inferred regulatory mutation The BRCA 1 gene is expressed in numerous tissues including breast and ovary and encodes a predicted protein of 1863 amino acids This protein contains a zinc finger domain in its amino terminal region but is otherwise unrelated to previously described proteins Identification of BRCA1 should facilitate early diagnosis of breast and ovarian cancer susceptibility in some individuals as well as a better understanding of breast cancer biology Breast cancer is one of the most common and important diseases affecting women Current estimates indicate that one in eight American women who reach age 95 will develop breast cancer 1 Treatment of Y Miki J Swensen K Yakumo C Lewis S Neuhausen and D Goldgar are in the Department of Medical Informatics University of Utah Medical Center Salt Lake City UT 84132 USA D Shattuck Eidens K Harshman S Tavtigian Q Liu W Ding R Bell J Rosenthal C 
Hussey T Tran M McClure C Frye T Hattier R Phelps H Katcher Z Gholami D Shaffer S Stone S Bayer C Wray R Bogden P Dayananth and A Kamb are at Myriad Genetics 421 Wakara Way Salt Lake City UT 84108 USA P A Futreal C Cochran L M Bennett A Huagen Strano J C Barrett and R Wiseman are at the Laboratory of Molecular Carcinogenesis National Institute of Environmental Health Sciences National Institutes of Health Research Trangle Park NC 27709 USA J Ward and L Cannon Albnght are in the Department of Internal Medicine University of Utah Medical Center Salt Lake City UT 84132 USA P Tonin and S Narod are in the Department of Medical Genetics McGill University Montreal Quebec H3G 1A4 Canada P K Bristow F H Norris L Helvering P Morrison P Rosteck and M Lai are at Lilly Research Laboratories Eli Lilly and Company Indianapolis IN 46285 USA M H Skolnick is in the Department of Medical Informatics University of Utah Medical Center and Myriad Genetics Salt Lake City UT 84108 USA To whom correspondence should be addressed 
Genetic factors contribute to an ill defined proportion of breast cancer incidence estimated to be about 5 of all cases but approximately 25 of cases diagnosed before age 30 2 Breast cancer has been subdivided into two types early onset and late onset a division that is based on an inflection in the age specific incidence curve around age 50 Mutation of one gene BRCA1 is thought to account for approximately 45 of families with significantly high breast cancer incidence and at least 80 of families with increased incidence of both early onset breast cancer and ovarian cancer 3 Intense efforts to isolate the BRCA1 gene have proceeded since it was first mapped to chromosome arm 17q in 1990 4 5 A second locus BRCA2 recently mapped to chromosome arm 13q 6 appears to account for a proportion of earlyonset breast cancer roughly equal to that resulting from BRCA1 Unlike BRCA1 however BRCA2 may not influence ovarian cancer risk The remaining susceptibility to early onset breast cancer is likely attributable to 
unmapped genes for familial cancer and rare germline mutations in genes such as TP53 which encodes the tumor suppressor protein p53 7 It has also been suggested that heterozygote carriers of defective forms of the gene predisposing to ataxia telangiectasia are at higher risk for breast cancer 8 9 Late onset breast can 66 SCIENCE advanced breast cancer is often futile and disfiguring making early detection a high priority in medical management of the disease Ovarian cancer although less frequent than breast cancer is often rapidly fatal and is the fourth most common cause of cancer mortality in American women VOL 266 7 OCTOBER 1994 cer is often familial in origin although the risks in relatives are not as high as those for early onset breast cancer 10 1 1 The percentage of such cases that are due to genetic susceptibility is unknown Like many other genes involved in familial cancer BRCA1 appears to encode a tumor suppressor a protein that acts as a negative regulator of tumor growth Cancer predisposing 
alleles typically carry mutations that cause loss or reduction of gene function Predisposition to cancer is inherited as a dominant genetic trait whereas the predisposing allele generally behaves as a recessive allele in somatic cells Thus a single inherited copy of the mutant allele causes predisposition and loss or inactivation of the wild type allele completes one of the steps in progression toward malignancy When chromosome loss is observed in breast and ovarian tumors from patients who carry BRCAI predisposing alleles the wild type copy of BRCA1 is invariably lost while the presumptive mutant allele is retained 12 14 This finding supports the hypothesis that BRCA1 is a tumor suppressor gene and suggests that the functional BRCA1 protein is present in normal breast and ovarian epithelium tissue and is altered reduced or absent in some breast and ovarian tumors Genetic analysis of recombinant chromosomes in members of large kindreds allowed localization of BRCA1 initially to a region of 1 to 2 megabases 
on chromosome 17q 1517 and subsequently to a region of about 600 kilobase pairs kb 18 between markers D17S1321 and D17S1325 19 A physical map comprised of overlapping yeast artificial chromosomes YACs P1 bacterial artificial chromosomes BACs and cosmid clones was generated for this region 18 Identification of a strong BRCAI candidate gene Several strategies were used to develop a detailed map of transcripts for the 600 kb region of 17q21 between D17S1321 and D17S1325 Sixty five candidate expressed sequences 20 within this region were identified Expressed sequences were characterized by DNA sequence database comparison transcript size expression pattern genomic structure and most importantly DNA sequence analysis in individuals from kindreds that segregate 17q linked breast and ovarian cancer susceptibility Three expressed sequences eventually were merged into a single transcription unit whose characteristics strongly suggest that it is BRCA1 21 This transcription unit is located in the center of the 600 kb 
region Fig 1 spanning D17S855 and will be referred to herein as BRCA1 A combination of sequences obtained from complementary DNA cDNA clones hybrid selected sequences and amplified polymerase chain reaction PCR products Downloaded from www sciencemag org on February 12 2008 RESEARCH ARTICLES a JD17S250 D ing is coordinated with alternative splicing farther downstream and whether all the splice variants produce proteins with an identical NH2 terminus are questions that remain to be explored We also probed genomic DNA samples from several different species with BRCA1 sequences devoid of the zinc finger region Low stringency blots revealed strongly hybridizing fragments in tissues from humans mice rats rabbits sheep and pigs but not chickens Fig 5 These results suggest that A BRCA1 is conserved in mammals Germline BRCA1 mutations in 17qlinked kindreds Identification of a candidate gene as BRCAI requires a demonstration of potentially disruptive mutations in that gene in carrier individuals from kindreds that 
segregate 1 7q linked susceptibility to breast and ovarian cancer Such individuals must contain BRCA1 alleles that differ from the wild type sequence The set of DNA samples used in this analysis consisted of DNA from individuals represent v v 0 MDLSALRVEEVQNVINAMQKILECPICLELIKEPVSTKCDHIFCKFCMLKLLNQKKGPSQ v v 60 CPLCKNDITKRSLQESTRFSQLVEELLKIICAFQLDTGLEYANSYNFAKKENNSPEHLKD v 120 EVSIIQSMGYRNRAKRLLQSEPENPSLQETSLSVQLSNLGTVRTLRTKQRIQPQKTSVYI 180 240 300 360 420 480 540 600 660 720 780 840 900 960 1020 1080 1140 1200 ELGSDSSEDTVNKATYCSVGDQELLQITPQGTRDEISLDSAKKAACEFSETDVTNTEHHQ PSNNDLNTTEKRAAERHPEKYQGSSVSNLHVEPCGTNTHASSLQHENSSLLLTKDRMNVE KAEFCNKSKQPGLARSQHNRWAGSKETCNDRRTPSTEKKVDLNADPLCERKEWNKQKLPC SENPRDTEDVPWITLNSSIQKVNEWFSRSDELLGSDDSHDGESESNAKVADVLDVLNEVD EYSGSSEKIDLLASDPHEALICKSDRVHSKSVESNIEDKIFGKTYRKKASLPNLSHVTEN LIIGAFVSEPQIIQERPLTNKLKRKRRPTSGLHPEDFIKKADLAVQKTPEMINQGTNQTE QNGQVMNITNSGHENKTKGDSIQNEKNPNPIESLEKESAFKTKAEPISSSISNELELNIM HNSKAPKKNRLRRKSSTRHIHALELVVSRNLSPPNCTELQIDSCSSSEEIKKKKYNQMPV 
RHSRNLQLMEGKEPATGAKKSNKPNEQTSKRHDSDTFPELKLTNAPGSFTKCSNTSELKE FVNPSLPREEKEEKLETVKVSNNAEDPKDLMLSGERVLQTERSVESSSISLVPGTDYGTQ ESISLLEVSTLGKAKTEPNKCVSQCAAFENPKGLIHGCSKDNRNDTEGFKYPLGHEVNHS RETSIEMEESELDAQYLQNTFKVSKRQSFAPFSNPGNAEEECATFSAHSGSLKKQSPKVT FECEQKEENQGKNESNIKPVQTVNITAGFPVVGQKDKPVDNAKCSIKGGSRFCLSSQFRG NETGLITPNKHGLLQNPYRIPPLFPIKSFVKTKCKKNLLEENFEEHSMSPEREMGNENIP STVSTISRNNIRENVFKEASSSNINEVGSSTNEVGSSINEIGSSDENIQAELGRNRGPKL NAMLRLGVLQPEVYKQSLPGSNCKHPEIKKQEYEEVVQTVNTDFSPYLISDNLEQPMGSS HASQVCSETPDDLLDDGEIKEDTSFAENDIKESSAVFSKSVQKGELSRSPSPFTHTHLAQ GYRRGAKKLESSEENLSSEDEELPCFQHLLFGKVNNIPSQSTRHSTVATECLSKNTEENL 1260 LSLKNSLNDCSNQVILAKASQEHHLSEETKCSASLFSSQCSELEDLTANTNTQDPFLIGS 1320 SKQMRHQSESQGVGLSDKELVSDDEERGTGLEENNQEEQSMDSNLGEAASGCESETSVSE 1380 DCSGLSSQSDILTTQQRDTMQHNLIKLQQEMAELEAVLEQHGSQPSNSYPSIISDSSALE 1440 DLRNPEQSTSEKVLQTSQKSSEYPISQNPEGXSADKFEVSADSSTSKNKEPGVERSSPSK 1500 CPSLDDRWYMHSCSGSLQNRNYPPQEELIKVVDVEEQQLEESGPHDLTETSYLPRQDLEG 1560 TPYLESGISLFSDDPESDPSEDRAPESARVGNIPSSTSALKVPQLKVAESAQSPAAAHTT v 1620 
DTAGYNAMEESVSREKPELTASTERVNKRMSMVVSGLTPEEFMLVYKFARKHHITLTNLI 1680 TEETTHVVMKTDAEFVCERTLKYFLGIAGGKWVVSYFWVTQSIKERKMLNEHDFEVRGDV R V c v 1740 VNGRNHQGPKRARESQDRKIFRGLEICCYGPFTNMPTDQLEWMVQLCGASVVKELSSFTL v v 1800 GTGVHPIVVVQPDAWTEDNGFHAIGQMCEAPVVTREWVLDSVALYQCQELDTYLIPQIPH 1860 SHY Downloaded from www sciencemag org on February 12 2008 allowed the construction of a composite full length BRCAJ cDNA The cDNA clone extending farthest in the 3 direction contains a polyadenylate tract preceded by a polyadenylation signal Conceptual translation of the cDNA revealed a single long open reading frame with a presumptive initiation codon flanked by sequences resembling the Kozak consensus sequence 22 This reading frame encodes a protein of 1863 amino acids Fig 2A Smith Waterman 23 and BLAST 24 searches identified a sequence near the NH2 terminus that has considerable similarity to zinc finger domains 25 Fig 2B This sequence contains cystine and histidine residues present in the consensus Cys3 His Cys4 C3HC4 zinc finger 
motif and shares many other residues with zinc finger proteins in the databases The BRCA1 gene is composed of 22 coding exons distributed over roughly 100 kb of genomic DNA Fig 3 Hybridization of RNA blots to labeled fragments of BRCA1 cDNA revealed a single transcript of 7 8 kb This transcript is most abundant in testis and thymus but is also present in breast and ovary Fig 4 The cDNA clones derived from the 5 onethird of BRCAJ transcripts display a complex pattern of alternative splicing Four alternative splices were observed downstream of the start codon as independent cDNA clones P3 P4 B31 and B21 in Fig 3 three of these patterns were detected in breast cDNA P3 B31 and B21 and two in ovary cDNA P3 and B21 In addition PCR analysis of cDNA samples prepared from breast ovary testis and lymphocyte messenger RNA mRNA indicates that there is considerable heterogeneity in splice junction usage near the 5 end of BRCAl transcripts upstream of the presumptive initiation codon How this alternative splic fD17S800 J 
U177S73b7 4 BRCA1 I I B t D17S855 D17S1327 CA 125 RNU2 D17S1325 D1 7S579 Fig 1 Schematic map of human chromosome 17 The pertinent region containing BRCA 1 is expanded to indicate the relative positions of two previously identified genes CA 125 34 and RNU2 45 D1 7S855 is located within BRCA1 CPICLELIKEPVSTK CDHIFCKFCMLKLLNQKK GPSQCPLCK BRCA1 RPT1 RIN1 RFP1 C3HC4 motif CPICLELLKEPVSAD CNHSFCRACITLNYESNRNTDGKGNCPVCR CPICLDMLKNTMTTKECLHRFCSDCIVTALRS GNKECPTCR CPVCLQYFAEPMMLD CGHNICCACLARCWGTAC TNVSCPQCR C C H C C C Fig 2 Predicted amino acid sequences for BRCA1 46 A Conceptual translation of the BRCA1 open reading frame indicating the approximate positions of introns triangles above sequence and the locations of germline mutations boldface residues The 1 1 bp deletion in kindred 1901 is shown by an asterisk the nonsense mutation in kindred 2082 is shown by a star the frameshift in kindred 1910 is shown by c and the missense mutation in kindred 2099 is shown by R The BRCA1 nucleotide sequence is deposited in 
GenBank with accession number Ul 4680 PCR primer sequences are available via anonymous FTP at the following internet address morgan med utah edu in the directory pub BRCA1 or by fax at the following number 801 584 3650 B Alignment of the BRCA1 zinc finger domain with three other zinc finger domains that scored highest in a Smith Waterman alignment RPT1 is a protein that appears to be a negative regulator of the interleukin 2 receptor in mice 47 RIN1 is a DNA binding protein that includes a RING finger motif related to the zinc finger 48 RFP1 is a putative transcription factor comprising the NH2 terminal domain of the REToncogene product 49 The C3HC4 motif shows the positions of the cystines and the histidine that form the zinc binding pockets SCIENCE VOL 266 7 OCTOBER 1994 67 ing eight different BRCA1 kindreds Table 1 The lod scores likelihood ratios for linkage in these kindreds range from 9 49 to 0 44 for a set of markers in 17q21 Four of the families have convincing lod scores for linkage and four have 
low positive or negative lod scores The latter kindreds were included because they demonstrate haplotype sharing at chromosome 17q21 for at least three affected members Furthermore all kindreds in the set display earlyonset breast cancer and four of the kindreds include at least one case of ovarian cancer both hallmarks of BRCAI kindreds Kindred 2082 has nearly equal incidence of breast and ovarian cancer an unusual occurrence given the relative rarity of ovarian cancer in the population 17 All but two of the kindreds were ascertained in Utah Kindred 2035 is from the midwestern United States Kindred 2099 is an African American kindred from the southern United States all other kindreds are Caucasian In the initial screen for predisposing mutations in BRCA1 DNA from one individual carrying the predisposing haplotype from each kindred was tested The 21 coding exons and associated splice junctions were amplified from either genomic DNA samples or cDNA prepared from lymphocyte mRNA 26 When the amplified DNA 
sequences were compared to the wildtype sequence four of the eight kindred samples were found to contain sequence variants Table 2 All four sequence variants are heterozygous and each appears in only one of the kindreds Kindred 1901 contains an 11 base pair bp deletion in exon 2 Cys24 frameshift to 36 Stop Kindred 2082 contains a nonsense mutation in coding exon 11 Gln1313 to Stop Fig Fig 3 Diagram of BRCA 1 mRNA showing the locations of introns and the variants of BRCA1 mRNA produced by alternative splic BRCA1 a 6A Kindred 1910 contains a single nucleotide insertion in coding exon 20 Gln1756 frameshift to 1829 Stop Fig 6B and kindred 2099 contains a missense mutation in coding exon 21 Metl775Arg The frameshift and nonsense mutations are likely to disrupt the function of the BRCA1 proteins The protein encoded by the insertion allele in kindred 1910 would contain an altered sequence beginning 107 amino acids residues from the wild type COOHterminus The effect of the 1 1 bp deletion in kindred 1901 would be 
even more dramatic because it occurs at the twenty fourth codon This deletion removes the last 11 bp of exon 2 and begins at the first cystine of the zinc finger motif thereby removing the zinc finger domain The mutant allele in kindred 2082 would encode a protein missing 548 residues from the COOH terminus The missense mutation observed in kindred 2099 is potentially disruptive as it substitutes a large charged amino acid Arg for a small hydrophobic amino acid Met Five common polymorphisms were also identified in the BRCA1 coding sequence Table 3 The individual studied in kindred 2035 is likely to carry a regulatory mutation in BRCAl In her cDNA two polymorphic sites PM1 and PM7 appeared homozygous whereas her genomic DNA revealed heterozygosity at these positions Fig 6C One possible explanation for this observation is that mRNA from her mutant BRCA1 allele is absent because of a mutation that affects RNA production or stability We explored this possibility further by examining three additional polymorphic 
sites PM6 PM7 and PM2 in the BRCA1 Downloaded from www sciencemag org on February 12 2008 MM Table 1 Kindred descriptions and associated lod scores 50 Br breast cancer Br 50 breast cancer diagnosed under age 50 Ov ovarian cancer Cases n Kindred Toetalod Sporadic Ov O Br Total 50 Br n n M Markers L score 2082 2099 31 22 20 14 22 2 7 0 9 49 2 36 2035 1901 1925 1910 10 10 4 5 8 7 3 4 1 1 0 0 0 0 0 0 2 25 1 50 0 55 0 36 1911 1927 8 5 5 4 0 0 1 1 0 20 0 44 D17S1327 D1 7S800 and D1 7S855t D1 7S1327 D17S855 D1 7S579 D17S579 and D1 7S250t D1 7S250 D1 7S250 Kindred contains one individual who had both breast and ovarian cancer this individual is counted as both a breast tNumber of women with breast cancer diagnosed under age 50 or cancer case and as an ovarian cancer case tBoth markers were used to ovarian cancer diagnosed at any age who do not share the BRCA 1 linked haplotype calculate multipoint lod scores v v v W W 392x3F F1Z T 5TO P3 A t v v W a 13 14 1 15 ii p4 v W 16 _V AA W _ v v _ W _ v v 17 18 191 20 21i 
2223i t 1 TGA 24 3 ing The top cDNA BRCA1 Breast CDNAi is the composite used to Ovary cDNAi TY31 generate the protein sequence in Fig 2 Intron locaT TE tions are shown by filled triM B21 angles and the exons are B31 numbered below the composite cDNA Alternative TY4 F103 mRNAs identified as cDNA 61 clones or in hybrid selection experiments are shown beF191 low the composite The start F31 I codon ATG and stop codon B9 TGA are indicated The zinc finger region is denoted noted by a double line V lines connecting exons indicate the absence of internal exons All exons are drawn proportionally except exon 11 indicated with a dotted line Upward pointing unfilled triangles show the position of a single codon CAG found at the start of exons 8 and 14 in some cDNAs Leftwardand rightward pointing unfilled triangles represent partial exons in some cDNAs P3 and P4 are cDNA clones isolated from a placental cDNA library 68 i 1 at TY3 and TE2 are 5 RACE clones from thymus and testis respectively B21 and B9 are cDNA clones 
from a normal breast cDNA library B31 is a hybridselected cDNA clone from breast cDNA TY4 and TY6 are cDNA clones isolated from a thymus cDNA library and F1 91 F1 03 and F3 are cDNA clones isolated from a fetal brain library The BRCA 1 variants labeled breast cDNA and ovary cDNA are the major forms detected in these tissues by PCR SCIENCE VOL 266 7 OCTOBER 1994 I including both carriers and noncarriers of the predisposing haplotype Fig 6 In each kindred the corresponding mutant allele was detected only in individuals carrying the BRCAl associated haplotype In the case of the potential regulatory mutation in kindred 2035 cDNA and genomic DNA from carriers in the kindred were compared for heterozygosity at polymorphic sites In every instance the extinguished allele in the cDNA sample was shown to lie on the chromosome that carries the BRCA1 predisposing allele To exclude the possibility that the mutations were simply common polymorphisms in the population we used allelespecific oligonucleotides ASOs for each 
mutation to screen a set of control DNA samples 27 The actual mutation in kindred 2035 has not been identified so we could not determine its frequency in the general population Gene frequency estimates in Caucasians were based on random samples from the Utah population Gene frequency estimates in African Americans C0 o X eJ9 C g 0 ZD CO C CO U _ 9 50 9 S07 50 7 50 4 40 2 40 4 40 Fig 4 Tissue expression pattern of BRCA 1 The blots were obtained from Clontech Palo Alto CA and contain RNA from the indicated tissues Hybridization conditions were those recommended by the manufacturer and the probe was a BRCA 1 cDNA fragment corresponding to nucleotides 3575 to 3874 Note that these tissues are heterogeneous and the percentage of relevant epithelial cells in breast and ovary can be variable Size standards are in kilobases Fig 5 Blot showing hybridization of a BRCA1 probe to genomic DNA fragments from various species DNA was digested with Eco RI subjected to electrophoresis through a 0 65 agarose gel and transferred 
to a nylon membrane which was then hybridized 32 to a probe consisting of random primed a 32P labeled BRCA 1 cDNA sequences comprising a total of 4 6 kb The probe excluded the zinc finger region The final wash was at 550C in x2 SSPE and 1 SDS for 20 min Size standards are in kilobases Table 2 Predisposing mutations in BRCA1 NA indicates not applicable as the regulatory mutation is inferred and the position has not been identified Mutation Kindred 1901 2082 1910 2099 2035 Codon 24 1313 1756 1775 NA Nucleotide change 11 bp C T Extra C T G Table 3 Neutral polymorphisms in BRCA 1 For the frequency in control chromosomes the number of chromosomes with a particular base at the indicated polymorphic site is shown A C G orT Frequency in control chromosomes effect Frameshift or splice Gln Stop Frameshift Met Arg Loss of transcript Name 0 180 0 170 0 162 0 120 NA Coding SCIENCE VOL 266 were based on 39 samples used in linkage studies and on samples from 20 African Americans from Utah 28 None of the four potential 
predisposing mutations tested was found in the appropriate control population indicating that they are rare in the general population Table 2 Thus both important requirements for BRCA1 susceptibility alleles are fulfilled by the candidate predisposing mutations cosegregation of the mutant allele with diseases and absence of the mutant allele in controls indicating a low frequency in the general population 29 Phenotypic expression of BRCA1 mutations The effect of the mutations on the BRCA1 protein correlates with differences in the observed phenotypic expression in the BRCA1 kindreds Most BRCA1 kindreds have a moderately increased ovarian cancer risk and a smaller subset have a high risk of ovarian cancer comparable to that for breast cancer 3 Four of the five kindreds in which BRCA1 mutations were detected fall into the former category and the fifth kindred 2082 falls into the group with high ovarian cancer risk The BRCA1 nonsense mutation found in kindred 2082 has an interesting phenotype Kindred 2082 has a 
high incidence of ovarian cancer and the mean age of breast cancer diagnosis is older than that in the other kindreds 17 This difference in age of onset could be due to an ascertainment bias in the smaller more highly penetrant families or it could reflect tissue specific differences in the behavior of BRCA1 mutations The other four kindreds that segregate known BRCA1 mutations have on average 1 ovarian cancer for every 10 cases of breast cancer but have a high proportion of breast cancer cases diagnosed at an early age late 20s or early 30s Kindred 1910 which has a 1 bp insertion mutation is noteworthy because three of the four affected individuals had bilateral breast cancer and in each case the second tumor was diagnosed within a year of the first occurrence Kindred 2035 which segregates the potential regulatory BRCA1 mutation might also be expected to have a Downloaded from www sciencemag org on February 12 2008 coding region which are separated by as much as 3 5 kb in the BRCA1 transcript In all cases 
where her genomic DNA appeared heterozygous for a polymorphism her cDNA appeared homozygous In individuals from other kindreds and in nonhaplotype carriers in kindred 2035 these polymorphic sites appeared heterozygous in cDNA implying that amplification from cDNA was not biased in favor of one allele This analysis indicates that a BRCA1 mutation in kindred 2035 either prevents transcription or causes instability or aberrant splicing of the BRCA1 transcript Cosegregation of BRCAI mutations with BRCA1 haplotypes and population frequency analysis In addition to potential disruption of protein function a sequence variant must meet two other criteria to qualify as a candidate predisposing mutation It must be present in members of the kindred who carry the predisposing haplotype and absent from other members of the kindred and it must be rare in the general population To test for cosegregation of mutations with the corresponding BRCA1 susceptibility allele we screened several individuals from kindreds 1901 1910 
2082 and 2099 PM1 PM6 PM7 PM2 PM3 7 OCTOBER 1994 AS Codon Base in locacodon tion 317 878 1190 1443 1619 2 2 2 3 1 Frequency in control chromosomes A C G T 0 10 0 152 0 55 0 100 0 0 53 109 0 115 0 58 0 52 0 116 dramatic phenotype Eighty percent of breast cancer cases in this kindred occur under age 50 This figure is as high as any in the set suggesting that this BRCAJ mutant allele has a high penetrance Table 1 Kindred 1901 displays a phenotypic pattern similar to that of kindred 2035 It is likely that the 1 1 bp deletion beginning at codon 24 carried in kindred 1901 results in a loss of gene function similar to the effect of the regulatory mutation in kindred 2035 Although the mutations described in this research article are clearly deleterious causing breast cancer in women at very young ages each of the four kindreds with mutations includes at least one woman who carried the mutation but lived until age 80 without developing a malignancy It will be of utmost importance in future studies to identify other 
genetic factors or environmental factors that may ameliorate the effects of BRCA1 mutations In addition in three of the eight putative BRCAl linked kindreds potential predisposing mutations were not found All of these kindreds have lod scores for BRCAJ linked markers of less than 0 55 and thus may not truly segregate BRCAl predisposing alleles Alternatively the mutations in these three kindreds may lie in noncoding regions of BRCA and therefore have escaped detection The role of BRCA1 in cancer Most A P a b c d e f g mutant tumor suppressor genes identified to date encode proteins that are absent nonfunctional or reduced in function The majority of TP53 mutations are missense some of these have been shown to produce abnormal p53 molecules that interfere with the function of the wild type product 30 31 A similar dominant negative mechanism of action has been proposed for some adenomatous polyposis coli APC alleles that produce truncated molecules 32 and for point mutations in the Wilms tumor gene WTJ which 
alter DNA binding of the WT1 protein 33 The nature of three mutations observed in the BRCAI coding sequence is consistent with production of either dominant negative proteins or nonfunctional proteins All three mutations are located in the COOH terminal half of the protein The regulatory mutation inferred in kindred 2035 cannot be dominant negative rather this mutation likely causes reduction or complete loss of BRCA1 expression from the affected allele Similarly the 11 bp deletion in kindred 1901 likely produces a nonfunctional product The BRCAI protein contains a C3HC4 zinc finger domain similar to domains found in numerous nucleic acid binding proteins The first 180 amino acids of BRCA1 contain five more basic residues than acidic residues In contrast the remainder of the molecule is very acidic with a net excess of 70 acidic residues The excess negative charge is particularly concentrated near the COOH terminus Thus one possibility is that BRCA1 encodes a transcription factor with an NH2 terminal DNA 
binding domain and a COOH terminal acidic blob domain with transactivational activity Interestingly the product of another familial tumor suppressor gene WTl also contains zinc finger domains 34 and these are altered by many cancer predisposing mutations in the gene 33 35 The WT1 gene encodes a transcription factor and alternative splicing of exons that encode parts of the zinc finger domains alters the DNA binding properties of WT1 36 Some alternatively spliced forms of WT1 mRNA generate WT1 proteins that act as transcriptional repressors 37 Differential splicing of BRCA1 may alter the zinc finger motif Fig 3 raising the possibility that a regulatory mechanism similar to that occurring in WT1 may apply to BRCA1 The identification of a gene that i falls within the interval known from genetic studies to include BRCA1 and ii contains frameshift nonsense and regulatory mutations that cosegregate with predisposing BRCA1 alleles strongly indicates that this gene is BRCA1 The observation of potential predisposing 
mutations in individuals C B 1353 1078 872603 14 310r r_ 7 p Downloaded from www sciencemag org on February 12 2008 m PM1 Fig 6 Mutation and cosegregation analysis in BRCA 1 kindreds Carrier individuals are represented as filled cirG cles and squares in the pedigree diagrams A Nonsense mutation in kinA 1 dred 2082 P indicates the person origG inally screened b and c are haplotype carriers a d e f and g do not carry the BRCA 1 haplotype The C to T mutation results in a stop codon and creates a A site for the restriction enzyme Avr II PCR amplification products were cut G C l A IIT G with this enzyme subjected to electroA phoresis through 1 0 agarose gels and stained with ethidium bromide The carriers are heterozygous for the site and therefore show three bands The G PCR products of noncarriers remain uncut by Avr II and therefore show one band Size standards are in base pairs B Frameshift mutation in kindred 1910 Sequencing reactions were loaded side by side as A C G and T reactions The first three lanes for 
each nucleotide set contain sequence PM1 and PM7 Table 3 Samples were examined for heterozygosity in the ladders from noncarriers Lanes 1 to 3 contain A ladders from carrier germ line and compared to the heterozygosity of lymphocyte mRNA The individuals Lane 4 contains the A ladder from a kindred member who does top two rows of each panel contain PCR products amplified from genomic DNA and the bottom two rows contain PCR products amplified from not carry the BRCA 1 mutation The frameshift resulting from the cytosine insertion is apparent in lanes 1 through 3 The diamond shape in the lymphocyte cDNA A and G are the two alleles detected by the ASOs pedigree diagram is used to protect the confidentiality of the transmitting The dark spots indicate that a particular allele is present in the sample The parent C Inferred regulatory mutation in kindred 2035 ASO analysis of first three lanes of the lower panel represent the three genotypes from unrelated individuals There is no cDNA for sample 4 in either panel 
haplotype carriers and noncarriers used two different polymorphisms a 70 SCIENCE VOL 266 7 OCTOBER 1994 EM40 m liffilim REFERENCES AND NOTES 1 American Cancer Society Cancer Facts Figures 1994 American Cancer Society Atlanta GA 1994 p 13 2 E B Claus W D Thompson N Risch Am J Hum Genet 48 232 1991 3 D F Easton D T Bishop D Ford B P Crockford Breast Cancer Linkage Consortium ibid 52 678 1993 J M Hall et al Science 250 1684 1990 S A Narod et a Lancet 338 82 1991 R Wooster et al Science 265 2088 1994 D Malkin et al ibid 250 1233 1990 M Swift L Sholman M Perry C Chase Cancer Res 36 209 1976 9 M Swift P J Reitnauer D Morrell C L Chase N Engl J Med 325 1831 1991 10 L Cannon Albright et al Cancer Res 54 2378 4 5 6 7 8 1994 11 C Mettlin l Croghan N Natarajan W Lane Am J Epidemiol 131 973 1990 12 H S Smith et a J Cell Biochem Suppl 14G 144 1993 13 D P Kelsell D M Black D T Bishop N K Spurr Hum Mol Genet 2 1823 1993 14 S Neuhausen et a Cancer Res in press 15 A M Bowcock et al Am J Hum Genet 52 718 1993 16 J Simard et 
al Hum Mol Genet 2 1193 1993 17 D E Goldgar et a J Nat Cancer Inst 86 200 1994 18 S L Neuhausen et al Hum Mol Genet in press 19 D E Goldgar et a unpublished results 20 Candidate expressed sequences are defined as DNA sequences obtained by i direct screening of breast fetal brain lymphocyte or ovary cDNAs 39 or ii random sequencing of genomic DNA 40 and prediction of coding exons by XPOUND 41 These expressed sequences in many cases were assem bled into contigs composed of several independently identified sequences Candidate genes may comprise more than one of these candidate expressed sequences 21 Three independent contigs of expressed sequences 1141 1 649 bp 694 5 213 bp and 754 2 1079 bp were isolated by hybrid selection and eventually shown to represent portions of BRCA 1 When expressed sequence tags ESTs for 1 141 1 and 754 2 were used as hybridization probes for RNA blots a single transcript of approximately 7 8 kb was observed in normal breast mRNA which suggested that they encode different portions of 
a single gene Screens of breast fetal brain thymus testis lymphocyte and placental cDNA libraries and PCR experiments with breast mRNA linked the 1141 1 694 5 and 754 2 contigs 5 RACE experiments with thymus testis and breast mRNAs extended the contig to the putative 5 end yielding a composite full length sequence PCR and direct sequencing of P1 s and BACs in the region were used to identify the location of introns and allowed the determination of splice donor and acceptor sites 22 M Kozak Nucleic Acids Res 15 8125 1987 23 T F Smith and M S Waterman J Mol Biol 147 195 1981 24 S F Altschul W Gish W Miller E W Myers D J Lipman ibid 215 195 1990 25 We performed database comparison by using i BLAST alignment algorithms 24 on the National Center for Biotechnology Information NCBI databases ii Smith Waterman alignment algorithms 23 on a MasPar computer to search the SwissProt database at MasPar and at the European Molecular Biology Laboratory and iii Smith Waterman algorithms on a Compugen Biocelerator at the 
Weizmann Institute to search the GenBank nucleotide and SwissProt protein databases 26 The templates for PCR were lymphocyte cDNA or genomic DNA from members of BRCA1 kindreds who carried the predisposing haplotype Sequences of PCR primers used to amplify each exon of BRCA1 are available upon request The PCR conditions were one cycle at 950C 5 min four cycles at 950C 10 s with the annealing temperature Tan at 680C for 10 s and at 720C for 10 s four cycles with Tann 66 C four cycles with Tann 64 C four cycles with Tann 62 C and 30 cycles with Tann 60 C The buffer conditions were as described 42 The PCR products were purified from 1 0 agarose gels with Qiaex beads QIAGEN analyzed by cycle sequencing with a 32P deoxy adenosine triphosphate 43 and subjected to electrophoresis on 6 polyacrylamide gels Polymorphisms were initially detected by eye on both strands and subsequently confirmed by ASO analysis 27 27 PCR products were generated as described 26 and quantified after electrophoresis through 2 agarose gels 
containing ethidium bromide by comparison with DNA standards PCR product 10 gi was added to 110 ul of denaturant 7 5 ml of H20 6 0 ml of 1 N NaOH 1 5 ml of 0 1 bromophenol blue and 75 ml of 0 5 mM EDTA and incubated for 10 min at room temperature Samples 30 u were then blotted onto Hybond membrane Amersham with a dot blotting apparatus Gibco BRL The DNA was fixed on the membrane by exposure to ultraviolet light Stratagene Prehybridization was carried out at 45 C in x5 SSPE 0 75 M NaCI 0 05 M NaH2Pu4 H2O and 0 005 M EDTA and 2 SDS 34 Wild type and mutant ASOs were labeled by incubation at 37 C for 10 min in a reaction that included 5 pCi of 32P adenosine triphosphate 100 ng of ASO 10 units of T4 polynucleotide kinase Boehringer Mannheim and kinase buffer 44 Labeled ASO 20 ng was used in an overnight hybridization reaction in the same buffer used for prehybridization Each blot was washed twice in x5 saline sodium citrate and 0 1 SDS for 10 min at room temperature and then for 30 min at progressively higher 
temperatures until nonspecific hybridization signals were eliminated Blots were exposed to x ray film for 40 min without an intensifying screen 28 The African American samples from Utah were from a newborn screening program 29 A reviewer suggested the possibility that this gene which we call BRCA 1 may contain frameshift mutations other than those detected here at a significant frequency in members of the general population 30 E Shaulian A Zauberman D Ginsberg M Oren Mol Cell Biol 12 5581 1992 31 S Srivastava S Wang Y A Tong Z M Hao E H Chang Cancer Res 53 4452 1993 32 L K Su et al ibid p 2728 33 M H Little et a Hum Mol Genet 2 259 1993 34 D A Haber et a Cell 61 1257 1990 35 M H Little et al Proc Nat Acad Sci U S A 89 4791 1992 36 W A Bickmore et a Science 257 235 1992 37 I A Drummond et al Mol Cell Biol 14 3800 1994 38 P A Futreal et a Science 266 120 1994 39 M Lovett J Kere L M Hinton Proc Nat Acad Sci U S A 88 9628 1991 P A Futreal eta Hum Mol Genet 3 1359 1994 40 A Kamb et a Nature Genet 8 22 1994 41 A 
Thomas and M H Skolnick IMA J Math Appl Med Biol in press 42 J Weaver Feldhaus et a Proc Natl Acad Sci U S A 91 7563 1994 43 J Sambrook E F Fritsch T Maniatis Molecular Cloning A Laboratory Manual Cold Spring Harbor Laboratory Cold Spring Harbor New York ed 2 1989 44 I G Campbell eta Hum Mol Genet 3 589 1994 45 G Westin et a Proc Nat Acad Sci U S A 81 3811 1984 46 Abbreviations for the amino acid residues are A Ala C Cys D Asp E Glu F Phe G Gly H His I lie K Lys L Leu M Met N Asn P Pro Q Gin R Arg S Ser T Thr V Val W Trp and Y Tyr 47 R Patarca et a Proc Natl Acad Sci U S A 85 2733 1988 48 R Lovering et al ibid 90 2112 1993 49 M Takahashi Y Inaguma H Hiai F Hirose Mo Cell Biol 8 1853 1988 50 Two of the kindreds referred to here have been studied independently J Feunteun et a Am J Hum Genet 52 736 1993 D Anderson unpublished results 51 D Goldgar unpublished results 52 We are grateful for the cooperation of the individuals from the BRCA 1 kindreds and for the assistance of our clinic coordinators P Fields L 
Steele M MacDonald and K Brown and for the help of C J Marshall We thank D Ballinger K Foumier W Gilbert L Norton G Omen J Rine J Simard R Williams and B Wold for scientific advice and F Bartholomew H Brownlee S Burgett J Collette B S Dehoff I L Jenkins A Leavitt K Richardson and K Rowe for technical support Linkage study controls were kindly provided by M Pericak Vance Supported in part by NIH grants CA 55914 M H S CA 54936 CA 4871 1 CA 42014 CN 0522 RR00064 and HG 00571 D G the National Cancer Institute of Canada the Canadian Genetic Diseases Network S N and the Cedars Cancer Institute of the Royal Victoria Hospital P T Downloaded from www sciencemag org on February 12 2008 whose early onset breast or ovarian cancer was not ascertained by family history supports the view that many early onset cases are due to mutations at the BRCAJ locus 38 The role of BRCAI in cancer progression may now be addressed with molecular precision The large size and fragmented nature of the coding sequence will make exhaustive 
searches for new mutations challenging Nevertheless the percentage of total breast and ovarian cancer caused by mutant BRCAI alleles will soon be estimated and individual mutation frequencies and penetrances may be established This in turn may permit accurate genetic screening for predisposition to a common deadly disease Although such research represents an advance in medical and biological knowledge it also raises numerous ethical and practical issues both scientific and social that must be addressed by the medical community Note added in proof Analysis of kindred 1911 indicates a possible linkage to BRCA2 suggesting that early breast cancer in this kindred is not due to a mutation of BRCAI 51 s 2 September 1994 accepted 14 September 1994 For information about an audio conference on the topic of the breast cancer gene BRCA1 see page 15 SCIENCE VOL 266 7 OCTOBER 1994 71 
52	b	Research Letters Results of the ATAC Arimidex Tamoxifen Alone or in Combination trial after completion of 5 years adjuvant treatment for breast cancer ATAC Trialists Group The standard adjuvant endocrine treatment for postmenopausal women with hormone receptor positive localised breast cancer is 5 years of tamoxifen but recurrences and side effects restrict its usefulness The aromatase inhibitor anastrozole was compared with tamoxifen for 5 years in 9366 postmenopausal women with localised breast cancer After a median follow up of 68 months anastrozole signiﬁcantly prolonged disease free survival 575 events with anastrozole vs 651 with tamoxifen hazard ratio 0 87 95 CI 0 78 0 97 p 0 01 and time to recurrence 402 vs 498 0 79 0 70 0 90 p 0 0005 and signiﬁcantly reduced distant metastases 324 vs 375 0 86 0 74 0 99 p 0 04 and contralateral breast cancers 35 vs 59 42 reduction 12 62 p 0 01 Almost all patients have completed their scheduled treatment and fewer withdrawals occurred with anastrozole than with 
tamoxifen Anastrozole was also associated with fewer side effects than tamoxifen especially gynaecological problems and vascular events but arthralgia and fractures were increased Anastrozole should be the preferred initial treatment for postmenopausal women with localised hormone receptor positive breast cancer A All patients HR patients Favours anastrozole Favours tamoxifen Members listed at http image thelancet com extras 04let11120webappendix pdf Correspondence to Prof Anthony Howell anthony howell christie tr nwest nhs uk Hazard ratio All patients HR patients Disease free survival 0 87 0 83 Time to recurrence 0 79 0 74 Time to distant recurrence 0 86 0 84 Overall survival 0 97 0 97 Time to breast cancer death 0 88 0 87 Contralateral breast cancer 0 58 0 47 0 2 0 4 0 6 0 8 1 0 1 2 1 5 Hazard ratio A T and 95 CI 2 0 25 B Proportion with recurrence The standard adjuvant endocrine treatment for postmenopausal women with hormone receptor positive localised breast cancer is 5 years of tamoxifen Nevertheless 
recurrences and side effects limit its usefulness 1 The Arimidex Tamoxifen Alone or in Combination ATAC trial a double blind randomised trial compared 5 years of the aromatase inhibitor anastrozole alone with tamoxifen alone or the combination as adjuvant therapy in 9366 postmenopausal women with localised breast cancer 2 3 Primary objectives were to discover whether anastrozole is at least as effective as tamoxifen in postmenopausal women with localised breast cancer and or offers beneﬁts in safety or tolerability over tamoxifen in this group of patients Initial analyses of the ATAC trial at 33 and 47 months of median follow up showed that anastrozole signiﬁcantly prolonged disease free survival and time to recurrence and reduced the incidence of contralateral breast cancer compared with tamoxifen 2 3 After these analyses the combination treatment arm was closed because of low efﬁcacy Details of the trial design methods primary objectives and endpoints have been reported previously 2 Here we compare the 
efﬁcacy and tolerability of anastrozole with that of tamoxifen after a median follow up of 68 months This time extends beyond the planned 5 year treatment period and only 8 of patients remain on trial treatment Treatment with anastrozole led to signiﬁcant improvements compared with tamoxifen for diseasefree survival 575 vs 651 events hazard ratio 0 87 95 CI 0 78 0 97 p 0 01 and time to recurrence 402 vs 498 0 79 0 70 0 90 p 0 0005 A greater advantage was seen in disease free survival 0 83 0 73 0 94 p 0 005 and in time to recurrence 0 74 0 64 0 87 p 0 0002 in hormone receptor positive patients ﬁgure 1A This 26 risk reduction over tamoxifen for time to Published online December 8 2004 http image thelancet com extras 04let11120web pdf Hazard ratio 0 74 95 CI 0 64 0 87 p 0 0002 20 Tamoxifen 15 Anastrozole 10 5 0 0 1 2 2540 2516 2448 2398 3 Follow up time years 4 5 6 Numbers at risk Anastrozole 2618 Tamoxifen 2598 Absolute difference 2355 2304 1 7 2268 2189 2 4 2014 1932 2 8 830 774 3 7 Figure A Efﬁcacy endpoints 
for all patients and hormone receptor positive patients and B time to recurrence in hormone receptor positive patients A anastrozole T tamoxifen HR hormone receptor positive Odds ratio calculated instead of hazard ratio www thelancet com Published online December 8 2004 http image thelancet com extras 04let11120web pdf For personal use Only reproduce with permission from Elsevier Ltd 1 Research Letters Number of patients Odds ratio anastrozole p vs tamoxifen 95 CI Anastrozole n 3092 Hot ﬂushes Nausea and vomiting Fatigue tiredness Mood disturbances Arthralgia Vaginal bleeding Vaginal discharge Endometrial cancer Fractures Hip Spine Wrist Colles All other sites Ischaemic cardiovascular disease Ischaemic cerebrovascular events Venous thromboembolic events Deep venous thromboembolic events Cataracts Tamoxifen n 3094 1104 35 7 393 12 7 575 18 6 597 19 3 1100 35 6 167 5 4 109 3 5 5 0 2 340 11 0 37 1 2 45 1 5 72 2 3 220 7 1 127 4 1 1264 40 9 384 12 4 544 17 6 554 17 9 911 29 4 317 10 2 408 13 2 17 0 8 237 7 7 31 1 
0 27 0 9 63 2 0 142 4 6 104 3 4 0 80 0 73 0 89 1 03 0 88 1 19 1 07 0 94 1 22 1 10 0 97 1 25 1 32 1 19 1 47 0 50 0 41 0 61 0 24 0 19 0 30 0 29 0 11 0 80 1 49 1 25 1 77 1 20 0 74 1 93 1 68 1 04 2 71 1 15 0 81 1 61 1 59 1 28 1 98 1 23 0 95 1 60 0 0001 0 7 0 3 0 2 0 0001 0 0001 0 0001 0 02 0 0001 0 5 0 03 0 4 0 0001 0 1 62 2 0 88 2 8 0 70 0 50 0 97 0 03 87 2 8 140 4 5 0 61 0 47 0 80 0 0004 48 1 6 74 2 4 0 64 0 45 0 93 0 02 182 5 9 213 6 9 0 85 0 69 1 04 0 1 In favour of tamoxifen n 2229 for anastrozole 2236 for tamoxifen excluding patients with hysterectomy at baseline recorded at any time Patients with one or more fractures occurring at any time before recurrence includes patients no longer receiving treatment Patients may have had one or more fractures at different sites Table Prespeciﬁed adverse events on treatment or within 14 days of discontinuation recurrence is in addition to the 47 risk reduction previously shown for 5 years of tamoxifen versus placebo in adjuvant studies 4 No signiﬁcant differences were 
noted in effect according to subgroup at the 1 level and the hazard rate was lower for anastrozole in all subgroups except for patients who were hormonereceptor negative or whose hormone receptor status was unknown Absolute differences in recurrence rates increased over time even beyond 5 years of scheduled treatment suggesting that there is a carryover effect for anastrozole similar to that observed for tamoxifen 4 at least in the short term ﬁgure 1B The beneﬁts of anastrozole were seen at all times after the ﬁrst year of follow up In particular the high hazard rate seen in years 1 3 for tamoxifen was substantially suppressed by anastrozole We noted a signiﬁcant overall beneﬁt in time to distant recurrence for anastrozole 324 vs 375 events hazard ratio 0 86 95 CI 0 74 0 99 p 0 04 with a similar trend in the subset of hormone receptorpositive patients 0 84 0 70 1 00 p 0 06 The incidence of contralateral breast cancer was substantially reduced by anastrozole compared with tamoxifen all patients 35 vs 59 42 
reduction 95 CI 12 62 p 0 01 hormone receptor positive patients 53 25 71 p 0 001 Since tamoxifen shows a 50 reduction in the occurrence of these tumours in hormone receptor positive patients compared with placebo 4 the ﬁndings from the ATAC study suggest that 2 anastrozole treatment might prevent 70 80 of hormone receptor positive tumours in women at high risk of breast cancer 831 women died 500 60 after recurrence of breast cancer and 331 40 without recurrence and due to other causes Overall survival was similar for anastrozole and tamoxifen hazard ratio 0 97 95 CI 0 85 1 12 p 0 7 a 12 reduction in deaths from breast cancer in the anastrozole group was not signiﬁcant 0 88 0 74 1 05 p 0 2 However since the trial population had a relatively good prognosis 5695 61 of patients were lymph node negative and 5959 64 had tumours 2 cm or smaller in diameter it is too early to expect a difference in survival For example it took at least 7 years to show a signiﬁcant survival advantage for tamoxifen versus placebo in 
previous adjuvant studies 5 The signiﬁcant reductions in recurrence and distant recurrence associated with anastrozole strongly suggest that a reduction in deaths from breast cancer will eventually be seen Since almost all patients have completed their scheduled 5 years of therapy the safety and tolerability data during treatment can be deemed ﬁnal Withdrawals due to adverse events were signiﬁcantly less common with anastrozole 344 11 1 than with tamoxifen 442 14 3 p 0 0002 Drug related serious adverse events were also signiﬁcantly less common with anastrozole 146 4 7 than with tamoxifen 271 9 0 p 0 0001 Treatment with anastrozole was associated with signiﬁcant reductions in the incidence of endometrial cancer thromboembolic events ischaemic cerebrovascular events vaginal bleeding hot ﬂushes and vaginal discharge compared with tamoxifen table Tamoxifen was associated with fewer fractures and less arthralgia than was anastrozole Fracture rates per 1000 woman years were 22 6 for anastrozole and 15 6 for 
tamoxifen hazard ratio 1 44 95 CI 1 21 1 68 p 0 0001 The incidence of hip fracture was low and similar for anastrozole and tamoxifen table Findings of several studies show that bisphosphonates are effective in maintaining bone density1 in women with breast cancer The risk ratios for all the prespeciﬁed adverse events in the present report were similar to those noted in previous analyses 2 3 suggesting that the safety proﬁle of anastrozole remains unchanged during the 5 year treatment period No new safety concerns emerged This analysis of the ATAC trial conﬁrms the efﬁcacy and tolerability beneﬁts of anastrozole as initial adjuvant treatment for postmenopausal women with localised breast cancer The results are only applicable to anastrozole since it is unknown how differences between the aromatase inhibitors affect their clinical usefulness Results from studies evaluating anastrozole or exemestane after 2 3 years of adjuvant tamoxifen compared with continuing tamoxifen suggest that it is www thelancet com 
Published online December 8 2004 http image thelancet com extras 04let11120web pdf For personal use Only reproduce with permission from Elsevier Ltd Research Letters reasonable to switch patients currently on tamoxifen to an aromatase inhibitor 1 The present data suggest that it is not appropriate to wait 5 years to start an aromatase inhibitor Furthermore the higher rates of recurrence especially in years 1 3 and the increased numbers of adverse events and treatment withdrawals associated with tamoxifen lend support to the approach of offering the most effective and well tolerated therapy at the earliest opportunity 5 years of anastrozole should now be considered as the preferred initial adjuvant endocrine treatment for postmenopausal women with hormone receptor positive localised breast cancer ATAC writing committee Prof A Howell MD Chairman of the ATAC Steering Committee Christie Hospital Manchester UK Prof J Cuzick PhD independent statistician Cancer Research UK London UK Prof M Baum ChM University 
College London London UK A Buzdar MD University of Texas MD Anderson Cancer Center Houston USA Prof M Dowsett PhD Royal Marsden Hospital London UK Prof J F Forbes FRACS University of Newcastle Newcastle Mater Misericordiae Hospital NSW Australia G Hoctin Boes MD AstraZeneca Pharmaceuticals Wilmington USA J Houghton BSc Clinical Trials Group of the Department of Surgery University College London London UK G Y Locker MD Evanston Northwestern Healthcare Northwestern University Feinberg School of Medicine Evanston IL USA Prof J S Tobias MD Meyerstein Institute of Clinical Oncology Middlesex Hospital London UK ATAC Trialists Group Full list of trialists and principal and main co investigators in the ATAC Trial available at http image the lancet com extras 04let11120webappendix pdf Contributors A Howell is the current principal investigator and chairs the writing and steering committees M Baum was the original principal investigator and played an active role in protocol design and trial governance A Buzdar 
participated in the trial management and data interpretation J Cuzick was responsible for the statistical analysis and participated in trial design and data interpretation M Dowsett participated in the trial design and conduct and in data interpretation J F Forbes coordinated entry of patients from the Australia New Zealand Breast Cancer Trials Group to the trial G Hoctin Boes participated in the coordination of the trial J Houghton participated in design issues the overall operational management of the trial and the preparation of trial results for analysis G Y Locker participated in the management of the trial J S Tobias contributed to the design of the study All contributors participated in writing the paper Conﬂict of interest statement A Howell has received honoraria and appeared on speakers bureaux for AstraZeneca M Baum has received travel grants honoraria for lectures and occasional consultancy fees for AstraZeneca A Buzdar has received research grants travel awards and honoraria from AstraZeneca J 
Cuzick is statistical consultant to and has received research funds from AstraZeneca M Dowsett has received paid consultancy from AstraZeneca and is in receipt of grants from AstraZeneca for work done in his laboratory J F Forbes has received honoraria from AstraZeneca Novartis and Lilly for attendance at advisory board meetings He is responsible for the undertaking of clinical trials by the Australia New Zealand Breast Cancer Trials Group which have been supported by education research grants from various pharmaceutical companies G Hoctin Boes is an employee of AstraZeneca J Houghton holds a contract with AstraZeneca for operational management and to support some of the monitoring of the trial She has also received travel awards and honoraria from AstraZeneca G Y Locker has received research grants and appeared on speakers bureaux for AstraZeneca J S Tobias has received occasional honoraria and travel expenses from AstraZeneca All have seen and approved the ﬁnal version of the manuscript Acknowledgments 
First and foremost we thank all the patients for their participation in the trial We also thank the trial investigators nurses data managers pharmacists and other support staff at the local sites the monitors and data management staff of AstraZeneca and the various collaborative groups We also thank the members of the International Steering Committee the Independent Data Monitoring Committee and the International Project Team References 1 Winer EP Hudis C Burstein H et al American Society of Clinical Oncology Technology on the use of aromatase inhibitors as adjuvant therapy for postmenopausal women with hormonereceptor positive breast cancer status report 2004 J Clin Oncol 2004 published online Nov 15 DOI 10 1200 JCO 2005 09 121 2 The ATAC Trialists Group Anastrozole alone or in combination with tamoxifen versus tamoxifen alone for adjuvant treatment of postmenopausal women with early breast cancer ﬁrst results of the ATAC randomised trial Lancet 2002 359 2131 39 3 The ATAC Trialists Group Anastrozole alone 
or in combination with tamoxifen versus tamoxifen alone for adjuvant treatment of postmenopausal women with early stage breast cancer Results of the ATAC Arimidex Tamoxifen Alone or in Combination trial efﬁcacy and safety update analyses Cancer 2003 98 1802 10 4 Early Breast Cancer Trialists Collaborative Group Tamoxifen for early breast cancer an overview of the randomised trials Lancet 1998 351 1451 67 5 Fisher B Dignam J Bryant J Wolmark N Five versus more than ﬁve years of tamoxifen for lymph node negative breast cancer updated ﬁndings from the National Surgical Adjuvant Breast and Bowel Project B 14 randomized trial J Natl Cancer Inst 2001 93 684 90 www thelancet com Published online December 8 2004 http image thelancet com extras 04let11120web pdf For personal use Only reproduce with permission from Elsevier Ltd 3 Research Letters Web Appendix The ATAC Trialists group Steering Committee Membership Professor A Howell Chairman Christie Hospital Manchester UK Professor Judith Adams University of 
Manchester Manchester UK Professor M Baum University College London UCL London UK Professor AR Bianco Universita Degli Studi Di Napoli Federico II Napoli Italy Dr A Buzdar Vice Chairman The University of Texas MD Anderson Cancer Center Houston USA Professor D Cella Northwestern University Evanston Illinois USA Dr M Coibion Institut Bordet Bruxelles Belgium Professor R Coleman Cancer Research Centre Weston Park Hospital Shefﬁeld UK Dr M Constenla Hospital Montecelo Pontevedra Spain Professor J Cuzick Independent Statistician Cancer Research UK London UK Professor Dr W Distler Frauenklinik Carl Gustav Carus Universität Dresden Dresden Germany Professor M Dowsett The Royal Marsden Hospital London UK Mr S Duffy St James s University Hospital Leeds UK Professor R Eastell University of Shefﬁeld Shefﬁeld UK Professor LJ Fallowﬁeld University of Sussex Brighton UK Professor J Forbes Newcastle Mater Misericordiae Hospital NSW Australia Professor WD George Beatson Oncology Centre Western Inﬁrmary Glasgow UK Sr J Gray 
Belfast City Hospital Belfast UK Dr J P Guastalla Centre Léon Bérard Lyon France Mr R Hellmund Dr G Hoctin Boes AstraZeneca Pharmaceuticals Wilmington USA Mrs J Houghton Trial Secretary Dr N Williams Clinical Trials Group of the Department of Surgery UCL London UK Professor Dr JGM Klijn Dr Daniel den Hoed Kliniek and University Hospital Rotterdam Rotterdam The Netherlands Dr GY Locker Evanston Northwestern Healthcare Northwestern University Feinberg School of Medicine Evanston IL USA Dr J Mackey Cross Cancer Institute Edmonton Alberta Canada Professor RE Mansel University of Wales College of Medicine Cardiff UK Professor JM Nabholtz Hartman Oncology Institute Levallois Perret France Dr T Nagykalnai Uzsoki U Hospital Budapest Hungary Dr A Nicolucci Givio Co ordinating Centre Consorzio Mario Negri Sud Centro Di Ricerchi Farmacologichi E Biomedichi Chieta Italy Dr U Nylen Radiumhemmet Karolinska Sjukhuset Stockholm Sweden Mr R Sainsbury University College London UCL London UK Dr F Sapunar Dr VJ Suarez Mendez 
AstraZeneca Pharmaceuticals Macclesﬁeld UK Professor JS Tobias The Meyerstein Institute of Clinical Oncology Middlesex Hospital London UK International Project Team A Doe Dr F Sapunar Dr VJ Suarez Mendez AstraZeneca Pharmaceuticals Macclesﬁeld UK E Foster ISD Cancer Clinical Trials Team Edinburgh UK J Houghton N Williams Clinical Trials Group of the Department of Surgery UCL London UK A Nicolucci Mario Negri Institute Chieta Italy S Pollard Clinical Trials Research Unit Leeds UK Data Monitoring Committee Dr M Buyse International Institute for Drug Development IDDI Brussels Belgium Professor J Cuzick Independent statistician Mr C Wale Cancer Research UK London UK Dr R Margolese McGill University The Sir Mortimer B Davis Jewish General Hospital Montreal Québec Canada Professor JJ Body Institute J Bordet Bruxelles Belgium Collaborative Operational Groups JF Forbes Group Co ordinator JK Wakeham Study Coordinator Australian New Zealand Breast Cancer Trials Group Operations Ofﬁce S de Placido Study Co ordinator C 
Carlomagno Study Co ordinator Universita Degli Studi Di Napoli Federico II Italy A Nicolucci Group Co ordinator M Belﬁglio Study Co ordinator M Valentini Study Co ordinator GIVIO Group Consorzio Mario Negri Sud Italy E Foster Principal Trial Co ordinator and CCTT contact ISD Cancer Clinical Trials Team Edinburgh UK Liz Foster isd csa scot nhs uk S Pollard Head of Pharmaceutical Collaboration Clinical Trials Research Unit University of Leeds Leeds UK J Houghton Senior Lecturer in Clinical Trials and CTG contact N Williams Trial Coordinator Clinical Trials Group of the Department of Surgery UCL London UK j houghton ctg ucl ac uk Principal and main co investigators in the ATAC Trial Argentina Dr F Coppola Dr C Bas Hospital Aleman Capital Federal Buenos Aires Dr J Itala Dr G Cortese Hospital de Clinicas Buenos Aires University Capital Federal Buenos Aires Dr A Nuñez de Pierro Dr D Allemand Hospital General de Agudos Juan Fernandez Capital Federal Buenos Aires Dr H Guixa Dr R Testa Hospital Italiano Capital 
Federal Buenos Aires Dr J Lebron University of Buenos Aires Capital Federal Buenos Aires Australia Professor G Gill Dr J Kollias Royal Adelaide Hospital Adelaide SA Dr J Chirgwin Dr M Leyden Box Hill Hospital Box Hill VIC Dr J Beith Dr A Sullivan Royal Prince Albert Hospital Camperdown NSW Dr S Della Fiorentina Dr A Goldrick Liverpool Hospital Liverpool NSW Dr J Chirgwin Maroondah Hospital Maroondah VIC Associate Professor G Richardson Dr S Hart Monash Medical Centre Melbourne VIC Associate Professor G Toner Dr P Francis Peter MacCallum Cancer Institute East Melbourne VIC Dr R Snyder Dr I Burns St Vincents Hospital Melbourne VIC Professor M Friedlander Dr D Goldstein Prince of Wales Hospital Randwick NSW Professor J Forbes Dr D Jackson University of Newcastle Newcastle Mater Misericordiae Hospital Waratah NSW Belgium Dr A Makar Dr D Van den Weyngaert AZ Middelheim Antwerpen Professor D Gangji Dr T Velu Hôpital Erasme Brussels Dr M Coibion Dr J M Nogaret Institut Bordet Brussels Dr P Neven Dr Laurent St Jan 
Ziekenhuis Brussels Dr J De Mol Dr F Van Aelst Heilig Hart Ziekenhuis Roeselare Canada Dr SR Sehdev MD Queen Lynch Medical Centre Brampton Ontario Dr R Simard Complexe Hospitalier de la Sagami Chicoutimi Québec J Mackey MD Cross Cancer Institute Edmonton Alberta Dr J Dufresne MD CHUS Hospital Fleurimont Fleurimont Québec Dr WS Lofters Kingston Regional Cancer Center Kingston Ontario Dr DR Holland Lethbridge Cancer Centre Lethbridge Alberta Dr HL Solow Markham Stouffville Health Centre Markham Ontario Dr JA Gapski Trillium Health Centre Mississauga Ontario Dr SH Rubin South East Health Care Oncology Department Moncton Hospital Moncton New Brunswick Dr A Robidoux CHUM Campus Hotel Dieu Montreal Québec B L Esperance MD Hôpital du Sacre Coeur de Montreal Québec Dr LC Panasci McGill Department of Oncology Montreal Québec Dr LA Zibdawi MD Southlake Regional Health Centre Newmarket Ontario Dr J Chang Lakeridge Health Oshawa Ontario Dr S Arif MD Penticton Regional Hospital Penticton British Columbia Dr RF Wierzbicki 
Peterborough Regional Health Center Oncology Clinic Peterborough Ontario Dr BP Findlay Hotel Dieu Hospital Oncology Department Québec City Québec Dr J Robert CHAUQ Hospital du St Sacrement Québec City Québec Dr S Lebel Hospital Laval Québec City Québec Dr M Jancewicz Allan Blair Cancer Center Regina Saskatchewan Dr MJ Burnell Atlantic Health Sciences Corp Saint John New Brunswick Dr A Sami Saskatoon Cancer Centre Saskatoon Saskatchewan Dr PLD Walde Algoma Regional Cancer Program Sault Ste Marie Ontario Dr PK Ganguly H Bliss Murphy Cancer Centre St Johns Newfoundland Dr CJ Germond MD Northeastern Ontario Regional Cancer Center Sudbury Ontario Dr Y Rahim Toronto East General Hospital Toronto Ontario Dr JJ Wilson Humber River Regional Hospital Weston Ontario Dr AL Cooke Manitoba Cancer Treatment and Research Foundation Winnipeg Manitoba Czech Republic Dr K Petrakova Dr R Demlova Masarykuv onkologicky ustav Brno Dr P Vodvarka Dr T Kysela FNsP Ostrava Ostrava Poruba Dr B Konopasek Dr P Mares Vseobecna fakultni 
nemocnice Praha France Professor J E Mention Centre de Gyneco Obst CHU Amiens Dr D Serin Dr Y Goubely Brewer Clinique Ste Catherine Avignon Professor J P Labat Dr J P Malhaire CHU Brest Brest Dr G Devulder Dr S Mirdat Dako Centre Hospitalier Laënnec Creil Professor JM Nabholtz Hartman Oncology Institute Levallois Perret France Professor D Houze De L Aunoit Dr J Y Charvolin Hôpital St Philibert Lomme Dr J P Guastalla Dr J Guastalla Centre Léon Bérard Lyon Dr R Coquard Dr B Velay Clinique St Jean Lyon Dr C Lejeune Dr D Hadjadj Aoul Hôpital de la Conception Marseille Dr M Untereiner Hôpital Clinique Claude Bernard Metz Professor PI Laffargue Hôpital Arnaud de Villeneuve Montpellier Dr M Rios Centre Alexis Vautrin Nancy Dr J M Vannetzel Dr R Mahjoubi Centre Chirurgical Henri Hartmann Neuilly sur Seine www thelancet com Published online December 8 2004 http image thelancet com extras 04let11120webappendix pdf For personal use Only reproduce with permission from Elsevier Ltd 1 Research Letters Dr R Samak Cabinet 
Medical Nice Dr F Morvan Dr F Rousseau Centre Hospitalier Pontoise Dr C Veyret Dr JP Julien Centre Henri Becquerel Rouen Dr P Quetin Centre Paul Strauss Strasbourg Professor J P Brettes Dr C Mathelin Hôpital Civil Strasbourg Germany Professor Dr W Distler Dr A Schindelhauer Frauenklinik Carl Gustav Carus Universität Dresden Professor MW Beckmann Universität Erlangen Nuernberg Erlangen Dr C Oberhoff Dr D Hanisch Universitätklinikum Essen Essen Professor A Schneider Friedrich Schiller Universität Jena Jena Professor Dr W Eiermann Dr G Raab Frauenklinik vom Roten Kreuz München Hungary Dr C Polgar National Oncology Institute Budapest Dr K Moskovits Dr Z Nagy St Imre Hospital Budapest Dr T Nagykalnai Dr L Landherr Uzsoki st Hospital Budapest Dr T Pinter Dr G Herodek Petz Aladar Hospital Gyor Dr B Piko Dr I Szegedi Pandy Kalman County Hospital Gyula Dr J Szanto Dr L Marazi BAZ County Hospital Miskolc Dr Z Kahan SZTE Oncotherapy Clinic Szeged Ireland Mr E McDermott Professor N O Higgins St Vincent s Hospital Dublin 
Professor T Gorey The Mater Hospital Dublin Ireland Professor F Given Dr S Tormey University College Galway Galway Italy Dr M Bonsignori Dr S Rossini Ospedale Torrette Ancona Dr F Di Vito Dr M Cucchi Ospedale Regionale Aosta Dr F Testore Mrs L Giaretto Dr Ospedale Civile Asti Dr F Recchia Dr S De Filippis Ospedale Civile Avezzano Dr LR Vito 1st Oncologia Mater Dei Bari Bari M De Lena Dr F Schittulli Istituto Scientiﬁco Oncologico Bari Dr A Martoni Dr E Piana Ospedale S Orsola M Malpighi Bologna Dr G Marini Dr P Marpicati Spedali Civili Brescia Dr M Pintus Dr A Tedde Ospedale Oncologico A Businco Cagliari Dr M Botta Dr D Degiovanni Ospedale S Spirito Casale Monferrato Dr L Basilico Dr M Taraborrelli Ospedale SS Annunziata Chieti Dr S Bravi Dr F Biagioni Ospedale Civile Città di Castello Dr D Cosentino Ospedale S Anna Como Dr G Scognamiglio Dr A Beretta Ospedale Valduce Como Professor P Marchetti Dr ME D Addario Università degli Studi Coppito L Aquila Dr M Obialero Dr F Peradotto Ospedale Civile Cuorgné 
Professor P Malacame Arcispedale S Anna Ferrara Dr A Nuzzo Dr L Laudadio Ospedale Floraspe Renzetti Lanciano Dr M D Aprile Dr M Natali Ospedale S Maria Goretti Latina Dr G Cruciani Dr E Montanari Ospedale Umberto I Lugo Dr E Aitini Dr G Cavazzini Ospedale C Poma Mantova Professor V Adamo Professor G Altavilla Policlinico Universitario G Martino Messina Dr G Gardani Ospedale S Gerardo Monza Dr AR Bianco Universita Degli Studi Di Napoli Federico II Napoli Dr A De Matteis Dr G Landi Istituto Nazionale Studi e Cura Tumori G Pascale Napoli Dr G D Aiuto Dr R Thomas Istituto Nazionale Tumori G Pascale Napoli Dr R Lauria Dr M De Laurentis Universita Federico II Napoli Dr A Fornasiero Dr S Monfardini Ospedale Civile Padova Dr G Brignone Dr L Mesi Ospedale Oncologico M Ascoli Palermo Professor A Riccardi Dr P Pugliese Policlinico S Matteo Pavia Dr G Pavia Dr T Porro Ospedale Civile Rho Dr F Cognetti Dr P Papaldo Istituto Regina Elena Roma Dr G Gasparini Dr MA Castellana Ospedale S Filippo Neri Roma Dr M Mattarei Dr S 
Robbiati Ospedale Civile S Maria delle Grazie Rovereto Professor A Farris Dr G Sanna Istituto Clinical Medica Sassari Dr C Vucusa Dr M Viglietta Ospedale SS Annunziata Savigliano Dr G Fornari Dr A Turletti Ospedale Evangelico Valdese Torino Dr G Nastasi Ospedale Civile S Isidoro Trescore Balneario Professor M Tordiglione AO Ospedale di Circolo e Fondazione Macchi Varese New Zealand Mr I Campbell Dr R Gannaway Waikato Hospital Breast Care Centre Hamilton Poland Dr J Tujakowski Dr M Osmanska Regional Oncology Centre Bydoszcz Dr P Koralewski Dr M Urbanska Ludwick Rydygier Memorial Hospital Krakow Dr B Karczmarek Borowska Dr B Kukielka Budny Regional Oncology Centre Lublin Dr M Teresiak Dr P Laski Regional Oncology Centre Pozan Dr M Krzakowski Dr E Pucula Maria Sklodowska Curie Memorial Oncology Centre Warszawa Portugal Dr D Jardim da Pena Dr R Nabiço Hospital de S Marcos Braga Portugal Dr M Barros Centro Hospitalar das Caldas da Rainha Caldas de Rainha Dr M Chumbo Hospital Condes de Castro Guimaraes Cascais Dr 
O Campos Dr H Gervasio IPO Hospital de Dia Coimbra Dr I Botto Maternidade Bissaya Barreto Coimbra Portugal 2 Professor O de Carlos Hospitais da Universidade de Coimbra Coimbra Portugal Dr O Candeias Dr B da Costa Hospital de Sta Maria Lisboa Dr RA Alves Dias dos reis Hospital de S Joao Porto Portugal Dr A Alcazar Hospital Reynaldo dos Santos Vila Franca de Xira Portugal Slovak Republic Professor S Spanik Dr I Vochyanova Onkologicky ustav sv Alzbety Bratislava Dr M Wagnerova Dr I Andrasina FNsP L Pasteura Kosice South Africa Dr A Maxwell Berea Professor L Goedhals Dr L Smith Nationale Hospital Bloemfontein Professor I Werner Dr E Murray Groote Schuur Hospital Cape Town Dr J Apffelstaedt Dr I Loubser Tyerberg Hospital Cape Town South Africa Dr D Hacking Durban Oncology Centre Westridge Durban Dr G Landers Parklands Hospital Durban Dr D Vorobiof Sandton Oncology Centre Sandton Johannesburg Spain Dr L Giner Hospital San Juan de Alicante Alicante Dr B Molins Hospital Germans Trias i Pujol Barcelona Dra C Alonso 
Muñoz Hospital Santa Cruz y San Pablo Barcelona Dr J Baselga Dr J Tabernero Hospital Vall D Hebron Barcelona Dr M Beltrán Dr E Canals Hospital Josep Trueta Gerona Dr S Menjon Virgen de las Nieves Granada Dra L Calvo Hospital Juan Canalejo La Coruña Dr M Dolores Hospital General de Galicia La Coruña Dr JR Mel Dr G Quintero Hospital Xeral de Logo Lugo Dr P España Hospital Clinica Puerta de Hierro Madrid Dr D Manga Dr A Alonso Hospital Gregorio Marañón Madrid Dr M Repolles Escarda Hospital Ramón y Cajal Madrid Dr P Aramburo Dr JE Alés Hospital Ruber Internacionale Madrid Dr R Alvarez Hospital Central De Asturias Oviedo Dr M Constenla Dr R Garcia Hospital Mentecelo Pontevedra Dr JM Lopez Vega Hospital Marques de Vadecilla Santander Dr JA Moreno Nogueira Dr P Borrego Hospital Virgen del Rocio Sevilla Dr J Montalar Dra A Santaballa Hospital La Fe Valencia Dr V Guillem Dr A Llombart Instituto Valenciano de Oncologia Valencia Dr G Huidoboro Vence Dr J Casal Hospital Meixoeiro Vigo Sweden Dr S Å Olsson Dr L Ryden 
Sjukhuset Ängelholm Associate Professor S Rotstein Dr D Pettersson Sköld Danderyds sjukhus Stockholm Dr B Börjesson B Bengt Länssjukhuset Halmstad Dr P E Jönsson Dr M Malmberg Helsingborg Hospital Helsingborg Dr L Lovén Dr I Grybäck Centralsjukhuset Kristianstad Dr C Ingvar Dr P Lindblom University Hospital Lund Dr S B Holmberg SU Mölndal Hospital Mölndal Sweden Dr U Nylén Dr E Lidbrink Karolinska sjukhuset Stockholm Dr T Fornander Dr G Winblad Södersjukhuset Stockholm Dr R Fernstad Dr L Löfgren St Görans Sjukhus Stockholm Dr T Ambré Dr M Nilsson Centrallasarettet Växjö Netherlands Dr L Siegenbeek van Heukelom Medisch Centrum Alkmaar Alkmaar Mrs AH Baan Dr D van Geldere Ziekenhuis Amstelveen Amstelveen Dr F Valster St Lievensberg Ziekenhuis Bergen Op Zoom Dr E Maartense Reinier de Graaf Gasthuis Delft Dr P van der Velden St het Van Weel Bethesda Ziekenhuis Dirksland Dr G van der Linden Albert Schweitzer Ziekenhuis Dordrecht Dr D Halkema Albert Schweitzer Ziekenhuis Dordrecht Dr C Dijkhuis Oosterschelde 
Ziekenhuis Goes Mrs Dr M van Hennik Beatrix Ziekenhuis Gorinchem Dr P Willemse Academisch Ziekenhuis Groningen Groningen Mrs Dr H de Graff Medisch Centrum Leeuwarden Leeuwarden Dr A de Boer Ljsselland Ziekenhuis Ljssel Dr E Bruggink Dr L Strobbe Nijmeegs Interconfessioneel Ziekenhuis Canisius Wilhelmina Nijmegen Dr D de Gooyer Ziekenhuis Franciscus Roosendaal Dr van der Gaast Academisch Ziekenhuis Rotterdam Dijlzigt Rotterdam Professor Dr J Klijn Mrs Dr C Seynaeve Academisch Ziekenhuis Rotterdam loc Daniel den Hoed Rotterdam Dr P Wismans Haven Ziekenhuis Rotterdam Dr M Baggen Ikazia Ziekenhuis Rotterdam Rotterdam Mrs Dr M Leijs St Clara Ziekenhuis Rotterdam Dr J Braun Schieland Ziekenhuis Schiedam Schiedam Dr F Erdkamp Maasland Ziekenhuis Sittard Dr F Kauw Albert Schweitzer Ziekenhuis Sliedrecht Dr A van Reisen Ziekenhuis Zeeuws Vlaanderen loc de Honte Terneuzen The Netherlands Dr C van der Heul St Elisabeth Ziekenhuis Tilburg Dr J Ruit Vlietland Ziekenhuis Vlaardingen Dr L Kerhofs Ziekenhuis Walcheren 
Vlissingen Dr R Hellingman Zweedse Rode Kruis Ziekenhuis Zierikzee Dr E Trommel Albert Schweitzer Ziekenhuis Zwijndrecht Dr J Coenen Isala Klinieken loc Sophia Zwolle Turkey Professor E Baltali www thelancet com Published online December 8 2004 http image thelancet com extras 04let11120webappendix pdf For personal use Only reproduce with permission from Elsevier Ltd Research Letters Hacettepe University Oncology Hospital Ankara Professor A Aydiner Istanbul University Oncology Institiute Faculty Istanbul UK Dr A Hutcheon Dr T Sarkar Aberdeen Royal Inﬁrmary Aberdeen Mr T Bates Mr N Grifﬁths South Kent Hospitals NHS Trust William Harvey Hospital Ashford Mr M Carr Cheviot Wansbeck NHS Trust Wansbeck General Hospital Ashington Dr C Alcock Stoke Mandeville Hospital NHS Trust Aylesbury Dr B Lavery Dr E Sugden Horton Hospital Radcliffe Hospitals NHS Trust Banbury Professor N Stuart Mr Derek Crawford Gwynedd Hospital NHS Trust Bangor Mr S Dolan Mr G Odling Smee Belfast City Hospital NHS Trust Belfast Mr A Aukland Dr 
D Spooner Sandwell Healthcare NHS Trust Sandwell District General Hospital Birmingham Mr H Bishop Mr R Salem The Royal Bolton Hospital Bolton Dr T Hickish Mr A Skene Royal Bournemouth Christchurch NHS Trust Royal Bournemouth Hospital Bournemouth Dr C Bradley Dr Dennis Parker Bradford Royal Inﬁrmary Bradford Mr S Cawthorn Dr M Shere Frenchay Healthcare NHS Trust Frenchay Hospital Bristol Dr S Goodman Dr E Whipp United Bristol Healthcare NHS Trust Bristol Royal Inﬁrmary Bristol Dr A Moody West Suffolk Hospital NHS Trust Bury St Edmunds Dr C Wilson Dr E Cox Addenbrookes s NHS Trust Hospital Cambridge Professor R Mansel Miss H Sweetland University Hospital of Wales NHS Trust Cardiff Dr P Sauven Dr S Chandrasekharan Mid Essex Hospital Services NHS Trust Chelmsford and Essex Centre Chelmsford Mr G Layer St Peters Hospital NHS Trust Chertsey Dr P Murray Miss F MacNeill Essex Rivers Healthcare NHS Trust Essex County Hospital Colchester Mr J Fox Castle Hill Hospital Cottingham Mr A Ball Crawley Horsham NHS Trust 
Crawley Hospital Crawley Mr G Rawsthorne Mid Cheshire Hospitals NHS Trust Leighton Hospital Crewe Mr R Burns Mr R Blunt Dudley Group of Hospitals NHS Trust Russells Hall Hospital Dudley Dr J Dewar Mr A Thompson Ninewells Hospital Medical School Dundee Dr W Taylor Mr A Cook North Durham NHS Tryst Dryburn Hospital Durham Dr I Kunkler Western General Hospital Edinburgh Dr D Cameron Professor R Leonard Western General Hospital Edinburgh Mr W Cunliffe Mr D Browell Gateshead Hospitals NHS Trust Queen Elizabeth Hospital Gateshead Professor WD George Dr AN Harnett Dr CT Twelves Western Inﬁrmary Glasgow Mr DC Smith Victoria Inﬁrmary Glasgow Mr M Kissin The Royal Surrey County and St Lukes NHS Trust Royal Surrey County Hospital Guildford Mr V Modgill Mr P Surtees Halifax Royal Inﬁrmary Halifax Mr R Knox Harrogate District Hospital Harrogate Dr J Joffe Huddersﬁeld Royal Inﬁrmary Huddersﬁeld Dr B Lavery Professor A Harris Oxford Radcliffe Hospital NHS Trust John Radcliffe Hospital Headington Oxford Mr S Raymond St 
Albans Hemel Hempstead NHS Trust Dr B Lavery Dr N Rowell South Bucks NHS Trust Wycombe General Hospital High Wycombe Dr J LeVay Mr T Archer Ipswich Hospitals NHS Trust Ipswich Mr A Nejim Dr I Hutchinson Airedale General Hospital Keighley Mr M Lansdown Dr T Perren St James s University Hospital Leeds Mr K Horgan Dr D Dodwell Leeds General Inﬁrmary Leeds Mr C Holcombe Royal Liverpool University Hospital NHS Trust Royal Liverpool Hospital Liverpool Mr J Rainey St Johns Hospital Howden Livingston Dr G Howard Western General Hospital Edinburgh Mr S Holt Mr Y Sharaiha Llanelli Dinefwr NHS Trust Price Phillip Hospital Llanelli Professor J Tobias Dr M Gaze UCL Hospitals NHS Trust The Middlesex Hospital London Dr J Mansi Mr N Sacks St Georges NHS Trust Hospital Tooting London Dr C Coulter Dr S Stewart St Marys NHS Trust Hospital London Dr A Jones Mr T Davidson Royal Free Hampstead NHS Trust The Royal Free Hospital London Mr JRC Sainsbury University College London London Dr A Jones Mr A Wilson Whittington Hospital NHS 
Trust London Professor I Smith Mr G Gui The Royal Marsden Hospital NHS Trust The Royal Marsden Hospital London Mr D Matheson Macclesﬁeld District General Hospital Macclesﬁeld Professor A Howell Dr A Wardley Christie Hospital Manchester Dr L Barr Dr N Bundred Withington Hospital Manchester Miss P Durning Mr A Clason James Cook University Hospital Middlesborough Mr R Souter Milton Keynes General Hospital Milton Keynes Mr C Grifﬁth Mr A Grifﬁths Newcastle Hospitals NHS Trust Royal Victoria Inﬁrmary Newcastle upon Tyne Mr C Gaffney Glan Hafren NHS Trust Royal Gwent Hospital Newport Mr M Stokes Daisy Hill Hospital Newry Mr J Dawson Mr S Powis Northampton General Hospital NHS Trust Northampton Mr I Goulbourne North Tyneside Health Care NHS Trust North Tyneside General Hospital North Shields Dr A Makris Dr E Maher Mount Vernon Watford Hospitals NHS Trust Mount Vernon Hospital Northwood Professor JR Robertson Professor R Blamey Nottingham City Hospital NHS Trust Nottingham Mr R Nangalia Mr H Bishop George Eliot 
Hospital NHS Trust Nuneaton Mr I McIntosh Royal Oldham Hospital Oldham Mr D Pinto Sperrin Lakelands Health Social Care Trust Tyrone County Hospital Omagh Dr B Lavery Dr A Jones Oxford Radcliffe Hospital NHS Trust The Churchill Hospital Oxford Mr C Tyrell Dr S Prance Plymouth Hospital NHS Trust Derriford Hospital Plymouth Dr C Tyrrell Dr P Macleod Plymouth Hospital NHS Trust Derriford Hospital Plymouth Mr C Yiangou Professor P Perry Portsmouth Hospital NHS Trust Portsmouth Mr I McIntosh Birch Hill Hospital Rochdale Dr M Quigley Havering Hospitals NHS Trust Oldchurch Hospital Romford Miss Z Saad Dr E Hoare Salford Royal Hospitals NHS Trust Hope Hospital Salford Professor R Coleman Mr S Kohlhardt Weston Park Hospital Shefﬁeld Mr B Harrison Northern General Hospital Shefﬁeld Dr R Agrawal Royal Shrewsbury NHS Trust Shrewsbury Dr M Galea Ealing Hospital NHS Trust Ealing Hospital Southall Dr G Fraser Stirling Royal Inﬁrmary Stirling Mr P England Stepping Hill Hospital Stockport Mr C Hennessy Mr A Peel North Tees 
General Hospital Stockton on Tees Mr AKR Al Debbagh Trafford General Hospital Trafford Dr S Kelly Dr K Stepp Royal Cornwall Hospitals NHS Trust Treliske Hospital Truro Dr S Kumar Pinderﬁeld Hospital Wakeﬁeld Dr R Grieve Mr T Waterworth Walsgrave Hospitals NHS Trust Walsgrave Mr G Copeland Warrington Hospital NHS Trust Warrington Dr D Jones South Warwickshire General Hospitals NHS Trust Warwick Dr A Robinson Dr C Trask Southend Healthcare NHS Trust Hospital Westcliffe onSea Professor P Barrett Lee Velindre NHS Trust Hospital Whitchurch Dr V Hall Royal Hampshire County Hospital Winchester Eastleigh Healthcare NHS Trust Winchester Mr D Berstock Dr R Errington Wirral Hospital NHS Trust Wirral Dr D Fairlamb Royal Wolverhampton Hospitals NHS Trust New Cross Hospital Wolverhampton Mr A Salman Mr A Johri Worthing Southlands Hospital NHS Trust Worthing Hospital Worthing Dr S Goodman Dr G Sparrow East Somerset NHS Trust Yeovil District Hospital Yeovil Mr S Nicholson York District Hospital York USA SJ Yee MD Arcadia CA 
J Feigert MD Arlington Fairfax Hemat Oncology Arlington VA RO Kerr MD Southwest Regional Cancer Center Austin TX K Tkaczuk MD University of Maryland Cancer Center Baltimore MD M Thant MD Maryland Hematology Oncology Associates Baltimore MD CE Hartz MD Eastern Maine Medical Center Cancer Care of Maine Bangor ME GP Miletello MD Baton Rouge General Regional Cancer Center Baton Rouge LA W Popovic MD Illinois Oncology Ltd Belleville IL DB Myers MD Billings Interhospital Oncology Project Billings MT MR Thomas MD Mid Dakota Clinic Bismarck ND P Radice Lynn Regional Cancer Center West Boca Raton FL MS Rubin MD Florida Cancer Specialists Bonita Springs FL E Levine MD Roswell Park Cancer Institute Buffalo NY CF White MD Lahey Clinic Department of Medical Oncology Burlington MA K Weeman MD Aultman Hospital Clinical Research Cancer Center Canton OH L Schlabach MD University Oncology Associates Chattanooga TN M Vohra MD Creticos Cancer Center Chicago IL P Silverman MD University Hospitals of Cleveland Ireland Cancer 
Center Cleveland OH DL Headley DO Cancer Center of Colorado Springs Colorado Springs CO MF Gonzalez MD Liberty Hematology Oncology Columbia SC LR Laufman MD Community Clinical Oncology Program Columbus OH JL Blum MD PhD Texas Oncology Dallas TX H Shaw MD Duke University Medical Center Durham NC H Puc MD Hematology Oncology Associates of CNY East Syracuse NY JM Rothman MD The Regional Cancer Center Erie PA GY Locker MD Evanston Northwestern Healthcare Northwestern University Feinberg School of Medicine Evanston IL N Robert MD Fairfax Northern Virginia Hematology Oncology Fairfax VA www thelancet com Published online December 8 2004 http image thelancet com extras 04let11120webappendix pdf For personal use Only reproduce with permission from Elsevier Ltd 3 Research Letters GR Justice MD Paciﬁc Coast Hematol Oncol Med Group Inc Fountain Valley CA K Yost MD Grand Rapids Clinical Oncology Program Grand Rapids MI AU Buzdar MD University of Texas MD Anderson Cancer Center Houston TX JK Hon MD Comprehensive Cancer 
Care Institute Huntsville AL USA M Trimble Hematology Oncology Associates Jackson MI S Sanal Florida Oncology Associates Hematology Oncology Association Jacksonville FL AM Grossman MD Knoxville Cancer Center Knoxville TN HP DeGreen MD Lancaster Cancer Center Ltd Lancaster PA USA NV Dimitrov MD Great Lakes Cancer Institute Breslin Cancer Center Lansing MI HJ Allen MD Comprehensive Cancer Center of Nevada Las Vegas NV JD Conroy Jr DO FACP FACOI Central PA Hematology Med Oncol Assoc Lemoyne PA K Pendergrass MD Kansas City Oncology and Hematology Group Lenexa KS JJ Sternberg MD Arkansas Oncology Associates Little Rock AR G Sarna MD Cedars Sinai Comprehensive Cancer Center Los Angeles CA L Bhupalam MD James Graham Brown Cancer Center University of Louisville Louisville KY W Bate MD Mercy Medical Center North Iowa Mercy Cancer Center Mason City IA PV Pickens MD Abington Hematology Oncology Assoc Meadowbrook PA USA M Schwartz MD Mount Sinai Comprehensive Cancer Center Miami Beach FL JP Singson MD St Francis Cancer 
Care Center Milwaukee WI JG Schneider MD Winthrop University Hospital Oncology Hematology Division Mineola NY D Schneider MD Virginia Piper Cancer Institute Minneapolis MN MW Meshad MD Oncology Center Med Group Mobile AL A Greco MD Sarah Cannon Cancer Center Nashville TN MJ Guarino MD Cancer Research Ofﬁce Newark DE KK Boatman MD INTEGRIS Oncology Services Oklahoma City OK PT Silberstein MD Creighton University Hematology Oncology Omaha NE USA AS Kelley MD Ventura County Hematology Oncology Specialists Oxnard CA E Camacho MD Comprehensive Cancer Centers at the Desert Regional Med Ctr Palm 4 Springs CA USA R Hirsch MD South Florida Hematology Oncology Pembroke Pines FL MS Roberts MD Hematology Associates Ltd Phoenix AZ JI Spector MD Berkshire Hematology Oncology PC Pittsﬁeld MA N Tirumali MD Central Interstate Medical Ofﬁce Portland OR USA MA Deutsch MD Raleigh Hematology Oncology Assoc Raleigh NC USA P Bushunow MD Rochester General Hospital Rochester NY T Woodlock MD St Mary s Hospital Unity Health System 
Rochester NY DM Sahasrabudhe MD University of Rochester Cancer Center Rochester NY WR Edwards MD ACT Medical Group Rockford IL I Jaiyesimi DO Cancer Clinical Trials Ofﬁce Royal Oak MI F Kass MD Cancer Center of Santa Barbara Santa Barbara CA W Keiser MD Redwood Regional Medical Group Santa Rosa CA G Burton MD Louisiana State Univ Medical Center Shreveport LA JC Michalak MD Siouxland Hematology Oncology Associates Sioux City IA MS McHale MD North Central Hematology Oncology Sioux Falls SD JR Goodman MD Providence Hospital Southﬁeld MI PD Byeff MD Hematology Oncology Southington CT K Hoelzer MD Regional Cancer Center Springﬁeld IL EP Lester MD Oncology Care Associates St Joseph MI M Woodson MD Hematology Oncology Consultants St Louis MO AP Lyss MD Missouri Baptist Cancer Center St Louis MO F Senecal MD Hematology Oncology Northwest Tacoma WA J Horton MB ChB Mofﬁtt Cancer Center Research Inst Tampa FL J Posada MD Scott White Clinic Temple TX IS Lowenthal MD Northwestern Connecticut Oncology Hematology 
Associates Torrington CT AN Dave MD Cancer Treatment Centers of America at Southwestern Regional Medical Center Tulsa OK G Grana MD Cooper Cancer Institute Voorhees NJ E Gelmann MD Georgetown University Medical Center Vincent T Lombardi Cancer Research Center Washington DC M Vukelich MD West Bend WI K Seetharaman MD Worcester Medical Center Worcester MA www thelancet com Published online December 8 2004 http image thelancet com extras 04let11120webappendix pdf For personal use Only reproduce with permission from Elsevier Ltd 
53	b	Articles Effects of radiotherapy and of differences in the extent of surgery for early breast cancer on local recurrence and 15 year survival an overview of the randomised trials Early Breast Cancer Trialists Collaborative Group EBCTCG Summary Background In early breast cancer variations in local treatment that substantially affect the risk of locoregional recurrence could also affect long term breast cancer mortality To examine this relationship collaborative metaanalyses were undertaken based on individual patient data of the relevant randomised trials that began by 1995 Methods Information was available on 42 000 women in 78 randomised treatment comparisons radiotherapy vs no radiotherapy 23 500 more vs less surgery 9300 more surgery vs radiotherapy 9300 24 types of local treatment comparison were identiﬁed To help relate the effect on local ie locoregional recurrence to that on breast cancer mortality these were grouped according to whether or not the 5 year local recurrence risk exceeded 10 Ͻ10 17 000 
women Ͼ10 25 000 women Lancet 2005 366 2087 2106 Collaborators listed at end of report Correspondence to EBCTCG secretariat Clinical Trial Service Unit CTSU Richard Doll Building Old Road Campus University of Oxford Oxford OX3 7LF UK bc overview ctsu ox ac uk Findings About three quarters of the eventual local recurrence risk occurred during the ﬁrst 5 years In the comparisons that involved little Ͻ10 difference in 5 year local recurrence risk there was little difference in 15 year breast cancer mortality Among the 25 000 women in the comparisons that involved substantial Ͼ10 differences however 5 year local recurrence risks were 7 active versus 26 control absolute reduction 19 and 15 year breast cancer mortality risks were 44 6 versus 49 5 absolute reduction 5 0 SE 0 8 2pϽ0 00001 These 25 000 women included 7300 with breast conserving surgery BCS in trials of radiotherapy generally just to the conserved breast with 5 year local recurrence risks mainly in the conserved breast as most had axillary clearance 
and node negative disease 7 versus 26 reduction 19 and 15 year breast cancer mortality risks 30 5 versus 35 9 reduction 5 4 SE 1 7 2p 0 0002 overall mortality reduction 5 3 SE 1 8 2p 0 005 They also included 8500 with mastectomy axillary clearance and node positive disease in trials of radiotherapy generally to the chest wall and regional lymph nodes with similar absolute gains from radiotherapy 5 year local recurrence risks mainly at these sites 6 versus 23 reduction 17 and 15 year breast cancer mortality risks 54 7 versus 60 1 reduction 5 4 SE 1 3 2p 0 0002 overall mortality reduction 4 4 SE 1 2 2p 0 0009 Radiotherapy produced similar proportional reductions in local recurrence in all women irrespective of age or tumour characteristics and in all major trials of radiotherapy versus not recent or older with or without systemic therapy so large absolute reductions in local recurrence were seen only if the control risk was large To help assess the life threatening side effects of radiotherapy the trials of 
radiotherapy versus not were combined with those of radiotherapy versus more surgery There was at least with some of the older radiotherapy regimens a signiﬁcant excess incidence of contralateral breast cancer rate ratio 1 18 SE 0 06 2p 0 002 and a signiﬁcant excess of non breast cancer mortality in irradiated women rate ratio 1 12 SE 0 04 2p 0 001 Both were slight during the ﬁrst 5 years but continued after year 15 The excess mortality was mainly from heart disease rate ratio 1 27 SE 0 07 2p 0 0001 and lung cancer rate ratio 1 78 SE 0 22 2p 0 0004 Interpretation In these trials avoidance of a local recurrence in the conserved breast after BCS and avoidance of a local recurrence elsewhere eg the chest wall or regional nodes after mastectomy were of comparable relevance to 15 year breast cancer mortality Differences in local treatment that substantially affect local recurrence rates would in the hypothetical absence of any other causes of death avoid about one breast cancer death over the next 15 years for 
every four local recurrences avoided and should reduce 15 year overall mortality Introduction In early breast cancer surgery can remove any disease that has been detected in or around the breast or regional lymph nodes but undetected deposits of disease may remain either locally ie in the residual breast tissue scar area chest wall or regional lymph nodes or at distant sites that could if untreated www thelancet com Vol 366 December 17 24 31 2005 develop into life threatening recurrence Many randomised trials over the past half century have studied the effects of radiotherapy and of the extent of surgery on local disease control and on cause speciﬁc mortality in early breast cancer This report updates previous meta analyses1 4 of the individual patient data from those trials 2087 Articles Post BCS radiotherapy After breast conserving surgery BCS a particularly common site of local recurrence is the conserved breast itself or the axilla if this has not been treated effectively The risk of recurrence in a 
conserved breast can be substantial even in node negative disease that has been conﬁrmed by axillary clearance and it can be greatly reduced by radiotherapy 4 5 Hence the recent National Institutes of Health NIH consensus conference on early breast cancer6 recommended that after BCS there should be radiotherapy to the conserved breast Recent surveys in North America and Europe indicate that this treatment is generally given 7 It is however not always given 8 since later recurrence in a conserved breast can usually be removed by further surgery Breast radiotherapy immediately after BCS could improve long term survival by comparison with a policy of watchful waiting for any local recurrence only if life threatening spread from tumour cells in the conserved breast would otherwise occur after BCS but before any clinically evident local recurrence was detected and treated or if the local disease could then not be controlled adequately Hence radiotherapy is likely to have little effect on early mortality whatever 
effect it might have on long term breast cancer mortality Post mastectomy radiotherapy Even after mastectomy an appreciable risk of local recurrence eg in the chest wall or lymph nodes can remain unless some reliable method of investigation such as axillary clearance has found no evidence of nodal involvement If axillary investigation reveals nodal involvement or if the axilla has not been adequately investigated post mastectomy radiotherapy can produce a substantial absolute reduction in this risk of local recurrence and previous trials9 12 and meta analyses2 4 have shown that although it has little effect on breast cancer mortality during the ﬁrst few years it can produce a moderate but deﬁnite reduction in longer term breast cancer mortality Hence the NIH consensus conference6 recommended radiotherapy after mastectomy for women at high risk of locoregional recurrence eg those with four or more involved lymph nodes was only marginally signiﬁcant in the trials of post BCS radiotherapy although more clearly 
signiﬁcant in those of post mastectomy radiotherapy Moreover in the data then available all cause mortality was not signiﬁcantly reduced by radiotherapy after either BCS or mastectomy More recently a review of just the published results from the post BCS radiotherapy trials found only a marginally signiﬁcant difference in all cause mortality but noted that an updated meta analysis of individual patient data would be more reliable 13 The present review of individual patient data from randomised trials of local treatments involves substantially longer follow up than our previous reviews 3 4 For the post BCS radiotherapy trials in particular many of which started relatively recently and for at least the most recent post mastectomy radiotherapy trials this longer followup should offer a much more reliable assessment of the long term effects on mortality The main results for these two particular comparisons are presented separately before the more general analyses that bring together data from all the local 
treatment comparisons The main aim of this report is to assess quantitatively the relationship between local control and long term breast cancer mortality It deals only semi quantitatively with the effects of some radiotherapy regimens on mortality several years later from other conditions eg heart disease and lung cancer14 16 and does not investigate the extent to which the long term fatal or nonfatal adverse effects of local treatment can be avoided by the substantial changes that have taken place over the past few decades in radiotherapy and surgery techniques 17 19 Methods Every 5 years since 1985 evidence from the randomised trials in early breast cancer has been reviewed centrally in a worldwide collaboration between the individuals now responsible for them as the Early Breast Cancer Trialists Collaborative Group EBCTCG An EBCTCG report published earlier this year20 gave the results up to the year 2000 from the trials that began by 1995 of systemic treatments chemotherapy or hormonal therapy for early 
breast cancer The present report gives the corresponding results from the trials of local treatments various types of surgery or radiotherapy or both using similar methods Long term follow up of mortality Moderate differences in mortality that take many years to emerge can best be assessed by systematic metaanalyses of the data on every individual patient in all relevant randomised trials Even this method of assessment however will yield reliable answers only if large numbers of relevant individuals have been randomised and followed up for many years Our previous reviews of individual patient data included follow up of the surgery trials only to 19903 and follow up of the radiotherapy trials4 only to 1995 In the latter review 4 the effect on long term breast cancer mortality 2088 Treatment comparisons and main outcomes Information was available table 1 from several trials of post BCS radiotherapy mostly to the conserved breast of post mastectomy radiotherapy mostly to the chest wall and locoregional lymph 
nodes after axillary clearance of more surgery versus less surgery in the absence of radiotherapy of more surgery versus less surgery in the presence of radiotherapy and of surgery versus radiotherapy ie more surgery versus less surgery plus additional radiotherapy Only unconfounded trials were considered ie trials in which there was to be no www thelancet com Vol 366 December 17 24 31 2005 Articles Treatments compared Available for analysis Trials RT versus no RT but the same surgery BCS generally with AC then RT versus no RT MastectomyϩAC then RT versus no RT MastectomyϩAS then RT versus no RT Mastectomy alone then RT versus no RT More surgery versus less surgery but the same or no RT IMC removal versus not both with mastectomy and no RT Pectoral muscle removal versus not both with mastectomy mainly CAMS China trial AC versus not in node positive disease both with mastectomy and some RT AC versus not in node negative disease both with mastectomy and no axillary RT MastectomyϩAC versus BCSϩAC neither with 
RT part of NSABP B 06 trial MastectomyϩAC versus BCSϩAC both with RT BCS with more versus less breast surgery neither with AC More surgery active versus less surgery plus RT control Nodal surgery versus RT MastectomyϩAC versus BCSϩRT Guy s Hospital trial Mastectomy versus BCSϩRT both with AC Total Not yet available Deaths Women Trials Women 10 25 4 7 1940 6265 360 3890 7311 9933 647 5597 3 2 0 0 1150 165 0 0 2 4 2 4 1 2 0 793 1347 240 757 660 185 0 1082 4925 266 1154 1432 428 0 0 2 5 0 0 0 3 0 200 552 0 0 0 216 9 1 7 78 2910 509 1675 21 531 4550 630 4125 42 080 1 0 3 19 100 0 540 2923 6 RT radiotherapy AC axillary clearance AS axillary sampling IMC internal mammary chain of lymph nodes Some trials eg NSABP B 06 about 700 mastectomyϩACϩRT vs about 700 mastectomyϩAC vs about 700 BCSϩAC contribute to more than one type of treatment comparison so their control group might be counted more than once in the total Without such double counting the total would be 70 trials available with 19 291 deaths among 38 047 
women 93 of total Numbers of trials known to be unavailable in such studies numbers randomised are by year 2000 and might be uncertain or wholly unavailable in which case they are taken as 100 since such studies might well be small In eight trials of postBCS RT all women were to have AC but in two 85B Scottish and 85D West Midlands only some were to do so In most trials of post BCS RT irradiation was generally just to the breast but in some the irradiated sites included axilla supraclavicular fossa and internal mammary chain AFϩIMC Table 1 Availability of data from unconfounded randomised trials of local therapy that began by 1995 difference between the treatment groups in the use of systemic therapy No speciﬁc studies of the relevance of newer diagnostic techniques such as sentinel lymph node biopsy 21 were available Webtables 1 3 give brief design details of each of the available treatment comparisons in the three main parts of table 1 For all unconfounded randomised trials that began recruitment by 1995 
information was sought for every patient on her initial characteristics allocated treatment and time to various outcomes These outcomes were breast cancer recurrence whether the ﬁrst such recurrence was a distant or an isolated local recurrence ie an ipsilateral locoregional recurrence occurring before any contralateral or other distant recurrence cause speciﬁc and overall mortality and the incidence of second primary cancers before breast cancer recurrence Data management procedures Trial identiﬁcation and data handling procedures were as in the EBCTCG report on systemic therapies 20 except that i more detail was sought of the surgical procedures radiotherapy regimens and deﬁnitions of local recurrence from protocols publications or correspondence see webtables 1 3 ii breast cancer in the contralateral breast was not counted as local recurrence iii more detail was sought by correspondence about the underlying causes of many of the deaths particularly from circulatory disease lung cancer or uncertain causes 
before any recurrence of breast cancer and iv more deﬁnite information was www thelancet com Vol 366 December 17 24 31 2005 sought by correspondence if it was unclear whether the ﬁrst recurrence was just an isolated local recurrence In treatment comparisons where the extent of axillary surgery was identical in both groups classiﬁcation of axillary nodal status as positive or negative was based on pathological information where available and on clinical information where not The few women with unknown nodal status were combined with those with clinically node positive disease In treatment comparisons where the extent of axillary surgery differed between the groups eg axillary surgery vs axillary radiotherapy classiﬁcation of nodal status was based only on clinical information to avoid bias For every randomised treatment comparison local recurrence was deﬁned in the same way for both groups In the trials of radiotherapy versus not this generally included recurrence or a new breast cancer in the residual breast 
tissue scar area chest wall or ipsilateral regional lymph nodes and in the trials involving surgery trial speciﬁc local recurrence deﬁnitions are given in webtables 2 and 3 Where recurrences just in a conserved breast or axilla had not originally been reported to the collaboration information on them was sought and they are now included as local recurrences Statistical analysis All analyses were stratiﬁed by trial by time since randomisation in single years and by nodal status negative or positive The main analyses of local recurrence breast cancer mortality and overall mortality were also stratiﬁed by age in 5 groups Ͻ40 40 49 50 59 2089 Articles Panel Webtables 1 4 and webﬁgures 1 10 on the Lancet website Webtables 1 3 provide brief details of every available trial including the anatomic sites treated surgically and the radiotherapy doses and sites irradiated and webtable 4 shows how the statistics for breast cancer mortality are derived by logrank subtraction ie subtraction of the logrank statistics for 
mortality from causes other than breast cancer from the logrank statistics for any death The 15 year time to event graphs in webﬁgures 1 3 provide more detail for some of the main metaanalyses including the logrank statistics for local recurrence breast cancer mortality and any death during years 0 4 5 9 10 14 and у15 webﬁgures 4 and 5 relate the effect on local recurrence to the proportional effect on breast cancer mortality and webﬁgure 6 gives various subgroup analyses Webﬁgure 7 radiotherapy side effects gives 15 year time to event graphs for the incidence of contralateral breast cancer and for mortality from causes other than breast cancer Finally the forest plots in webﬁgures 8 10 give summary results for every separate trial separating women with node negative and node positive disease for local recurrence breast cancer mortality and any death This report and the webtables and webﬁgures are also available on the EBCTCG website www ctsu ox ac uk projects ebctcg along with Powerpoint images of some of 
them 60 69 у70 years at randomisation Only two groups Ͻ50 and у50 years were used however for analyses that were further subdivided by tumour characteristics grade size oestrogen receptor ER status or actual number of involved nodes Other aspects of the statistical methods and the formats of the ﬁgures are as before 20 unless otherwise indicated and are described on the EBCTCG website see panel In early breast cancer most local recurrences become apparent within the ﬁrst few years but much of the distant recurrence and breast cancer mortality occurs later 4 The main analyses involve 5 year local recurrence risks and 15year breast cancer mortality risks Both are generally illustrated by 15 year graphs for comparability with the EBCTCG report20 on systemic therapies but the logrank observed minus expected O E values that yield the signiﬁcance tests associated with such graphs are based on events throughout the entire period of follow up both during and after the ﬁrst 15 years unless otherwise indicated For the 
major treatment comparisons results for overall mortality any death are also given mainly on the website Collaborative review Preliminary meta analyses of the trials of local treatments had been presented and discussed at a meeting of collaborators in September 2000 after which much additional detail was sought about methods and outcomes in these trials and restructured corrected meta analyses emerged in 2004 A draft of the present 2090 report was circulated for comment by the collaborating trialists in June 2005 was presented and discussed at a further meeting of collaborators in September 2005 and was available for further comment in October 2005 It was revised substantially in the light of these comments and recirculated when submitted for publication in November 2005 and during the editorial process page proofs were posted on the passwordprotected EBCTCG website Role of the funding sources This collaboration is funded from the general long term ﬁnancial support of the CTSU by organisations that had no 
role in study design data collection data analysis data interpretation or writing of the report The EBCTCG secretariat had full access to all the data and analyses and after consultation with the collaborators had ﬁnal responsibility for the decision to submit for publication Results Table 1 shows the numbers of trials and the numbers of randomised women who contributed to various local treatment comparisons The two most extensively studied aspects of local treatment are radiotherapy after BCS 7311 women in 10 trials and radiotherapy after mastectomy and axillary clearance 9933 women in 25 trials The results subdivided by nodal status thereby making four separate treatment comparisons for these two particular sets of trials are presented ﬁrst Then information from all the treatment comparisons in table 1 again subdivided by nodal status making a total of 24 comparisons is used to relate the magnitude of the effect on local recurrence to that on breast cancer mortality Finally the effects of the radiotherapy 
regimens in these trials on the incidence of second cancers and on mortality from diseases other than breast cancer are presented Radiotherapy after BCS Figure 1 gives for the ten trials of post BCS radiotherapy logrank analyses of the effects on local recurrence upper part of ﬁgure and on breast cancer mortality lower part Separate subtotals are given a for trials in which the conserved breast was the only site irradiated sometimes with an additional boost to the scar and b for those where other sites were also irradiated such as the axilla and supraclavicular fossa One of the ten trials contributed to both subtotals so there are 11 strata in ﬁgure 1 The reduction in local recurrence mainly in the conserved breast produced by allocation to radiotherapy is substantial and highly signiﬁcant pϽ0 00001 in every separate trial There is no signiﬁcant heterogeneity between the proportional reductions in local recurrence in the 11 different strata in ﬁgure 1 or in the two subtotals The recurrence rate ratio 
comparing those allocated radiotherapy with those not is about 0 3 in every trial corresponding to a proportional reduction of 70 Considering all ten trials together the 5 year risk of local www thelancet com Vol 366 December 17 24 31 2005 Articles Figure 1 Effect of radiotherapy RT after BCS ten trials on local recurrence and on breast cancer mortality event rate ratios O E observed expected BW breast chest wall S scar as site of RT boost AF axilla fossa IMC internal mammary chain Sites in parentheses not always treated Isolated local recurrence events woman years Events woman years Year started and study name RT sites Allocated BCSϩRT Allocated BCS BCSϩRT events Logrank O E Variance of O E Ratio of annual event rates BCSϩRT BCS a Radiotherapy only to conserved breast 14 node positive 1976 1981 1982 1984 1987 1989 1991 NSABP B 06 Uppsala Örebro St George s London Ontario COG INT Milan 3 NSABP B 21 Swedish BCCG BW BW BW BWϩS BWϩS BWϩS BW 125 6862 10 1636 12 1202 53 3543 19 2478 6 1810 33 3718 285 4991 43 
1511 31 1047 155 2754 60 2005 40 1729 92 3429 258 21 249 7 2 706 17 466 25 6 a Subtotal 5 year risk Ϫ93 3 Ϫ17 7 Ϫ11 5 Ϫ58 2 Ϫ25 1 Ϫ17 3 Ϫ30 8 84 8 12 7 9 6 48 2 18 2 11 2 30 5 Ϫ254 0 215 3 Some systemic adjuvant therapy same polychemotherapy and or tamoxifen in both groups 0 31 SE 0 04 2pϽ0 00001 99 CIs are given for trialspeciﬁc results black squares and 95 CIs are given for subtotals and totals white diamonds b Radiotherapy to conserved breast and other sites 24 node positive 1982 1985 1985 1986 St George s London Scottish West Midlands UK CRC UK 14 620 16 2598 42 2398 33 1604 30 380 83 2260 104 1929 77 1454 Ϫ10 9 Ϫ33 0 Ϫ36 8 Ϫ24 3 9 7 22 5 34 2 25 7 105 7220 7 7 294 6023 26 7 Ϫ105 0 92 1 0 32 SE 0 06 2pϽ0 00001 363 28 469 7 3 BWϩAF BWϩSϩ AF ϩIMC BWϩSϩAFϩIMC Various 1000 23 489 25 9 Ϫ359 0 307 4 0 31 SE 0 03 2pϽ0 00001 b Subtotal 5 year risk Total aϩb 5 year risk Heterogeneity between 11 strata 2 7 8 p 0 6 10 0 0 5 BCS RT better 1 0 1 5 BCS RT worse 2 0 Breast cancer mortality deaths women Deaths women 
Year started and study name RT sites Allocated BCSϩRT Allocated BCS BCSϩRT deaths Logrank O E Ratio of annual death rates BCSϩRT BCS Variance of O E a Radiotherapy only to conserved breast 14 node positive 1976 NSABP B 06 BW 267 731 305 719 Ϫ19 7 135 0 1981 1982 1984 1987 1989 1991 BW BW BW S BW S BWϩS BW 37 184 24 128 91 416 40 294 8 337 32 593 34 197 25 122 123 421 51 273 8 336 41 594 2 3 Ϫ2 5 Ϫ16 4 Ϫ6 2 0 5 Ϫ3 9 16 8 10 9 51 5 21 3 3 9 18 0 499 2683 28 0 587 2662 33 2 Ϫ45 8 257 4 28 70 78 296 107 349 89 261 Ϫ2 1 Ϫ5 0 Ϫ11 4 Ϫ8 3 12 2 30 2 45 3 37 6 254 990 302 976 Ϫ26 9 125 3 0 81 SE 0 08 2p 0 02 28 2 35 1 753 3673 30 5 889 3638 35 9 Ϫ72 7 382 7 0 83 SE 0 05 2p 0 0002 Uppsala Örebro St George s London Ontario COG INT Milan 3 NSABP B 21 Swedish BCCG a Subtotal 15 year risk 0 84 SE 0 06 2p 0 004 b Radiotherapy to conserved breast and other sites 24 node positive 1982 1985 1985 1986 St George s London Scottish West Midlands UK CRC UK b Subtotal 10 year risk Total aϩb 15 year risk BWϩAF 31 80 BWϩSϩ AF ϩIMC 59 
293 BWϩSϩAFϩIMC 88 358 Various 76 259 Heterogeneity between 11 strata 2 3 8 p 0 96 10 www thelancet com Vol 366 December 17 24 31 2005 0 0 5 BCS RT better 1 0 1 5 BCS RT worse 2 0 2091 Articles 6097 women with BCS and node negative disease 5 year gain 16 1 SE 1 0 60 50 Breast cancer mortality 50 Isolated local recurrence 15 year gain 5 1 SE 1 9 Logrank 2p 0 006 60 40 29 2 30 BCS 22 9 20 40 31 2 BCS 26 1 BCSϩRT 30 20 3 20 17 4 BCSϩRT 10 8 9 10 10 0 8 0 6 7 0 0 0 5 10 15 0 5 Time years 10 15 Time years 1214 women with BCS and node positive disease 5 year gain 30 1 SE 2 8 60 15 year gain 7 1 SE 3 6 Logrank 2p 0 01 60 55 0 BCS 46 5 50 45 2 47 9 BCSϩRT Breast cancer mortality Isolated local recurrence 50 BCS 41 1 40 30 20 40 36 5 30 24 3 20 20 9 BCSϩRT 10 11 0 13 1 10 0 0 0 5 10 15 Time years 0 5 10 15 Time years Figure 2 Effect of radiotherapy RT after BCS on local recurrence and on breast cancer mortality 15 year probabilities Data from 10 trials Vertical lines indicate 1 SE above or below the 5 10 and 15 year 
percentages recurrence is 7 among those allocated radiotherapy and 26 among those not corresponding to an absolute reduction of 19 in this 5 year risk The proportional risk reduction for breast cancer mortality is much less extreme than that for local recurrence and none of the trial speciﬁc breast cancer mortality results is clearly signiﬁcant on its own as each of the 99 CIs overlaps unity The total result at the bottom of ﬁgure 1 is however highly signiﬁcant breast cancer death rate ratio 0 83 SE 0 05 95 CI 0 75 0 91 2p 0 0002 indicating a reduction of about one sixth in the annual breast cancer mortality rate The 15 year risk of 2092 death from breast cancer in the hypothetical absence of other causes is 30 5 among those allocated post BCS radiotherapy and 35 9 among those not corresponding to an absolute reduction of 5 4 SE 1 7 The similarity of the subtotals a and b in the upper part of ﬁgure 1 is because all of the effect in a and much of that in b is from irradiating the conserved breast and the 
clear reduction in breast cancer mortality given in the total aϩb at the foot of ﬁgure 1 shows the effectiveness of breast irradiation in these patients The total results in ﬁgure 1 for local recurrence and for breast cancer mortality are plotted in ﬁgure 2 by year www thelancet com Vol 366 December 17 24 31 2005 Articles since randomisation separating node negative and node positive disease The 5 year risk of local recurrence is substantially bigger in node positive disease as is the absolute reduction in this recurrence risk ie the 5 year gain ﬁgure 2 The absolute reduction in breast cancer mortality also appears somewhat larger for women with node positive disease but the numbers are too small for this ﬁnding to be statistically reliable Radiotherapy after mastectomy and axillary clearance Figure 3 gives the corresponding results for women with axillary clearance in the trials of post mastectomy radiotherapy In the majority of these trials radiotherapy was given to the chest wall and to the lymph nodes in 
the axilla supraclavicular fossa and internal mammary chain webtable 1 webﬁgure 8 For women with node negative disease the 5 year local recurrence risk after mastectomy and axillary clearance was only 6 even in the absence of radiotherapy Although radiotherapy reduces it to 2 2p 0 0002 the absolute 5 year gain is only 4 and there is no signiﬁcant reduction in 15 year breast cancer mortality indeed there appears if anything to be a slight increase but the numbers of events are small By contrast for women with node positive disease the 5 year local recurrence risk after mastectomy and axillary 1428 women with mastectomy with AC and node negative disease 5 year gain 4 0 SE 1 1 60 50 Breast cancer mortality 50 Isolated local recurrence 15 year loss 3 6 SE 2 6 Logrank 2p 0 01 excluding data beyond year 15 logrank 2p 0 18 60 40 30 20 40 31 3 MastectomyϩACϩRT 30 20 20 8 12 5 6 3 8 0 2 3 10 3 1 5 10 10 MastectomyϩAC 27 7 MastectomyϩAC 22 3 11 3 MastectomyϩACϩRT 0 0 0 15 0 5 Time years 10 15 Time years 8505 women 
with mastectomy with AC and node positive disease 5 year gain 17 1 SE 0 9 60 15 year gain 5 4 SE 1 3 Logrank 2p 0 0002 60 60 1 MastectomyϩAC 54 7 MastectomyϩACϩRT 50 9 50 50 27 6 30 29 2 MastectomyϩAC 22 8 20 10 7 8 MastectomyϩACϩRT 7 5 5 8 Breast cancer mortality Isolated local recurrence 46 7 40 40 34 0 32 1 30 20 10 0 0 0 5 10 Time years 15 0 5 10 15 Time years Figure 3 Effect of radiotherapy RT after mastectomy and axillary clearance AC on local recurrence and on breast cancer mortality 15 year probabilities Data from 25 trials Vertical lines indicate 1 SE above or below the 5 10 and 15 year percentages www thelancet com Vol 366 December 17 24 31 2005 2093 Articles Type of local treatment comparison Isolated local recurrences events cumulative risk by year 5 after randomisation Events by year 5 women randomised Active Control 5 year risk actuarial Active Control Absolute reduction in 5 year risk control active Reduction SE Reduction 99 CI RT versus no RT but same surgery BCS then RT versus no RT 10 
trials Node negative 216 3071 Node positive 66 602 6 7 22 9 16 1 1 0 221 612 637 3026 11 0 41 1 30 1 2 8 MastectomyϩAC then RT versus no RT 25 trials Node negative 13 662 41 691 2 3 6 3 4 0 1 1 Node positive 214 4170 778 4170 5 8 22 8 17 1 0 9 MastectomyϩAS then RT versus no RT 4 trials Node negative 13 225 52 224 6 1 24 5 18 5 3 5 Node positive 11 95 43 103 13 8 50 1 36 3 7 5 Mastectomy alone then RT versus no RT 7 trials Node negative 70 1427 307 1477 Node positive 88 837 243 836 5 6 23 3 17 6 1 4 11 6 33 5 21 9 2 3 More surgery versus less surgery but the same or no RT IMC removal versus not neither with RT 2 trials Node negative 11 243 9 251 4 7 4 0 Ϫ0 7 1 9 Node positive 42 286 50 302 19 1 21 3 2 2 4 0 Pectoral muscle removal versus not both with same RT or no RT 4 trials Node negative 1 49 2 56 2 2 4 1 1 8 3 5 Node positive 59 330 60 309 22 2 22 9 0 8 3 8 7 5 13 5 6 1 4 6 11 9 23 0 11 1 2 5 AC versus not in node positive disease both with some RT 2 trials Node positive 7 129 13 137 AC versus not in 
node negative disease neither with axillary RT 4 trials Node negative 51 572 119 582 MastectomyϩAC versus BCSϩAC neither with RT part of NSABP B 06 Node negative 46 432 149 432 10 9 36 5 25 6 3 3 Node positive 46 281 128 287 18 9 52 1 33 1 5 0 MastectomyϩAC versus BCSϩAC both with RT 2 trials Node negative 2 59 4 60 5 7 5 2 Ϫ0 5 4 6 Node positive 5 153 10 156 4 2 8 0 3 9 3 1 More surgery active versus less surgery plus RT control Nodal surgery versus RT 9 trials Node negative 123 1343 113 1329 10 8 9 6 Ϫ1 2 1 3 Node positive 221 943 170 935 27 6 21 8 Ϫ5 8 2 3 MastectomyϩAC versus BCS aloneϩRT Guy s Hospital Node negative 15 241 52 233 Node positive 11 85 22 71 6 4 25 3 18 9 3 7 15 8 35 5 19 6 8 8 Mastectomy versus BCSϩRT both with AC 7 trials Node negative 71 1432 115 1438 5 3 8 6 3 3 1 0 Node positive 40 610 26 645 7 9 4 7 Ϫ3 1 1 5 30 20 Active better 10 0 Figure 4 Absolute reduction in 5 year local recurrence risk 78 randomised comparisons grouped into 24 types of local treatment comparison based on 
treatments compared and nodal status RT radiotherapy AC axillary clearance AS axillary sampling IMC internal mammary chain of lymph nodes A few trials did not provide data on local recurrence so in some comparisons numbers differ from table 1 clearance is 23 in the absence of radiotherapy which is substantial and radiotherapy reduces it to 6 Therefore although the proportional reduction in the local recurrence rate produced by radiotherapy is similar in node positive disease and in node negative disease the absolute 5 year gain is much larger 17 In node2094 positive disease the 15 year breast cancer mortality with and without post mastectomy radiotherapy is 54 7 versus 60 1 an absolute reduction of 5 4 SE 1 3 2p 0 0002 This analysis of the effects of post mastectomy radiotherapy in node positive disease is limited to the www thelancet com Vol 366 December 17 24 31 2005 Articles 8500 women who had had axillary clearance Its ﬁndings for local recurrence and for breast cancer mortality would not have been 
materially altered however by inclusion of the additional 2500 women who had had only axillary sampling or no axillary surgery webﬁgure 8b In every large trial of post mastectomy radiotherapy in women with node positive disease there was a similar proportional reduction in local recurrence showing that the radiotherapy regimens used in all the main trials recent or older were of comparable efﬁcacy in achieving local control webﬁgure 8b Hence when assessing the relevance of local control to long term breast cancer mortality it is appropriate to consider the evidence from both recent and older trials Comparison of post BCS and post mastectomy radiotherapy trials In the post BCS radiotherapy trials the site of local recurrence was generally available When it was over 90 578 of 636 of the local recurrences among controls involved the conserved breast as did over 90 of the effect of radiotherapy on local recurrence In the postmastectomy radiotherapy trials the site of local recurrence was not generally available 
However little breast tissue remains after mastectomy so the main effect of radiotherapy on local recurrence in these postmastectomy trials must involve other sites such as the chest wall or regional lymph nodes Coincidentally the 5 year risks of local recurrence without radiotherapy and the reduction in those risks produced by radiotherapy were similar among women with node negative disease in the post BCS trials and among women with node positive disease in the postmastectomy trials ﬁgure 2 upper panels and ﬁgure 3 lower panels The control 15 year breast cancer mortality was of course lower among women in the post BCS trials about 80 of whom had small tumours greatest dimension р20 mm and node negative disease than among women in the post mastectomy trials with node positive disease For both however it was substantial and for both the absolute reduction in breast cancer mortality with radiotherapy was about 5 The apparent similarity of the absolute reductions in 15 year breast cancer mortality in these two 
types of radiotherapy trial after similar absolute reductions in 5 year local recurrence risk suggests that the effect on long term survival of avoiding a recurrence in a conserved breast is approximately comparable with that of avoiding a recurrence at other locoregional sites Three categories of local treatment comparison To examine the general relationship between the effects of local treatment differences on local recurrence and their effects on breast cancer mortality all the treatment comparisons listed in table 1 were subdivided by nodal status making a total of 24 such comparisons These were then grouped arbitrarily into three categories according to www thelancet com Vol 366 December 17 24 31 2005 the absolute reduction Ͻ10 10 20 or Ͼ20 in the 5 year local recurrence risk The 24 white squares and their 99 CIs in ﬁgure 4 display these absolute reductions in risk The length of the side of each white square is inversely proportional to the standard error of the absolute reduction The vertical broken 
lines correspond to absolute reductions of 10 and 20 in risk and have been used as arbitrary cut points to group these 24 types of comparison into three categories according to the absolute reduction in this risk These categories involve respectively 17 000 20 000 and 5000 women with mean absolute reductions of 1 17 and 26 in the 5 year local recurrence risk Most of the substantial absolute reductions in local recurrence risk involved the addition of radiotherapy The others involved conservation of the breast or axilla or both without effective radiotherapy to the conserved tissue Furthermore almost all the comparisons of radiotherapy versus no radiotherapy involved substantial absolute reductions in local recurrence the one exception was that after mastectomy and axillary clearance in women with pathologically node negative disease the risk of local recurrence without radiotherapy was so low that no large absolute reduction was possible ﬁgures 3 and 4 In the lower part of ﬁgure 4 the four earliest trials 
those starting during 1951 1970 webﬁgure 10 had high local recurrence risks despite radiotherapy Omission of these early trials from subsequent analyses would make no material difference to the main conclusions Local control and long term breast cancer mortality The absolute reductions in breast cancer mortality that correspond to the three categories of local treatment comparison are shown in table 2 The differences in breast cancer mortality are greater at 15 years than at 5 years and the 15 year differences in breast cancer mortality in the three categories are approximately proportional to the differences in 5 year local recurrence risk The regression line through zero relating the absolute effects on local recurrence to those on breast cancer mortality suggests that a local treatment difference that reduces the 5 year local recurrence risk by 20 would reduce the 15 year breast cancer mortality by 5 2 SE 0 8 2pϽ0 00001 Breast cancer mortality 5 year risk active vs control 5 year absolute reduction SE 15 
year risk active vs control 15 year absolute reduction SE 18 8 vs 19 5 21 8 vs 23 3 24 9 vs 26 7 0 6 0 6 1 5 0 6 1 8 1 3 41 3 vs 42 3 44 0 vs 48 5 47 4 vs 53 4 1 0 0 9 4 5 0 8 6 0 1 6 Subtotal bϩc mean 19 22 4 vs 24 0 1 6 0 6 44 6 vs 49 5 5 0 0 8 a Ͻ10 mean 1 b 10 20 mean 17 c Ͼ20 mean 26 Weighted regression line through zero relating mortality reduction to recurrence reduction 5 2 SE 0 8 absolute reduction in 15 year breast cancer mortality for 20 absolute reduction in 5 year local recurrence risk Table 2 Breast cancer mortality risks by time since randomisation and by category of absolute reduction in 5 year local recurrence risk from ﬁgure 4 2095 Articles 12 comparisons with Ͻ10 local recurrence risk 16 804 women 43 with node positive disease 5 year gain 0 3 SE 0 6 60 50 50 40 30 20 11 1 10 15 3 Control 13 6 Active 14 0 12 9 10 8 Breast cancer mortality Isolated local recurrence 15 year gain 1 0 SE 0 9 Logrank 2p Ͼ0 1 60 5 10 40 33 6 32 8 30 19 5 20 18 8 10 0 0 42 3 Control 41 3 Active 0 15 0 5 Time years 
10 15 Time years 12 comparisons with Ͼ10 local recurrence risk 25 276 women 51 with node positive disease 5 year gain 18 7 SE 0 5 60 50 49 5 Control 50 40 33 2 Control 31 4 30 26 1 20 11 5 Active 10 10 3 7 4 0 Breast cancer mortality Isolated local recurrence 15 year gain 5 0 SE 0 8 Logrank 2pϽ0 00001 60 44 6 Active 39 6 40 35 9 30 24 0 20 22 4 10 0 0 5 10 15 Time years 0 5 10 15 Time years Figure 5 Local recurrence and breast cancer mortality for treatment comparisons that produce a less than 10 upper panels or more than 10 lower panels absolute reduction in 5 year local recurrence risk 15 year probabilities Vertical lines indicate 1 SE above or below the 5 10 and 15 year percentages A quantitatively similar conclusion can be obtained by combining the second and third categories b and c in table 2 and analysing the resulting total of 25 000 women Among them treatment reduced the 5 year local recurrence risk by a mean of 19 and reduced the 15 year breast cancer mortality by 5 0 SE 0 8 2pϽ0 00001 The ﬁndings 
for these 25 000 women are plotted against time since randomisation in ﬁgure 5 lower panels The effect on local recurrence is substantial and is seen rapidly indeed much of it is apparent within the ﬁrst two or three years In contrast 2096 there is no apparent effect on breast cancer mortality within the ﬁrst two or three years although there is a moderate but deﬁnite effect on 15 year breast cancer mortality Most of these 25 000 women were in trials of radiotherapy and half had node negative disease so the results for them are intermediate between those for postBCS radiotherapy in node negative disease ﬁgure 2 upper panels and post mastectomy radiotherapy in node positive disease ﬁgure 3 lower panels Further details of these comparisons are given on the website webtable 4 webﬁgures 3 6 For the treatment www thelancet com Vol 366 December 17 24 31 2005 Articles comparisons involving more than a 10 reduction in local recurrence risk logrank analyses by period of follow up provide formal conﬁrmation that the 
main reduction in local recurrence occurs during just the ﬁrst few years By contrast for breast cancer mortality there is no material effect during years 0 2 Subsequently however there are highly signiﬁcant reductions in breast cancer mortality 2pϽ0 00001 during each of the time periods 3 4 years and 5 9 years and 2p 0 0003 during the time period 10 14 years after randomisation After year 15 however there is no evidence of any further gain or loss of the earlier gain in breast cancer mortality webﬁgure 6c Among those of the 25 000 women who survived to year 15 the ratio treatment versus control of the annual breast cancer mortality rates in subsequent years was 1 03 SE 0 08 Tests of heterogeneity For each of the three categories of treatment comparison in table 2 webﬁgure 4 shows the breast cancer mortality ratios treatment versus control separately during the ﬁrst 5 years after randomisation and in later years giving a total of six mortality ratios For none of these six mortality ratios is there any 
signiﬁcant heterogeneity between the contributions to it from different types of treatment comparison webﬁgure 5 Moreover the sum 2 of the six heterogeneity test statistics 42 41 2 p 0 5 provides no signiﬁcant evidence of heterogeneity between the proportional effects on breast cancer mortality of local treatments that have similar absolute effects on local recurrence risks Such overall tests of heterogeneity with many degrees of freedom are however not very sensitive to any real heterogeneity that might exist A more relevant observation is that in 3 quite different circumstances the avoidance of local recurrence mainly during the ﬁrst 5 years appeared to be of comparable relevance to breast cancer mortality mainly after the ﬁrst 5 years i in the trials of post BCS radiotherapy ii in those of post mastectomy radiotherapy and iii in the aggregated results from the trials of breast conservation or axillary conservation without effective radiotherapy to the conserved tissue total logrank O E 28 9 15 7ϩ5 1ϩ8 1 
with variance 145 2 breast cancer mortality ratio 0 82 SE 0 08 2p 0 02 webﬁgure 5 Subgroup analyses Analyses of selected treatment comparisons in subgroups of age and of tumour characteristics grade size ER status and amount of nodal involvement where available are given in webﬁgure 6 Any apparent differences or similarities between the subgroup speciﬁc treatment effects are likely to be much more trustworthy for local recurrence than for breast cancer mortality because differences in local treatment can have such large effects on local recurrence rates For women with node negative disease in the trials of radiotherapy after BCS webﬁgure 6a and for women with node positive www thelancet com Vol 366 December 17 24 31 2005 5 year local recurrence risk in trials of a RT after BCS node negative b RT after mastectomy and AC node positive RT versus control Absolute reduction SE RT versus control Age years Ͻ50 50 59 60 69 у70 11 vs 33 7 vs 23 4 vs 16 3 vs 13 22 2 16 2 12 1 11 2 6 vs 23 6 vs 24 5 vs 23 17 1 18 2 18 
2 Tumour grade Well differentiated Moderately differentiated Poorly differentiated 4 vs 14 9 vs 26 12 vs 34 10 2 17 2 22 3 4 vs 22 4 vs 30 6 vs 40 18 3 26 2 34 4 Tumour size T category 1 20 mm T1 21 50 mm T2 Ͼ50 mm T3 or T4 5 vs 20 14 vs 35 15 1 21 3 5 vs 22 6 vs 30 8 vs 36 17 2 24 2 28 4 ER status ER poor ER positive 12 vs 30 6 vs 25 18 3 19 2 8 vs 28 6 vs 24 20 2 18 2 7 vs 23 16 1 4 vs 16 12 vs 26 6 vs 23 12 2 14 2 17 1 Number of involved nodes 1 3 у4 All women Absolute reduction SE See webﬁgures 6a and 6b for more details on characteristics including separate results for those in whom the relevant characteristic is not known T4 tumour of any size with direct extension to skin or chest wall Table 3 Effects of age and tumour characteristics on 5 year risks of local recurrence in trials of radiotherapy RT a after BCS in women with node negative disease and b after mastectomy and axillary clearance AC in women with node positive disease disease in the trials of radiotherapy after mastectomy and axillary 
clearance webﬁgure 6b radiotherapy produced similar proportional reductions in local recurrence risk irrespective of age tumour grade tumour size ER status or amount of nodal involvement Consequently within each subgroup the absolute beneﬁt produced by radiotherapy was determined principally by the magnitude of the local recurrence risk in unirradiated women Age Table 3 gives 5 year local recurrence risks for various subgroups in the trials of radiotherapy after BCS generally with axillary clearance in node negative disease and in the trials of radiotherapy after mastectomy and axillary clearance in node positive disease In the former most local recurrences are in the conserved breast and the 5 year risk of such recurrence in the breast is known to be about twice as great in younger as in older women 22 25 Hence the absolute effects of post BCS radiotherapy on local recurrence mainly in the conserved breast were greater in younger than in older women 5 year risk reductions of 22 16 12 and 11 for those aged 
Ͻ50 50 59 60 69 and 70 years respectively test for trend in absolute beneﬁts 2p 0 00002 By contrast there was no trend with age in the 5 year risks of local recurrence mainly in the chest wall or lymph nodes among women with mastectomy axillary clearance and 2097 Articles Site of cancer or cause of death and 3 digit ICD 9 code s Incidence of contralateral breast cancer By years since randomisation and for cases mean year of randomisation 0 4 1980 5 14 1980 у15 1975 By age at randomisation Ͻ50 years у50 years By use of systemic therapy With chemotherapy or tamoxifen Without chemotherapy or tamoxifen Total contralateral breast cancer Incidence of other speciﬁed cancers Lung cancer 162 Oesophagus cancer 150 Leukaemia 204 208 Soft tissue sarcoma 158 171 Thyroid cancer Bone cancer Other speciﬁed malignant disease Total other speciﬁed cancers Mortality before recurrence from causes other than breast cancer By cause Circulatory disease Heart disease etc Stroke Pulmonary embolism Other speciﬁed cause Lung cancer 
Oesophagus cancer Leukaemia Soft tissue sarcoma Respiratory disease 460 519 786 Other known cause Unspeciﬁed cause not breast cancer By years since randomisation and for deaths mean year of randomisation 0 4 1976 5 14 1975 у15 1970 By age at randomisation Ͻ50 years у50 years Total non breast cancer deaths Events Logrank O E Variance of O E Ratio of rates 2p 673 627 151 1 3 53 5 2 1 161 1 150 2 33 4 1 01 0 08 1 43 0 10 1 06 0 18 0 9 0 00001 0 7 600 851 11 7 45 1 143 0 201 3 1 09 0 09 1 25 0 08 0 3 0 002 649 802 1451 21 7 35 1 56 9 158 0 186 4 344 4 1 15 0 09 1 21 0 08 1 18 0 06 0 08 0 01 0 002 215 31 59 26 26 28 966 1351 24 3 5 4 7 5 5 4 Ϫ2 3 1 7 16 4 58 4 51 1 7 5 13 9 6 4 6 2 6 9 220 7 312 7 1 61 0 18 2 06 0 53 1 71 0 36 2 34 0 62 0 69 0 34 1 28 0 43 1 08 0 07 1 20 0 06 0 0007 0 05 0 04 0 03 0 4 0 5 0 3 0 001 1510 1106 345 59 1455 156 23 31 7 241 997 701 77 6 60 7 9 1 7 8 6 4 21 7 4 9 2 4 1 3 Ϫ1 0 Ϫ22 9 7 8 345 4 252 7 80 9 11 8 335 8 37 5 5 6 7 0 1 7 55 5 228 5 159 4 1 25 0 06 1 27 0 07 1 12 0 12 1 94 0 41 
1 02 0 06 1 78 0 22 2 40 0 68 1 40 0 45 2 13 1 14 0 98 0 13 0 90 0 06 1 05 0 08 0 00003 0 0001 0 3 0 02 0 7 0 0004 0 04 0 4 0 3 0 9 0 1 0 5 756 1513 1397 7 4 37 7 46 9 176 4 348 4 304 8 1 04 0 08 1 11 0 06 1 17 0 06 0 6 0 05 0 01 554 3112 3666 27 4 64 4 91 8 129 6 699 8 829 4 1 24 0 10 1 10 0 04 1 12 0 04 0 02 0 02 0 001 O E observed expected Approximate excess number of events in radiotherapy group is 2 O E Ratio of annual event rates SE irradiated versus unirradiated estimated from O E and its variance V as exp O E V 20 Primary cancers of all speciﬁed sites 140 194 200 208 except non melanoma skin 173 and breast Includes radiotherapy versus not 3 versus 2 thyroid cancer 193 1 versus 0 bone cancer 170 All circulatory 390 459 785 798 except stroke 430 438 and pulmonary embolism 415 451 453 673 Analyses in table and in corresponding webﬁgure 7 stratiﬁed by only two groups of age had they been stratiﬁed by ﬁve age groups as in main analyses and the node negative patients in the 80Y Edinburgh trial 
appropriately removed see footnotes added in proof to webtables 2 and 3 the mortality results would have changed only very slightly eg for total non breastcancer deaths the logrank O E would have been 93 4 with variance 789 2 rate ratio 1 126 SE 0 04 2p 0 0009 Table 4 Effect of radiotherapy on incidence of second cancers before recurrence of breast cancer and on mortality from causes other than breast cancer 23 500 women in 46 trials of adding radiotherapy and 9300 in 17 trials of radiotherapy vs more surgery node positive disease Hence the absolute effects of postmastectomy radiotherapy on the risk of such local recurrence were also approximately independent of age local recurrence reductions of 17 18 and 18 for women aged Ͻ50 50 59 and 60 69 years respectively there were few older women in these trials Tumour characteristics In both types of trial the 5 year local recurrence risk without radiotherapy was higher and the absolute reduction in this risk from radiotherapy was correspondingly greater in women 
with tumours that 2098 were large or with direct extension to the skin or chest wall T2 T3 T4 tumours or poorly differentiated but there was little relevance of ER status to these risks For women with mastectomy axillary clearance and nodepositive disease the number of involved nodes 1 3 or у4 was unavailable for more than half the women webﬁgure 6b Where it was available the 5 year local recurrence risks irradiated versus control were 4 versus 16 for women with one to three involved nodes reduction 12 SE 2 and 12 versus 26 for women with four or more involved nodes reduction 14 SE 2 table 3 The 15 year local recurrence reductions differed www thelancet com Vol 366 December 17 24 31 2005 Articles Although in the present analyses subgroup speciﬁc results derived for local recurrence might well be fairly reliable as the effects of local treatment on local recurrence can be so extreme subgroup speciﬁc results for breast cancer mortality might well not be Hence unduly selective emphasis on particularly 
favourable or unfavourable mortality results from particular subgroups or particular trials or even from particular types of treatment comparison could give rise to misleading over estimation or under estimation of the real relevance of local disease control to long term breast cancer mortality Instead the most reliable estimate of the effect on breast cancer mortality of a particular local treatment comparison in particular subgroups of women might come not from the apparent results for breast cancer mortality in those subgroups but from estimating the effect of that treatment comparison on local recurrence risk in those subgroups and then applying the general ﬁnding that a 20 absolute reduction in 5 year local recurrence risk leads to about a 5 absolute reduction in 15 year breast cancer mortality ie a four to one ratio of absolute effects women allocated radiotherapy that mainly involved contralateral breast cancer 2p 0 002 and lung cancer 2p 0 0007 and there was an excess mortality from causes other than 
breast cancer that mainly involved heart disease 2p 0 0001 and lung cancer 2p 0 0004 Based on much smaller numbers there was also a moderately signiﬁcant excess mortality from pulmonary embolism and excess incidence of oesophagus cancer leukaemia and soft tissue sarcoma The effects of these radiotherapy regimens on contralateral breast cancer and on mortality from causes other than breast cancer are plotted against time since randomisation in webﬁgure 7 The averaged effects on 15 year outcome are not large 9 3 vs 7 5 for contralateral breast cancer 15 9 vs 14 6 for nonbreast cancer mortality but they may well vary substantially from one regimen to another and the absolute 15 year mortality differences could also depend strongly on tumour laterality which can affect cardiac radiation dose smoking habits which affect both vascular and lung cancer risks other vascular risk factors and particularly on age The excess of contralateral breast cancer with radiotherapy appears mainly during the period 5 14 years 
after randomisation table 4 webﬁgure 7 and is signiﬁcant even among women aged 50 years or older when randomised table 4 When the excess mortality from causes other than breast cancer is subdivided by time since randomisation the proportional excess again appears to be less during the ﬁrst 5 years than in subsequent years but it is separately signiﬁcant for the periods 5 14 years and 15 years or more after randomisation The mean dates of randomisation for those who died 5 14 years and 15 years or more after randomisation were however 1975 and 1970 respectively and the radiotherapy regimens of the early 1970s may well have involved greater hazards than many current regimens The excess mortality from causes other than breast cancer is signiﬁcant both for women younger than 50 years of age and for women older than 50 years of age when randomised 2p 0 02 for both but the CIs for the age speciﬁc risks are wide The numbers are not sufﬁcient for the main hazards contralateral breast cancer lung cancer or heart 
disease to be reliably subdivided by both follow up duration and age Results of similar analyses of the trials of more versus less surgery indicate no signiﬁcant effect of more surgery on non breast cancer mortality mortality ratio 1 11 SE 0 09 Diseases other than the original breast cancer Overall mortality in radiotherapy trials Table 4 shows the incidence of second cancers and of mortality from causes other than breast cancer in all the trials in table 1 that tested radiotherapy ie all trials of radiotherapy vs not with the same surgery and all trials of more surgery vs radiotherapy with active and control reversed There was an excess cancer incidence among Figure 6 compares for the two main radiotherapy analyses the effects on breast cancer mortality with the effects on overall mortality In the post BCS radiotherapy trials the absolute reduction in 15 year overall mortality is about as large as that in 15 year breast cancer mortality For these post BCS trials there is as yet more substantially however 
and were 14 and 20 for women with one to three and for those with four or more involved nodes respectively webﬁgures 2d and 2e Systemic therapy In trials of systemic therapy 20 5 years of tamoxifen reduced the local recurrence rate by about one half in women with ER positive disease local recurrence rate ratio 0 47 SE 0 08 and irrespective of ER status polychemotherapy reduced it by about one third ratios 0 63 SE 0 08 and 0 70 SE 0 05 for women aged Ͻ50 and 50 69 years respectively webﬁgures 9R 4aR 4bR in the recent EBCTCG report20 on systemic therapy The local treatment comparisons that produced more than a 10 absolute reduction in 5 year local recurrence risk were however effective in the presence or in the absence of systemic therapy ie of chemotherapy or tamoxifen or both to both trial groups or to neither Among the women who received systemic therapy the mean absolute reduction in 5 year local recurrence risk was 20 8 vs 28 webﬁgure 6c and the 15 year reduction in breast cancer mortality was 5 9 SE 1 2 
49 1 vs 55 1 2pϽ0 00001 Thus better local treatment adds to the effects of systemic therapy on local recurrence and on breast cancer mortality Four to one ratio of absolute effects www thelancet com Vol 366 December 17 24 31 2005 2099 Articles RT after BCS generally with AC 7311 women 17 with node positive disease 80 80 15 year gain 5 4 SE 1 7 Logrank 2p 0 0002 70 70 60 60 50 15 year gain 5 3 SE 1 8 Logrank 2p 0 005 50 35 9 BCS 30 BCSϩRT 30 5 24 8 BCS 40 Any death Breast cancer mortality 40 5 40 BCSϩRT 35 2 27 7 30 24 5 20 20 20 7 12 9 11 4 10 10 10 1 0 0 5 10 15 11 9 0 20 0 5 Time years 10 15 20 Time years RT after mastectomy with AC 8505 women with node positive disease 80 80 15 year gain 5 4 SE 1 3 Logrank 2p 0 0002 70 15 year gain 4 4 SE 1 2 Logrank 2p 0 0009 70 66 4 60 MastectomyϩAC MastectomyϩACϩRT 64 2 MastectomyϩAC 60 1 72 3 68 8 MastectomyϩACϩRT 60 61 0 53 7 59 8 50 9 54 7 50 50 49 9 40 40 34 0 Any death Breast cancer mortality 46 7 32 1 30 35 5 33 6 30 20 20 10 10 0 0 0 5 10 Time years 15 20 0 5 10 
15 20 Time years Figure 6 Effect of radiotherapy RT on breast cancer mortality and on all cause mortality after BCS or after mastectomy with axillary clearance AC 15 year or 20 year probabilities Vertical lines indicate 1 SE above or below the 5 10 and 15 year percentages 2100 www thelancet com Vol 366 December 17 24 31 2005 Articles however little follow up beyond year 15 indeed many women have not yet been followed to year 15 In the trials of radiotherapy after mastectomy and axillary clearance in node positive disease the reduction in 15year all cause mortality is 4 4 SE 1 2 64 2 vs 59 8 2p 0 0009 This is less than the 5 4 reduction in 15 year breast cancer mortality At 20 years the reduction in breast cancer mortality remains unchanged at 5 4 66 4 vs 61 0 while that for allcause mortality although still signiﬁcant is only 3 5 72 3 vs 68 8 indicating a continuing excess of non breast cancer mortality long after treatment with the older radiotherapy regimens Discussion Main ﬁndings About three quarters of 
the local recurrence risk and more than three quarters of any treatment effects on local recurrence occurred during the ﬁrst 5 years after randomisation By contrast more than half the 15 year breast cancer mortality and much more than half of any such treatment effects on breast cancer mortality occurred after the ﬁrst 5 years Some local treatment comparisons eg axillary clearance vs effective axillary radiotherapy mastectomy vs BCS plus effective radiotherapy post mastectomy radiotherapy in nodenegative disease involved little Ͻ10 absolute difference in the 5 year risk of local recurrence and in aggregate these comparisons also involved little difference in 15 year breast cancer mortality ﬁgure 5 upper panel Local recurrence and breast cancer mortality The other local treatment comparisons are those that involved absolute differences of more than 10 in the 5 year risk of local recurrence eg post BCS radiotherapy mainly to the conserved breast post mastectomy radiotherapy in node positive disease 
conservation of the breast or axilla without effective radiotherapy to the conserved tissue In the aggregate of all such comparisons involving a total of 25 000 women the 5 year local recurrence risks were 7 active versus 26 control reduction 19 and the 15 year breast cancer mortality risks were 44 6 versus 49 5 reduction 5 0 SE 0 8 2pϽ0 00001 Treatment comparisons that produced similar sized effects on local recurrence tended to produce similar sized effects on breast cancer mortality webﬁgure 5 In particular both for the 7300 women in trials of post BCS radiotherapy mostly with axillary clearance and pathologically node negative disease and for the 8500 women in trials of post mastectomy radiotherapy after axillary clearance in node positive disease the absolute reductions in 5 year local recurrence and in 15 year breast cancer mortality were similar in magnitude to those in the aggregated results in all 25 000 women and were highly signiﬁcant This ﬁnding indicates that the avoidance of recurrence in a www 
thelancet com Vol 366 December 17 24 31 2005 conserved breast and the avoidance of other local recurrence eg in the chest wall or regional lymph nodes are of comparable relevance to 15 year breast cancer mortality In these two particular radiotherapy comparisons as in the aggregated results differences in local treatment that substantially affect locoregional recurrence would in the hypothetical absence of other causes of death avoid about one breast cancer death over the next 15 years for every four such recurrences avoided Moreover even when it does not affect survival avoiding a local recurrence can be of substantial beneﬁt Non breast cancer mortality and overall mortality The absence of other causes of death is of course not a realistic assumption particularly for older patients Even the general mortality that is not caused by breast cancer or its treatment makes the 15 year survival gain somewhat smaller for overall mortality than for breast cancer mortality as it reduces by a similar factor the 
proportion of 15 year survivors in both the treatment group and the control group Moreover most of the substantial differences in local recurrence in these trials were produced by radiotherapy and some of the radiotherapy regimens at least in the older trials of post mastectomy radiotherapy appreciably increased mortality more than 5 years later from diseases other than breast cancer with most of this excess mortality involving heart disease and lung cancer In addition this overview conﬁrms the previous evidence26 27 that radiotherapy can increase the incidence of contralateral breast cancer more than 5 years later which would slightly reduce its net beneﬁcial effect on 15 year breast cancer mortality We cannot ascertain from the present data whether therapeutic doses of radiation affect the incidence of new ipsilateral breast cancer in a conserved breast as new and recurrent tumours are not separated Nevertheless at least in the post BCS radiotherapy trials and among women with axillary clearance and node 
positive disease in the postmastectomy radiotherapy trials the radiotherapy regimens that were tested produced moderate but deﬁnite reductions not only in 15 year breast cancer mortality but also in 15 year overall mortality ﬁgure 6 Further effects after year 15 The evidence as to what will happen after year 15 is still limited Thus far these trials have shown that the treatments that substantially reduced the 5 year local recurrence risk moderately reduced 15 year breast cancer mortality and 15 year overall mortality They also suggest that there will be little additional gain or loss after year 15 in breast cancer mortality ratio treatment vs control of annual breast cancer death rates during the period after year 15 1 03 SE 0 08 webﬁgure 3b There is however evidence from the aggregate of all radiotherapy trials of a somewhat higher death rate during the period after year 15 from causes other than breast cancer ratio radiotherapy vs not of annual non breast cancer death rates 2101 Articles after year 15 1 
17 SE 0 06 but the mean date of randomisation for those dying in this late period was 1970 and the late hazards could well be substantially lower for modern radiotherapy regimens than for those of the 1960s and 1970s Breast cancer mortality rates remain substantial throughout at least the second decade after diagnosis and perhaps beyond as does the incidence of contralateral breast cancer while lung cancer and heart disease rates increase with advancing age If long term follow up of many of these trials is continued to 20 or more years or even to 30 or more years distinguishing between different causes of death and to the extent possible between new and recurrent tumours in a conserved breast the ensuing data will clarify substantially the long term risks and beneﬁts of the post BCS radiotherapy regimens in these trials as three quarters of the women were still alive in the present analyses table 1 It will also help clarify substantially the beneﬁts and risks of both the older and the more recent 
postmastectomy radiotherapy regimens in these trials Low and high local recurrence risks Radiotherapy produces its greatest absolute effects on local recurrence in women who are at greatest risk of local recurrence table 3 ﬁgures 2 and 3 For whether the underlying risk is low or high about 70 of it can be avoided by radiotherapy In the trials of post BCS radiotherapy the risk of local recurrence among controls depended strongly on nodal status 5 year risks 23 node negative 41 node positive and among those with node negative disease young age poor tumour differentiation and large tumour size all indicated a high local recurrence risk table 3 The large majority 78 of the node negative tumours in the post BCS radiotherapy trials were small 1 20 mm in their longest diameter but even with such small tumours without radiotherapy the 5 year risk of local recurrence was 20 table 3 In the trials of radiotherapy after mastectomy and axillary clearance the 5 year risk of local recurrence among the controls depended 
strongly on the number of involved nodes where this information was available risks 6 16 and 26 respectively for 0 1 3 and 4 involved nodes Among women with mastectomy axillary clearance and node negative disease the absolute reduction in 5 year local recurrence risk after radiotherapy was only 4 2 vs 6 so if one death from the original breast cancer is avoided for every four local recurrences avoided then the expected reduction in 15 year breast cancer mortality after radiotherapy would be only 1 less the adverse effects of any increase in contralateral disease Relatively few such women were randomised however and among them the apparent effect of radiotherapy on breast cancer mortality happened to be slightly unfavourable Only where the absolute effects of radiotherapy on local recurrence are substantial can they be used to help 2102 quantify any proportional relationship between effects on local control and on breast cancer mortality Among all women with mastectomy axillary clearance and node positive 
disease the absolute effects of radiotherapy on the 5 year local recurrence risk were substantial 6 vs 23 particularly if the tumour was poorly differentiated or large and breast cancer mortality was correspondingly reduced In these postmastectomy trials however age was of little or no relevance to local recurrence mainly in the nodes or chest wall even though in the post BCS trials age was of substantial relevance to local recurrence mainly in the conserved breast table 3 Generalisability of ﬁndings Changes in practice There have been and will continue to be substantial changes in the use or methods of screening surgery pathology radiotherapy and systemic adjuvant therapy since many of these trials began 28 30 In particular tumour sizes are generally smaller systemic therapy is more effective radiotherapy is less likely to be given to the internal mammary chain of lymph nodes or to a surgically cleared axilla and there has been increasing recognition of the late side effects of radiotherapy and of the need 
when treating early breast cancer to limit doses to the heart and lungs Hence depending mainly on the doses to the heart lungs and contralateral breast the late hazards of current and future radiotherapy regimens might well be much lower than those of the regimens studied in the older trials Moreover advances in early diagnosis surgery and systemic therapy mean that the 5 year risks of local recurrence might well be much less than in these trials Nevertheless some risk is likely to remain since the desire to control local recurrence after either BCS or mastectomy has to be balanced not only against the late adverse effects but also against the cosmetic and functional effects of excessive local treatment Prediction of absolute risks and beneﬁts Prediction from these trials of the long term risks of current radiotherapy regimens will depend on approximate comparison of current and previous radiation doses to the heart lungs etc while prediction of the eventual effects on breast cancer mortality will depend on 
what the local recurrence risks would currently be without radiotherapy The absolute risks of local recurrence in these trials and the absolute beneﬁts and hazards of radiotherapy in these trials cannot be generalised because of the continuing changes in practice since the trials began Nevertheless the quantitative relationship in these trials between local disease control and 15 year breast cancer mortality should still be relevant to current and future treatment decisions Where it is possible to estimate the absolute risk of a particular type of local recurrence after www thelancet com Vol 366 December 17 24 31 2005 Articles a particular type of surgery it is also possible to estimate the absolute reduction in this risk that effective radiotherapy would achieve as radiotherapy avoids about 70 of the risk of recurrence in the irradiated sites or that would have been avoided by more extensive surgery as surgery eliminates the possibility of recurrence in the excised tissue From the absolute reduction in 
local recurrence the absolute reduction in breast cancer mortality can be inferred For example if additional local treatment led to an estimated reduction in the 5 year local recurrence risk of say about 12 then from the general four to one relationship between effects on local recurrence and on breast cancer mortality it could reasonably reliably be inferred that the 15 year reduction in breast cancer mortality would be about 3 even though directly randomised proof of such a small mortality difference would be difﬁcult to obtain Combination of effects of local and systemic therapy Likewise as the risk of recurrence in a conserved breast is about twice as great in younger as in older women it could reasonably reliably be inferred that radiotherapy to a conserved breast or in the absence of radiotherapy mastectomy rather than BCS would have a correspondingly greater effect on breast cancer mortality in younger than in older women even though the age speciﬁc subgroup analyses of mortality have wide conﬁdence 
intervals webﬁgure 6a Furthermore avoidance of death from breast cancer gains more additional years of life expectancy for younger than for older women Systemic therapy can approximately halve the 5 year risks of both local and distant recurrence 20 In the absence of radiotherapy the risk of local recurrence although reduced by surgery and systemic therapy may still be substantial If it is then addition of radiotherapy or in some cases more extensive surgery would further reduce it by a substantial amount and thereby further reduce 15 year breast cancer mortality by a moderate amount 10 12 31 32 Indeed webﬁgure 6c suggests that the relationship between local control and breast cancer mortality is much the same with or without systemic therapy This conclusion may be of general validity even though it is based on the methods of local control and types of systemic therapy studied in these particular trials If so the moderate differences in 15 year breast cancer mortality produced by better local control can be 
combined with the moderate differences produced by chemotherapy and hormonal therapy and probably by newer systemic therapies yielding in total quite substantial effects on 15 year breast cancer mortality Hence although for the addition of radiotherapy or for other ways of improving local control the effects on breast cancer mortality are only moderate several such moderate reductions in mortality from earlier diagnosis from improvements in local control from the introduction of systemic therapy and from progressive changes www thelancet com Vol 366 December 17 24 31 2005 in its efﬁcacy may in combination approximately halve a middle aged patient s 15 year risk of death from breast cancer In some countries the introduction of several such improvements in diagnosis or treatment has in aggregate already led to substantial reductions since 1990 in the national breast cancer mortality rates in middle age 20 Conclusion The main purpose of the present overview is to help predict the effects of different treatment 
strategies on long term survival It makes no treatment recommendations nor does it assess the costs or the functional cosmetic or psychological effects of different treatments In early breast cancer local treatments that substantially improve local control have little effect on breast cancer mortality during the ﬁrst few years but have deﬁnite although moderate effects by 15 years and avoidance of local recurrence in a conserved breast and elsewhere are of comparable relevance to 15 year breast cancer mortality These trials of radiotherapy and of the extent of surgery show that in the hypothetical absence of other causes of death about one breast cancer death over the next 15 years would be avoided for every four local recurrences avoided Although the management of early breast cancer continues to change it is reasonable to assume that this approximate four to one relationship will continue to apply and will still be of relevance to future treatment choices Contributors The main contributors are the 
individuals or collaborative groups who undertook or are now continuing follow up of the trials that are reviewed and who provided trial data or other relevant information including comments on previous versions of the manuscript Acting on their behalf the EBCTCG secretariat M Clarke R Collins S Darby C Davies P Elphinstone V Evans J Godwin R Gray C Hicks S James E MacKinnon P McGale T McHugh R Peto C Taylor Y Wang accept full responsibility for the overall content of this report on the data provided and on the other relevant information received S Darby P McGale R Peto and C Taylor also accept responsibility for the data and information provided on the trials of local treatments being accurately reported on the study website Conﬂict of interest statement The secretariat declare that they all have no conﬂict of interest Acknowledgments The main acknowledgment is to the tens of thousands of women who took part in the trials reviewed here Funding for the EBCTCG secretariat is by direct support from the UK 
Medical Research Council and a special grant from Cancer Research UK to the Clinical Trial Service Unit and Epidemiological Studies Unit CTSU in the Nufﬁeld Department of Clinical Medicine University of Oxford This paper is dedicated to Richard Doll 1912 2005 epidemiologist extraordinary 33 EBCTCG collaborators listed by institution or trial organisation ACETBC Tokyo Japan O Abe R Abe K Enomoto K Kikuchi H Koyama H Masuda Y Nomura K Sakai K Sugimachi T Tominaga J Uchino M Yoshida Addenbrooke s Hospital Cambridge UK J L Haybittle ATLAS Trial Collaborative Study Group Oxford UK C Davies Auckland Breast Cancer Study Group New Zealand V J Harvey T M Holdaway R G Kay B H Mason Australian New Zealand Breast Cancer Trials Group Sydney Australia J F Forbes N Wilcken 2103 Articles Austrian Breast Cancer Study Group Vienna Austria M Gnant R Jakesz M Ploner Beatson Oncology Centre Glasgow UK H M A Yosef Belgian Adjuvant Breast Cancer Project Liège Belgium C Focan J P Lobelle Berlin Buch Akademie der Wissenschaften 
Germany U Peek Birmingham General Hospital UK G D Oates J Powell Bordeaux Institut Bergonié France M Durand L Mauriac Bordet Institute Brussels Belgium A Di Leo S Dolci M J Piccart Bradford Royal Inﬁrmary UK M B Masood D Parker J J Price Breast Cancer Study Group of the Comprehensive Cancer Centre Limburg Netherlands P S G J Hupperets British Columbia Cancer Agency Vancouver Canada S Jackson J Ragaz Cancer and Leukemia Group B Washington DC USA D Berry G Broadwater C Cirrincione H Muss L Norton R B Weiss Cancer Care Ontario Canada H T Abu Zahra Cancer Research Centre of the Russian Academy of Medical Sciences Moscow Russia S M Portnoj Cancer Research UK London UK M Baum J Cuzick J Houghton D Riley Cardiff Trialists Group UK R E Mansel Case Western Reserve University Cleveland OH USA N H Gordon Central Oncology Group Milwaukee WI USA H L Davis Centre Claudius Regaud Toulouse France A Beatrice J Mihura A Naja Centre Léon Bérard Lyon France Y Lehingue P Romestaing Centre Paul Lamarque Montpellier France J B 
Dubois Centre Regional François Baclesse Caen France T Delozier J Mace Lesec h Centre René Huguenin Paris St Cloud France P Rambert Charles University Prague Czech Republic L Petruzelka O Pribylova Cheltenham General Hospital UK J R Owen Chemo N0 Trial Group Germany N Harbeck F Jänicke C Meisner Chicago University IL USA P Meier Christie Hospital and Holt Radium Institute Manchester UK A Howell G C Ribeiro deceased R Swindell Clinical Trial Service Unit Oxford UK J Burrett M Clarke R Collins S Darby C Davies P Elphinstone V Evans J Godwin R Gray C Harwood C Hicks D Jackson S James E MacKinnon P McGale T McHugh G Mead deceased P Morris J Oulds R Peto C Taylor Y Wang Coimbra Instituto de Oncologia Portugal J Albano C F de Oliveira H Gervásio J Gordilho Copenhagen Radium Centre Denmark H Johansen H T Mouridsen Dana Farber Cancer Institute Boston MA USA R S Gelman J R Harris I C Henderson C L Shapiro Danish Breast Cancer Cooperative Group Copenhagen Denmark P Christiansen B Ejlertsen H T Mouridsen S Møller M 
Overgaard Danish Cancer Registry Copenhagen Denmark B Carstensen T Palshof Düsseldorf University Germany H J Trampisch Dutch Working Party for Autologous Bone Marrow Transplant in Solid Tumours Groningen Netherlands O Dalesio E G E de Vries S Rodenhuis H van Tinteren Eastern Cooperative Oncology Group Boston MA USA R L Comis N E Davidson R Gray N Robert G Sledge D C Tormey W Wood Edinburgh Breast Unit UK D Cameron U Chetty P Forrest W Jack Elim Hospital Hamburg Germany J Rossbach Erasmus MC Daniel den Hoed Cancer Center Rotterdam Netherlands J G M Klijn A D Treurniet Donker W L J van Putten European Institute of Oncology Milan Italy A Costa U Veronesi European Organization for Research and Treatment of Cancer Brussels Belgium H Bartelink L Duchateau C Legrand R Sylvester J A van der Hage C J H van de Velde Evanston Hospital IL USA M P Cunningham Fox Chase Cancer Centre Philadelphia PA USA R Catalano R H Creech French Adjuvant Study Group GFEA Guyancourt France J Bonneterre P Fargeot P Fumoleau P Kerbrat M 
Namer German Adjuvant Breast Group GABG Frankfurt Germany W Jonat M Kaufmann M Schumacher G von Minckwitz 2104 German Breast Cancer Study Group BMFT Freiburg Germany G Bastert H Rauschecker R Sauer W Sauerbrei A Schauer M Schumacher Ghent University Hospital Belgium A de Schryver L Vakaet GIVIO Interdisciplinary Group for Cancer Care Evaluation Chieti Italy M Belﬁglio A Nicolucci F Pellegrini M Sacco M Valentini Glasgow Victoria Inﬁrmary UK C S McArdle D C Smith Gruppo Oncologico Clinico Cooperativo del Nord Est Aviano Italy E Galligioni Gruppo Ricerca Ormono Chemio Terapia Adiuvante GROCTA Genova Italy F Boccardo A Rubagotti Groote Schuur Hospital Cape Town South Africa D M Dent C A Gudgeon A Hacking Guadalajara Hospital de 20 Noviembre Mexico A Erazo J Y Medina Gunma University Japan M Izuo Y Morishita H Takei Guy s Hospital London UK I S Fentiman J L Hayward R D Rubens D Skilton Heidelberg University I Germany H Scheurlen Heidelberg University II Germany M Kaufmann D von Fournier Hellenic Cooperative 
Oncology Group Athens Greece U Dafni G Fountzilas Helsinki Deaconess Medical Centre Finland P Klefstrom Helsinki University Finland C Blomqvist T Saarto Innsbruck University Austria R Margreiter Institut Curie Paris France B Asselain R J Salmon J R Vilcoq Institut Gustave Roussy Paris France R Arriagada C Hill A Laplanche M G Lê M Spielmann Instituto Nazionale per la Ricerca sul Cancro Genova Italy P Bruzzi E Montanaro R Rosso M R Sertoli M Venturini Instituto Oncologico Romagnolo Forli Italy D Amadori Integraal Kankercentrum Amsterdam Netherlands J Benraadt M Kooi A O van de Velde J A van Dongen J B Vermorken International Breast Cancer Study Group Ludwig Bern Switzerland M Castiglione F Cavalli A Coates J Collins J Forbes R D Gelber A Goldhirsch J Lindtner K N Price C M Rudenstam H J Senn International Collaborative Cancer Group Charing Cross Hospital London UK J M Bliss C E D Chilvers R C Coombes E Hall M Marty Israel NSABC Tel Aviv Israel R Borovik G Brufman H Hayat E Robinson N Wigler Istituto Nazionale 
per lo Studio e la Cura dei Tumori Milan Italy G Bonadonna T Camerini G De Palo M Del Vecchio F Formelli P Valagussa Italian Cooperative Chemo Radio Surgical Group Bologna Italy A Martoni F Pannuti Italian Oncology Group for Clinical Research Parma Italy G Cocconi A Colozza R Camisa Japan Clinical Oncology Group Breast Cancer Study Group Matsuyama Japan K Aogi S Takashima Japanese Foundation for Multidisciplinary Treatment of Cancer Tokyo Japan O Abe T Ikeda K Inokuchi K Kikuchi K Sawa Kawasaki Medical School Japan H Sonoo Krakow Institute of Oncology Poland S Korzeniowski J Skolyszewski Kumamoto University Group Japan M Ogawa J Yamashita Leuven Akademisch Ziekenhuis Gasthuisberg Belgium J Bonte R Christiaens R Paridaens W Van den Bogaert Marseille Laboratoire de Cancérologie Biologique APM France P Martin S Romain Memorial Sloan Kettering Cancer Center New York NY USA T Hakes C A Hudis L Norton R Wittes Metaxas Memorial Cancer Hospital Athens Greece G Giokas D Kondylis B Lissaios Mexican National Medical 
Centre Mexico City Mexico R de la Huerta M G Sainz National Cancer Institute Bethesda MD USA R Altemus K Cowan D Danforth A Lichter M Lippman J O Shaughnessy L J Pierce S Steinberg D Venzon J A Zujewski National Cancer Institute Bari Italy A Paradiso M De Lena F Schittulli National Cancer Institute of Canada Clinical Trials Group Kingston Ontario Canada J D Myles J L Pater K I Pritchard T Whelan National Kyushu Cancer Center Japan Y Nomura www thelancet com Vol 366 December 17 24 31 2005 Articles National Surgical Adjuvant Breast and Bowel Project NSABP Pittsburgh PA USA S Anderson G Bass A Brown J Bryant J Costantino J Dignam B Fisher C Redmond S Wieand N Wolmark Nolvadex Adjuvant Trial Organisation London UK M Baum I M Jackson deceased M K Palmer North Central Cancer Treatment Group Mayo Clinic Rochester MN USA J N Ingle V J Suman North Sweden Breast Cancer Group Umea Sweden N O Bengtsson H Jonsson L G Larsson North Western British Surgeons Manchester UK J P Lythgoe R Swindell Northwick Park Hospital 
London UK M Kissin Norwegian Breast Cancer Group Oslo Norway B Erikstein E Hannisdal A B Jacobsen J E Varhaug Norwegian Radium Hospital Oslo Norway B Erikstein S Gundersen M Hauer Jensen H Høst A B Jacobsen R Nissen Meyer Nottingham City Hospital UK R W Blamey A K Mitchell D A L Morgan J F R Robertson Oncofrance Paris France M Di Palma G Mathé J L Misset Ontario Clinical Oncology Group Hamilton Canada R M Clark M Levine K I Pritchard T Whelan Osaka City University Japan K Morimoto Osaka National Hospital Japan K Sawa Y Takatsuka Churchill Hospital Oxford UK E Crossley A Harris D Talbot M Taylor Parma Hospital Italy G Cocconi B di Blasio Petrov Research Institute of Oncology St Petersburg Russia V Ivanov V Semiglazov Piedmont Oncology Association Winston Salem NC USA J Brockschmidt M R Cooper Prefectural Hospital Oita Japan H Ueo Pretoria University South Africa C I Falkson Royal Marsden Hospital Institute of Cancer Research London UK R A Hern S Ashley T J Powles I E Smith J R Yarnold St George s Hospital 
London UK J C Gazet St Luke s Hospital Dublin Ireland N Corcoran Sardinia Oncology Hospital A Businico Cagliari Sardinia N Deshpande L di Martino SASIB International Trialists Cape Town South Africa P Douglas A Hacking H Høst A Lindtner G Notter Saskatchewan Cancer Foundation Regina Canada A J S Bryant G H Ewing L A Firth J L Krushen Kosloski Scandinavian Adjuvant Chemotherapy Study Group Oslo Norway R Nissen Meyer Scottish Cancer Therapy Network Edinburgh UK L Foster W D George H J Stewart P Stroner South Sweden Breast Cancer Group Lund Sweden P Malmström T R Möller S Rydén I Tengrup L Tennvall Nittby South East Sweden Breast Cancer Group Linköping Sweden J Carstenssen M Dufmats T Hatschek B Nordenskjöld M Söderberg South Eastern Cancer Study Group and Alabama Breast Cancer Project Birmingham AL USA J T Carpenter South West Oncology Group San Antonio TX USA K Albain J Crowley S Green S Martino C K Osborne P M Ravdin Stockholm Breast Cancer Study Group Sweden U Glas U Johansson L E Rutqvist T Singnomklao A 
Wallgren Swiss Group for Clinical Cancer Research SAKK Bern and OSAKO St Gallen Switzerland M Castiglione A Goldhirsch R Maibach H J Senn B Thürlimann Tel Aviv University Israel H Brenner A Hercbergs Tokyo Cancer Institute Hospital Japan M Yoshimoto Toronto Edmonton Breast Cancer Study Group Canada G DeBoer A H G Paterson K I Pritchard Toronto Princess Margaret Hospital Canada J W Meakin T Panzarella K I Pritchard Tumour Hospital Chinese Academy of Medical Sciences Beijing People s Republic of China in collaboration with the Oxford CTSU Y Shan Y F Shao X Wang D B Zhao CTSU ZM Chen HC Pan Tunis Institut Salah Azaiz Tunisia J Bahi UK Multicentre Cancer Chemotherapy Study Group London UK M Reid M Spittle www thelancet com Vol 366 December 17 24 31 2005 UK Asia Collaborative Breast Cancer Group London UK G P Deutsch F Senanayake D L W Kwong University Federico II Naples Italy A R Bianco C Carlomagno M De Laurentiis S De Placido University of Texas MD Anderson Cancer Center Houston TX USA A U Buzdar T Smith 
Uppsala Örebro Breast Cancer Study Group Sweden J Bergh L Holmberg G Liljegren J Nilsson Vienna University Hospital 1st Department of Gynaecology Austria M Seifert P Sevelda C C Zielinsky Wessex Radiotherapy Centre Southampton UK R B Buchanan M Cross G T Royle West Midlands Oncology Association Birmingham UK J A Dunn R K Hills M Lee J M Morrison D Spooner West of Scotland Breast Trial Group Glasgow UK A Litton Western Cancer Study Group Torrance CA USA R T Chlebowski Würzburg University Germany H Cafﬁer References 1 Cuzick J Stewart H Peto R et al Overview of randomized trials of postoperative adjuvant radiotherapy in breast cancer Cancer Treat Rep 1987 71 15 29 2 Cuzick J Stewart H Rutqvist L et al Cause speciﬁc mortality in long term survivors of breast cancer who participated in trials of radiotherapy J Clin Oncol 1994 12 447 53 3 Early Breast Cancer Trialists Collaborative Group EBCTCG Effects of radiotherapy and surgery in early breast cancer an overview of the randomized trials N Engl J Med 1995 333 
1444 55 4 Early Breast Cancer Trialists Collaborative Group EBCTCG Favourable and unfavourable effects on long term survival of radiotherapy for early breast cancer an overview of the randomised trials Lancet 2000 355 1757 70 5 Fisher B Anderson S Bryant J et al Twenty year follow up of a randomized trial comparing total mastectomy lumpectomy and lumpectomy plus irradiation for the treatment of invasive breast cancer N Engl J Med 2002 347 1233 41 6 National Institutes of Health Consensus Development Panel National Institutes of Health Consensus Development Conference Statement adjuvant therapy for breast cancer November 1 3 2000 J Natl Cancer Inst 2001 93 979 89 7 Ceilley E Jagsi R Goldberg S et al Radiotherapy for invasive breast cancer in North America and Europe results of a survey Int J Radiat Oncol Biol Phys 2005 61 365 73 8 Nattinger AB Hoffman RG Kneusel RT Schapira MM Relation between appropriateness of primary therapy for earlystage breast carcinoma and increased use of breast conserving surgery 
Lancet 2000 356 1148 53 9 Arriagada R Rutqvist LE Mattsson A Kramar A Ratstein S Adequate locoregional treatment for early breast cancer may prevent secondary dissemination J Clin Oncol 1995 13 2869 78 10 Overgaard M Hansen PS Overgaard J et al Postoperative radiotherapy in high risk premenopausal women with breast cancer who receive adjuvant chemotherapy N Engl J Med 1997 337 949 55 11 Overgaard M Jensen M B Overgaard J et al Postoperative radiotherapy in high risk postmenopausal breast cancer patients given adjuvant tamoxifen Danish Breast Cancer Cooperative Group DBCG 82c randomised trial Lancet 1999 353 1641 48 12 Ragaz J Olivotto IA Spinelli JJ et al Locoregional radiation therapy in patients with high risk breast cancer receiving adjuvant chemotherapy 20 year results of the British Columbia randomized trial J Natl Cancer Inst 2005 97 116 26 13 Vinh Hung V Verschraegen C for the Breast Conserving Surgery Project Breast conserving surgery with or without radiotherapy pooled analysis for risks of 
ipsilateral breast tumor recurrence and mortality J Natl Cancer Inst 2004 96 115 21 14 Darby SC McGale P Taylor CW Peto R Long term mortality from heart disease and lung cancer after radiotherapy for early breast cancer prospective cohort study of about 300 000 women in US SEER cancer registries Lancet Oncol 2005 6 557 65 15 Carr ZA Land CE Kleinerman RA et al Coronary heart disease after radiotherapy for peptic ulcer disease Int J Radiat Oncol Biol Phys 2005 61 842 50 2105 Articles 16 17 18 19 20 21 22 23 24 2106 Deutsch M Land SR Begovic M et al The incidence of lung carcinoma after surgery for breast carcinoma with and without postoperative radiotherapy Results of National Surgical Adjuvant Breast and Bowel Project NSABP clinical trials B 04 and B 06 Cancer 2003 98 1362 68 Solin LJ Fowble BL Martz KL Goodman RL Hanks GE Results of the 1983 patterns of care process survey for deﬁnitive breast irradiation Int J Radiat Oncol Biol Phys 1991 20 105 11 Shank B Moughan J Owen J Wilson F Hanks GE The 1993 94 
patterns of care process survey for breast irradiation after breastconserving surgery comparison with the 1992 standard for breast conservation treatment Int J Radiat Oncol Biol Phys 2000 48 1291 99 Taghian A Jagsi R Makris A et al Results of a survey regarding irradiation of internal mammary chain in patients with breast cancer practice is culture driven rather than evidence based Int J Radiat Oncol Biol Phys 2004 60 706 14 Early Breast Cancer Trialists Collaborative Group EBCTCG Effects of chemotherapy and hormonal therapy for early breast cancer on recurrence and 15 year survival an overview of the randomised trials Lancet 2005 365 1687 717 Krag D Ashikaga T The design of trials comparing sentinel node surgery and axillary resection N Engl J Med 2003 349 603 05 Arriagada R Lê MG Guinebretiere J M Dunant A Rochard F Tursz T Late local recurrences in a randomised trial comparing conservative treatment with total mastectomy in early breast cancer patients Ann Oncol 2003 14 1617 22 Bartelink H Horiot J C 
Poortmans P et al Recurrence rates after treatment of breast cancer with standard radiotherapy with or without additional radiation N Engl J Med 2001 345 1378 87 Fisher ER Anderson S Tan Chui E Fisher B Eaton L Wolmark N Fifteen year prognostic discriminants for invasive breast carcinoma National Surgical Adjuvant Breast and Bowel Project Protocol B 06 Cancer 2001 91 suppl 8 1679 87 25 26 27 28 29 30 31 32 33 Voogd AC Nielsen M Peterse JL et al Differences in risk factors for local and distant recurrence after breast conserving therapy or mastectomy for stage I and II breast cancer pooled results of two large European randomized trials J Clin Oncol 2001 19 1688 97 Boice JD Jr Harvey EB Blettner M Stovall M Flannery JT Cancer in the contralateral breast after radiotherapy for breast cancer N Engl J Med 1992 326 781 85 Storm HH Andersson M Boice JD Jr et al Adjuvant radiotherapy and risk of contralateral breast cancer J Natl Cancer Inst 1992 84 1245 50 Kini VR Vicini FA Victor SJ Dmuchowski CF Rebner M 
Martinez AA Impact of the mode of detection on outcome in breast cancer patients treated with breast conserving therapy Am J Clin Oncol 1999 22 429 35 Pierce LJ Moughan J White J Winchester DP Owen J Wilson JF 1998 1999 patterns of care study process survey of national practice patterns using breast conserving surgery and radiotherapy in the management of stage I II breast cancer Int J Radiat Oncol Biol Phys 2005 62 183 92 Fuller SA Haybittle JL Smith REA Dobbs HJ Cardiac doses in post operative breast irradiation Radiother Oncol 1992 25 19 24 Ragaz J Jackson SM Le N et al Adjuvant radiotherapy and chemotherapy in node positive premenopausal women with breast cancer N Engl J Med 1997 337 956 62 Whelan TJ Julian J Wright J Jadad AR Levine ML Does locoregional radiation therapy improve survival in breast cancer A meta analysis J Clin Oncol 2000 18 1220 29 Kinlen LJ Richard Doll epidemiologist extraordinary Nature 2005 438 41 www thelancet com Vol 366 December 17 24 31 2005 
54	b	letters to nature 24 Chorvatova A Gendron L Bilodeau L Gallo Payet N Payet M D A Ras dependent chloride current activated by adrenocorticotropin in rat adrenal zona glomerulosa cells Endocrinology 141 684 692 2000 25 Tong J et al NF1 regulated adenylyl cyclase pathway Soc Neurosci Abstr abstract no 345 9 Society for Neuroscience New Orleans 2000 26 Ingram D A et al Hyperactivation of p21 ras and the hematopoietic speci c Rho GTPase Rac2 cooperate to alter the proliferation of neuro bromin de cient mast cells in vivo and in vitro J Exp Med 194 57 69 2001 27 Jacks T et al Tumour predisposition in mice heterozygous for a targeted mutation in Nf1 Nature Genet 7 353 361 1994 28 Umanoff H Edelmann W Pellicer A Kucherlapati R The murine N ras gene is not essential for growth and development Proc Natl Acad Sci USA 92 1709 1713 1995 29 Voikar V Koks S Vasar E Rauvala H Strain and gender differences in the behaviour of mouse lines commonly used in transgenic studies Physiol Behav 72 271 281 2001 30 Blanton M G Lo 
Turco J J Kriegstein A R Whole cell recording from neurons in slices of reptilian and mammalian cerebral cortex J Neurosci Methods 30 203 210 1989 Acknowledgements We thank V Manne for the BMS191563 and E Friedman for technical assistance in earlier experiments We are grateful to M Barad D Buonomano T Cannon J Colicelli P Frankland L Kaczmarek A Matynia M Sanders and D Smith for discussions and to C Brannan and S Schlussel for encouragement R M C received support from the Graduated Program in Basic and Applied Biology GABBA of the University of Oporto the Portuguese Foundation for Science and Technology FCT and the National Neuro bromatosis Foundation NNF This work was also supported by a generous donation from K M Spivak and by grants from the NIH R01 NS38480 Neuro bromatosis Inc National Illinois Mass Bay Area Minnesota Arizona Kansas and Central Plains Mid Atlantic and Texas chapters the Merck and the NNF foundations to A J S Competing interests statement The authors declare that they have no competing 
nancial interests Correspondence and requests for materials should be addressed to A J S e mail Silvaa mednet ucla edu Gene expression pro ling predicts clinical outcome of breast cancer Laura J van t Veer Hongyue Dai Marc J van de Vijver Yudong D He Augustinus A M Hart Mao Mao Hans L Peterse Karin van der Kooy Matthew J Marton Anke T Witteveen George J Schreiber Ron M Kerkhoven Chris Roberts Peter S Linsley Rene Bernards Stephen H Friend Divisions of Diagnostic Oncology Radiotherapy and Molecular Carcinogenesis and Center for Biomedical Genetics The Netherlands Cancer Institute 121 Plesmanlaan 1066 CX Amsterdam The Netherlands Rosetta Inpharmatics 12040 115th Avenue NE Kirkland Washington 98034 USA These authors contributed equally to this work Breast cancer patients with the same stage of disease can have markedly different treatment responses and overall outcome The strongest predictors for metastases for example lymph node status and histological grade fail to classify accurately breast tumours according 
to their clinical behaviour1 3 Chemotherapy or hormonal therapy reduces the risk of distant metastases by approximately one third however 70 80 of patients receiving this treatment would have survived without it4 5 None of the signatures of breast cancer gene expression reported to date6 12 allow for patient tailored therapy strategies Here we used DNA microarray analysis on primary breast tumours of 117 young patients and applied supervised classi cation to identify a gene expression signature strongly predictive of a short interval to distant metastases poor prognosis signature in patients without tumour cells in local lymph nodes at diagnosis lymph node negative In addition we established a signature that identi es tumours of BRCA1 carriers The poor prognosis signature consists of genes regulating cell cycle invasion metastasis and 530 angiogenesis This gene expression pro le will outperform all currently used clinical parameters in predicting disease outcome Our ndings provide a strategy to select 
patients who would bene t from adjuvant therapy We selected 98 primary breast cancers 34 from patients who developed distant metastases within 5 years 44 from patients who continued to be disease free after a period of at least 5 years 18 from patients with BRCA1 germline mutations and 2 from BRCA2 carriers All sporadic patients were lymph node negative and under 55 years of age at diagnosis From each patient 5 mg total RNA was isolated from snap frozen tumour material and used to derive complementary RNA cRNA A reference cRNA pool was made by pooling equal amounts of cRNA from each of the sporadic carcinomas Two hybridizations were carried out for each tumour using a uorescent dye reversal technique on microarrays containing approximately 25 000 human genes synthesized by inkjet technology13 Fluorescence intensities of scanned images were quanti ed normalized and corrected to yield the transcript abundance of a gene as an intensity ratio with respect to that of the signal of the reference pool14 Some 5 000 
genes were signi cantly regulated across the group of samples that is at least a twofold difference and a P value of less than 0 01 in more than ve tumours An unsupervised hierarchical clustering algorithm allowed us to cluster the 98 tumours on the basis of their similarities measured over these approximately 5 000 signi cant genes Similarly the 5 000 genes were clustered on the basis of their similarities measured over the group of 98 tumours Fig 1a In the dendrograms shown in Fig 1a left and top the length and the subdivision of the branches displays the relatedness of the breast tumours left and the expression of the genes top Two distinct groups of tumours are the dominant feature in this two dimensional display top and bottom of plot representing 62 and 36 tumours respectively suggesting that the tumours can be divided into two types on the basis of this set of 5 000 signi cant genes Notably in the upper group only 34 of the sporadic patients were from the group who developed distant metastases within 
5 years whereas in the lower group 70 of the sporadic patients had progressive disease Fig 1b Thus using unsupervised clustering we can already to some extent distinguish between good prognosis and poor prognosis tumours To gain insight into the genes of the dominant expression signatures we associated them with histopathological data for example oestrogen receptor ER a expression as determined by immunohistochemical IHC staining Fig 1b Out of 39 IHCstained tumours negative for ER a expression ER negative 34 clustered together in the bottom branch of the tumour dendrogram In the enlargement shown in Fig 1c a group of downregulated genes is represented containing both the ER a gene ESR1 and genes that are apparently co regulated with ER some of which are known ER target genes A second dominant gene cluster is associated with lymphocytic in ltrate and includes several genes expressed primarily by B and T cells Fig 1d Sixteen out of eighteen tumours of BRCA1 carriers are found in the bottom branch intermingled 
with sporadic tumours This is consistent with the idea that most BRCA1 mutant tumours are ER negative and manifest a higher amount of lymphocytic in ltrate15 The two tumours of BRCA2 carriers are part of the upper cluster of tumours and do not show similarity with BRCA1 tumours Neither high histological grade nor angioinvasion is a speci c feature of either of the clusters Fig 1b We conclude that unsupervised clustering detects two subgroups of breast cancers which differ in ER status and lymphocytic in ltration A similar conclusion has also been reported previously7 16 The 78 sporadic lymph node negative patients were selected speci cally to search for a prognostic signature in their gene expression pro les Forty four patients remained free of disease 2002 Macmillan Magazines Ltd NATURE VOL 415 31 JANUARY 2002 www nature com 0 0 6 BRCA1 d Contig 37571RC KIAA0882 CA12 ESR1 GATA3 MYB P28 FLJ20262 AL133619 Contig 56390RC CELSR1 Contig 58301RC UGCG AL049265 BCL2 EMAP 2 HSU79303 Contig 51994RC Contig 237RC 
Contig 47045RC XBP1 HNF3A VAV3 Contig 54295RC AL133074 Contig 53968RC Contig 49342RC ZFP103 AL110139 FLJ12538 ERBB3 FBP1 Contig 50297RC FLJ20273 AL080192 TCEB1L D5S346 AL137761 TEGT Contig 41887RC c Figure 1 Unsupervised two dimensional cluster analysis of 98 breast tumours a Twodimensional presentation of transcript ratios for 98 breast tumours There were 4 968 signi cant genes across the group Each row represents a tumour and each column a single gene As shown in the colour bar red indicates upregulation green downregulation black no change and grey no data available The yellow line marks the subdivision into two dominant tumour clusters b Selected clinical data for the 98 patients in a BRCA1 germline mutation carrier or sporadic patient ER expression tumour grade 3 versus grade 1 and 2 lymphocytic in ltrate angioinvasion and metastasis status White indicates positive black negative and grey denotes tumours derived from BRCA1 NATURE VOL 415 31 JANUARY 2002 www nature com Contig 27915RC Contig 14390RC 
POU2AF1 PIM2 LOC51237 LOC57823 LOC57823 AJ249377 X93006 U96394 X79782 AF063725 IGLL1 IGL IGL AJ225092 IGKV3D 15 AF103458 AJ225093 Contig 10268RC Contig 44195RC AF058075 IGL IGKC TLX3 Contig 42547 Contig 20907RC ICAP 1A FLJ20340 AF103530 MTR1 CD19 CD19 IGHM VPREB3 BM040 KIAA0167 TRD IRF5 Contig 50634RC 0 6 Clustering of 98 breast tumours Log10 expression ratio 000 ER Clustering of 5 000 significant genes Metastases b Agioinvasion a Grade 3 Lymphocytic infiltrate letters to nature germline carriers who were excluded from the metastasis evaluation The cluster below the yellow line consists of 36 tumours of which 34 are ER negative total 39 ER negative and 16 are carriers of the BRCA1 mutation total 18 c Enlarged portion from a containing a group of genes that co regulate with the ER a gene ESR1 Each gene is labelled by its gene name or accession number from GenBank Contig ESTs ending with RC are reverse complementary of the named contig EST d Enlarged portion from a containing a group of co regulated genes that 
are the molecular re ection of extensive lymphocytic in ltrate and comprise a set of genes expressed in T and B cells Gene annotation as in c 2002 Macmillan Magazines Ltd 531 letters to nature Sporadic breast tumours patients 55 years tumour size 5 cm lymph node negative LN0 Correlation to average good prognosis profile a 25 000 genes on the microarray The correlation coef cient of the expression for each gene with disease outcome was calculated and 231 genes were found to be signi cantly associated with disease outcome correlation coef cient 0 3 or 0 3 In the second step these 231 genes were rank ordered on the basis of the magnitude of the correlation coef cient Third the number of genes in the prognosis classi er was optimized by sequentially adding subsets of 5 genes from the top of this rank ordered list and Prognosis reporter genes No distant metastases 5 years Distant metastases 5 years Metastases after their initial diagnosis for an interval of at least 5 years good prognosis group mean follow up of 
8 7 years and 34 patients had developed distant metastases within 5 years poor prognosis group mean time to metastases 2 5 years Fig 2a To identify reliably good and poor prognostic tumours we used a powerful three step supervised classi cation method similar to those used previously8 17 18 In brief approximately 5 000 genes signi cantly regulated in more than 3 tumours out of 78 were selected from the b 70 60 Tumours 50 40 30 20 10 Tumours AL080059 Contig 63649RC LOC51203 Contig 46218RC Contig 38288RC AA555029RC Contig 28552RC FLT1 MMP9 DC13 EXT1 AL137718 PK428 HEC ECT2 GMPS Contig 32185RC UCH37 Contig 35251RC KIAA1067 GNAZ SERF1A OXCT ORC6L L2DTL PRC1 AF052162 COL4A2 KIAA0175 RAB6B Contig 55725RC DCK CENPA SM20 MCM6 AKAP2 Contig 56457RC RFC4 DKFZP564D0462 SLC2A3 MP1 Contig 40831RC Contig 24252RC FLJ11190 Contig 51464RC IGFBP5 IGFBP5 CCNE2 ESM1 Contig 20217RC NMU LOC57110 Contig 63102RC PECI AP2B1 CFFM4 PECI TGFB3 Contig 46223RC Contig 55377RC HSA250839 GSTM3 BBC3 CEGP1 Contig 48328RC WISP1 ALDH4 KIAA1442 
Contig 32125RC FGF18 0 1 0 1 15 10 5 Figure 2 Supervised classi cation on prognosis signatures a Use of prognostic reporter genes to identify optimally two types of disease outcome from 78 sporadic breast tumours into a poor prognosis and good prognosis group for patient data see Supplementary Information Table S1 b Expression data matrix of 70 prognostic marker genes from tumours of 78 breast cancer patients left panel Each row represents a tumour and each column a gene whose name is labelled between b and c Genes are ordered according to their correlation coef cient with the two prognostic groups Tumours are ordered by the correlation to the average pro le of the good prognosis group middle panel Solid line 532 1 1 c prognostic classi er with optimal accuracy dashed line with optimized sensitivity Above the dashed line patients have a good prognosis signature below the dashed line the prognosis signature is poor The metastasis status for each patient is shown in the right panel white indicates patients who 
developed distant metastases within 5 years after the primary diagnosis black indicates patients who continued to be disease free for at least 5 years c Same as for b but the expression data matrix is for tumours of 19 additional breast cancer patients using the same 70 optimal prognostic marker genes Thresholds in the classi er solid and dashed line are the same as b See Fig 1 for colour scheme 2002 Macmillan Magazines Ltd NATURE VOL 415 31 JANUARY 2002 www nature com letters to nature evaluating its power for correct classi cation using the leave oneout method for cross validation see Supplementary Information Classi cation was made on the basis of the correlations of the expression pro le of the leave one out sample with the mean expression levels of the remaining samples from the good and the poor prognosis patients respectively The accuracy improved until the optimal number of marker genes was reached 70 genes The expression pattern of the 70 genes in the 78 samples is shown in the colour plot of Fig 2b 
left panel where tumours were ordered by rank according to their correlation coef cients with the average good prognosis pro le Fig 2b middle panel The classi er predicted correctly the actual outcome of disease for 65 out of the 78 patients 83 with respectively 5 poor prognosis and 8 good prognosis patients assigned to the opposite category Fig 2b threshold optimal accuracy solid line However for the selection of patients eligible for adjuvant systemic therapy a lower number of poor prognosis patients assigned to the good prognosis category should be attained For this purpose we set a threshold that resulted in misclassi cation of no more than 10 of the poor prognosis patients 3 patients out of 34 of the poor prognosis group This optimized sensitivity threshold resulted in a total of 15 misclassi cations 3 poor prognosis tumours were classi ed as good prognosis and 12 good prognosis tumours were classi ed as poor prognosis Fig 2b dashed line We classi ed tumours having a gene expression pro le with a 
correlation coef cient above the optimized sensitivity threshold dashed line as a good prognosis Figure 3 Supervised classi cation on ER and BRCA1 signatures a Outline of a two level classi cation system 98 breast tumours are rst classi ed into an ER positive group and an ER negative group which is further divided into BRCA1 mutation and sporadic tumours b Expression data matrix of the 98 sporadic tumours across 550 optimal ER reporter genes The contrasting patterns discriminate between tumours with an ERnegative signature below solid line and an ER positive signature above solid line The reporter genes were ordered on the basis of their level of contribution to the classi ers Tumours are arranged according to the leave one out correlation coef cients to the average signatures of the classi er The ER status as determined by IHC and microarray are indicated in the two right panels c Expression data matrix of 38 ER negative tumours de ned by the ER classi er over the 100 optimal BRCA1 reporter genes The degree 
of the patterns divides the tumours in the ER negative group into two subgroups BRCA1 like and sporadic like Patients above the solid line are characterized by a BRCA1 signature The classi cation for each tumour was based on the leave one out procedure The BRCA1 germline mutation status is indicated in the right panel white indicates mutation See Fig 1 for colour scheme NATURE VOL 415 31 JANUARY 2002 www nature com 2002 Macmillan Magazines Ltd 533 letters to nature signature and below this threshold as a poor prognosis signature Even small primary tumours without lymph node metastases can display the poor prognosis signature indicating that they are already programmed for this metastatic phenotype The functional annotation for the genes provides insight into the underlying biological mechanism leading to rapid metastases Genes involved in cell cycle invasion and metastasis angiogenesis and signal transduction are signi cantly upregulated in the poor prognosis signature for example cyclin E2 MCM6 
metalloproteinases MMP9 and MP1 RAB6B PK428 ESM1 and the VEGF receptor FLT1 see Fig 2b If we evaluate all 231 prognostic reporter genes more genes belonging to these functional categories become apparent for example RAD21 cyclin B2 PCTAIRE CDC25B CENPF VEGF PGK1 MAD2 CKS2 BUB1 for a complete list see Supplementary Information Table S2 Many clinical studies have correlated alterations in expression of individual genes with breast cancer disease outcome often with contradictory results Examples include cyclin D1 ER a UPA PAI 1 HER2 neu and c myc19 22 Surprisingly none of these genes are present in our set of 70 marker genes This could be due to the fact that here we determine gene expression at the level of transcription whereas most previous studies measured protein levels However it is more likely that these genes in isolation have only limited predictive power which highlights the need for an approach based on many genes To validate the prognosis classi er an additional independent set of primary tumours 
from 19 young lymph node negative breast cancer patients was selected This group consisted of 7 patients who remained metastasis free for at least ve years and 12 patients who developed distant metastases within ve years The disease outcome was predicted by the 70 gene classi er and resulted in 2 out of 19 incorrect classi cations using both the optimal accuracy threshold Fig 2c solid line and the optimized sensitivity threshold Fig 2c dashed line Thus the classi er showed a comparable performance on the validation set of 19 independent sporadic tumours and con rmed the predictive power and robustness of prognosis classi cation using the 70 optimal marker genes Fisher s exact test for association P 0 0018 The prediction of the classi er presented in Fig 2b would indicate that women under 55 years of age who are diagnosed with lymphnode negative breast cancer that has a poor prognosis signature have a 28 fold odds ratio OR 95 con dence interval CI 7 107 P 1 0 10 8 to develop a distant metastasis within 5 
years compared with those that have the good prognosis signature see Methods for odds ratio de nition This estimate however is based on the same series of patients that the classi er was derived from and therefore this odds ratio represents an upper limit A performance cross validation procedure in which the leave one out sample is not involved in selecting the prognosis reporter genes and the number of reporter genes is not optimized results in an odds ratio of 15 for a short interval to metastases 95 CI 4 56 P 4 1 10 6 see Supplementary Information This crossvalidated predictive value of our classi er is superior to the currently available clinical and histopathological prognostic factors high grade odds ratio OR 6 4 95 CI 2 1 19 P 0 0008 tumour size greater than 2 cm OR 4 4 95 CI 1 7 11 P 0 0028 angioinvasion OR 4 2 95 CI 1 5 12 P 0 01 age 40 OR 3 7 95 CI 1 3 11 P 0 02 and ER negative OR 2 4 95 CI 0 9 6 6 P 0 13 Furthermore the evaluation of the cross validated classi er in a multivariate model that 
includes all classical prognostic factors indicates that it is an independent factor in predicting outcome of disease logistic regression OR 18 3 3 94 P value of likelihood ratio test 1 4 10 4 Studying a large and unselected cohort of breast cancer patients is required to provide a more accurate estimate of the metastatic risk associated with the prognosis signature Unsupervised cluster analysis distinguishes between ER positive 534 and ER negative tumours Fig 1a To investigate the expression patterns associated with the immunohistochemical staining of ER and to explore the differences between the sporadic and BRCA1 tumours that fall into the ER negative cluster Fig 1a a supervised two layer classi cation was performed Fig 3a Figure 3b shows that 550 genes optimally report the dominant pattern associated with ER status including genes such as keratin 18 BCL2 ERBB3 and ERBB4 see Supplementary Information Table S3 The leaveone out analysis shows that only two ER positive and three ERnegative tumours as 
determined by IHC were classi ed in the opposite gene expression group 95 correct classi cation Fig 3b middle panel However in all ve discordant cases the abundance of ER messenger RNA measured by the microarray agrees with the classi cation Fig 3b right panel An ER status reporter signature was also determined by others using a similar classi cation method8 and their ER signature gene set overlaps with ours 21 out of their 50 ER status reporter genes are present in our set of 550 ER reporters Our observation in the unsupervised analysis that ER clustering has predictive power for prognosis is also valid for the ER supervised classi cation although it does not reach the level of signi cance of the prognosis classi er ER signature prediction for prognosis OR 3 7 95 CI 1 3 11 P 0 02 data not shown Figure 3c shows the leave one out classi cation of the 38 ERnegative tumours into sporadic cases and BRCA1 associated cases based on an optimal set of 100 genes This set is enriched in lymphocyte speci c genes see 
Supplementary Information Table S4 The classi cation into sporadic and BRCA1 tumours was caused mainly by the differences in levels of gene expression amplitude in concordance with recent ndings that BRCA1 mediates ligand independent transcriptional repression of the ER23 95 accuracy 2 38 misclassi ed Fig 3c The one sporadic tumour that was classi ed as a BRCA1 tumour was shown to contain methylation of the BRCA1 promoter indicating an epigenetic modi cation of BRCA124 data not shown Notably the discordant BRCA1 tumour is from a patient where the germline mutation has only altered the last 29 amino acids of the BRCA1 protein BRCA1 mutation 5 622del62 which abolishes transcriptional activation by BRCA125 One previous study de ned a gene expression signature associated with BRCA1 germline mutations using a panel of seven tumours26 however the study was unable to appreciate the overlap in signatures between the ER negative and BRCA1 tumours Furthermore the nine BRCA1 status reporter genes26 were not present in 
our set of 100 optimal reporter genes The two layer cluster analysis that we have used and the larger number of tumours we analysed may account for these differences Our results indicate that breast cancer prognosis can already be derived from the gene expression pro le of the primary tumour Recent consensus conferences on treatment of breast cancer in Europe and the USA St Gallen2 and NIH consensus3 have developed guidelines for the eligibility of adjuvant chemotherapy based on histological and clinical characteristics Following these Table 1 Breast cancer patients eligible for adjuvant systemic therapy Patient group Consensus Total patient group n 78 Metastatic disease at 5 yr n 34 Disease free at 5 yr n 44 64 78 82 72 78 92 43 78 55 33 34 97 32 34 94 31 34 91 31 44 70 40 44 91 12 44 27 18 44 41 St Gallen NIH Prognosis pro le The conventional consensus criteria are tumour 2 cm ER negative grade 2 3 patient 35 yr either one of these criteria St Gallen consensus tumour 1 cm NIH consensus Number of tumours 
having a poor prognosis signature using our microarray pro le de ned by the optimized sensitivity threshold in the 70 gene classi er see Fig 2b Number of tumours with a poor prognosis signature in the group of disease free patients when the cross validated classi er is applied 2002 Macmillan Magazines Ltd NATURE VOL 415 31 JANUARY 2002 www nature com letters to nature guidelines up to 90 of lymph node negative young breast cancer patients are candidates for adjuvant systemic treatment As 70 80 of these patients would not have developed distant metastases without adjuvant treatment these patients may not bene t from the treatment and may potentially suffer from the side effects We applied the St Gallen and NIH consensus criteria on our patient group to compare the ef cacy of the microarray classi er for the selection of patients for adjuvant systemic treatment Table 1 shows that the prognosis classi er selects just as effectively those high risk patients that would bene t from adjuvant therapy but signi 
cantly reduces the number of patients that receive unnecessary treatment Thus the prognostic pro le potentially provides a powerful tool to tailor adjuvant systemic treatment that could greatly reduce the cost of breast cancer treatment both in terms of adverse side effects and health care expenditure Furthermore the signature that de nes ER status can be used to decide on adjuvant hormonal therapy and the signature that reveals BRCA1 status may further improve the diagnosis of hereditary breast cancer Finally genes that are overexpressed in tumours with a poor prognosis pro le are potential targets for the rational development of new cancer drugs Identi cation of such targets may improve the ef ciency of developing M therapeutics for many tumour types Methods Breast tumour selection criteria The criteria for the sporadic patients n 97 were primary invasive breast carcinoma less than 5 cm T1 or T2 no axillary metastases N0 age at diagnosis less than 55 years calendar year of diagnosis 1983 1996 no previous 
malignancies all patients were treated by modi ed radical mastectomy n 35 or breast conserving treatment n 62 including axillary lymph node dissection followed by radiotherapy Five patients of the metastases group received adjuvant systemic therapy consisting of chemotherapy n 3 or hormonal therapy n 2 all other patients did not receive additional treatment All patients were followed at least annually for a period of at least 5 years The criteria for hereditary patients n 20 were carriers of a germline mutation in BRCA1 or BRCA2 and primary invasive breast carcinoma no other selection criterion was applied This study was approved by the Medical Ethical Committee of the Netherlands Cancer Institute For complete patient data see Table S1 in Supplementary Information Clinical parameters of breast tumours Tumour material was snap frozen in liquid nitrogen within 1 h after surgery A haematoxylin and eosin stained section was prepared before and after cutting slides for RNA isolation for assessment of the 
percentage of tumour cells Only samples with greater than 50 tumour cells were selected mean 67 and median 70 for all groups studied Formalin xed paraf n embedded tumour tissue was used to evaluate the following tumour type according to the World Health Organisation classi cation histological grade grade 1 3 and the presence of angioinvasive growth and extensive lymphocytic in ltrate ER expression was determined by immunohistochemical staining negative when less than 10 of the nuclei showed staining all others ER positive RNA isolation We used 30 sections of 30 mm thickness for total RNA isolation Total RNA was isolated with RNAzolB and nally dissolved in RNase free H2O Twenty ve micrograms of total RNA was treated with DNase using the Qiagen RNase free DNase kit and RNeasy spin columns Total RNA treated with DNase was dissolved in RNase free H2O to a nal concentration of 0 2 mg ml 1 cRNA labelling cRNA was generated by in vitro transcription using T7 RNA polymerase on 5 mg total RNA and labelled with Cy3 or 
Cy5 CyDye Amersham Pharmacia Biotech 13 Five micrograms of Cy labelled cRNA from one breast cancer tumour was mixed with the same amount of reverse colour Cy labelled product from a pool which consisted of an equal amount of cRNA from each individual sporadic patient Expression pro ling using microarray Labelled cRNAs were fragmented to an average size of approximately 50 100 nucleotides by heating at 60 8C in the presence of 10 mM ZnCl2 added to a hybridization buffer containing 1 M NaCl 0 5 sodium sarcosine 50 mM MES pH 6 5 and formamide to a nal concentration of 30 nal volume 3 ml at 40 8C Hu25K microarrays represented the 24 479 biological oligonucleotides plus 1 281 control probes Sequences for microarrays were selected from RefSeq a collection of non redundant mRNA sequences http www ncbi nlm nih gov LocusLink refseq html and from expressed sequence tag EST contigs http www phrap org est_assembly human gene_number_methods html Each mRNA or EST contig was represented on the Hu25K microarray by a single 
60 polymer NATURE VOL 415 31 JANUARY 2002 www nature com oligonucleotide chosen by the oligonucleotide probe design programme13 After hybridization slides were washed and scanned using a confocal laser scanner Agilent Technologies Fluorescence intensities on scanned images were quanti ed corrected for background noise and normalized13 Microarray data are available at http www rii com publications default htm Method of unsupervised two dimensional clustering In the two dimensional cluster analysis gene clustering and tumour clustering were performed independently using an agglomerative hierarchical clustering algorithm For gene clustering pairwise similarity metrics among genes are calculated on the basis of expression ratio measurements across all tumours Similarly for tumour clustering pairwise similarity measures among tumours are calculated based on expression ratio measurements across all signi cant genes for details see Supplementary Information Method of supervised classi cation We developed a method 
for classifying breast tumours into prognostic or diagnostic categories based on gene expression pro les This method includes the following three steps 1 selection of discriminating candidate genes by their correlation with the category 2 determination of the optimal set of reporter genes using a leave one out cross validation procedure 3 prognostic or diagnostic prediction based on the gene expression of the optimal set of reporter genes for details see Supplementary Information Statistical analysis The odds ratio is the ratio of the odds in favour of developing distant metastases within 5 years for a patient in this study with a tumour characterized by the poor prognosis signature to the odds in favour of developing metastases without this signature 2 2 table P values associated with odds ratios are calculated by Fisher s exact test In the multivariate analysis a logistic model was applied with outcome of disease as the dependent variable and the P value for the relevant parameter is derived from the 
likelihood ratio test in the model see Supplementary Information Received 24 August accepted 22 November 2001 1 McGuire W L Breast cancer prognostic factors evaluation guidelines J Natl Cancer Inst 83 154 155 1991 2 Goldhirsch A Glick J H Gelber R D Senn H J Meeting highlights international consensus panel on the treatment of primary breast cancer J Natl Cancer Inst 90 1601 1608 1998 3 Eifel P et al National institutes of health consensus development conference statement adjuvant therapy for breast cancer November 1 3 2000 J Natl Cancer Inst 93 979 989 2001 4 Early Breast Cancer Trialists Collaborative Group Polychemotherapy for early breast cancer an overview of the randomised trials Lancet 352 930 942 1998 5 Early Breast Cancer Trialists Collaborative Group Tamoxifen for early breast cancer an overview of the randomised trials Lancet 351 1451 1467 1998 6 Perou C M et al Distinctive gene expression patterns in human mammary epithelial cells and breast cancers Proc Natl Acad Sci USA 96 9212 9217 1999 7 Perou 
C M et al Molecular portraits of human breast tumours Nature 406 747 752 2000 8 Gruvberger S et al Estrogen receptor status in breast cancer is associated with remarkably distinct gene expression patterns Cancer Res 61 5979 5984 2001 9 Martin K J et al Linking gene expression patterns to therapeutic groups in breast cancer Cancer Res 60 2232 2238 2000 10 Zajchowski D A et al Identi cation of gene expression pro les that predict the aggressive behavior of breast cancer cells Cancer Res 61 5168 5178 2001 11 Sorlie T et al Gene expression patterns of breast carcinomas distinguish tumor subclasses with clinical implications Proc Natl Acad Sci USA 98 10869 10874 2001 12 West M et al Predicting the clinical status of human breast cancer by using gene expression pro les Proc Natl Acad Sci USA 98 11462 11467 2001 13 Hughes T R et al Expression pro ling using microarrays fabricated by an ink jet oligonucleotide synthesizer Nature Biotechnol 19 342 347 2001 14 Roberts C J et al Signaling and circuitry of multiple MAPK 
pathways revealed by a matrix of global gene expression pro les Science 287 873 880 2000 15 Lakhani S R et al Multifactorial analysis of differences between sporadic breast cancers and cancers involving BRCA1 and BRCA2 mutations J Natl Cancer Inst 90 1138 1145 1998 16 Brenton J D Aparicio S A Caldas C Molecular pro ling of breast cancer portraits but not physiognomy Breast Cancer Res 3 77 80 2001 17 Khan J et al Classi cation and diagnostic prediction of cancers using gene expression pro ling and arti cial neural networks Naure Med 7 673 679 2001 18 He Y D Friend S H MicroarraysÐthe 21st century divining rod Nature Med 7 658 659 2001 19 Bieche I et al Genetic alterations in breast cancer Genes Chromosomes Cancer 14 227 251 1995 20 Steeg P S Zhou Q Cyclins and breast cancer Breast Cancer Res Treat 52 17 28 1998 21 Janicke F et al Randomized adjuvant chemotherapy trial in high risk lymph node negative breast cancer patients identi ed by urokinase type plasminogen activator and plasminogen activator inhibitor 
type 1 J Natl Cancer Inst 93 913 920 2001 22 van Diest P J et al Cyclin D1 expression in invasive breast cancer Correlations and prognostic value Am J Pathol 150 705 711 1997 23 Zheng L Annab L A Afshari C A Lee W H Boyer T G BRCA1 mediates ligand independent transcriptional repression of the estrogen receptor Proc Natl Acad Sci USA 98 9587 9592 2001 24 Esteller M et al Promoter hypermethylation and BRCA1 inactivation in sporadic breast and ovarian tumors J Natl Cancer Inst 92 564 569 2000 25 Chapman M S Verma I M Transcriptional activation by BRCA1 Nature 382 678 679 1996 26 Hedenfalk I et al Gene expression pro les in hereditary breast cancer N Engl J Med 344 539 548 2001 2002 Macmillan Magazines Ltd 535 letters to nature Supplementary Information accompanies the paper on Nature s website http www nature com a AE7 5B6 DO 11 7A5 4E8 DO11 Tc1 D10G4 5B6 DO 11 W3D8 W4F3 DO11 Tc2 Th1 Acknowledgements We thank D Atsma and D Majoor for assistance with the histological analyses and the preparation of tumour RNA T 
van der Velde W van Waardenburg and O Dalesio for medical record data extraction D Slade J McDonald J Koch T Erkkila M Parrish and others at Rosetta s High Throughput Gene Expression Pro ling Facility for microarray experiments R Stoughton F van Leeuwen M Rookus P Nederlof F Hogervorst and D Voskuil for suggestions and A Berns L Hartwell J Radich and S Rodenhuis for support and reading of the manuscript This work was supported by a grant from the Center for Biomedical Genetics Competing interests statement The authors declare competing nancial interests details accompany the paper on Nature s website http www nature com Th2 Tim 3 b Signal peptide 1 MFSGLTLNCVLLLLQLLLARSLEDGYKVEVGKNAYLPCSYTLPTSGTLVPMCWGKGFCPW mTim 3 1 MFSHLPFDCVLLLLLLLLTRSSEVEYRAEVGQNAYLPCFYTPAAPGNLVPVCWGKGACPV hTIM 3 IgV domain Correspondence and requests for materials should be addressed to S H F e mail stephen_friend merck com 61 SQCTNELLRTDERNVTYQKSSRYQLKGDLNKGDVSLIIKNVTLDDHGTYCCRIQFPGLMN mTim 3 61 FECGNVVLRTDERDVNYWTS 
RYWLNGDFRKGDVSLTIENVTLADSGIYCCRIQIPGIMN hTIM 3 Mucin domain Th1 speci c cell surface protein Tim 3 regulates macrophage activation and severity of an autoimmune disease Transmembrane region 180 ADEIKDS GETIRTAIHIGVGVSAGLTLALIIGVLILKWYSCKKKKLSSLSL mTim 3 180 ANELRDSRLANDLRDSGATIRIGIYIGAGICAGLALALIFGALIFKWYSHSKEKIQNLSL hTIM 3 Cytoplasmic region 231 ITLANLPPGGLANAGAVRIRSEENIYTIEENVYEVENSNEYYCYVNS QQPS mTim 3 240 ISLANLPPSGLANAVAEGIRSEENIYTIEENVYEVEEPNEYYCYVSSRQQPSQPLGCRFA hTIM 3 282 mTim 3 300 MP hTIM 3 c CHO mock Laurent Monney Catherine A Sabatos Jason L Gaglia Akemi Ryu Hanspeter Waldner Tatyana Chernova Stephen Manning Edward A Green eld Anthony J Coyle Raymond A Sobel Gordon J Freeman Vijay K Kuchroo Activation of naive CD4 T helper cells results in the development of at least two distinct effector populations Th1 and Th2 cells1 3 Th1 cells produce cytokines interferon IFN g interleukin IL 2 tumour necrosis factor TNF a and lymphotoxin that are commonly associated with cell mediated immune responses 
against intracellular pathogens delayed type hypersensitivity reactions4 and induction of organ speci c autoimmune diseases5 Th2 cells produce cytokines IL 4 IL 10 and IL 13 that are crucial for control of extracellular helminthic infections and promote atopic and allergic diseases4 Although much is known about the functions of these two subsets of T helper cells there are few known surface molecules that distinguish between them6 We report here the identi cation and characterization of a transmembrane protein Tim 3 which contains an immunoglobulin and a mucin like domain and is expressed on differentiated Th1 cells In vivo administration of antibody to Tim 3 enhances the clinical and pathological severity of experimental autoimmune encephalomyelitis EAE a Th1 dependent autoimmune disease and increases the number and activation level of macrophages Tim 3 may have an important role in the induction of autoimmune diseases by regulating macrophage activation and or function In addition to their distinct roles 
in disease Th1 and Th2 cells cross regulate each other s expansion and functions Thus preferential induction of Th2 cells inhibits autoimmune diseases7 8 and 536 CHO mTim 3 No of cells Center for Neurologic Diseases Department of Neurology Brigham and Women s Hospital and Harvard Medical School and Department of Adult Oncology Dana Farber Cancer Institute and Department of Medicine Harvard Medical School Boston Massachusetts 02115 USA Millennium Pharmaceuticals 640 Memorial Drive Cambridge Massachusetts 02139 USA VA Health Care System Palo Alto and Department of Pathology Stanford University School of Medicine Stanford California 95305 USA d Relative expression 121 DKKLELKLDIKAAKVTPAQTAHGDSTTASPRTLTTERNG SETQTLVTLHNNNGTKISTW mTim 3 120 DEKFNLKLVIKPAKVTPAPTLQRDFTAAFPRMLTTRGHGPAETQTLGSLPDINLTQISTL hTIM 3 540 2 445 200 100 0 Tim 3 7 4 3 ck 7 1 9 ells ells ells AE 10G Tim mo 64 SC 02 T c c c lls s D O m HO aw 2 D2 LS 1ive ive B 11b e 1 c ell H C s R cells ells Na Na CD e Th h2 c C ive ag ritic B c T h Na rop end 
c D Ma Figure 1 Cloning of a Th1 speci c cell surface protein Tim 3 a Th1 Th2 Tc1 and Tc2 cells were stained with monoclonal antibody to Tim 3 solid line or rat IgG isotype control dotted line b Deduced amino acid sequence of murine and human Tim 3 Shading indicates regions of homology IgV variable region of immunoglobulin c CHO cells transfected using either Tim 3 cDNA CHO mTim 3 or vector alone CHO mock Stable puromycin resistant cells were stained with monoclonal antibody to Tim 3 solid line or rat IgG isotype control dotted line d Total RNA from various cell lines and cells puri ed from SJL mice was isolated and transcribed to cDNA by reverse transcription and cDNA was used for Taqman PCR The gure shows expression of Tim 3 RNA relative to control GAPDH expression predominant induction of Th1 cells can regulate induction of asthma atopy and allergies9 10 Several groups have reported the association of chemokine and co stimulatory receptors with Th1 refs 11 14 and Th2 refs 12 13 15 18 cells however the 
nature of the differences in expression of most of these molecules is quantitative To identify new Th1 speci c cell surface proteins we immunized Lewis and Lou M rats with Th1 T cell clones and lines including the established Th1 speci c clone AE7 and in vitro differentiated Th1 cell lines derived from 5B6 ref 19 and DO11 10 T cell receptor TCR transgenic mice A panel of approximately 20 000 monoclonal antibodies was generated and screened on Th1 and Th2 cells Two of the monoclonal antibodies 8B 2C12 and 25F 1D6 that selectively stained Th1 cells were further characterized These 2002 Macmillan Magazines Ltd NATURE VOL 415 31 JANUARY 2002 www nature com 
55	b	Tamoxifen for Prevention of Breast Cancer Report of the National Surgical Adjuvant Breast and Bowel Project P 1 Study Bernard Fisher Joseph P Costantino D Lawrence Wickerham Carol K Redmond Maureen Kavanah Walter M Cronin Victor Vogel Andre Robidoux Nikolay Dimitrov James Atkins Mary Daly Samuel Wieand Elizabeth Tan Chiu Leslie Ford Norman Wolmark and other National Surgical Adjuvant Breast and Bowel Project Investigators from administration of tamoxifen its use as a breast cancer preventive agent is appropriate in many women at increased risk for the disease J Natl Cancer Inst 1998 90 1371 88 On June 1 1992 the National Surgical Adjuvant Breast and Bowel Project NSABP implemented a randomized clinical trial to evaluate the worth of tamoxifen for the prevention of breast cancer in women considered to be at increased risk for the disease The term prevention as used in this article indicates a reduction in the incidence risk of invasive breast cancer over the period of the study Although tamoxifen prevented 
the appearance of a substantial number of breast cancers over the duration of this study the term prevention does not necessarily imply that the initiation of breast cancers has been prevented or that the tumors have been permanently eliminated The primary aim of the NSABP Breast Cancer Prevention Trial BCPT P 1 was to determine whether tamoxifen administered for at least 5 years prevented invasive breast cancer in women at increased risk Secondary aims were to determine whether tamoxifen administration would lower the incidence of fatal and nonfatal myocardial infarctions and reduce the incidence of bone fractures Additional objectives were to evaluate breast cancer mortality and tamoxifen s adverse effects in order to assess the benefits and risks from the drug and in keeping with recent advances to obtain information with regard to breast cancer genetics Tamoxifen was chosen as the agent to be evaluated because of its demonstrated benefit when used alone as well as in combination with chemotherapy to 
treat advanced breast cancer 1 5 and because of its proven efficacy in reducing tumor re Affiliations of authors B Fisher National Surgical Adjuvant Breast and Bowel Project NSABP and Allegheny University of the Health Sciences Pittsburgh PA J P Costantino C K Redmond W M Cronin V Vogel University of Pittsburgh D L Wickerham N Wolmark NSABP and Allegheny General Hospital M Kavanah Boston Medical Center MA A Robidoux HotelDieu de Montreal Quebec Canada N Dimitrov Michigan State University East Lansing J Atkins Southeast Cancer Control Consortium Winston Salem NC M Daly Fox Chase Cancer Center Cheltenham PA S Wieand NSABP Biostatistical Center University of Pittsburgh E Tan Chiu Allegheny University of the Health Sciences L Ford National Cancer Institute Bethesda MD Correspondence to Bernard Fisher M D Scientific Director Allegheny University of the Health Sciences Four Allegheny Center Suite 602 Pittsburgh PA 15212 5234 e mail BFISHER1 aherf edu See Notes following References Oxford University Press Journal 
of the National Cancer Institute Vol 90 No 18 September 16 1998 ARTICLES 1371 Downloaded from http jnci oxfordjournals org at University of Florida on September 1 2010 Background The finding of a decrease in contralateral breast cancer incidence following tamoxifen administration for adjuvant therapy led to the concept that the drug might play a role in breast cancer prevention To test this hypothesis the National Surgical Adjuvant Breast and Bowel Project initiated the Breast Cancer Prevention Trial P 1 in 1992 Methods Women N 13 388 at increased risk for breast cancer because they 1 were 60 years of age or older 2 were 35 59 years of age with a 5 year predicted risk for breast cancer of at least 1 66 or 3 had a history of lobular carcinoma in situ were randomly assigned to receive placebo n 6707 or 20 mg day tamoxifen n 6681 for 5 years Gail s algorithm based on a multivariate logistic regression model using combinations of risk factors was used to estimate the probability risk of occurrence of breast 
cancer over time Results Tamoxifen reduced the risk of invasive breast cancer by 49 two sided P 00001 with cumulative incidence through 69 months of follow up of 43 4 versus 22 0 per 1000 women in the placebo and tamoxifen groups respectively The decreased risk occurred in women aged 49 years or younger 44 50 59 years 51 and 60 years or older 55 risk was also reduced in women with a history of lobular carcinoma in situ 56 or atypical hyperplasia 86 and in those with any category of predicted 5 year risk Tamoxifen reduced the risk of noninvasive breast cancer by 50 two sided P 002 Tamoxifen reduced the occurrence of estrogen receptor positive tumors by 69 but no difference in the occurrence of estrogen receptor negative tumors was seen Tamoxifen administration did not alter the average annual rate of ischemic heart disease however a reduction in hip radius Colles and spine fractures was observed The rate of endometrial cancer was increased in the tamoxifen group risk ratio 2 53 95 confidence interval 1 35 4 
97 this increased risk occurred predominantly in women aged 50 years or older All endometrial cancers in the tamoxifen group were stage I localized disease no endometrial cancer deaths have occurred in this group No liver cancers or increase in colon rectal ovarian or other tumors was observed in the tamoxifen group The rates of stroke pulmonary embolism and deep vein thrombosis were elevated in the tamoxifen group these events occurred more frequently in women aged 50 years or older Conclusions Tamoxifen decreases the incidence of invasive and noninvasive breast cancer Despite side effects resulting METHODS Planning and Initiation of the Trial In June 1990 the National Cancer Institute NCI invited proposals from clinical cooperative groups for a feasibility pilot study that if approved would permit the design and conduct of a protocol for a breast cancer prevention trial These proposals were to be reviewed by the Cancer Control Protocol Review Committee in the NCI Division of Cancer Prevention and Control 
by the Cancer Therapy Evaluation Program Review Committee by representatives of the National Heart Lung and Blood Institute and by other NCI National Institutes of Health staff In addition external peer review was to be conducted by an ad hoc Special Review Committee convened by the Division of Extramural Activities of the NCI In February 1991 the NCI and the National Cancer Advisory Board approved the application submitted by the NSABP on July 3 1991 the NSABP received approval from the Food and Drug Administration Investigators from 131 clinical centers throughout the United States and Canada see Appendix A were selected by a peer review process to be contributors to the trial All investigations conducted were approved by review boards at each institution and were in accord with an assurance filed with and approved by the U S Department of Health and Human Services Each of the 131 clinical centers had on site auditing to monitor and assess data quality Screening for breast cancer risk eligibility was 
initiated on April 22 1992 and randomization was begun on June 1 1992 During the first year of accrual i e from June 1 1992 through May 31 1993 nearly half 48 of the 16 000 women the number originally projected as being necessary to accomplish the study goal were accrued to the study During the last 7 months of 1993 and the first 3 months of 1994 nearly 3300 additional participants were enrolled Thus by the end of March 1994 approxi 1372 ARTICLES mately 11 100 women had either been randomly assigned or had agreed to participate in the study At that time accrual was interrupted and was not resumed until March 1995 Randomization was completed on September 30 1997 More detailed information regarding participant accrual has been published 29 Conditions for Participant Eligibility Women were deemed acceptable for the P 1 study if they met certain eligibility criteria defined in the protocol and were enrolled at one of the NSABP institutions that had been selected as contributors to the study To be eligible for 
the trial the participants had to have 1 signed a consent document that had been witnessed and dated before randomization 2 been either 60 years of age or older or between the ages of 35 and 59 years with a 5 year predicted risk for breast cancer of at least 1 66 or had a history of lobular carcinoma in situ LCIS 3 had a life expectancy of at least 10 years 4 had a breast examination that demonstrated no clinical evidence of cancer 5 had a mammogram within 180 days before randomization that showed no evidence of breast cancer 6 had normal white blood cell and platelet counts and normal hepatic and renal function tests 7 not been pregnant upon entry into the study or planned not to become pregnant while on protocol therapy 8 been accessible for follow up 9 undergone an endometrial sampling before randomization if they had a uterus and were randomly assigned after July 8 1994 Endometrial sampling upon study entry was optional for participants randomly assigned before that date 10 taken no estrogen or 
progesterone replacement therapy oral contraceptives or androgens for at least 3 months before randomization and 11 had no history of deep vein thrombosis or pulmonary embolism Breast Cancer Risk Assessment The algorithm for estimating breast cancer risk was based on the work of Gail et al 30 who developed a multivariate logistic regression model in which combinations of risk factors were used to estimate the probability of occurrence of breast cancer over time The variables included in the model were age number of first degree relatives with breast cancer nulliparity or age at first live birth number of breast biopsies pathologic diagnosis of atypical hyperplasia and age at menarche In its original form the model predicted the combined risk of invasive and noninvasive breast cancers for white women Making appropriate modifications to account for a different attributable risk we applied the risk ratio RR for each of the parameters used in the Gail model to the expected rates of invasive breast cancer only 
Modifications to allow for race specific determinations of breast cancer risk were also incorporated into the model The 1984 1988 Surveillance Epidemiology and End Results SEER 1 rates of invasive breast cancer were used as the expected rates The total U S mortality rates for the year 1988 were used to adjust for the age specific competing risk of death from causes other than breast cancer Risk Benefit Each woman screened was provided with a risk profile that identified her breast cancer risk and displayed a plot of projected risk over her lifetime Fig 1 To enable the women to make a more informed decision about their participation in the trial each of them received information about the potential number of breast cancer and coronary artery cases that might be prevented from the use of tamoxifen as well as the number of cases of endometrial cancer and pulmonary embolism that might be caused by the drug Statistical Methods Randomization of participants in a double blind fashion was performed centrally by the 
NSABP Biostatistical Center and participants were stratified by age 35 49 years 50 59 years ജ60 years race black white other history of LCIS yes no and breast cancer RR 2 5 2 5 3 9 ജ4 0 To avoid imbalances in treatment assignment within a clinical center an adaptive randomization scheme using the biased coin method of Efron 31 was used The trial was monitored by an independent data monitoring committee known as the Endpoint Review Safety Monitoring and Advisory Committee ERSMAC which was composed of representatives with expertise in clinical trial methodology from a variety of disciplines including oncology gynecology cardiology biostatistics epidemiology and research ethics The design of the study included formal interim monitoring for early stopping based on the primary end point of the trial i e the incidence of invasive breast cancer The stopping rule of Fleming et al 32 was employed by the use of bounds that used less than 1 of alpha In addition as an informal tool to facilitate the monitoring Journal 
of the National Cancer Institute Vol 90 No 18 September 16 1998 Downloaded from http jnci oxfordjournals org at University of Florida on September 1 2010 currence and prolonging survival when administered as postoperative adjuvant therapy in stages I and II disease 6 10 Findings indicating that tamoxifen treated patients had a statistically significantly lower incidence of contralateral breast cancer 9 13 and that most patients used tamoxifen safely with good compliance and minimal side effects also provided justification for its evaluation as a preventive agent 14 Equally compelling was the extensive information related to the drug s pharmacokinetics metabolism and antitumor effects that had been observed in experimental animals and humans 15 18 In addition there was evidence to indicate that tamoxifen interfered with the initiation and promotion of tumors in experimental systems and inhibited the growth of malignant cells by a variety of mechanisms 19 21 Because tamoxifen had been shown to alter lipid and 
lipoprotein metabolism 22 26 which could reduce the risk of coronary artery disease it seemed appropriate that the incidence of and mortality from ischemic heart disease also be assessed In addition there was evidence to indicate that perhaps because of its estrogen agonist activity 27 28 tamoxifen might have a beneficial effect on osteoporosis Consequently the decision was made to determine whether tamoxifen reduced the incidence of bone fractures at selected sites By September 30 1997 13 388 women aged 35 years and older had been randomly assigned in the P 1 trial Because this number was considered adequate to meet the study objectives as they related to breast cancer participant entry was terminated On March 24 1998 an independent data monitoring committee which had provided oversight for the study since its inception determined that in accordance with prespecified rules for stopping the study the findings indicating a reduction in breast cancer risk were sufficiently strong to justify disclosure of the 
results This article is the first published report of the findings obtained from the P 1 study invasive breast cancer noninvasive breast cancer and invasive endometrial cancer were determined by use of the exact method assuming that the events came from a Poisson distribution and conditioning on the total number of events and the person years at risk 34 Under these conditions the expected proportion of events in the tamoxifen group p has a binomial distribution and was defined as the number of person years in the tamoxifen group PYtam divided by the total number of person years in both groups PYtam PYplac The observed proportion of events po was defined as the number of events in the tamoxifen group ntam divided by the total number of events in both groups ntam nplac The P value for testing a difference in the event rates between the groups was then computed as an exact binomial test of the hypothesis that p ס po Event rates in the two treatment groups were also compared by use of the RR and 95 confidence 
intervals CIs in which the rate in the tamoxifen group was contrasted with that in the placebo group CIs for RRs were also determined assuming that the events followed a Poisson distribution conditioning on the total number of events and person years at risk Under this circumstance the CI for an RR was determined by first finding the upper pU and lower pL limits of the CI for po where po ס RR PYtam RR PYtam PYplac and RR 1 ס Then the CI for the RR was determined by solving the equation RR ס p PYplac 1 p PYtam where pU and pL were substituted as the value of p respectively Cumulative incidence rates by follow up time were determined accounting for competing risk due to death 35 RESULTS Study Screening Accrual and Follow up Information Breast cancer risk assessments were used to determine the eligibility of women for the study From April 22 1992 through Table 1 Summary of screening accrual and follow up information for the study Screening accrual and follow up information Placebo Tamoxifen Total Breast cancer 
risk assessments 98 018 Women meeting risk eligibility requirement 57 641 Medical eligibility assessments 14 453 Women meeting both risk and medical eligibility requirements 13 954 Women randomly assigned Not at risk for breast cancer Without follow up Included in analysis Average follow up time mo Median follow up time mo followed for 36 mo followed for 48 mo followed for 60 mo Person years of follow up 6707 0 108 6599 47 7 54 6 74 0 66 7 37 1 26 247 6681 1 104 6576 47 7 54 5 73 7 67 0 36 4 26 154 13 388 1 212 13 175 47 7 54 6 73 9 67 0 36 8 52 401 See text for details Based on time at risk for death Journal of the National Cancer Institute Vol 90 No 18 September 16 1998 ARTICLES 1373 Downloaded from http jnci oxfordjournals org at University of Florida on September 1 2010 May 20 1997 risk assessments were performed for 98 018 women Table 1 57 641 58 8 of these women were deemed eligible on the basis of their risk for participation in the trial Of this group 14 453 women agreed to be medically evaluated for 
complete eligibility A total of 13 954 women met all eligibility requirements Of those 13 388 95 9 were randomly assigned to receive in a doubleblind fashion 20 mg per day of either tamoxifen or placebo for 5 years 6707 were to receive placebo and 6681 were to receive tamoxifen Table 1 Both tamoxifen and placebo were supplied by Zeneca Pharmaceuticals Wilmington DE After one of the participants had been randomly assigned it was discovered that she had invasive breast cancer rather than a noninvasive lesion LCIS as had originally been reported following mammographic and pathologic examination Therefore she was not at risk for development Fig 1 Example of a breast cancer risk profile NSABP ס National Surgical Adjuvant Breast and of breast cancer and was not included in the Bowel Project UNK ס unknown Reproduced from Cancer Control 1997 4 78 86 with permission analyses At the time of analysis there were from the copyright holder 212 participants with no follow up 108 in the placebo group and 104 in the 
tamoxifen group All of the 13 175 women at risk and with follow up were inof multiple potential beneficial and detrimental outcomes the ERSMAC adopted a form of global monitoring using a global index modeled after the one proposed cluded in the analyses In each study group 7 2 of the particiby Freedman et al 33 for the Women s Health Initiative trial The use of this pants withdrew their consent but were followed until consent supplemental monitoring tool was not included in the protocol design but was withdrawal When the treatment groups were combined 21 6 adopted by the ERSMAC before the time of the first formal interim analysis All analyses were based on assigned treatment at the time of randomization of the participants discontinued their assigned therapy for rearegardless of treatment status at the time of analysis All randomly assigned sons not specified in the protocol The proportion of women who participants with follow up were included in the analyses Average annual event stopped their therapy was 
greater in the tamoxifen group i e rates for the study end points were calculated for each treatment group by means 19 7 in the placebo group versus 23 7 in the tamoxifen of a procedure in which the number of observed events was divided by the group Also 1 6 of the participants in each study group were number of observed event specific person years of follow up P values twolost to follow up When the consent withdrawals were excluded sided for tests of differences between the treatment groups for the rates of Participant Characteristics Of the 13 175 participants included in the analysis 39 3 were 35 49 years old at randomization 30 7 were 50 59 years old and 30 0 were 60 years of age or older Table 2 Only 2 6 of the participants were 35 39 years of age and 6 0 were 70 years of age or older Almost all participants were white 96 4 more than one third 37 1 had had a hysterectomy 6 3 had a history of LCIS and 9 1 had a history of atypical hyperplasia The distribution of participants among the placebo and 
tamoxifen groups relative to these characteristics was similar Almost one fourth 23 8 of the participants had no firstdegree relatives with breast cancer More than one half 56 8 had one first degree relative with breast cancer 16 4 had two and 3 0 had three or more About one quarter of the women had a 5 year predicted breast cancer risk that was 2 00 or less Almost three fifths 57 6 had a 5 year risk between 2 01 and 5 00 and 17 4 had a risk of more than 5 00 Breast Cancer Events A total of 368 invasive and noninvasive breast cancers occurred among the 13 175 participants 244 of these occurred in the placebo group and 124 in the tamoxifen group Fig 2 There was a highly significant reduction in the incidence of breast cancer as a result of tamoxifen administration that decrease was observed for both invasive and noninvasive disease For invasive breast cancer there was a 49 reduction in the overall risk There were 175 cases of invasive breast cancer in the placebo group as compared with 89 in the tamoxifen 
group P 00001 The cumulative incidence through 69 months was 43 4 per 1000 women and 22 0 per 1000 women in the two groups respectively For noninvasive breast cancer the reduction in risk was 50 there were 69 cases in women receiving placebo and 35 in 1374 ARTICLES Table 2 Participant characteristics at time of randomization for women included in the analyses Placebo Tamoxifen Characteristic No No Age y 35 39 40 49 50 59 60 69 ജ70 185 2411 2017 1590 396 2 8 36 5 30 6 24 1 6 0 159 2422 2031 1571 393 2 4 36 8 30 9 23 9 6 0 Race White Black Other 6359 111 129 96 4 1 7 2 0 6347 109 120 96 5 1 7 1 8 No of first degree relatives with breast cancer 0 1 2 ജ3 1595 3731 1092 181 24 2 56 5 16 5 2 7 1540 3754 1069 213 23 4 57 1 16 3 3 2 Prior hysterectomy No Yes 4194 2405 63 6 36 4 4097 2479 62 3 37 7 History of lobular carcinoma in situ No Yes 6188 411 93 8 6 2 6161 415 93 7 6 3 History of atypical hyperplasia in the breast No Yes 5985 614 90 7 9 3 5997 579 91 2 8 8 5 y predicted breast cancer risk ഛ2 00 2 01 3 00 3 01 
5 00 ജ5 01 1660 2031 1791 1117 25 2 30 8 27 1 16 9 1636 2057 1714 1169 24 9 31 3 26 1 17 8 6599 100 0 6576 100 0 Total those receiving tamoxifen P 002 Through 69 months the cumulative incidence of noninvasive breast cancer among the placebo group was 15 9 per 1000 women versus 7 7 per 1000 women in the tamoxifen group The average annual rate of noninvasive breast cancer per 1000 women was 2 68 in the placebo group compared with 1 35 in the tamoxifen group yielding an RR of 0 50 95 CI 77 0 33 0 ס The reduction in noninvasive cancers related to a decrease in the incidence of both ductal carcinoma in situ DCIS and LCIS No survival differences were observed Nine deaths were attributed to breast cancer i e six in the group that received placebo and three in the tamoxifen group To assess the consistency of the effect of tamoxifen across the population rates of invasive breast cancer were calculated for several subgroups of women When age history of LCIS history of atypical hyperplasia and levels of predicted risk 
of breast cancer were taken into consideration tamoxifen was found to be effective in preventing breast cancer in all subgroups Table 3 The reduction in the incidence of invasive breast cancer associated with tamoxifen ranged from 44 among women who were 49 years of age or younger at the time of randomization to 55 among those who were 60 years of age or older at randomization Among women with a history of LCIS the reduction in risk was 56 The reduction was particularly noteworthy among those with a history of atypical hyperplasia there were 23 cases Journal of the National Cancer Institute Vol 90 No 18 September 16 1998 Downloaded from http jnci oxfordjournals org at University of Florida on September 1 2010 the percent of participants with complete follow up was 92 4 in the placebo group and 92 3 in the tamoxifen group The study was designed to maintain statistical power even if the rate of noncompliance defined as permanently discontinuing tamoxifen therapy was as high as 10 per year of follow up While 
the cumulative rate of noncompliance was below the planned level the interruption of accrual in 1994 resulted in a substantial increase in the rates of noncompliance and of consent withdrawal In the 6 month interval following the interruption the proportion of women who became noncompliant or who withdrew their consent was two to three times higher than before or after that interval The mean time on the study for the 13 175 participants who were included in the analysis was 47 7 months 73 9 had a follow up exceeding 36 months 67 0 were followed for more than 48 months and 36 8 had follow up exceeding 60 months The median follow up time was 54 6 months All data included in this article are based on information received as of July 31 1998 concerning follow up through March 31 1998 This was the cutoff point selected because it was the day before the trial was unblinded On April 1 1998 investigators were provided with lists identifying the treatment assignment for each participant tamoxifen group there was a 
substantial reduction in risk for each year of follow up in the latter group The observed rates of reduction by year were 33 55 39 49 69 and 55 Tumor Characteristics Table 3 Average annual rates for invasive breast cancer by age history of lobular carcinoma in situ LCIS history of atypical hyperplasia 5 year predicted breast cancer risk and number of first degree relatives with breast cancer No of events Rate per 1000 women 95 confidence interval Placebo Tamoxifen Placebo Tamoxifen Risk ratio 175 89 6 76 3 43 0 51 0 39 0 66 68 50 57 38 25 26 6 70 6 28 7 33 3 77 3 10 3 33 0 56 0 49 0 45 0 37 0 85 0 29 0 81 0 27 0 74 History of LCIS No Yes 157 18 81 8 6 41 12 99 3 30 5 69 0 51 0 44 0 39 0 68 0 16 1 06 History of atypical hyperplasia No Yes 152 23 86 3 6 44 10 11 3 61 1 43 0 56 0 14 0 42 0 73 0 03 0 47 5 y predicted breast cancer risk ഛ2 00 2 01 3 00 3 01 5 00 ജ5 01 35 42 43 55 13 29 27 20 5 54 5 18 5 88 13 28 2 06 3 51 3 88 4 52 0 37 0 68 0 66 0 34 0 18 0 72 0 41 1 11 0 39 1 09 0 19 0 58 No of first degree 
relatives with breast cancer 0 1 2 ജ3 38 90 37 10 17 46 20 6 6 45 6 00 8 68 13 72 2 97 3 03 4 75 7 02 0 46 0 51 0 55 0 51 0 24 0 84 0 35 0 73 0 30 0 97 0 15 1 55 Patient characteristic All women Age y ഛ49 50 59 ജ60 Journal of the National Cancer Institute Vol 90 No 18 September 16 1998 ARTICLES 1375 Downloaded from http jnci oxfordjournals org at University of Florida on September 1 2010 Rates of invasive breast cancer by selected tumor characteristics are compared in Fig 4 The annual rate of estrogen receptor ER positive breast cancers was 69 less in women in the tamoxifen group The rates were 5 02 per 1000 women in the placebo group compared with 1 58 per 1000 women in the tamoxifen group RR 59 13 0 ס CI 54 0 22 0 ס There was no evidence of a significant difference in the rates of tumors presenting as ER negative 1 20 per 1000 women in the placebo group and 1 46 per 1000 women in the tamoxifen group RR 59 22 1 ס CI 30 2 47 0 ס Of the seven invaFig 2 Cumulative rates of invasive and noninvasive breast 
cancers occurring in particisive breast cancers that occurred among black women pants receiving placebo or tamoxifen The P values are two sided four were ER negative and three were ER positive Of those that were ER positive two were in the placebo of invasive breast cancer in the placebo group and three in the group and one was in the tamoxifen group The rate of invasive breast cancer among women in the tamoxifen group When related to the level of predicted risk among participants the reduction of cancer risk ranged from tamoxifen group was less than that among women in the placebo 32 to 66 Because the proportion of nonwhite women ran group in all tumor size categories The greatest difference bedomly assigned in the trial was small 3 6 only nine invasive tween treatment groups was evident in the occurrence of tumors breast cancer events were observed in this population Seven that were 2 0 cm or less in size at the time of diagnosis The events occurred in black women and two in women of other observed rates 
of occurrence of tumors of 1 0 cm or smaller were races Of the seven tumors that occurred among blacks two were 2 43 per 1000 women in the placebo group and 1 43 per 1000 women in the tamoxifen group The rates of occurrence of tuin the placebo group and five were in the tamoxifen group The effectiveness of tamoxifen in preventing invasive breast mors 1 1 2 0 cm were 2 63 and 1 04 per 1000 women respeccancer was assessed by means of a comparison of the rates of the tively The rates of occurrence of tumors of 2 1 3 0 cm were occurrence of that disease during each of the first 6 yearly in 0 85 per 1000 women in the placebo group and 0 54 per 1000 tervals of follow up Fig 3 When the average annual rate per women in the tamoxifen group for tumors 3 1 cm or larger the 1000 women in the placebo group was compared with that in the rates were 0 73 and 0 42 per 1000 women respectively fen group The rates of breast cancers presenting without nodal involvement were 4 48 and 2 31 per 1000 women in the placebo and 
tamoxifen groups respectively The rates of occurrence of tumors presenting with one to three involved nodes were 1 39 and 0 54 per 1000 women respectively The rates for cancers presenting with four or more positive axillary nodes were the same in both study groups Endometrial Cancer The rate of invasive breast cancer by nodal status at the time of diagnosis differed in the two treatment groups Because axillary dissection was not performed for all cases of invasive breast cancer pathologic nodal status was not available for 12 women in the placebo group and for three women in the tamoxi Invasive Cancers Other Than Cancer of the Breast and Uterus Endometrium Invasive cancers at sites other than the breast and endometrium were equally distributed with 97 cases in each group RR 59 00 1 ס CI 53 1 57 0 ס Table 5 At no site was there evidence of a disproportionate number of events Of particular importance were the observations that no liver cancers occurred in either group and that there was no increase in the 
incidence of colon rectal ovarian or other genitourinary tumors The greatest incidence of tumors occurred in the lung trachea and bronchus 17 in the placebo group and 20 in the tamoxifen group Ischemic Heart Disease Fig 4 Rates of invasive breast cancer occurring in participants receiving placebo or tamoxifen by tumor size lymph node status and estrogen receptor status Numbers above the bars indicate numbers of events UNK ס unknown Path ס pathologic Neg ס negative Pos ס positive 1376 ARTICLES Women who experienced more than one ischemic heart disease event were categorized according to the most severe event in decreasing order from fatal myocardial infarction to acute ischemic syndrome The number of participants who had a myocardial infarction in the placebo and tamoxifen groups was 28 and 31 respectively Eight 29 of the 28 events that occurred in the placebo group were fatal as compared with seven 23 of the 31 events in the group that received tamoxifen Table 6 Likewise the number of participants who had 
angina requiring a coronary artery bypass graft or angioplasty was 14 in the placebo group and 13 in the tamoxifen group The number of women reported as having acute ischemic syndrome was 20 in the placebo group and 27 in the tamoxifen group RR 63 1 ס 95 CI 55 2 37 0 ס Of the total number of events related to ischemic heart disease 62 occurred in the placebo group five in women aged ഛ49 years and 57 in women aged ജ50 years 71 events occurred in the tamoxifen group 10 and 61 in the two age Journal of the National Cancer Institute Vol 90 No 18 September 16 1998 Downloaded from http jnci oxfordjournals org at University of Florida on September 1 2010 Fig 3 Rates of invasive breast cancer occurring in participants receiving placebo or tamoxifen by yearly interval of follow up Numbers above the bars indicate numbers of events Participants who received tamoxifen had a 2 53 times greater risk of developing an invasive endometrial cancer 95 CI ס 1 35 4 97 than did women who received placebo an average annual rate 
per 1000 participants of 2 30 in the former group and 0 91 in the latter group Table 4 The increased risk was predominantly in women 50 years of age or older The RR of women aged 49 years or younger was 1 21 95 CI 14 0 ס 3 60 whereas it was 4 01 95 CI 09 01 07 1 ס in women aged 50 years or older The increase in incidence after tamoxifen administration was observed early in the follow up period Fig 5 Through 66 months of follow up the cumulative incidence was 5 4 per 1000 women and 13 0 per 1000 women in the placebo and tamoxifen groups respectively Fourteen 93 of the 15 invasive endometrial cancers that occurred in the placebo group were International Federation of Gynecology and Obstetrics FIGO stage I and one 7 was FIGO stage IV All 36 invasive endometrial cancers that occurred in the group receiving tamoxifen were FIGO stage I Four in situ endometrial cancers were reported three of these occurred in the placebo group and one in the tamoxifen group Table 4 Average annual rates of invasive and in situ 
endometrial cancer No of events Type of event Invasive cancer Age y ഛ49 ജ50 In situ cancer Rate per 1000 women Placebo Tamoxifen Placebo Tamoxifen Risk ratio 95 confidence interval 15 36 0 91 2 30 2 53 1 35 4 97 8 7 3 9 27 1 1 09 0 76 0 18 1 32 3 05 0 06 1 21 4 01 0 35 0 41 3 60 1 70 10 90 0 01 4 38 Women at risk nonhysterectomized groups respectively Overall the average annual rate of ischemic heart disease was 2 37 per 1000 women in the placebo group and 2 73 per 1000 women in the tamoxifen group Fig 5 Cumulative rates of invasive endometrial cancer occurring in participants receiving placebo or tamoxifen The P value is two sided Table 5 Distribution of invasive cancers other than breast and uterine endometrial cancer No of cancers Primary cancer site Placebo Tamoxifen Mouth pharynx larynx Stomach Gallbladder Pancreas Retroperitoneum Colon Rectum Liver Lung trachea bronchus Lymphatic hematopoietic systems Ovary fallopian tube Other genital Urinary bladder Kidney Connective tissue Skin Nervous system 
Thyroid gland Unknown 2 2 1 7 1 9 3 0 17 11 11 4 1 3 2 9 3 5 6 3 1 0 4 0 11 4 0 20 14 10 4 3 2 1 11 1 4 4 Total Average annual rate per 1000 women Risk ratio 95 confidence interval 97 3 72 1 00 0 75 1 35 97 3 73 International Classification of Diseases code 9 68 Fractures of the hip and radius Colles were defined in the protocol as the primary fracture events to be evaluated in the trial Soon after initiation of the study fractures of the spine were also included These three fracture sites were selected a priori as those that would most likely be associated with osteoporosis Also when the radiology reports were reviewed to identify fractures of the radius that were Colles fractures it became evident that without the actual x ray films it was difficult to determine whether some of the lower radial fractures were Colles or not Thus to ensure that reporting was complete a fourth category of fractures i e fractures of the lower radius other than Colles was included A total of 955 women experienced bone fractures 
483 and 472 in the placebo and tamoxifen groups respectively Fewer osteoporotic fracture events combined hip spine and lower radius occurred in women who received tamoxifen than in those who received placebo Overall 111 women in the tamoxifen group experienced fractures at one or more of these sites as compared with 137 women in the placebo group this represents a 19 reduction in the incidence of fractures a reduction that almost reached statistical significance RR 59 18 0 ס CI 50 1 36 0 ס Table 7 There was a 45 reduction in fractures of the hip RR 59 55 0 ס CI 52 0 ס 1 15 a 39 reduction in Colles fractures RR 59 16 0 ס CI 32 1 92 0 ס no reduction in other lower radial fractures RR 59 50 1 ס CI 15 1 37 0 ס and a 26 reduction in fractures of the spine RR 59 47 0 ס CI 23 1 14 0 ס The overall reduction was greater in the older age group ജ50 years at entry RR 59 97 0 ס CI 50 1 06 0 ס Vascular Events Women who experienced both a stroke and a transient ischemic attack or both a pulmonary embolism and a deep vein 
thrombosis were categorized according to the most severe event i e stroke or pulmonary embolism respectively While not statistically significant at the traditional level 95 CI the incidence of stroke increased from 24 events in the placebo group to 38 events in the tamoxifen group i e from 0 92 per 1000 participants per year in the former group to 1 45 per 1000 participants per year in the latter group Table 8 The RR was 1 59 and the 95 CI was 0 93 2 77 Fourteen of the 24 strokes that occurred in the placebo group were reported as being the result of vascular occlusion and six were considered to be hemor Journal of the National Cancer Institute Vol 90 No 18 September 16 1998 ARTICLES 1377 Downloaded from http jnci oxfordjournals org at University of Florida on September 1 2010 Fractures Table 6 Average annual rates of ischemic heart disease No of events Type of event Rate per 1000 women Placebo Tamoxifen Placebo Tamoxifen Risk ratio 95 confidence interval Myocardial infarction Fatal Nonfatal 28 8 20 31 7 24 
1 07 0 30 0 76 1 19 0 27 0 92 1 11 0 88 1 20 0 65 1 92 0 27 2 77 0 64 2 30 Severe angina 14 13 0 53 0 50 0 93 0 40 2 14 Acute ischemic syndrome 20 27 0 77 1 03 1 36 0 73 2 55 62 71 2 37 2 73 1 15 0 81 1 64 Total International Classification of Diseases codes 410 414 68 Requiring angioplasty or coronary artery bypass graft New Q wave on electrocardiogram without angina or elevation of serum enzymes or angina requiring hospitalization without surgery No of events Site of fracture Hip Spine Radius Colles Other lower radius Total ഛ49 y of age at entry ജ50 y of age at entry Rate per 1000 women Placebo Tamoxifen Placebo Tamoxifen Risk ratio 95 confidence interval 22 31 23 63 12 23 14 66 0 84 1 18 0 88 2 41 0 46 0 88 0 54 2 54 0 55 0 74 0 61 1 05 0 25 1 15 0 41 1 32 0 29 1 23 0 73 1 51 137 23 114 111 20 91 5 28 2 24 7 27 4 29 1 98 5 76 0 81 0 88 0 79 0 63 1 05 0 46 1 68 0 60 1 05 Excludes women who had a Colles fracture One woman had a hip fracture and a Colles fracture and one woman had a hip fracture and another 
lower radial fracture One woman had a hip fracture and a Colles fracture one woman had a hip fracture and a spine fracture and two women had hip fractures and other lower radial fractures Table 8 Average annual rates of vascular related events by age at study entry No of events Type of event by age at entry Rate per 1000 women Placebo Tamoxifen Placebo Tamoxifen Risk ratio 95 confidence interval Stroke ഛ49 y old ജ50 y old 24 4 20 38 3 35 0 92 0 39 1 26 1 45 0 30 2 20 1 59 0 76 1 75 0 93 2 77 0 11 4 49 0 98 3 20 Transient ischemic attack ഛ49 y old ജ50 y old 25 4 21 19 3 16 0 96 0 39 1 32 0 73 0 30 1 01 0 76 0 76 0 76 0 40 1 44 0 11 4 49 0 37 1 53 Pulmonary embolism ഛ49 y old ജ50 y old 6 1 5 18 2 16 0 23 0 10 0 31 0 69 0 20 1 00 3 01 2 03 3 19 1 15 9 27 0 11 119 62 1 12 11 15 Deep vein thrombosis ഛ49 y old ജ50 y old 22 8 14 35 11 24 0 84 0 78 0 88 1 34 1 08 1 51 1 60 1 39 1 71 0 91 2 86 0 51 3 99 0 85 3 58 Seven cases were fatal three in the placebo group and four in the tamoxifen group Three cases in the 
tamoxifen group were fatal All but three cases in each group required hospitalization rhagic in origin The etiology of four was unknown Two deaths occurred in women who had the occlusive type and one death occurred in a woman who had a stroke that was hemorrhagic in origin Of the 38 strokes that occurred in the group receiving tamoxifen 21 were occlusive 10 were hemorrhagic in origin and seven were of unknown etiology Three of the hemorrhagic strokes were fatal One death occurred among the seven women who experienced stroke of unknown etiology Thus three of the 1378 ARTICLES deaths that occurred in the placebo group and four that occurred in the tamoxifen group were related to stroke When the distribution of strokes was examined according to age the number of events in women aged 49 years or younger was similar i e four in the placebo group and three in the tamoxifen group Among women aged 50 years or older 20 strokes occurred in those who received placebo and 35 in those who received tamoxifen In that age 
group the RR was 1 75 and the 95 CI was 0 98 3 20 Journal of the National Cancer Institute Vol 90 No 18 September 16 1998 Downloaded from http jnci oxfordjournals org at University of Florida on September 1 2010 Table 7 Annual rates for fracture events among participants Cataracts More than 1 5 years before the trial was stopped and the treatment assignments were unblinded October 1996 the ERSMAC released information to the NSABP leadership with regard to an excess risk of cataracts and cataract surgery observed among women in the tamoxifen group The NSABP leadership then informed officials of the NCI the Office for Protection From Research Risks and the principal investigators and participants in the trial It was also provided by the NCI to chairpersons of the local Institutional Review Boards responsible for oversight of all breast cancer treatment trials in which tamoxifen was administered The status regarding these outcomes at the time of this analysis is summarized in Table 9 Information on the 
development of cataracts was based on unconfirmed self reporting However information regarding cataract surgery was verified and documented by examination of medical records The rate of cataract development among women who were cataract free at the time of randomization was 21 72 per 1000 women in the placebo group and 24 82 per 1000 women in the tamoxifen group This represents an RR of 1 14 with CIs that indicate marginal statistical significance 95 CI 92 1 10 1 ס There was also a difference by treatment group with respect to cataract surgery In the placebo and tamoxifen groups the rates of developing cataracts and undergoing cataract surgery were 3 00 and 4 72 per 1000 women respectively RR 59 75 1 ס CI 41 2 61 1 ס A total of 943 women reported having cataracts at entry into the study The RR of cataract surgery in these women was similar to that experienced by women who developed cataracts after randomization This excess risk was observed primarily among women in the older age group Quality of Life At each 
follow up visit participants were evaluated relative to tamoxifen related non life threatening side effects that could affect their quality of life Information was collected with regard to the occurrence of hot flashes vaginal discharge irregular menses fluid retention nausea skin changes diarrhea and weight gain or loss A self administered depression scale developed by the Center for Epidemiological Studies CES D 36 was used to estimate the relation of tamoxifen to the occurrence of mental depression Also self reported at each visit were data from the Medical Outcomes Study Short Form 36 MOSSF 36 and the Medical Outcomes Study MOS Sexual Functioning Scale 37 The only symptomatic differences noted between the placebo and tamoxifen groups were related to hot flashes and vaginal discharge both of which occurred more often in the latter group Table 10 The proportion of women who reported hot flashes as being quite a bit or extremely bothersome was 45 7 in the tamoxifen group as compared with 28 7 in the placebo 
group The proportion reporting vaginal discharge that was moderately bothersome or worse was 29 0 in the tamoxifen group as compared with 13 0 in the placebo group There were no notable differences between the two groups relative to any of the findings obtained from the various self reporting instruments Of particular note are the findings for depression scores determined from the CES D scale The distribution of participants in the two groups according to the various levels of clinical depression was almost identical The highest depression score observed was less than or equal to 15 for 65 4 of the women in each group and the proportion of women with a score that was greater than or equal to 30 was 9 0 in the placebo group and 8 8 in the tamoxifen group The findings regarding quality of life will be presented in a subsequent publication Causes and Demographics of Deaths Seventy one deaths occurred among participants in the placebo group and 57 occurred among women in the tamoxifen group RR 59 18 0 ס CI 61 1 
65 0 ס Table 11 Forty two deaths in the placebo group and 23 deaths in the tamoxifen group were due to cancer Aside from the breast uterus ovary and lung a small number of deaths were related to cancer occurring at a variety of other sites such as the brain colon pancreas Table 9 Average annual rates of cataracts and cataract surgery among participants No of women Status of participants Without cataracts at randomization Developed cataracts Developed cataracts and underwent cataract surgery Rate per 1000 women Placebo Tamoxifen Placebo Tamoxifen Risk ratio 95 confidence interval 6131 507 73 6101 574 114 21 72 3 00 24 82 4 72 1 14 1 57 1 01 1 29 1 16 2 14 Journal of the National Cancer Institute Vol 90 No 18 September 16 1998 ARTICLES 1379 Downloaded from http jnci oxfordjournals org at University of Florida on September 1 2010 Twenty five transient ischemic attacks occurred in the placebo group and 19 in the tamoxifen group Table 8 Pulmonary emboli were observed in almost three times as many women in the 
tamoxifen group as in the placebo group 18 versus six RR 59 10 3 ס CI 72 9 51 1 ס Table 8 When the incidence of pulmonary embolism was related to the age of participants there was an increase in those events in postmenopausal women who received tamoxifen In women aged 49 years or younger one event occurred in the placebo group and two events occurred in the tamoxifen group RR 59 30 2 ס CI ס 0 11 119 62 in contrast in those aged 50 years or older five events occurred in the former group and 16 in the latter group RR 59 91 3 ס CI 51 11 21 1 ס More women who received tamoxifen developed deep vein thrombosis than did women who received placebo 35 versus 22 cases respectively Table 8 The average annual rates per 1000 women were 1 34 versus 0 84 RR 59 06 1 ס CI 19 0 ס 2 86 The excess risk appeared to be greater among women aged 50 years or older For women aged 49 years or younger the number of cases was eight in the placebo group versus 11 in the tamoxifen group RR 59 93 1 ס CI 99 3 15 0 ס In women 50 years of age 
or older the number of cases was 14 versus 24 with an RR of 1 71 95 CI 85 3 58 0 ס Table 10 Distribution of participants in the placebo and tamoxifen groups by highest level of hot flashes vaginal discharge and depression reported of participants Tamoxifen n 6646 ס Hot flashes bothersome No Slightly Moderately Quite a bit Extremely 31 4 18 2 21 7 18 6 10 1 19 4 14 1 21 8 28 1 17 6 Vaginal discharge bothersome No Slightly Moderately Quite a bit Extremely 65 2 21 8 8 5 3 3 1 2 44 8 26 2 16 6 9 3 3 1 Depression CES D 0 15 16 22 23 29 30 36 ജ37 65 4 16 1 9 5 5 4 3 6 65 4 15 6 10 1 5 1 3 7 The quality of life questionnaire that was used was a self reporting instrument Some participants opted not to complete the questionnaires Thus information is not available for 101 women in the placebo group and 110 in the tamoxifen group CES D refers to a self administered depression scale developed by the Center for Epidemiological Studies 36 Table 11 Distribution of causes of death No of deaths Cause Placebo Tamoxifen Cancer 
Brain Breast Colon Uterus endometrium Lung Ovary Lymphatic system Pancreas Extrahepatic bile duct Kidney Melanoma Thyroid gland Primary site unknown 42 3 6 1 1 11 1 4 6 1 2 0 1 5 23 1 3 1 0 8 2 2 2 0 0 1 0 3 Cardiac and vascular disease Heart disease ischemic and other Stroke Pulmonary embolus Arterial disease 15 12 3 0 0 22 13 4 3 2 Other Amyotrophic lateral sclerosis Automobile accident Miscellaneous 11 different causes Unknown 14 2 2 6 4 12 0 1 7 4 Total deaths Average annual rate per 1000 women Risk ratio 95 confidence interval 71 2 71 0 81 0 56 1 16 57 2 17 1380 ARTICLES DISCUSSION Although in the past consideration had been given to primary prevention the aim of which was to prevent cancer by identifying and eliminating cancer causing agents and to secondary prevention which involved screening individuals at increased risk for cancer in the hope that early detection and treatment would affect survival it was not until the mid 1980s that serious attention was given to chemoprevention an approach aimed 
at reducing cancer risk by the administration of natural or synthetic clinical compounds that prevent reverse or suppress carcinogenesis in individuals at increased risk for the disease 38 Although biologic and clinical considerations related to chemoprevention have received much attention 39 41 almost no studies have been directed toward evaluating the concept as it relates to breast cancer Although information obtained in the l980s provided support for the theory that dietary fat might be associated with the occurrence of breast cancer and that restricting fat intake could perhaps reduce the incidence of the disease 42 a trial to test that hypothesis has only recently been implemented The use of retinoids for the prevention of breast cancer began to receive attention in 1987 when a study was initiated to evaluate the effectiveness of fenretinide 4 HPR 43 To date as far as we are aware no information with regard to breast cancer end points has been reported from that trial The findings in this article 
provide the first information from a randomized clinical trial to support the hypothesis that breast cancer can be prevented in a population of women at increased risk for the disease They show that tamoxifen administration reduced the risk of invasive and noninvasive breast cancers by almost 50 in all age groups Of particular importance is the finding that a benefit from tamoxifen was identified among women at various levels of risk within the spectrum of risks associated with participants in the P 1 study Because of the importance of knowing whether or not the finding that tamoxifen reduces the incidence of tumors can be generalized to all women extensive effort was directed toward recruiting nonwhite participants Despite great effort the number of nonwhite participants was small and there were few events among those women For these reasons the size of the treatment effect estimated from the total population 49 reduction of breast cancer risk may not be a reliable estimate for nonwhite women Also of 
importance are the findings obtained in women who had a history of LCIS or atypical hyperplasia pathologic entities thought to increase the risk of invasive breast cancer Although the present study was not designed to address these issues it provides the only quantitative information available from a clinical trial about the magnitude of the risk of invasive cancer in women with a reported history of LCIS or atypical hyperplasia Journal of the National Cancer Institute Vol 90 No 18 September 16 1998 Downloaded from http jnci oxfordjournals org at University of Florida on September 1 2010 Placebo n 8946 ס Symptom thyroid gland and kidney Fifteen deaths in the placebo group and 22 deaths in the tamoxifen group were from causes related to the vascular system Four women died of stroke in the tamoxifen group whereas three women died of stroke in the placebo group Two women in the tamoxifen group and none in the placebo group died of arterial disease other than stroke Three women in the tamoxifen group and none in 
the placebo group died as a result of pulmonary embolism employing the drug for the treatment of breast cancer was inconclusive The P 1 study findings that failed to demonstrate that tamoxifen reduced the risk of and mortality from ischemic heart disease differ from those obtained in the Stockholm 48 and the Scottish 49 studies in which it was reported that tamoxifen reduced cardiac morbidity in breast cancer patients These findings are similar however to those observed in the NSABP B 14 trial In that study 50 although there was a trend that suggested the possibility of such an effect statistically significant differences in cardiovascular mortality were not observed in tamoxifen treated patients Thus although tamoxifen can improve lipid profiles its effect on the reduction of cardiovascular disease in women taking the drug remains uncertain While the current findings suggest that tamoxifen does not play a role in preventing ischemic heart disease they do show that at least during the duration of the P 1 
study the drug did not have a detrimental effect on the heart One of the original aims of the P 1 study was to determine whether tamoxifen reduced the risk of fractures of the hip radius Colles and spine The current findings indicating a 45 39 and 26 reduction in fractures at those sites cannot be viewed as inconsequential When considered in light of the estimate made in 1990 that 24 million American women suffer from osteoporosis that 1 3 million fractures per year occur secondary to that disease and that the estimate of the cost of treating such patients is 6 1 billion per year the prevention of fractures is important for women at increased risk for breast cancer who are also at risk for osteoporosis as they age 51 Because the findings with regard to fractures are based on a relatively small number of events definitive conclusions relative to the effect of tamoxifen on the rate of fractures must await additional information Whether the benefit achieved from tamoxifen in the P 1 study was due to the drug s 
interference with the initiation and promotion of tumors or to hindrance of the growth of occult tumors is unknown Because it is likely that a broad spectrum of molecular biologic and pathologic changes in breast tissue existed among participants at the time of their entry into the trial it might be assumed that both mechanisms were responsible for the finding Nonetheless the absence of specific information to resolve the issue does not detract from the evidence indicating that tamoxifen did in fact prevent the clinical expression of tumors i e the goal of primary disease prevention The length of tamoxifen administration is another concern It has been speculated that tamoxifen administration for only 5 years may merely delay tumor growth for a short time and that if the drug fails to affect the process of tumor initiation and promotion tumors will subsequently appear In view of the time required for a tumor to become clinically evident another concern that has been raised is that the administration of 
tamoxifen for only 5 years may be inadequate Information from NSABP B 14 which indicated that the benefit from 5 years of tamoxifen administered to women with stage I ER positive tumors remained through 10 years of follow up fails to support that concern 52 Since the findings in that study also demonstrated that more than 5 years of tamoxifen did not enhance the drug s effect in the P 1 study the drug was administered for only 5 years However additional studies with more prolonged tamoxifen administration and follow up time are necessary before a hypothetical issue such as this one can be resolved Journal of the National Cancer Institute Vol 90 No 18 September 16 1998 ARTICLES 1381 Downloaded from http jnci oxfordjournals org at University of Florida on September 1 2010 and presents the only information to demonstrate that tamoxifen can reduce the magnitude of that risk When compared with women who had no history of LCIS or atypical hyperplasia the finding of a 100 increase in the average annual rate of 
invasive cancer among women in the placebo group who had a history of LCIS and of a nearly 57 increase in this rate among women with a history of atypical hyperplasia clearly indicates that these pathologic entities are associated with a substantial increase in a woman s risk for invasive breast cancer Even more important is the finding that tamoxifen administration dramatically reduced the risk of invasive cancer in women with a history of LCIS or atypical hyperplasia Although the findings indicating the extent to which the invasive cancer risk was reduced are compelling the occurrence of a 50 reduction in the risk of noninvasive breast cancer is equally important for the following reasons The expanded use of mammography has resulted in the more frequent detection of DCIS In view of the cost involved and the effort required to diagnose these tumors and in light of the debate about both the initial and subsequent treatment of patients with DCIS and the putative relationship between DCIS and the subsequent 
occurrence of invasive breast cancer a reduction in the risk of DCIS must be viewed as an important finding since prevention of that disease would obviate the above considerations Moreover the reduction in the incidence of DCIS provokes consideration of the biologic significance of that finding Cells comprising most DCIS lesions have been demonstrated to be ER positive 44 45 Consequently if DCIS is indeed a precursor of invasive cancer at least some of the invasive tumors that were prevented by tamoxifen in the P 1 study could be the result of the elimination of occult DCIS by the drug In that regard the findings regarding the characteristics of the invasive breast cancers that occurred among the participants in the P 1 study are of importance When the findings from tumors that occurred in the two groups were compared it was observed that in the tamoxifen group there was a decreased rate of invasive cancers that were ER positive that were 2 0 cm or less in size or that were associated with negative lymph 
nodes These observations provide insight relative to the biologic nature of the tumors that were prevented These findings are consistent with the thesis that the benefit from tamoxifen results from its inhibition of the growth and progression of tumors that are ER positive i e those that are more likely to exhibit slower growth and less likely to be associated with axillary nodal involvement It is also of interest that LCIS and atypical hyperplasia are most often ER positive 46 47 and that there was a marked reduction in tumors that occurred in women with a history of those lesions In view of these findings a question to be answered relates to when cells in the biologic cascade of events leading from tumor initiation to the phenotypic expression of invasive tumors express their ER status and thus may be affected by tamoxifen Although the P 1 study was not designed to have the power to evaluate specifically the hypothesis that tamoxifen reduced the rate of heart disease a secondary goal was to obtain 
information regarding the incidence of fatal and nonfatal myocardial infarctions When the study was being designed there was evidence that tamoxifen altered lipid and lipoprotein metabolism 22 26 However information about tamoxifen s effect on the cardiovascular system that had been obtained from clinical trials 1382 ARTICLES ment of cataracts among women who were cataract free at the time of randomization An increase in the rate of cataracts was found in the tamoxifen group We do not consider the ophthalmic toxicities from tamoxifen administration sufficiently great to warrant withholding the drug from women such as those who participated in the P 1 trial Finally as we 10 62 and others 63 64 have noted in previous investigations certain vascular related events reported in the P 1 study were more frequent in older women who received tamoxifen than in those who received placebo While there was an overall increase in the average annual rate of stroke in women 50 years of age or older uncertainty exists 
regarding the mechanism responsible for these results There is also uncertainty regarding the cause of death in women who had a pulmonary embolism Although three deaths were reported as being due to pulmonary embolism all were associated with comorbid conditions that could have accounted for those deaths On the basis of the P 1 findings and this commentary it is necessary to consider the question of who should receive tamoxifen to decrease their risk of breast cancer The findings in this article indicate that women 50 years of age or younger who would have been eligible for the P 1 study are candidates for the drug Similarly women with a history of LCIS or atypical hyperplasia and postmenopausal women at high risk for breast cancer who have had a hysterectomy should be considered eligible for tamoxifen Women who have a history of DCIS may also be appropriate candidates for tamoxifen Findings from other NSABP trials B 17 and B 24 have demonstrated that the risk for an invasive breast cancer in women with 
localized DCIS is at least as high if not higher than that for women with a history of LCIS In the current study women in the placebo group who had a history of LCIS had an annual rate per thousand for breast cancer of 12 98 The annual rate of invasive cancer among women who underwent lumpectomy for DCIS was 23 7 B 17 and among those treated with lumpectomy and radiation therapy it was 14 4 B24 In both of those studies the risk of developing an invasive cancer was considerable That risk could be substantially reduced by tamoxifen administration Another group of women who might also be candidates for tamoxifen are those at high risk for breast cancer because they carry BRCA1 or BRCA2 genetic mutations In the P 1 study blood that was obtained from participants for the conduct of future scientific investigations is now being used to determine how many of them had these mutations and whether tamoxifen decreased their breast cancer risk While that information is as yet unavailable offering women who carry these 
mutations the option of taking tamoxifen may be considered since doing so provides an alternative to bilateral mastectomy Many women 50 years of age or older who have stopped menstruating have not had a hysterectomy and have no history of LCIS DCIS or atypical hyperplasia may also be eligible for tamoxifen The decision relative to which of these women should or should not receive tamoxifen for breast cancer prevention is complex The primary determinant for making such a decision relates to each woman s projected risk for breast cancer The higher the risk the more likely that tamoxifen would confer a benefit Women whose breast cancer risk is sufficiently Journal of the National Cancer Institute Vol 90 No 18 September 16 1998 Downloaded from http jnci oxfordjournals org at University of Florida on September 1 2010 Another question that has been raised by the study results relates to the timing of tamoxifen administration In women at sufficient risk for receiving the drug the issue of timing should not be 
considered critical On the other hand it is likely that the biologic changes that occurred in breast cells were present when participants who subsequently developed tumors were enrolled in the trial Consequently it is not unexpected that such tumors began to be diagnosed early in the follow up period Thus it does not seem justified to delay administration of the drug to women such as those in the P 1 study who were at increased risk for breast cancer It is appropriate to consider whether the benefit from tamoxifen in reducing the incidence of breast cancer is sufficiently great to justify its use as a chemopreventive agent despite the risk of undesirable side effects From the onset of the P 1 study there has been considerable emphasis on the adverse effects of tamoxifen particularly with regard to endometrial cancer and vascular related toxic effects which predominate in postmenopausal women Recent reviews and individual studies of the relationship between tamoxifen and endometrial cancer indicate that the 
concern with regard to the level of excess risk of endometrial cancer may have been exaggerated and that when endometrial cancers do occur in women who receive tamoxifen they have as favorable a prognosis as those in women who do not receive the drug or who receive estrogen replacement therapy 53 57 In the P 1 trial the average annual rate of invasive endometrial cancer in women 50 years of age or older who received tamoxifen was similar to what we had noted in the B 14 trial i e about 2 per 1000 women per year Of particular importance are the observations in this study that refute the claim that endometrial cancers occurring in tamoxifen treated women are more aggressive are less easily manageable and cause more deaths than endometrial cancers that occur in non tamoxifen treated women or in those who have received hormone replacement therapy 58 There is no evidence either from this study or from any other NSABP trial 59 60 to support those contentions To date all of the invasive endometrial cancers noted in 
the P 1 study in women who received tamoxifen were FIGO stage I i e localized tumors Thus our findings fail to show that such tumors carry an unfavorable prognosis Nonetheless because of the increased risk of endometrial cancer women receiving tamoxifen should have regular gynecologic examinations and should see their physicians if they experience abnormal vaginal bleeding Reports have appeared about the dangers of liver damage hepatoma colon cancer and retinal toxicity resulting from tamoxifen administration As the findings in this article and in reports from other NSABP studies attest such concerns have not been substantiated To date no primary liver cancers have been reported in the P 1 trial and no increase in the incidence of either colon or any other second cancer other than cancer of the uterus has been observed Also no differences in the self reporting of macular degeneration were observed 59 cases in the placebo group and 60 cases in the tamoxifen group Reports suggesting that tamoxifen 
administration might be associated with ocular changes led to the conduct of a Tamoxifen Ophthalmic Evaluation Study in NSABP B 14 A recent report 61 from that study indicated that no cases of vision threatening toxicity occurred among tamoxifen treated women although posterior subcapsular opacities were more frequently observed in that group In this article information is presented relative to the develop It is likely that the paucity of events in the European studies was due to the relatively small number of participants and to the fact that the risk of breast cancer occurring among women in these trials was lower than that among participants in the P 1 trial Because the criteria used for selecting participants in the Italian and the British studies were different from those used in the P 1 trial women in those studies had a different risk for breast cancer than did P 1 trial participants in that the expected proportion of ERnegative tumors could have been higher in them This difference is important 
because tamoxifen is unlikely to prevent the occurrence of ER negative tumors The true statistical power of a study to detect an effect of tamoxifen would be a function of the number of tumors that are ER positive rather than a function of the total number of breast cancer events Thus if the expected proportion of ER negative tumors is high then the ability to show an effect of tamoxifen would be substantially reduced since the statistical power that is based on the total number of events would be diminished The fewer the number of events the more likely it is that this reduction in statistical power is a critical factor affecting the ability to detect a difference between the study groups Noncompliance is another factor that affects the ability to detect differences since it will result in a decrease of the anticipated effect of a drug The rates of noncompliance were appreciable in the European trials With small numbers of participants and relatively small numbers of events as occurred in those trials a 
high level of noncompliance will result in a substantial reduction in the likelihood of identifying a treatment effect In the P 1 study a high rate of noncompliance was used for samplesize estimates 10 per year of follow up Thus the sample size was planned to be sufficiently large to preserve adequate power even in the presence of a high rate of noncompliance Perhaps the most important reason for the failure of the European studies to provide an adequate test of tamoxifen s effect could be due to the fact that 41 of the women in the British trial and 14 in the Italian study received hormone replacement therapy This introduced a potential confounding factor that could have interfered with testing of the hypothesis that gave rise to the conduct of both trials The use of hormone replacement therapy was considered to be a protocol violation in the P 1 trial Until a clinical trial evaluating the efficacy of using tamoxifen with hormone replacement therapy is conducted it is difficult to assess the relevance of 
findings from trials using that regimen The issue has been raised that the P 1 trial was stopped prematurely and that the findings were reported too early The trial was stopped only when the independent monitoring committee for that study ERSMAC on the basis of stopping rules established before the onset of the trial concluded that the primary study hypothesis had been confirmed beyond a reasonable doubt i e that tamoxifen decreased the incidence rate of invasive breast cancer P 00001 It was concluded that additional follow up would not have resulted in improved estimates of treatment effects that would have justified withholding from the participants on placebo the knowledge that tamoxifen was an effective prophylactic agent This allows those women on placebo to consider taking tamoxifen While additional studies are needed to address the issues that have arisen as a result of our findings we consider it highly inappropriate to not offer tamoxifen to women who are similar to those in the P 1 study and who 
may benefit from its use as a breast cancer preventive agent Journal of the National Cancer Institute Vol 90 No 18 September 16 1998 ARTICLES 1383 Downloaded from http jnci oxfordjournals org at University of Florida on September 1 2010 high to offset the potential detrimental effects of tamoxifen would be candidates for the drug However women whose breast cancer risk is not as high should evaluate their individual benefits and risks with their physicians in order to make an informed decision with regard to the use of tamoxifen One way in which the benefit from tamoxifen can be estimated is to subtract the overall number of unfavorable events from the overall number of cancers prevented Whether such a risk benefit analysis is appropriate in deciding if tamoxifen should be used in the prevention setting is questionable It seems inappropriate to view an endometrial cancer as being equivalent to a breast cancer since when endometrial cancers occur in women who receive tamoxifen they are most often curable by 
hysterectomy and the mortality rate is minimal Consequently in the P 1 study the breast cancers that would have occurred had tamoxifen not been used would have resulted in an estimated mortality rate that would likely have been higher than that observed from the undesirable effects of the drug Moreover the morbidity after hysterectomy would likely have been less than that resulting from the surgery radiation chemotherapy and tamoxifen used to treat the unprevented breast cancer Tools that can be used for determining a woman s breast cancer risk and the net effect from tamoxifen when used to prevent breast cancer are currently being developed As has been observed with the successive use of newer chemotherapeutic agents for the treatment of breast cancer it is likely that new prevention agents will improve upon the benefits achieved with tamoxifen The new NSABP chemoprevention trial P 2 represents such an effort That trial will compare the toxicity risks and benefits of the selective ER modulator SERM 
raloxifene with those of tamoxifen Raloxifene which has been shown to prevent osteoporosis will be evaluated in postmenopausal women to determine its value in preventing breast cancer without increasing the risk of endometrial cancer 65 The uncertainty of the clinical application of the current findings is analogous to uncertainties related to the use of systemic adjuvant therapy for breast cancer With each demonstration of the worth of such therapy questions continue to arise as to who should receive the treatment i e who will benefit and who will not who will not need the therapy because they will never demonstrate a treatment failure how much of a benefit is worthwhile and whether or not the toxicity and mortality encountered justify its administration Despite these uncertainties the use of adjuvant therapy was considered to be a major advance in the treatment of early stage breast cancer The use of a chemopreventive agent denotes a similar advance in that it is being employed at an even earlier stage i e 
during the origin and development of a phenotypically expressed cancer before its diagnosis Before submission of this article for publication the results of two European studies were published 66 67 that failed to confirm the P 1 study findings None of the information presented in them alters our conclusion that tamoxifen significantly reduces the probability of breast cancer in women at increased risk for the disease The three studies are too dissimilar in design population enrolled and numerous other aspects to permit making valid comparisons among them For a variety of reasons it is unlikely that the European studies provided an adequate test of tamoxifen s effectiveness as a preventive agent There were relatively few breast cancer events 70 in the British trial and 49 in the Italian study as compared with 368 events in the P 1 study Appendix A Clinical centers participating Name of center 1384 ARTICLES Program coordinator A Desai N Wolmark J Hamm D Alberts C Geyer Jr J Lesesne F Brescia C Austin G Elias 
L Schwartzberg T Gaskin III N Abramson A Nafziger J Luce M Grant M Prout R Graham U Kuusk A Hatfield K Wright J Wade J Kirshner S Jubelirer L Wagman P Raich K Lanier L Laufman P DeFusco R Myers J Mailliard A Lees J Garber J Ross H Gross L Sutton R Dalton D Lannin P Brooks S Standiford N Robert M Daly J Evans C Isaacs D Mastrianni D King J Giguere A Arnold R Chlebowski R Zera P Loehrer J Ward A Benson III S Taylor R Margolese J Polikoff M Gittleman C Forsthoff C Presant D Booser V Stark Vancs T Frazier D Bowman J Hoehn R Wheeler T Wozniak C Desch A Heerdt E Barksdale D Gosik B MacCracken H Fritz P Hagan Jones R Hallett C Shulman P Remke S Honts T Stewart K Hawkins P Stokes L Stragand M Milian Menendez B Quast L Pottier C Mullen L Fearn L Foster P Brockschink S Shonkwiler K Shedlock E Javins D Hooks C McElfatrick L Birenbaum S Oxley J Kulko S Neville C Reed C Danbrook B Cahoon J Strohbehn E Craddick K Warren D Swan M Edwards A Hayes Crosby T Schulte M Dittberner J James M Lamey J Dritschilo P Maksymik P Wade J 
Martin S Holohan V Marsoobian L Tatro F Monaco C Walker S French P Wellmann N Warrington M Wolgast E Ladd F Magy M Aldana D Weber E Fisher L O Neill K McDonald N Goldberg B Roedig A Steele G Parker R Gross Journal of the National Cancer Institute Vol 90 No 18 September 16 1998 Downloaded from http jnci oxfordjournals org at University of Florida on September 1 2010 Albert Einstein Cancer Center Philadelphia PA Allegheny Cancer Center Network Pittsburgh PA Alliant Health System Louisville KY Arizona Cancer Center Tucson Arrington Cancer Research and Treatment Center Lubbock TX Atlanta Breast Cancer Prevention Program Atlanta Community Women s Health Project GA Atlanta Regional CCOP Baltimore Clinical Center MD Baptist Cancer Institute CCOP Memphis TN Baptist Health System Birmingham AL Baptist Regional Cancer Institute Jacksonville Bassett Hospital Cooperstown NY Bay Area Cancer Control Consortium CA Baylor Sammons Cancer Center Dallas TX Boston University Medical Center MA Breast Health Center New England 
Medical Center Boston MA British Columbia Cancer Agency Vancouver Carle Cancer Center CCOP Urbana IL Cedar Rapids Oncology Project CCOP IA Central Illinois CCOP Springfield Central New York Group Syracuse Charleston Morgantown Groups WV City of Hope Duarte CA Colorado Cancer Research Program CCOP Denver Columbia River CCOP Portland OR Columbus CCOP OH Connecticut Task Force Hartford Credit Valley Hospital Mississauga ON Creighton Cancer Center Omaha NE Cross Cancer Institute Edmonton AB Dana Farber Consortium Boston MA Dartmouth Hitchcock Medical Center Lebanon NH Dayton Clinical Oncology CCOP OH Duke University Medical Center Durham NC Duluth CCOP MN E Carolina University Greenville NC E Maine Medical Center Bangor Ellis Fischel Cancer Center University of Missouri Columbia Fairfax Hospital Falls Church VA Fox Chase Cancer Center Philadelphia PA Geisinger Breast Clinic of Danville PA Georgetown University Lombardi Cancer Center Washington DC Glens Falls Hospital Cancer Program NY Greater Phoenix CCOP AZ 
Greenville CCOP SC Hamilton Regional Cancer Center ON Harbor UCLA Torrance CA Hennepin County Medical Center Minneapolis MN Hoosier Oncology Group Indianapolis IN Huntsman Cancer Institute Salt Lake City UT Illinois Cancer Center Chicago IL Illinois Masonic Cancer Center Chicago Jewish General St Mary s Montreal PQ Kaiser Permanente CCOP San Diego CA Lehigh Valley Hospital Allentown PA Long Beach Memorial Cancer Institute CA Los Angeles Oncologic Institute CA M D Anderson Cancer Center Houston TX M D Anderson Cancer Network Ft Worth TX Main Line Health System CCOP Wynnewood PA Manitoba Cancer Foundation Winnipeg MB Marshfield Clinic CCOP WI Mayo Clinic CCOP Scottsdale AZ Medical Center of Delaware CCOP Wilmington Medical College of Virginia MBCCOP Richmond Memorial Sloan Kettering Cancer Center New York NY Principal investigator Appendix A continued Clinical centers participating Name of center Program coordinator M Hyzinski P Flynn J Paradelo W Donegan D Myers N Dimitrov R Bornstein M Wallack R Rosenbluth L 
Weiselberg I Pierce C Kardinal W Farrar K Boatman L Deschenes R Clarfeld S Edge H Shibata J Wolter E Davila J Ellerton R Farah W Caceres K Kimmey L Tschetter J Atkins G Schnetzer III A Greco Jr R Levitt S Tchekmedyian R Carlson M Osborne V Caggiano E Cobos T Panella P Schaefer A Paterson P Goss J Carpenter Jr S Klimberg J Goodnight Jr P Ganz R Arenas B Aron R Oishi P Jochimsen W Jewell E Romond L Baker A Robidoux S Bernard A Mangalik M Torosian M Conrad A Cruz J Stewart R Sticca D Spicer D Krag V Vogel III M Simon H Hynes S Wilks L Lickley V Pauli A Deshler M Goodpaster J Jensen S Hall C Robins L Mamounas M Montegari J Behr D Mayberry K Hart M Bateman J Bennett M Watson A Christen J Machia P Burke R Santos M Escobar F Cenciarelli K VanWagenen J Atchley D Cuadrado E Lagow J Norman R Burgess S Segler C Antinora D Pilon D Jackson C Schurman R Weiss L Ayer Rand S Dixon J Rodgers D Frie A Hades J Smith L Crosby M Colvert L Clawson B Kahn B Bulliner A Kelminski M Spaight E Spizman M Ashki B Golden L Robitaille B 
Kaluzny A Parsons P O Neill M Grove I Presas T Fass K Queen E Sales S Dion L Robertson C Kresge M Good B Chaparro M Oldfield CCOP ס Community Clinical Oncology Program MBCCOP ס Minority Based Community Clinical Oncology Program CGOP ס Cooperative Group Outreach Program Journal of the National Cancer Institute Vol 90 No 18 September 16 1998 ARTICLES 1385 Downloaded from http jnci oxfordjournals org at University of Florida on September 1 2010 Mercy Hospital CCOP Scranton PA Metro Minnesota Center St Louis Park Midwest BCPT Kansas City MO Milwaukee Group WI Montana Group Billings Michigan State University East Lansing MI N E Ohio BCPT Group Cleveland New York Consortium St Vincent s Hospital Guttman N New Jersey CCOP Hackensack N Shore University Hospital CCOP Manhasset NY N W Virginia Mason CCOP Tacoma WA Ochsner CCOP New Orleans LA Ohio State James Cancer Hospital Columbus Oklahoma City Consortium OK Project for Prevention of Cancer Sein PQ Puget Sound Oncology Consortium Seattle WA Roswell Park Cancer 
Institute Buffalo NY Royal Victoria Hospital Montreal PQ Rush Presbyterian St Luke Medical Center Chicago IL S Florida Group Miami Beach S Nevada CCOP Las Vegas San Joaquin Valley CGOP Fresno CA San Juan MBCCOP Puerto Rico Scott White Texas A M Temple TX Sioux Community Cancer Consortium CCOP Sioux Falls SD Southeast Cancer Control Consortium CCOP Winston Salem NC St Francis Program CCOP Tulsa OK St Louis Cape Girardeau CCOP MO St Luke s Hospitals CCOP Fargo ND St Mary Long Beach Community CA Stanford University Palo Alto CA Strang Cancer Prevention Center New York NY Sutter California Healthcare System Center Sacramento CA Texas Tech University Health Sciences Center Southwest Cancer Center Lubbock Thompson Cancer Center Knoxville TN Toledo CCOP OH Tom Baker Cancer Centre Calgary AB Toronto Hospital Breast Group ON University of Alabama at Birmingham University of Arkansas for Medical Science Little Rock University of California Davis Cancer Center Sacramento University of California Los Angeles Center for 
Health Sciences University of Chicago IL University of Cincinnati Medical Center OH University of Hawaii Honolulu University of Iowa Iowa City University of Kansas Kansas City University of Kentucky Consortium Lexington Clinic University of Michigan Ann Arbor University of Montreal PQ University of North Carolina Chapel Hill University of New Mexico Cancer Center Albuquerque University of Pennsylvania Cancer Center Philadelphia University of South Alabama MBCCOP Mobile University of Texas Health Science Center San Antonio University of Wisconsin Comprehensive Cancer Center Madison Upstate Carolina CCOP Spartanburg SC USC Norris Comprehensive Cancer Center Los Angeles CA Vermont Cancer Center University of Vermont Burlington W Pennsylvania Project Pittsburgh PA Wayne State University Detroit MI Wichita CCOP KS Wilford Hall Medical Center TX Women s College Hospital Toronto ON Principal investigator Appendix B The following key personnel were involved in the planning implementation conduct and analysis of the 
National Surgical Adjuvant Breast and Bowel Project NSABP Breast Cancer Prevention Trial BCPT BCPT Steering Committee Jeffrey Abrams Nancy Brinker Susan Braun Walter Cronin Mary Daly Nikolay V Dimitrov Bernard Fisher John Flack Leslie Ford Patricia Ganz Charles Geyer Jr Andrew Glass William Harlan Elizabeth Hart Brian Henderson Martin Abeloff Michele Carter Theodore Colton Polly Feigl Laurence Freedman Lawrence Friedman Elsie Anderson Judith Bingham Karen Brennan Barbara Capuzelo Mary Ellen Gorman Sandra Kanicki Elizabeth Lee Titas Marquez Jeannie Morice Robin Burgess Anita Hades Donna Jackson Joan James Elisabeth Ladd Deborah Lifsey Joelle Machia Mary Pat Matisko Nancy Morton Stewart Anderson Alan Burshell Sol Epstein C Conrad Johnston Jr Carl Kardinal Joan James C Conrad Johnston Jr Carl Kardinal Maureen Kavanah Joan McGowan Richard Margolese Carol Redmond Andre Robidoux Phillip Stott Victor Vogel D Lawrence Wickerham H Samuel Wieand Norman Wolmark Endpoint Review Safety Monitoring and Advisory Committee 
ERSMAC Barbara Hulka Howard Judd Elliot Rapaport Carol Redmond Barbara Tilley Marty Smith Romenza Kaye Thomas Lonnie Williams Helene Wilson Barbara Simonick Connie Szczepanek Diane Weber Marilyn Zack Joan McGowan Janet Wolter Harvey Schipper Sally Schumaker Victor Vogel John Ware Jr D Lawrence Wickerham H Samuel Wieand Participant Advisory Board Beverly Munn Rici Rutkoff Mary Sankolewicz Gwendolyn Parker Crystal Rabbas Sidney Shonkwiler Osteoporosis Committee Robert Lindsay Quality of Life Committee David Cella Walter Cronin Richard Day Patricia Ganz Jean Clause Lasry Elizabeth Maunsell Carol Moinpour A H G Paterson Wendy Schain Gynecology Committee Joseph Costantino Mary Daly Charles Geyer Jr Maureen Kavanah Lawrence Levy George Lewis Jr Erwin Bettinghaus Cathy Coleman Joseph Costantino Paul Engstrom Jr Leslie Ford V Craig Jordan Mary Ketner Amy Langer Joseph Costantino John Flack William Harian Lewis Kuller Santica Marcovina Steven Reis Santica Marcovina Tess McMillan Carolyn Runowicz Recruitment Promotion 
and Compliance Committee Rose Mary Padberg Lori Psillidis Sherrie Reynolds Edmund Ricci Rodger Winn Antronette Yancey D Lawrence Wickerham H Samuel Wieand Cardiovascular Committee Russell Tracy Northwest Lipid Research Laboratories Katherine Rosecrans Tricia Speer Epicare Center Farida Rautaharju Pentti Rautaharju Advisors Consultants Zora Brown Les Butler Joyce Cramer Michael Gorin Kathy Crosson Barbara Dunn Alfred Fallavollita Jennifer Flach Leslie Ford Peter Greenwald Karen Johnson Sunita Kallarakal Richard Klausner Lynne Anderson Stewart Anderson Gordon Bass Wayne Baughman Joseph Costantino Walter Cronin Deborah Darnbrough Richard Day Arthur DeCillis Kenneth Duff Janet Famiglietti Jennifer Aikin Jill Bowlus Lora Ann Bray Joan Dash Bernard Fisher Gladys Hurst Mary Ketner Jacek Kopek Terry Mamounas Mary Pat Matisko Pat Halpin Murphy Mary Claire King Steven Reis Jackie McNulty Rose Mary Padberg Judy Patt Donna Shriner Kara Smigel Crystal Wolfrey Darlene Kiniry Paul Magee Mary Passarello Michele Randolph 
Carol Redmond H Samuel Wieand Lori Psillidis Donna Szczepankowski Elizabeth Tan Chiu D Lawrence Wickerham Amy Wolenski Norman Wolmark National Cancer Institute NCI Susan Nayfield Eleanor Nealon Barnett S Kramer NSABP Biostatistical Center Lynn Holman Regina Hopkins Michael Hritz NSABP Operations Center 1386 ARTICLES Colleen Meyers Joyce Mull Debra Pollak Journal of the National Cancer Institute Vol 90 No 18 September 16 1998 Downloaded from http jnci oxfordjournals org at University of Florida on September 1 2010 BCPT Coordinator Committee REFERENCES Journal of the National Cancer Institute Vol 90 No 18 September 16 1998 ARTICLES 1387 Downloaded from http jnci oxfordjournals org at University of Florida on September 1 2010 1 Heuson JC Current overview of EORTC clinical trials with tamoxifen Cancer Treat Rep 1976 60 1463 6 2 Mouridsen H Palshof T Patterson J Battersby L Tamoxifen in advanced breast cancer Cancer Treat Rev 1978 5 131 41 3 Legha SS Buzdar AU Hortobagyi GN Wiseman C Benjamin RS Blumenschein GR 
Tamoxifen Use in treatment of metastatic breast cancer refractory to combination chemotherapy JAMA 1979 242 49 52 4 Margreiter R Wiegele J Tamoxifen Nolvadex for premenopausal patients with advanced breast cancer Breast Cancer Res Treat 1984 4 45 8 5 Jackson IM Litherland S Wakeling AE Tamoxifen and other antiestrogens In Powles TJ Smith IE editors Medical management of breast cancer London U K Martin Dunitz 1991 p 51 61 6 Controlled trial of tamoxifen as adjuvant agent in management of early breast cancer Interim analysis at four years by Nolvadex Adjuvant Trial Organisation Lancet 1983 1 257 61 7 Controlled trial of tamoxifen as single adjuvant agent in management of early breast cancer Analysis at six years by Nolvadex Adjuvant Trial Organisation Lancet 1985 1 836 40 8 Fisher B Redmond C Brown A Fisher ER Wolmark N Bowman D et al Adjuvant chemotherapy with and without tamoxifen in the treatment of primary breast cancer 5 year results from the National Surgical Adjuvant Breast and Bowel Project Trial J 
Clin Oncol 1986 4 459 71 9 Adjuvant tamoxifen in the management of operable breast cancer the Scottish Trial Report from the Breast Cancer Trials Committee Scottish Cancer Trials Office MRC Edinburgh Lancet 1987 2 171 5 10 Fisher B Costantino J Redmond C Poisson R Bowman D Couture J et al A randomized clinical trial evaluating tamoxifen in the treatment of patients with node negative breast cancer who have estrogen receptorpositive tumors N Engl J Med 1989 320 479 84 11 CRC Adjuvant Breast Trial Working Party Cyclophosphamide and tamoxifen as adjuvant therapies in the management of breast cancer Br J Cancer 1988 57 604 7 12 Rutqvist LE Cedermark B Glas U Mattsson A Skoog L Somell A et al Contralateral primary tumors in breast cancer patients in a randomized trial of adjuvant tamoxifen therapy J Natl Cancer Inst 1991 83 1299 306 13 Fisher B Redmond C New perspective on cancer of the contralateral breast a marker for assessing tamoxifen as a preventive agent editorial J Natl Cancer Inst 1991 83 1278 80 14 
Powles TJ Hardy JR Ashley SE Farrington GM Cosgrove D Davey JB et al A pilot trial to evaluate the acute toxicity and feasibility of tamoxifen for prevention of breast cancer Br J Cancer 1989 60 126 31 15 Furr BJ Patterson JS Richardson DN Slater SR Wakeling AE Tamoxifen review In Goldberg ME editor Pharmacological and biochemical properties of drug substances Vol 2 Washington DC American Pharmaceutical Association 1979 p 355 99 16 Adam HK Pharmacokinetic studies with Nolvadex Reviews on Endocrine Related Cancer 1981 9 Suppl 131 43 17 Wakeling AE Valcaccia B Newboult E Green LR Non steroidal antioestrogens receptor binding and biological response in rat uterus rat mammary carcinoma and human breast cancer cells J Steroid Biochem 1984 20 111 20 18 Jordan VC Fritz NF Tormey DC Long term adjuvant therapy with tamoxifen effects on sex hormone binding globulin and antithrombin III Cancer Res 1987 47 4517 9 19 Terenius L Effect of anti oestrogens on initiation of mammary cancer in the female rat Eur J Cancer 1971 
7 65 70 20 Jordan VC Effect of tamoxifen ICI 46 474 on initiation and growth of DMBA induced rat mammary carcinomata Eur J Cancer 1976 12 419 24 21 Jordan VC Allen KE Evaluation of the antitumour activity of the nonsteroidal antioestrogen monohydroxytamoxifen in the DMBA induced rat mammary carcinoma model Eur J Cancer 1980 16 239 51 22 Rossner S Wallgren A Serum lipoproteins and proteins after breast cancer surgery and effects of tamoxifen Atherosclerosis 1984 52 339 46 23 Bertelli G Pronzato P Amoroso D Cusimano MP Conte PF Montagna G et al Adjuvant tamoxifen in primary breast cancer influence on plasma lipids and antithrombin III levels Breast Cancer Res Treat 1988 12 307 10 24 Bruning PF Bonfrer JM Hart AA de Jong Bakker M Linders D van Loon J et al Tamoxifen serum lipoproteins and cardiovascular risk Br J Cancer 1988 58 497 9 25 Love RR Newcomb PA Wiebe DA Surawicz TS Jordan VC Carbone PP et al Effects of tamoxifen therapy on lipid and lipoprotein levels in postmenopausal patients with node negative 
breast cancer J Natl Cancer Inst 1990 82 1327 32 26 Bagdade JD Wolter J Subbaiah PV Ryan W Effects of tamoxifen treatment on plasma lipids and lipoprotein lipid composition J Clin Endocrinol Metab 1990 70 1132 5 27 Furr BJ Jordan VC The pharmacology and clinical uses of tamoxifen Pharmacol Ther 1984 25 127 205 28 Love RR Mazess RB Barden HS Epstein S Newcomb PA Jordan VC et al Effect of tamoxifen on bone mineral density in postmenopausal women with breast cancer N Engl J Med 1992 326 852 6 29 Fisher B Costantino J Highlights of the NSABP breast cancer prevention trial Cancer Control 1997 4 78 86 30 Gail MH Brinton LA Byar DP Corle DK Green SB Schairer C et al Projecting individualized probabilities of developing breast cancer for white females who are being examined annually J Natl Cancer Inst 1989 81 1879 86 31 Efron B Forcing a sequential experiment to be balanced Biometrika 1971 58 403 17 32 Fleming TR Harrington DP O Brien PC Designs for group sequential tests Controlled Clin Trials 1984 5 348 61 33 
Freedman L Anderson G Kipnis V Prentice R Wang CY Rossouw J et al Approaches to monitoring the results of long term disease prevention trials examples from the Women s Health Initiative Controlled Clin Trials 1996 17 509 25 34 Rosner B Fundamentals of biostatistics 4th ed Boston Duxbury Press 1995 p 590 4 35 Korn EL Dorey FJ Applications of crude incidence curves Stat Med 1992 11 813 29 36 Radloff LF The CES D scale a self report depression scale for research in the general public Appl Psychol Meas 1977 1 385 401 37 Stewart AL Ware JE Jr editors Measuring functioning and well being the Medical Outcomes Study approach Durham NC Duke University Press 1992 38 Sporn MB Roberts AB Role of retinoids in differentiation and carcinogenesis J Natl Cancer Inst 1984 73 1381 7 39 Szarka CE Grana G Engstrom PF Chemoprevention of cancer Curr Probl Cancer 1994 18 6 79 40 Bernstein L Ross RK Henderson BE Prospects for the primary prevention of breast cancer Am J Epidemiology 1992 135 142 52 41 Lippman SM Benner SE Hong WK 
Chemoprevention Strategies for the control of cancer Cancer 1993 72 3 Suppl 984 90 42 Prentice RL Kakar F Hursting S Sheppard L Klein R Kushi LH Aspects of the rationale for the Women s Health Trial J Natl Cancer Inst 1988 80 802 14 43 Veronesi U De Palo G Costa A Formelli F Marubini E Del Vecchio M Chemoprevention of breast cancer with retinoids J Natl Cancer Inst Monogr 1992 12 93 7 44 Bur ME Zimarowski MJ Schnitt SJ Baker S Lew R Estrogen receptor immunohistochemistry in carcinoma in situ of the breast Cancer 1992 69 1174 81 45 Poller DN Snead DR Roberts EC Galea M Bell JA Gilmour A et al Oestrogen receptor expression in ductal carcinoma in situ of the breast relationship to flow cytometric analysis of DNA and expression of the c erbB 2 oncoprotein Br J Cancer 1993 68 156 61 46 Giri DD Dundas SA Nottingham JF Underwood JC Oestrogen receptors in benign epithelial lesions and intraductal carcinomas of the breast an immunohistological study Histopathology 1989 15 575 84 47 Barnes R Masood S Potential value 
of hormone receptor assay in carcinoma in situ of breast Am J Clin Pathol 1990 94 533 7 48 Rutqvist LE Mattsson A Cardiac and thromboembolic morbidity among postmenopausal women with early stage breast cancer in a randomized trial of adjuvant tamoxifen The Stockholm Breast Cancer Study Group J Natl Cancer Inst 1993 85 1398 406 49 McDonald CC Alexander FE Whyte BW Forrest AP Stewart HJ Cardiac and vascular morbidity in women receiving adjuvant tamoxifen for breast 50 51 52 53 54 55 57 58 59 60 61 62 1388 ARTICLES 63 Saphner T Tormey DC Gray R Venous and arterial thrombosis in patients who received adjuvant therapy for breast cancer J Clin Oncol 1991 9 286 94 64 Nolvadex Adjuvant Trial Organisation Controlled trial of tamoxifen as a single adjuvant agent in the management of early breast cancer Br J Cancer 1988 57 608 11 65 Cummings SR Norton L Eckert S Grady D Cauley J Knickerbocker R et al Raloxifene reduces the risk of breast cancer and may decrease the risk of endometrial cancer in post menopausal women 
Two year findings from the Multiple Outcomes of Raloxifene Evaluation MORE trial abstract Proc Am Soc Clin Oncol 1998 17 2a 66 Veronesi U Maisonneuve P Costa A Sacchini V Maltoni C Robertson C et al Prevention of breast cancer with tamoxifen preliminary findings from the Italian randomised trial among hysterectomised women Italian Tamoxifen Prevention Study Lancet 1998 352 93 7 67 Powles T Eeles R Ashley S Easton D Chang J Dowsett M et al Interim analysis of the incidence of breast cancer in the Royal Marsden Hospital tamoxifen randomised chemoprevention trial Lancet 1998 352 98 101 68 International classification of diseases 9th rev Clinical modification 5th ed Salt Lake City Medicore Publications 1997 p 115 7 NOTES 1 Editor s note SEER is a set of geographically defined population based central tumor registries in the United States operated by local nonprofit organizations under contract to the National Cancer Institute NCI Each registry annually submits its cases to the NCI on a computer tape These 
computer tapes are then edited by the NCI and made available for analysis This investigation was supported by Public Health Service grants U10 CA37377 and U10 CA 69974 from the National Cancer Institute National Institutes of Health Department of Health and Human Services We thank Tanya Spewock for editorial assistance Mary Hof for preparation of the manuscript and Lynne Anderson and Gordon Bass for assistance with the analysis We gratefully acknowledge the courage and commitment of the 13 388 women who agreed to participate in this trial Without their support and efforts the results of the study would not have been possible Acknowledgement of additional contributions is presented in Appendix B Manuscript received July 29 1998 revised August 27 1998 accepted August 28 1998 Journal of the National Cancer Institute Vol 90 No 18 September 16 1998 Downloaded from http jnci oxfordjournals org at University of Florida on September 1 2010 56 cancer in a randomised trial The Scottish Cancer Trials Breast Group BMJ 
1995 311 977 80 Costantino JP Kuller LH Ives DG Fisher B Dignam J Coronary heart disease mortality and adjuvant tamoxifen therapy J Natl Cancer Inst 1997 89 776 82 Melton LJ 3d Eddy DM Johnston CC Jr Screening for osteoporosis Ann Intern Med 1990 112 516 28 Fisher B Dignam J Bryant J DeCillis A Wickerham DL Wolmark N et al Five versus more than five years of tamoxifen therapy for breast cancer patients with negative lymph nodes and estrogen receptor positive tumors J Natl Cancer Inst 1996 88 1529 42 Stearns V Gelmann EP Does tamoxifen cause cancer in humans J Clin Oncol 1998 16 779 92 ACOG committee on Gynecologic Practice Committee opinion Washington DC The American College of Obstetrics and Gynecologists 1996 169 1 3 Assikis VJ Neven P Jordan VC Vergote I A realistic clinical perspective of tamoxifen and endometrial carcinogenesis Eur J Cancer 1996 32A 1464 76 Katase K Sugiyama Y Hasumi K Yoshimoto M Kasumi F The incidence of subsequent endometrial carcinoma with tamoxifen use in patients with primary 
breast carcinoma Cancer 1998 82 1698 703 Ragaz J Coldman A Survival impact of adjuvant tamoxifen on competing causes of mortality in breast cancer survivors with analysis of mortality from contralateral breast cancer cardiovascular events endometrial cancer and thromboembolic episodes J Clin Oncol 1998 16 2018 24 Magriples U Naftolin F Schwartz PE Carcangiu ML High grade endometrial carcinoma in tamoxifen treated breast cancer patients J Clin Oncol 1993 11 485 90 Fisher B A commentary on endometrial cancer deaths in tamoxifen treated breast cancer patients J Clin Oncol 1996 14 1027 39 Fisher B Costantino JP Redmond CK Fisher ER Wickerham DL Cronin WM Endometrial cancer in tamoxifen treated breast cancer patients findings from the National Surgical Adjuvant Breast and Bowel Project NSABP B 14 J Natl Cancer Inst 1994 86 527 37 Gorin MB Day R Costantino JP Fisher B Redmond CK Wickerham L et al Long term tamoxifen citrate use and potential ocular toxicity Am J Ophthalmol 1998 125 493 501 Fisher B Redmond C 
Systemic therapy in node negative patients updated findings from NSABP clinical trials National Surgical Adjuvant Breast and Bowel Project J Natl Cancer Inst Monogr 1992 11 105 16 
56	b	Linkage of Early Onset Familial Breast Cancer to Chromosome 17q21 Author s Jeff M Hall Ming K Lee Beth Newman Jan E Morrow Lee A Anderson Bing Huey Mary Claire King Reviewed work s Source Science New Series Vol 250 No 4988 Dec 21 1990 pp 1684 1689 Published by American Association for the Advancement of Science Stable URL http www jstor org stable 2878541 Accessed 03 12 2011 17 28 Your use of the JSTOR archive indicates your acceptance of the Terms Conditions of Use available at http www jstor org page info about policies terms jsp JSTOR is a not for profit service that helps scholars researchers and students discover use and build upon a wide range of content in a trusted digital archive We use information technology and tools to increase productivity and facilitate new forms of scholarship For more information about JSTOR please contact support jstor org American Association for the Advancement of Science is collaborating with JSTOR to digitize preserve and extend access to Science http www jstor org 
Research Articles inkage of Early Onset Familial Breast to Chromosome 17q21 Cancer JEFF M HALL MING K LEE BETH NEWMAN JAN E MoRRow LEE A ANDERSON BING HUEY MARY CLAIRE KING unavoidable epidemiologic realities The disease is common but only a small proportion of cases in the general population are attributable to inherited susceptibility Thus families may have multiple cases of breast cancer without inherited susceptibility and sporadic cases may occur even in families with inherited disease In addition the disease is not completely penetrant among susceptible persons with expression depending on gender age and nongenetic risk factors Finally both epidemiological and molecular evidence suggests heterogeneity We have tested simultaneously for genetic linkage and heterogeneity of breast cancer in families and our results suggest both the presence of a gene for early onset breast cancer on chromosome 17q21 and linkage heterogeneity of the disease Families and inheritance of susceptibility Our genetic analysis is 
based on 23 extended families with 146 cases of breast cancer Figs 1 and 2 All persons in our analysis are Caucasian and from a variety of original ancestries The 329 participating relatives now live in and were therefore sampled from 40 states of the United States Puerto Rico Canada the United Kingdom and Colombia H s UMAN DISEASE GENES CAN BE LOCATED BY LINKAGE These families share the epidemiological features that are characteristic of familial versus sporadic breast cancer 2 younger age at analysis of families in which the incidence of the disease is high Linkage analysis can reveal the chromosomal loca diagnosis frequent bilateral disease and more frequent occurrence tion of the genes of interest by identifying polymorphic genetic of disease among men markersof known location that are coinherited with the disease in Our statistical model for the inheritance of susceptibility to breast families 1 Among the common cancers breast cancer is particu cancer was derived from our previous complex segregation 
analysis larly suited for this approach because family history of the disease is of a population based series of 1500 families with breast cancer 4 a significant risk factor in all populations epidemiological evidence Inherited susceptibility to breast cancer in that series could be fully consistently indicates that a woman s risk of breast cancer is inexplained by a rare autosomal dominant allele with a major effect on creasedby the occurrence of the disease in her mother or sisters The risk risk of breast cancer in genetically susceptible women was younger the ages at diagnosis of her relatives the greaterthe increase estimated to be 0 37 by age 40 0 66 by age 55 and 0 82 over the in a woman s risk 2 entire lifetime In contrast risk of breast cancer in women without The transformation of breast ductal epithelial cells to malignant genetic susceptibility was estimated to be 0 004 by age 40 0 028 by growth results from alterations in their DNA that may be either age 55 and 0 081 over the entire lifetime 
Females less than 15 years inherited or somatic 3 Mapping genes for familial breast cancer is of age and all males had a negligible risk less than 0 001 The important because alterations at the same loci may also be respon estimated proportion of breast cancer cases in the sample that were sible for sporadic disease Individuals with inherited susceptibility to attributableto inherited susceptibility was only 4 percent the great breast cancer are completely asymptomatic for decades before the majority of cases resulting purely from somatic events Among onset of disease the effects of critical inherited alterations are thus younger patients however the proportion of inherited cases is likely latent for an extended period Among women with no inherited to be considerably higher Disease allele frequencies q between susceptibility to the disease these same alterations may be the initial 0 004 and 0 02 yield virtually identical results those for q equals lesions of breast tumorigenesis with disease expression being 
simi0 01 are described larly dependent on subsequent genetic alterationsor tumor promotDefinition of the breast cancer phenotype For any complex ing steps disease it is essential to adequately define the phenotype the Mapping genes for human breast cancer has been complicated by inheritance of which will be traced in families Real linkages can be missed and spurious linkages suggested either by defining the phenotype too broadly so that persons without inherited susceptibility to disease are mistakenly categorized as affected or simply by The authors are at the School of Public Health University of California Berkeley CA making errors in diagnosis To minimize errors in diagnosis we 94720 Human breast cancer is usually caused by genetic alterations of somatic cells of the breast but occasionally susceptibility to the disease is inherited Mapping the genes responsible for inherited breast cancer may also allow the identificationof earlylesions that are criticalfor the development of breast cancer in the 
general population Chromosome 17q21 appears to be the locale of a gene for inherited susceptibilityto breast cancerin families with early onsetdisease Genetic analysisyields a lod score logarithm of the likelihood ratio for linkage of 5 98 tor linkage of breast cancersusceptibilityto Dl 7S74 in early onsetfamilies and negative lod scores in families with late onset disease Likelihood ratios in favor of linkage heterogeneity among families ranged between 2000 1 and greaterthan 106 1 on the basis of multipoint analysisof four loci in the region 1684 SCIENCE VOL 250 reviewed existing pathology records of all family members on whom accurately estimated Therefore we also tested for linkage by breast surgery had been performed For deceased persons reported including disease information only for the affected relatives that is by their relatives to have had breast cancer but for whom no the breast cancer cases in each family all unaffected subjects were pathology records were available we obtained hospital records 
or assigned to the lowest risk class so that only their marker informadeath certificates For living subjects who had not undergone breast tion was incorporated Autosomal dominance was still assumed and surgery we relied on self report of no breast cancer for deceased lifetime risk of sporadic disease was not altered persons with no history of breast surgery we relied on death 3 In order to evaluate linkage without imposing any specific certificates and reports of relatives The affected phenotype was genetic model we counted alleles shared by descent for affectedpairs defined as all histologic types of invasive breast cancer No other of female relatives This analysis was possible because one marker cancer sites were included 5 was highly polymorphic Pairs of affected relativeswere sisters firstStatistical methods Four approaches to evaluating linkage were degree relatives aunt niece and grandmother granddaughter secapplied ond degree relatives and first cousins third degree relatives 1 Lod scores logarithms 
of the likelihood ratios for linkage for Relative pairs were stratified by the average age of breast cancer linkage of individual markers to disease were estimated with the diagnosis for the pair LIPED program multipoint mapping was performed with the 4 We applied the affected pedigree membermethod to evaluate LINKAGE program homogeneity of recombination fractions was sharing of alleles by state among the breast cancer cases in each evaluated with the B test for two point linkage data and direct family 7 Only cases whose markergenotypes could be determined comparison of likelihoods for multipoint data 1 6 Homogeneity with certainty were included in the analysis of affected pedigree of linkage was tested for all families the sample was not a priori members subdivided Linkage analyseswere based on an autosomal dominant Typing of DNA polymorphisms For each of the 329 informamodel with the age and sex specific risks described above for tive relatives we obtained 35 milliliters of fresh blood and prepared 
hypothetically susceptible and nonsusceptible individuals LIPED immortalized lymphocytes by Epstein Barrviral transformation 8 was modified to incorporate four liability classes for each genotype Genomic DNA was prepared as described 9 Probes were labeled The risk group liability class for each woman was defined by her by random primer extension 10 and hybridized to DNA according age at diagnosis of breast cancer at death if deceased without breast to standardprocedures 11 Parentage was confirmed by consistent cancer or at most recent interview if living and unaffected All men inheritance of 183 polymorphic markers Markers at chromosome were assigned to the lowest risk group 17q21 include D17S74 a VNTR variable number of tandem 2 In determining a plausible model for inheritance of suscepti repeats defined by the probe CMM86 and Hinfi D17S40 defined bility to a complex disease it is always possible that the underlying by the probe LEW 101 and Msp I D17S41 defined by the probe genetic model may be correct but 
that penetrance may not be LEW 102 and Pst I and D17S78 defined by the probe p131 and 2 dx44 1 71 DD 6 6 4 dx45 dx36 dx3l EH BC 3 EH ED 49 DD FG Ov 16i dx39 BD dx45 BA 65 AE BD Ov 72 BA EA 6 6 Od4i e 43 HC dx29 4B HC EB28 EB HAH EB EB HC dx2 HC dx37 EF EF 40 HF HF 5 2dx4 x2d3 AA AE 39 BA BE DE dx23 dx4l dx3 AA dx25 EA dx3l AC 5 4 6 dx47 BE 6 72 BC 6 dx39 BE 60 AC CD dx45 dx4l 92 CE dx40 C BC 4b 86 CE DG 77 CB dx42 C DF 31 73 68 59 dx59 dx40 EC ED EC EE CC CC EC CD CE BD 21 DECEMBER 1990 4 dx45 CB dx32 breast cancer open circles fe familiesand are letteredsequentialy within each family from largestto smallestfragmentsize are Alleles in parentheses based on reconstructed genotypes CE dx40 BE 51 6 6 6 leles of D17S74 are shown for all 4 dx43 BE 62 dx41 cancer familiesI to Fig 1 Breast 7 Solid circles females with cancer open maleswithoutbreast squares males without breast aiven cancer cancer The age given for each canwomanis age at first breast cer diagnosis dx if affected age inat deathif deceased deceased are 
by dividuals represented diagonal lines through symbols or age at most recent interviewif alive without breast cancer Al BC CB DD 46 46 32 B DE dx45 CB 4 dx30 BD 4 36 BD CD dx36 7 6 82 AE 1 dx46 CA BD 77 x CD C i BD AC 1 dx43 CE AD dx58 AF dx 6 dx45 CA dx46 CE dx41 CB 46 BB dx5l D 1 dx46 BC dx48 BC 42 BC dx38 BD 54 DA dx33 BC 44 EB RESEARCH ARTICLES 1685 Msp I 12 On the basis of analysisof the CEPH families the order and approximate recombination distances of the markers are 17cen D17S78 0 10 D17S41 0 06 D17S74 0 12 D17S40 17qter 13 D17S74 a highly polymorphic VNTR with heterozygosity greater than 0 90 is extremely useful for linkage analysis but presents technical challenges that are common to VNTRs Linkage analysisis critically sensitive both to errors in assigning genotypes and to marker allele frequencies The lengths of the Hinf restriction fragments that define D17S74 alleles are continuously distributed between 1 and 5 kilobase pairs kb For our analyses D17S74 alleles were identified by analyzing DNA 
samples from all members of a family on the same Southern blot placing relatives with fragments of similar size adjacentto one another 14 Population 10 9 8 d 4 ii 661 75 61 AT AD dx53 dx56 BB 6 dx 32 D BD dx62 dx28 dx5O AB D AE 72 71I DE EE EA 94 dx65 DF DE 82 dx49 CH DB 46 nb dx66 DE dx4 CG dx57 D AD BE 70 dx50 DA 51 dx35 DG DB dx32 38 33 DH BC DG 12 35 dx53 AF C dx45 dx49 dx 40 AD 57 GG dx33CdxE9 GG GG AC AC AC D D E AD BC 30 CD CE dx 70 73 dx56 dx63 AB CB CD dx64 CD 54 B EB DE dx43 dx 48 BD dx 71 CB dx49 D CD CE FD FD 27 27 dx6l CB DG 6 62 BC dx4O DE 18 dx 56 AE i dx65 HJ 69 EJ 1 dx66 77 GK 71 dx35 dx63 62 52 dx44 dx41 ED DC CD dx32 FE dx58 71 AC AD ED AD AC 0 G DF 56 AK EG t dx47 E 1 AB 51 dx44 X B BB dx38 Ab6 dx 1 S e i dx 75 71 dx49 66 C F 70 dx56 68B dx55 B F dx87 SDd5 01 Z o dx 70 91 79 x5 dx52 AC 14A 38 dx47 EIH Eg BB A BB dx47 O dx58 AF EF dx7l BE dx45 41 5 59 dxSB BC BD BD dx36 DB EA 22 23 6 4 4 40 dx72 dx54 dx57 AC AC AB AB 6 dx46 AB dx49 BC BD BC dxSS CE 66 AC d 63 DF 34 Xdx79 AC BD 5 70 BE BE 
EG 21 70 AA AO o dx0 B AA H ETH qH IE 20 E EA dx 61 FK GK JK 19 S O dx48 FK 37 EG B 85 El dx64 dx66 dxSO dx32dx35 3 CD A 4 W 66 BD 79 dx44 CD 38 BD 17 72 dx65 DF 45 BD dx66 CD DD CH 16 82 DB dx 42 CD dx62 CB dx 44 2 21 DH 15 75 AD AC GH dx0 dx30 dx77 44 AC 14 3 dx57dx52 T 66dx55dx33 GG 31 BE 6 38 AD dx65 BD EF 33 BE 13 66 73 A AD DC 6 dx48 G AD D dx28 11 G BC dx44 d FC dx6 I dx56 dx4O 56 CE dx37 dx65 AB CD dx4B BC FG dx78 DE BD DE dx70 AC 6 75 CF dx57 dx5Bdx62 EG EG DD dx62 dx57 EE BC Fig 2 Breastcancerfamilies8 to 23 Notation as for Fig 1 with solid Genotypes for D17S74 are shown for all families D17S78 genotypes for squaresin families 16 and 19 representingmales with breast cancer family 8 and D17S40 genotypes for family 19 are also shown 1686 SCIENCE VOL 250 frequencies of the D17S74 alleles in this sample were estimated by selecting subjects from different families whose D17S74 fragments appearedto be of similar size on the basis of their familyblots and then analyzing the DNA from these unrelated 
persons in neighboring lanes on the same blots Some samples were included several times in order to identify distortions in the gels These population blots were analyzed without reference to sample numbers in order to determine which alleles could be consistently distinguished D17S74 had more than 30 distinguishable fragment lengths and hence more than 30 different alleles in our sample nine of which occurred more than once among unrelated individuals at frequencies ranging from 0 07 to 0 13 The other D17S74 alleles were only represented once in our sample but because extremely rare marker allele frequencies can have a major influence on estimates of lod scores and the T statistic 7 apparently unique alleles were each assigned the frequency 0 03 Results of linkage and heterogeneity analysis in the breast cancer families For the 23 families as a group homogeneity of linkage of breast cancer to D17S74 could be rejected at P equals 0 01 Multipoint analysisof linkage in the intervalD17S78 D17S41D17S74 D17S40 
yielded likelihood ratios in favor of heterogeneity of linkage among the 23 families between 2000 1 and 1 4 x 106 to 1 After adjusting for heterogeneity among all families the maximum two point lod score is 3 28 at recombination distance of 0 014 from D17S74 with disease linked to this locus in 40 percent of the families Figs 1 and 2 Heterogeneity of linkage in these families appears explicable by age of disease onset Breast cancer is linked to markers in this chromosomal region specifically in families with early onset breast cancer Among the seven families with a mean age of breast cancer diagnosis less than or equal to 45 the two point lod score for linkage of D17S74 and breast cancer is 5 98 at a distance of 0 001 recombination units with a 95 percent confidence interval of 0 001 to 0 09 Table 1 In contrast total lod scores for the families with late onset disease are negative It is characteristicof linkage in the presence of heterogeneity that a modest lod score in this case 2 35 for all families 
ignoring heterogeneity at a fairly large recombination distance 0 20 recombination units masks two curves one with a more positive lod score in the linked families 5 98 at a smaller recombination fraction 0 001 to 0 09 and the other negative 15 Two point lod scores for all four markers in this chromosomal region suggest that a gene for susceptibility in the early onset families is likely to be within approximately 10 percent recombination of D1 7S74 Table 2 Multipoint analysis of the fourmarker interval yields a maximum lod score of 5 41 near D17S74 for the earliest onset families Table 2 Again total lod scores for families with older ages at diagnosis are negative throughout the interval Linkage of breast cancer to D17S74 was also evaluated on the basis of only the individuals with breast cancer in each family For this analysis all women without breast cancer and all men were assigned to the lowest risk group For the 23 families as a group the P value for homogeneity of linkage is 0 06 For the families with 
average age at diagnosis less than or equal to 45 the maximum lod score is 4 69 at close linkage with a 95 percent confidence interval for the recombination fraction of 0 001 to 0 10 Lod scores at close linkage to D17S74 are 2 19 for families 8 to 15 and 5 22 for families 16 to 23 Analysis of alleles shared by descent among related pairs of women with breast cancer also suggested linkage of early onset breast cancerto D1 7S74 Table 3 In families 1 to 7 all three classes of relatives shared more alleles by descent than expected by chance Even in families 8 to 23 there was evidence for increased identity by 21 DECEMBER 1990 Table 1 Lod scoresfor linkageof breastcancerto D17S74 chromosome 17q21 For eachfamily M is the meanage of diagnosisof breastcancer 0 001 0 10 0 20 0 30 0 40 IZ at 0 0 001 2 36 0 50 0 40 1 14 0 50 1 38 0 70 0 00 0 31 0 04 1 51 0 06 0 41 0 65 0 35 2 71 0 13 0 75 2 56 1 71 0 65 0 85 0 07 1 89 0 35 0 29 0 91 0 25 1 06 0 58 0 02 0 03 0 06 0 41 0 03 0 09 0 18 0 08 0 56 0 04 0 38 0 93 1 01 0 50 0 
13 0 02 1 38 0 21 0 19 0 64 0 08 0 73 0 40 0 02 0 06 0 08 0 13 0 02 0 02 0 01 0 02 0 20 0 07 0 18 0 45 0 56 0 34 0 04 0 00 0 82 0 09 0 09 0 35 0 00 0 41 0 21 0 01 0 04 0 08 0 03 0 01 0 03 0 06 0 01 0 07 0 05 0 07 0 20 0 28 0 18 0 05 0 00 0 28 0 02 0 03 0 11 0 03 0 14 0 05 0 00 0 01 0 05 0 00 0 00 0 04 0 04 0 00 0 02 0 01 0 02 0 05 0 11 0 05 0 02 0 00 2 36 2 86 3 26 4 40 3 90 5 28 5 98 5 98 5 67 5 63 4 12 4 06 3 65 3 00 2 65 0 06 0 19 0 94 3 50 5 21 4 56 5 41 5 48 fraction Recombination Family M 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 32 7 37 2 37 3 39 8 42 6 44 2 45 4 47 0 47 4 47 6 49 3 50 2 50 4 51 4 51 8 52 0 53 5 53 6 55 8 56 4 58 7 59 4 63 3 descent among sisters with early onset disease but little or no evidence for increased identity by descent for second and thirddegree relatives Finally the affected pedigree member analysis of alleles shared by state among all individuals with breast cancer in each family also suggested linkage of early onset breast cancer to markersin this 
chromosomal region 16 These analyses defined families with early onset disease as those with an average age at breast cancer diagnosis of less than or equal to 45 However early onset could be defined in a variety of ways The cumulative lod scores for linkage of breast cancer to D1 7S74 as families with increasing age at disease onset are included in the analysis are indicated in Fig 3 and the right hand column of Table 1 Cumulative lod scores are above 5 0 for families with an average age at diagnosis of less than 48 remain positive for families with an average age at diagnosis of less than 52 and then drop sharply As specific alternative ways of defining early onset breast cancer we defined families with early onset disease to be those in which i most breast cancers in each family were diagnosed by age 50 families 1 to 7 9 10 12 13 21 or ii most breast cancersin each family were diagnosed by age 45 families 1 to 6 10 or iii the averageage at breast cancer diagnosis was less than 48 the mean for all the 
cases in the sample families 1 to 10 The critical results for all definitions were the same maximum lod scores for linkage of breast cancer to this chromosomal region are between 5 2 and 5 7 in the early onset families and negative in the late onset families and the disease gene appears to be within about 0 10 recombinatioi units of D17S74 Other risk factors for breast cancer in the families To determine whether the linked gene is expressed in the presence of any specific background of nongenetic risk factors for breast cancer we compared breast cancer risk factors for women in families with apparentlinkage of breast cancer to chromosome 17q versus women in families with evidence against this linkage Ages at first pregnancy number of children prevalence of fertility problems exposure to x rays use of oral contraceptives and ages at menopause were families after similarly distributed in linked and unmlinked adjusting for age and birth cohort of the women Similarly prevaRESEARCH ARTICLES 1687 of Fig 3 Linkage 
D17S74 in cancer families to breast based on the autosomal modeldescribed dominant lod in the test Cumulative scores Y2Z areshownfor all familiesfor which the meanage of breastcancer is diagnosis AM less than or equalto the age representedon the x axis Total than lod scoresaregreater or equalto 5 0 for families with M 48 and greater than or equal to 3 0 for families M _ 50 Each with dot abovethe x axisreprewiththatparsentsa family meanonset age ticular 6 Table 3 Identity by descent of two one or zero alleles of D17S74 among pairs of relatedindividuals with breastcancer Mean onset is the average of breastcancerdiagnosisfor the relatedpair Familynumbers age refer to Figs 1 and 2 Second degree 20 relativesare aunt nieceor grandmother granddaughter pairs third degree 30 relatives are first 4 cousins Mean onset of related pair years 2 Sisters Two One 45 46 55 14 5 4 8 56 3 6 45 46 55 20 Zero One 30 Zero One Zero 5 2 8 4 4 4 7 7 3 10 9 Families 1 to 7 2 0 17 0 6 2 0 0 2 0 1 0 45 46 55 5 5 Families 8 to 23 2 0 2 0 
8 7 5 2 2 3 2 4 56 3 6 7 3 10 N X 30 40 50 60 2 of Age onset years lences of specific cancers at other sites did not differ among women in linked and unlinked families although male breast cancers occurred only in unlinked families The only observed difference between women in linked versus unlinked families was age at breast cancer diagnosis Linkage analysis in families and loss of heterozygosity in tumors Comparisons of breast tumor tissue with normal tissue from the same individual have suggested that chromosomes lp 3p lip 13q 16q or 17p or some combination may harbor genes that are important for breast tumor progression 17 An earlier linkage analysisfrom our group based on fewer and less informative families suggested with a modest lod score that a gene for familial premenopausal breast cancer or ovarian cancer is present on chromosome 16q 18 We have excluded the other regions suggested by loss of heterozygosity in tumors as locales of genes for inherited susceptibility to breast cancer 9 19 A gene or 
genes on chromosome 5q appear to be responsible for inherited forms of colon cancer and quite possibly for the initial somatic lesions of other colon cancers with genes on 12p K ras 17p p53 18q DCC and possibly elsewhere being responsible for subsequent invasion and metastasis 20 This pattern of multiple sequential events determined by alterations on more than one chromosome may also apply to breast cancer Negative lod scores in the families with late onset breast cancers may reflect any or all of the following the existence of a different locus or loci responsible for inherited susceptibility in these families the chance occurrence of some families with multiple cases or a All families 0 19 0 9 2 0 2 7 higher prevalence of sporadic cases in families with late onset inherited disease Loci responsible for disease in families with late onset disease may be identifiable by continued simultaneous analysis of linkage and heterogeneity Candidate genes on chromosome 17q The ultimate goal of gene mapping of human 
traits is to move from a known chromosomal location to identification of the crucial gene and characterization of its critical alterations This region of chromosome 17q includes several plausible candidate genes 21 A gene for a truncated form of the human epidermal growth factor receptor her2 MIM 164870 Mendelian Inheritance in Man is identical to erbb2or neu MIM 190150 The gene her2 acts as an oncogene in NIH 3T3 cells and is amplified in many primary breast tumors her2 amplification is associated with poor prognosis at least for nodepositive tumors 22 Other candidate genes in this region include that for estradiol 17P dehydrogenase edhbl7 MIM 264300 which is the enzyme that catalyzes the conversion of estrone to estradiol the homeobox 2 gene hox2 MIM 142960 which is criticalfor murine embryological development nm23 whose expression is associated with lymph node metastasis in primary breast carcinomas the gene for retinoic acid receptor ax rara MIM 180240 a protein that binds the possible anticarcinogen 
retinoic acid and wnt3 MIM 165330 one of the integration sites activated by the mouse mammary tumor virus and which is homologous to Table 2 Linkage analysis of breast cancer to four markerson chromosome 17q by mean age M of breast cancer diagnosis Families 1 to 7 M 45 Two point lod scores at five recombination fractions Famles 8 to 15 46 M 51 Families 16 to 23 M 52 Marker 0 001 0 10 0 20 0 30 0 40 0 001 0 10 0 20 0 30 0 40 0 001 0 10 0 20 0 30 0 40 D17S78 D17S41 D17S74 D17S40 0 84 1 54 5 98 0 65 1 12 4 83 0 16 0 71 3 47 0 04 0 36 1 97 0 00 0 14 0 65 0 36 0 10 3 33 0 95 0 51 0 80 0 81 0 43 0 18 0 48 0 26 0 05 0 14 0 08 0 04 4 18 2 73 8 13 1 25 1 03 2 49 0 44 0 59 0 94 0 12 0 35 0 34 0 04 0 16 0 12 1 36 1 01 0 63 0 30 0 07 0 49 0 12 0 01 0 06 0 05 2 42 0 71 0 25 0 05 0 00 Families S78 000 0 020 Lod scores based on multipoint analysis of the interval on recombination units S74 S41 0 184 0 208 0 160 0 120 0 140 0 100 0 060 0 080 0 040 0 232 0 256 S40 0 280 1 to 7 8 to 15 16 to 23 2 83 0 30 6 70 3 09 0 07 5 80 3 
30 0 01 5 51 4 96 3 36 5 58 4 48 2 78 5 03 1688 3 47 0 03 5 52 3 57 0 05 5 89 3 41 0 20 6 98 4 46 1 58 6 60 4 60 2 71 7 94 5 24 9 14 15 21 5 41 5 61 8 94 5 24 4 24 6 79 SCIENCE VOL 250 the Drosophilawinglessness locus 23 If alterations in any of these genes are responsible for inherited breast cancer polymorphisms at the criticallocus may be in linkage disequilibriumwith the disease in the early onset families REFERENCES AND NOTES 1 J Ott Analysis of Human GeneticLinkage Johns Hopkins Univ Press Baltimore 1985 2 R Ottman M C Pike M C King J T Casagrande B E Henderson Am J Epidemiol 123 15 1986 N L Petrakis V L Ernster M C King in Cancer Epidemiology and Prevention D Schottenfeld and J F Fraumeni Jr Eds Saunders Philadelphia 1982 pp 855 870 A G Schwartz M C King S H Belle W A Satariano G M Swanson J Natl CancerInst 75 665 1985 3 A G Knudsen CancerRes 45 1437 1985 4 B Newman M A Austin M Lee M C King Proc Natl Acad Sci U S A 85 3044 1988 5 Four women with endometrial cancer from families 1 4 and 9 and two 
women with thyroid cancer from families 5 and 13 were omitted entirely from the analysis In family 3 two women have breast and ovarian cancer and are defined as affected at the ages of their breast cancer diagnoses two other women died with ovarian cancer only and could not be included 6 J Ott Ann Hum Genet 47 311 1983 G M Lathrop J M Lalouel C Julier J Ott Am J Hum Genet 37 482 1985 N Risch ibid 42 353 1988 7 D E Weeks and K Lange Am J Hum Genet 42 315 1988 8 L Louie and M C King ibid 41 A174 abstr 1987 9 J M Hall et al ibid 44 577 1989 10 A P Feinberg and B Vogelstein Anal Biochem 132 6 1983 ibid 137 266 1984 11 T Maniatis F F Fritsch J Sambrook MolecularCloning A Laboratory Manual Cold Spring Harbor Laboratory Press Cold Spring Harbor NY 1982 12 Y Nakamura et al Genomics2 302 1988 J Luty et al Nucleic Acids Res 16 6250 1988 13 Centre d Etude du Polymorphisme Humain CEPH Version3 Database Lod Scores and Recombination Estimates CEPH Paris 1989 14 An example of the difficulties in typing VNTR alleles is 
presented by family 18 in which allele K is alleged to appear in cousins without pedigree evidence for identity by descent in intervening relatives This assignment and others like it were made after analyzing several samples from the relatives on the same Southern blot genotyping them without reference to their sample numbers or positions in the pedigree and determining that the bands were indistinguishable among the four samples 15 L L Cavalli Sforza and M C King Am J Hum Genet 38 599 1985 16 For the 23 families as a whole the method of affected pedigree members yields a T value of 1 13 P 0 13 for the T statistic unadjusted for allele frequencies T equals 4 26 P 0 001 for the inverse square root weighting function and T equals 5 49 P 0 001 for the reciprocal weighting function The T statistic among the younger families was consistently significant with T values of 2 60 5 18 and 5 78 and empirical P values of 0 012 0 001 and 0 003 for the three weighting functions respectively 17 P Devilee et al Genomics 5 
554 1989 M Genuardi H Tsihira D E Anderson G F Saunders Am J Hum Genet 45 73 1989 I U Ali R Lidereau C Theillet R Callahan Science 238 185 1987 C Lundberg L Skoog W K Cavenee Proc Natl Acad Sci U S A 84 2372 1987 C Coles et al Lancet336 761 1990 T Sato et al Am J Hum Genet 47 A16 abstr 1990 18 M C King R C P Go R C Elston H T Lynch N L Petrakis Science208 406 1980 M C King et al J Natl Cancer Inst 71 463 1983 19 A M Bowcock J M Hall J M Hebert M C King Am J Hum Genet 46 12 1990 J M Hall et al Genomics6 181 1990 20 B Vogelstein et al Science244 207 1989 E R Fearon and B Vogelstein Cell 61 759 1990 21 V A McKusick Mendelian Inheritancein Man Johns Hopkins Univ Press Baltimore ed 10 1990 E Solomon and D F Barker Cytogenet Cell Genet 51 319 1989 22 L Coussens et al Science230 1132 1985 N C Popescu C R King M H Kraus Genomics 362 1989 P P Di Fiore et al Science237 178 1987 M 4 J van de Vijver and R Nusse Biochim Biophys Acta in press P Devilee and C J Comelisse CancerSurv in press 23 V Luu The et al Mol 
Endocrinol 268 1990 G Bevilacqua M E Sobel L 4 A Liotta P S Steeg CancerRes 49 5185 1989 A M Rosengard et al Nature 342 177 1989 H Roelink E Wagenaar S Lopes da Silva R Nusse Proc Natl Acad Sci U S A 87 4519 1990 M Petkovich N J Brand A Krust P Chambon Nature 330 444 1987 N Brand et al Nature 332 850 1988 24 We thank Y Nakamura M Litt and D Barker for probes the CEPH for information on genetic markers N L Petrakis and S Wolman for help with interpretation of breast tumor pathology J Ott and N Risch for statistical advice E Jones S Rowell P Zuppan and J Crandall for technical assistance and E Anderson and the Breast CancerTask Force of NCI for encouragement Supported by NIH grant CA27632 12 September 1990 accepted 10 November 1990 AAAS Newcomb Cleveland Prize To Be Awarded for an Article or a Report Published in Science The AAAS Newcomb Cleveland Prize is awarded to the author of an outstanding paper published in Science The value of the prize is 5000 the winner also receives a bronze medal The 
currentcompetiion period began with the 1 June 1990 issue and ends with the issue of 31 May 1991 Reports and Articles that include original research data theories or syntheses and are fundamental contributions to basic knowledge or technical achievements of far reaching consequence are eligible for considerationfor the prize The paper must be a first time publication of the author s own work Reference to pertinent earlier work by the author may be included to give perspective 21 DECEMBER 1990 Throughout the competition penod readers are invited to nominate papers appearing in the Reports or Articles sections Nominations must be typed and the following information provided the title of the paper issue in which it was published author s name and a brief statement of justification for nomination Nominations should be submitted to the AAAS Newcomb Cleveland Prize AAAS Room 924 1333 H Street NW Washington D C 20005 and must be received on or before 30 June 1991 Final selection will rest with a panel of 
distinguished scientists appointed by the editor of Science The award will be presented at the 1992 AAAS annual meeting In cases of multiple authorship the prize will be divided equally between or among the authors RESEARCH ARTICLES 1689 
57	b	It s very scary when you find out you have breast cancer You may have found a lump or had an abnormal mammogram and then a biopsy Your whole life stops nothing else matters At this point it seems like a mountain has just landed on the road you re travelling on and you simply can t climb over it lots of doctors waiting rooms information searching the web and obtaining information from various support groups help lines or outdated books in your local library Knowledge helps you gain control by learning nearly as much as your doctor about breast cancer in these vital first weeks after your diagnosis These are the critical steps to take on day one to regain some control over your situation How to get control Remember that breast cancer is not a medical emergency a few extra days to ensure that you get the right information and treatment won t harm you Understand your choices and don t get rushed into a decision you don t feel comfortable about Find the best option for your own individual situation Find a doctor 
who can deliver that option safely and efficiently without significant waiting times Take notes ask your support person to be a note taker or take a tape recorder with you Ask for a copy of letters or other correspondence doctors send to each other Keep a copy of all your test results and correspondence in ascending chronological order in a folder filled with clear plastic sheets Keep a copy of details of your insurance company policy number contact telephone numbers health care cards and a log of important dates including consultations dates of surgery when you started and finished radiation therapy chemotherapy or hormonal treatment Write your telephone number on all your X rays as they often get lost Inquire about cancer support groups and consider keeping a journal or blog of your thoughts experiences and feelings See if your team includes a breast care nurse who is usually more accessible than the doctor and ask for his or her business card or best contact number Ask for a business card from all the 
professionals you meet Better still find a doctor who works in a team a surgeon a radiation oncologist and a medical oncologist The team should work closely with other specialists such as a plastic surgeon and a geneticist and have access to expert nurses and other allied health staff If you live outside a large city it will be harder to see the whole team before your surgery Seek an opinion from a surgeon who has networks with a cancer centre or if you can afford the time money extra stress and inconvenience of leaving your home seek a second opinion Take control of your destiny particularly during follow up as most doctors don t have the time to provide the emotional support you need This should not turn into a doctor bashing exercise Rather it s about you understanding the medical aspects of the disease what is possible and what is not possible and what treatments can and cannot be given safely in a particular situation Do your research but don t go overboard Be particularly guided by your family doctor 
who knows you and your medical history Do a Google search of the doctor to whom you ve been referred See what articles he or she has written One way of doing this is via Google Scholar Visit www google com au and click more then find the Google Scholar link in the drop down menu Another way to search your doctor s publications is to do a PubMed search PubMed was developed at the National Library of Medicine located at the US National Institutes of Health NIH To start a PubMed search visit www ncbi nlm nih gov sites entrez go to the Search tool and enter the author s name Remember this will not include all of an individual s publications but it does give you a guide to what they have published 
58	b	In 1995 following the publication of the NHMRC Guidelines for the Management of Early Breast Cancer an invitation was sent to potentially interested clinicians and other parties in the ACT and surrounding NSW to participate in the forming of a local interest group The intention was for the group to look at issues surrounding the management and treatment of breast cancer within the local environment In this section BCTG patient information Publications Breast Cancer Treatment Project A regional group The unique nature of the region was seen to be an important and distinctive feature a small number of hospitals and located within the ACT and the area covered by the Southern Area Health Service of NSW provided the potential to establish a regional group not aligned to any institution or area health service Map of treatment region This map covers the area where data is collected for the study Of all cases between 1997 and 2001 65 of new breast cancer cases are from the ACT and 35 from the South Eastern of NSW 
region The initial meetings of the group focused on developing the terms of reference and the local protocols for the various clinical groups Very early on in the life of the group it was agreed that if we were to make full use of the NHMRC guidelines then we would need to be able to monitor what treatment was being provided to women with breast cancer in the region and to compare that treatment with the best practice guidelines In order to do this appropriate data collection mechanisms needed to be put in place Funding was sought and gained from the Commonwealth for a project officer to undertake the development of a data base and the ongoing collection of data relating to the treatment of breast cancer Other activities the group has focused on include refining the data set and hearing from each clinical group in turn about new developments in breast cancer treatment A number of guest speakers have been invited to attend the meetings and to speak on national issues relating to breast cancer The meetings 
continue to be well attended 14 years after the establishment of the group and the wealth of data collected will provide an ongoing source of information to ensure that women receive the best care available 
59	b	Lymphoedema Breast Cancer App A complete mobile guide to understanding monitoring for Lymphoedema Lymphedema after Breast Cancer Home FAQ More Info Media Links App will be great value to any woman affected by breast cancer Breast Surgeons Australia New Zealand Inc Brillant mobile health care resource iTunes customer review Impressive Innovative Carolyn Linkedin The Lymphoedema Breast Cancer App provides information about what Lymphoedema Lymphedema is and how to reduce your risk Users can relax knowing that they have 24 7 access to all that they need to know and all at the touch of a button You can save your arm measures in the App and refer to these next time you log in allowing you to monitor your arm for Lymphoedema Lymphedema over time 
60	b	Newly diagnosed early breast cancer An update on pre operative assessment and staging Volume 41 No 11 November 2012 Pages 871 874 Article E Letters 0 Send E letter Download article Download Citations Newly diagnosed early breast cancerMeagan Brennan BMed FRACGP DFM FASBP is a breast physician The Poche Centre North Sydney and Clinical Senior Lecturer Northern Clinical School Sydney Medical School University of Sydney New South Wales Nehmat Houssami MMBS Hons FASBP FAFPHM MPH PhD is Associate Professor and Principal Research Fellow Screening and Test Evaluation Program University of Sydney Sydney New South Wales Background Most breast cancer seen in developed nations is diagnosed at an early stage and surgery is the recommended first line treatment in most cases Objective This article reviews the current approach and related evidence on pre operative assessment of women with newly diagnosed breast cancer It discusses the use of conventional assessment tools mammography ultrasound and needle biopsy for staging 
the breast and axilla the evidence relating to breast magnetic resonance imaging and the indications for staging investigations for distant metastatic disease It highlights recent changes in practice including areas of nonconsensus and informs general practitioners on evolving issues in the pre operative care of the newly diagnosed breast cancer patient Discussion Once a breast cancer diagnosis has been established appropriate pre operative evaluation to assess the extent of disease locally and sometimes systemically helps guide surgical management and decisions on adjuvant therapy Each year around 1 4 million women worldwide are diagnosed with breast cancer Breast cancer represents approximately 10 of all new cancers and nearly a quarter 23 of all female cancer cases 1 With increased breast awareness and the widespread implementation of population screening programs the majority of breast cancer in developed nations is diagnosed at an early stage 2 3 Surgery remains the recommended first line treatment for 
most early breast cancers Once a breast cancer diagnosis has been established appropriate pre operative evaluation to assess the extent of disease and for staging purposes can guide surgical management and decisions on adjuvant therapy This article reviews the current approach and related evidence on pre operative assessment of women with newly diagnosed breast cancer It highlights changes in practice including areas of nonconsensus to inform general practitioners on evolving issues in pre operative care of the newly diagnosed breast cancer patient Conventional breast assessment and imaging Standard assessment of the breast over the past few decades has followed the triple testing strategy of clinical examination breast imaging mammography and ultrasound needle biopsy fine needle or core needle 4 Once a cancer diagnosis is established these conventional tests provide information critical to planning treatment for example mammography and ultrasound help guide selection to breast conservation 5 and core needle 
biopsy allows tissue testing for hormone receptor and human epidermal growth factor receptor 2 HER2 status6 to support decisions on adjuvant systemic treatment The more recent evolution of imaging and image guided biopsy methods has allowed opportunity to further refine pre operative testing including extension of breast ultrasound scanning to cover the axilla in women with invasive breast cancer 7 Pre operative axillary lymph node assessment The presence or absence of metastatic cancer in the axillary lymph nodes commonly termed node positive cancer remains a key prognostic feature and determinant of adjuvant treatments The majority of women with early stage breast cancer will be clinically lymph node negative at diagnosis ie no clinically palpable malignant nodes and will be managed with sentinel lymph node biopsy These women will undergo full axillary lymph node dissection ALND only if there is proven malignancy in the sentinel node s identified with intra operative lymph node assessment cytology or 
frozen section or on final histological assessment Women presenting with positive lymph nodes at diagnosis will usually be recommended to undergo planned up front ALND at the time of primary tumour excision Pre operative ultrasound examination of the axilla is part of the routine assessment of women with invasive breast cancer Guidelines from the National Institute for Health and Clinical Excellence recommend axillary ultrasound in all cases 7 While the sensitivity of ultrasound for detecting lymph node metastases is modest 61 4 in a meta analysis of 30 studies it has relatively high specificity 82 8 Of importance ultrasound guided needle biopsy can be performed on lymph nodes with an abnormal appearance This has moderate to high sensitivity 79 6 but very high specificity 98 3 and positive predictive value 97 1 8 The high specificity and implications of a positive ultrasound guided needle biopsy for changing surgical management have made it an acceptable strategy to triage patients to sentinel node based 
management versus ALND When used in this way ultrasound and biopsy will correctly triage 55 2 of histologically node positive newly diagnosed invasive cancer cases directly to ALND avoiding unnecessary sentinel node biopsy and allowing planning of adjuvant therapy at an early stage 8 The use of sentinel node based management in women with larger or multifocal multicentric tumours is debated Based on limited evidence accuracy may be similar to that for smaller unifocal tumours but the node positivity rates are high for the sentinel node s as well as the nonsentinel axillary nodes so only a minority of these patients will avoid ALND 9 Clinical trials are ongoing These women may also benefit from pre operative ultrasound assessment of the axilla with needle biopsy of abnormal nodes and the literature suggests that these tests may have a higher accuracy in this situation when there is a higher underlying risk of lymph node metastases 8 Breast magnetic resonance imaging The use of breast magnetic resonance 
imaging MRI has increased dramatically over recent years Initial evidence supported the use of breast MRI as a screening test in conjunction with mammography in asymptomatic young women at high hereditary risk of breast cancer as MRI is able to detect additional cancers compared to mammography alone 10 Breast MRI in high risk screening should be distinguished from the use of MRI pre operatively in newly diagnosed breast cancer to map out the extent of disease within the affected breast and to screen the contralateral breast at the time of diagnosis The indications for breast MRI in newly diagnosed cancer are controversial and there is no consensus on the best use of this test Breast MRI detects additional cancer in the breast compared to mammography in around 16 of new breast cancers 11 It may show the index lesion to be larger than on conventional imaging or may show separate smaller foci not seen on conventional imaging The sensitivity of MRI for detecting additional cancer in this setting has led to some 
investigators recommending the routine use of pre operative breast MRI where breast conservation is planned with the intention of adapting the surgical plan to include a larger excision or mastectomy if additional disease is seen on MRI However there is evidence that this approach leads to additional surgery conversion from wide local excision to mastectomy in 8 1 and wider excision in 11 3 of cases 11 without proof that the additional surgery has clinical or prognostic benefit 5 12 It is argued that most of these additional small foci of disease would have been adequately treated with adjuvant whole breast radiotherapy that is part of routine treatment following breast conserving surgery 5 12 There has been a worldwide increase in mastectomy rates in recent years and some authors have implicated the introduction of breast MRI as a possible factor contributing to this trend 13 14 A potential benefit of pre operative breast MRI may be a reduced rate of re excision A cancer that is shown to be larger on breast 
MRI than estimated using conventional imaging may be treated with wider excision in theory the need for a second operation to obtain clear margins could be reduced by the use of pre operative MRI The available evidence two randomised trials however does not support this theory 15 16 Breast MRI finds lesions suspicious of malignancy in the contralateral breast in 9 3 of cases around half of these will be false positive tests and among the true malignant lesions detected the majority are small lesions or ductal carcinoma in situ DCIS 17 Use of MRI to screen the contralateral breast is limited by its poor specificity and the fact that the lesions it detects tend to be lesions that may not be of clinical significance and or may be treated adequately by adjuvant systemic treatments given for the index cancer There is some evidence that women who are found to have suspicious contralateral breast lesions will choose to undergo bilateral mastectomy without a biopsy to assess the additional MRI detected lesion 17 The 
role of breast MRI in the context of newly diagnosed breast cancer therefore has been rigorously debated 12 18 21 National Institute for Health and Clinical Excellence guidelines recommend against the routine use of MRI and suggest it be considered selectively where there is a discrepancy in other pre operative tests where the breast tissue is extremely dense and in some cases of invasive lobular carcinoma 7 It is also reasonable to consider the use of breast MRI for screening the contralateral breast in pre menopausal women with a strong family history of breast cancer or a proven BRCA1 or BRCA2 gene mutation when they are diagnosed with breast cancer A pre operative diagnosis of ductal carcinoma in situ The incidence of ductal carcinoma in situ DCIS noninvasive disease has increased since the introduction of breast screening It now represents around 25 of breast malignancy and 20 of cases detected in the United Kingdom screening program 2 While the local treatment for DCIS is similar to that for invasive 
breast cancer there are differences As DCIS is a local rather than a systemic disease assessment of the axillary lymph nodes and the use of chemotherapy are not part of the usual management One of the main challenges in the management of DCIS is making an accurate pre operative diagnosis As most DCIS presents as asymptomatic mammographic microcalcification core needle biopsy CNB usually under stereotactic guidance is often required for diagnosis Where stereotactic biopsy is unavailable or the lesion is not accessible open surgical biopsy may be necessary to make the diagnosis When DCIS presents as a palpable mass or is visible on ultrasound biopsy may be performed under clinical or ultrasound guidance A challenge in the management of DCIS is underestimation where CNB shows DCIS but subsequent excision histology shows invasive breast cancer This occurs in 25 of all CNB diagnoses of DCIS 22 Understaged DCIS cases have a higher chance of needing a second operation compared to patients who have concordant CNB 
and excision histology findings 23 In addition these women undergo considerable emotional upset as their diagnosis changes from in situ disease to potentially life threatening invasive disease In this respect an understanding of the complexity of a CNB diagnosis of DCIS and the risk of underestimated cancer is helpful to allow discussion about the potential change in diagnosis following excision Underestimation is more likely when a lesion presents with a breast symptom is palpable is larger than 20 mm in diameter on imaging or shows a mass lesion on mammography rather than microcalcification alone 22 Ductal carcinoma in situ assessed with CNB under ultrasound or clinical guidance is more likely to represent understaged invasive disease than that biopsied under stereotactic guidance An awareness of factors associated with underestimation allows treatment planning This includes considering the use of sentinel lymph node biopsy at the time of lesion excision even if invasive disease has not been confirmed pre 
operatively 22 Screening for distant metastases as part of initial staging Current guidelines for the management of women with early breast cancer generally recommend against the routine use of staging imaging studies to detect asymptomatic distant metastases at the time of diagnosis 24 27 These recommendations are based on early studies that showed the incidence of detectable metastatic disease in women at breast cancer diagnosis is extremely small using chest X ray bone scan and ultrasound of the liver 28 29 A recent review of this area incorporating newer technology such as positron emission tomography PET and positron emission tomography computed tomography PET CT showed similar findings and further supports the view that routine screening for distant metastases in newly diagnosed women is not warranted In this review the median prevalence of metastatic disease on conventional imaging in Stage I breast cancer was 0 2 Stage II breast cancer 1 2 and Stage III breast cancer 8 0 30 The incidence was highest 
in inflammatory breast cancer 30 5 and 48 8 Studies using PET or integrated PET CT had a higher accuracy than CT X ray bone scan and ultrasound The review concluded that the routine use of staging scans in cases of early breast cancer could not be justified however it may be considered in more advanced presentations such as inflammatory cancer and more advanced Stage III cases with a large number of axillary lymph nodes involved by cancer at presentation 30 Conclusion The appropriate use of pre operative staging investigations can guide surgical management and adjuvant therapy decisions Assessment with triple testing using conventional modalities is essential Ultrasound assessment of the axilla is recommended for women with invasive breast cancer with biopsy of abnormal looking lymph nodes as this can select patients for sentinel node based management Breast MRI may have a role in the pre operative assessment of disease in selected cases but is not recommended for the majority of cases The management of DCIS 
is complicated by the common situation of underestimation where invasive breast cancer is present but is not detected on pre operative core biopsy The routine use of imaging studies to look for distant metastases is not indicated in the vast majority of breast cancer presentations General practitioners are well placed to discuss these issues with the newly diagnosed breast cancer patient 
61	b	Student Zachary Dumesny raises money for breast cancer research on Pink Ribbon Day October 28 2013 ABOUT 14 000 women across the nation are diagnosed with breast cancer each year Today thousands of South Australians will dig deep to help find a cure Today is Pink Ribbon Day the biggest event on the Cancer Council s fundraising calendar and hundreds of events including morning teas and girls nights in will be held across the state Woodend Primary School student Zachary Dumesny 11 is pitching in after the shock of a staff member s diagnosis led him to want to be part of a 9 4 million drive to find a cure Zachary said he was shocked and upset to lean a staff member was diagnosed with breast cancer But after talking to his mum the Year 6 student made a decision to fundraise for the National Breast Cancer Foundation with a little bit of help from his friends Zachary organised a small working committee and with the four Year 6 7 classes began to plan several fun activities around the colour pink to raise funds The 
team organised a pink casual day a pink jelly bean competition and a Pink Day where a group of Year 6 7 students who gained sponsorship had their hair coloured pink or shaved Zachary raised 50 and chose to have his hair shaved off to raise more money and because he felt empathy towards cancer patients who lose their hair Two people in our class are getting their hair shaved he said I wanted to see how cancer patients feel and to help raise more money Principal Brian Marshall said the school holds two major fundraisers but there are also opportunities for classes to fundraise for a charity of their choice and this year all the Year 6 7 classes worked together to raise funds for the National Breast Cancer Foundation It s a fantastic example of student enterprise and students initiating action he said We want students to understand about being empathetic and showing compassion and this is wonderful of putting it into action Mr Marshall said while the parents and teachers supported the fundraiser without the 
decision from Zachary and other students it may not have happened He has a genuine empathy for this experience and for people going through this and it s good to feel that and then influence other people he said The committee hopes to raise around 1000 
62	b	Breast Cancer charity rejects US7000 donation raised by men motorboating their faces into women s breasts THREE men who raised US7000 7243 for breast cancer research by motorboating women s breasts say their donation was rejected because of haters Self styled pickup artists Jason Jesse and Kong run SimplePickup to teach men how to approach and ask out women They launched a fundraising effort earlier this month donating US20 to breast cancer research to each woman who let them shove their faces into their breasts They raised more than US2000 in one day and promised to chip in an extra US100 for every 100 000 views of the video on YouTube bringing the total to US7000 Do you love boobs We sure do And this month is breast cancer awareness month so today we re working to save some boobies they say in the video After seeing the video the charity asked them to take down the video refunded the money and asked the self proclaimed ladykillers not to use their name anymore It s obvious they had to do this because they 
were getting pressured by a small minority of haters who thought this video was offensive says Kong who says critics are completely out of line So congratulations haters breast cancer researchers literally just lost 7000 because of your personal problems with this video The trio are now soliciting ideas for what to do with that 7000 and suggest spending the money on another breast cancer charity or using the cash to commit random acts of kindness 
63	b	Breast Cancer Breast Cancer What is Breast Cancer Breast cancer occurs when the cells lining the breast ducts or lobules grow abnormally and out of control A tumour can form in the ducts or lobules of the breast When the cells that look like breast cancer are still confined to the ducts or lobules of the breast it is called pre invasive breast cancer Most breast cancers are found when they are invasive This means the cancer has spread outside the ducts or lobules of the breast into surrounding tissue Read more about breast cancer Breast cancer symptoms Signs to look for include a lump lumpiness or thickening changes to the nipple such as a change in shape crusting a sore or an ulcer redness unusual discharge or a nipple that turns in inverted when it used to stick out changes to the skin of the breast such as dimpling of the skin unusual redness or other colour changes an increase or decrease in the size of the breast a change to the shape of the breast swelling or discomfort in the armpit persistent unusual 
pain that is not related to your normal monthly menstrual cycle remains after a period and occurs in one breast only Read more about the symptoms of breast cancer Breast cancer statistics Breast cancer is the most common cancer in Australian women About 13 000 women are diagnosed each year One in nine women will be diagnosed with breast cancer by the age of 85 It is more common in older women The average age at diagnosis is 60 About one quarter of women who are diagnosed are younger than 50 years of age Breast cancer is rare in men About 100 men are diagnosed in Australia each year This represents less than 1 of all breast cancers 
64	b	One colour doesn t always fit all with breast cancer Date October 18 2013 Comments 14 Carole Renouf inShare Pin It submit to reddit Email article Print Reprints permissions Ads by Google Invest in Queensland business qld gov au grow Never been a better time to invest employ grow in QLD Find out why Do pink politics and marketing over emphasise breast cancer survivorship Do pink politics and marketing over emphasise breast cancer survivorship I was deeply saddened a few days ago to hear from the mother of a young woman with advanced breast cancer that her daughter had died after living with the disease for several years Her daughter was only in her 30s and leaves three small children Also sad were the instructions her daughter left for her funeral Anyone attending was forbidden to wear any pink This was her last message to all of us and this month breast cancer awareness month I promised her mother to make it count Both mother and daughter had spoken to me about living with advanced breast cancer where the 
cancer has spread to other organs typically bones lungs brain or liver This is what eventually causes death It is difficult enough living with the disease they said but what they couldn t tolerate was the experience of being rendered invisible and inaudible Many women with metastatic disease feel that globally pink politics and marketing over emphasise survivorship that special club to which so many now belong but from which they are excluded They say their experience is just too scary to be acknowledged and does not fit within the ideology of hope that has been honed to perfection since the breast cancer movement began And theirs are not the bright shiny stories the media likes to tell Just as women with early breast cancer once banded together to draw attention to their needs women with metastatic disease have started to do the same It s become a pink splinter movement and one of its leaders and advocates is Musa Mayer Her book Advanced Breast Cancer emphasises Breast cancer strikes such terror into the 
hearts of women that they turn instinctively towards those who are deemed cured On the one hand women with advanced disease often silence and hide themselves so that they do not depress the newly diagnosed They feel they are somehow failures On the other those who have secured entry into the survivor club often feel a dreadful guilt in the face of those others who will not make it Research has brought us to a place where in Australia today we have five year breast cancer survival rates of 89 per cent A study published in the Medical Journal of Australia last year tells us that one in 20 women with disease confined to the breast and one in six women whose breast cancer has reached adjacent soft tissue or lymph nodes at diagnosis will eventually progress to metastatic disease Research has delivered the medical regimes for many of these women to live for years It has not yet got us to a place where they will not die but that is the next horizon Women who confront us with the riddle that you can be living with 
and dying of breast cancer at the same time are a presence in our lives As a society we are largely not ready for this and that needs to change Pink is all about women supporting and advocating for each other It makes me sad to hear that today some women living with metastatic disease feel excluded by it We need to find a more inclusive form of language one that is not all about battling and winning We need a new ideology embracing those who will survive and those who will endure for as long as possible Every day these women must walk the thin edge between living in the moment and living with illness between striving to be well and striving to accept their time remains limited while research continues the search for answers Let s not make their task any harder because our fear of dying gets in the way That s the pink elephant in the room Surely this October 28 years after the breast cancer awareness month was established internationally we can acknowledge that pink can have many shades some lighter some 
darker but all the same colour And that not all Pink Ribbon stories will have a happy ending but they all deserve to be told Read more http www smh com au comment one colour doesnt always fit all with breast cancer 20131017 2vpj9 html ixzz2j6Z34iKA 
65	b	Studying the genes for breast cancer Studying the genetic variations that predispose individuals to breast cancer has been the fuel for study for Honours student Julian Fang from Arundel The Bachelor of Biomedical Science student is currently in his third year of the honours accelerated program meaning that he can complete his degree with honours in just three years rather than the usual four Having just completed his 20 000 word thesis An association study of polymorphisms in the IL13 TCF7L2 SULF1 DUSP13 and GPC3 genes and breast cancer Julian says he is looking forward to the day when it may be possible to do a quick blood test to test an individual for their genetic susceptibility to breast cancer I had great teachers at high school which fostered my interest in science so it s really exciting to now be studying how different genetic variations of individuals could lead to altered risk of developing breast cancer In the future this work may lead to new developments in the research and treatment targets of 
breast cancer he says Honours on a part time basis Putting himself under pressure to complete his undergraduate program in a shorter timeframe has so far paid off for Julian with the honours component being completed on a part time basis during the second and third years of his study I always love to challenge myself and keep myself busy The opportunity to join the Griffith Honours College was a goal I worked towards in my first year of study because of the opportunities available for members says Julian Designed for outstanding students the Griffith Honours College provides students with enriching experiences in combination with undergraduate degree studies Having never travelled overseas on his own before Julian was also given the chance to go on a study exchange to the UK where he completed a semester of his degree at Leeds University That was really one of the best decisions I have ever made as it allowed me to experience the culture of other universities and network with other like minded people in my 
particular study area During my first year in the Honours College I was fortunate enough to travel to Cambodia on a trip organised by Griffith Honours College through ENACTUS an organisation which aims to change the world by making a difference in our local community through student run community engagement projects There we built a house for a widower looking after five children Unfortunately this family lived in extremely primitive conditions but with the assistance of some Cambodian builders we managed to construct a new house for the family in which they could live Now weighing up his career choices Julian says he is looking at options including medicine and research whilst keeping an interest in possible business pursuits I am excited about where the program is taking me and my development over the three years It s a competitive world out there so acquiring this depth of knowledge helps to gain an edge and opens up opportunities especially in the medical and healthcare world I am thankful that Griffith 
has provided me with so many opportunities and so much experience in just three years 
66	b	THE 2013 Walk 4Â Breast Cancer NOW ONLINE REGISTRATIONS HAVE NOW CLOSED BUT YOU CAN STILL REGISTER ON THE DAY The walk will go ahead rain hail or shine Where Tickle Park Coolum Beach When Saturday 29th June 2013 Time 20km Walk 7am Registration for a 8am start at Tickle Park 10km half walk 8am Registration for a 9am start at Castaways Beach Entry Fee 30 per person Parking Limited parking available in Coolum so please car pool where possible Courtesy busses will take you back to your car from Noosa Heads Surf Life Saving Club Enquiries walk4breastcancer cancerqld org au 1300 65 65 85 We look forward to seeing you there Walk For Breast Cancer 2013 OUR GOAL THIS YEAR IS TO RAISE 100 000 Â A CANCER FREE FUTURE WITHOUT BREAST CANCER IS WORTH WALKING FOR 
67	b	About breast cancer Breast cancer is a malignant tumour which starts in breast tissue It affects both women and much less commonly men who make up less than one percent of all breast cancer cases There are several types of breast cancer which most commonly begins in the milk ducts and or the milk lobules of the breast Some breast cancers are found when they are in situ or pre invasive meaning they have not spread outside the duct where they began Most breast cancers however are found when they are invasive meaning the cancer has grown beyond the duct or lobule into the breast tissue or to other parts of the body Breast cancer which spreads out of the breast may also spread to lymph nodes in the armpit nearest the breast affected by cancer axillary lymph nodes Breast cancer which is found before it appears to have spread beyond the breast and axillary lymph nodes is known as early breast cancer Breast cancer that has spread to other parts of the body such as the bones and liver is known as metastatic 
secondary or advanced breast cancer Breast cancer can be divided into four major stages Early breast cancer Early breast cancer is often detected through screening mammography or following a change or lump noticed in the breast Early stage breast cancer can usually be treated very effectively Locally advanced breast cancer Locally advanced breast cancer occurs when there is extensive growth of the breast cancer within the breast and or the axillary lymph nodes but no spread to other parts of your body This is a less common form of presentation for breast cancer Metastatic breast cancer Breast cancer that has spread from the breast and axillary lymph nodes to other parts of the body is known as metastatic breast cancer This may occur sometime after treatment of the primary breast cancer Much less commonly it may be apparent at the time of an initial visit to a healthcare professional There are a number of treatment options for this stage of disease Pre invasive breast cancer In pre invasive breast cancer the 
tumour is yet to become completely malignant Pre cancerous change also known as ductal carcinoma in situ DCIS does not have the ability to spread but if left untreated over time may develop into invasive breast cancer Treatment information Depending on the extent of disease treatment can involve surgery radiation therapy and or chemotherapy Click here to find further information about treatments About Peter Mac s Breast Service Peter Mac s Breast Service provides comprehensive and individualised care for a range of breast diseases from benign non cancerous breast conditions to advanced breast cancer This care is supported by a multi disciplinary team and a vibrant research program Patients will receive care from expert surgery oncologists radiation oncologists and medical oncologists in association with specialist breast nurse coordinators psychologists social workers radiation therapists dietitians occupational therapists palliative care professionals specialist breast pathologists and radiologists Your 
case will be discussed at a multi disciplinary team meeting ensuring you receive the best care and treatment plan As a tertiary training institution education is a primary focus for Peter Mac During your time at Peter Mac you will meet health professionals training at varies stages of education Your care will always be delivered under the direct supervision of senior specialist staff Clinic times Breast Service clinics run at East Melbourne each Monday and Friday morning and on Wednesday afternoons Clinical trials Peter Mac supports and recommends all patients consider participation in appropriate ethically approved clinical trials Referral information Your general practitioner or specialist will organise relevant referrals for you to Peter Mac s Breast Service For further information about referrals click here 
68	b	Accidental results excite breast cancer researchers A drug that was originally developed in Newcastle to kill parasites in meat and livestock is instead proving to be a potent weapon against breast cancer See your ad here It has been four years since Dr Jennette Sakoff lab researcher at Calvery Mater Hospital embarked on a project with the intention that it would help agriculture She never imagined it would instead show all the signs of becoming a targeted killer of breast cancer cells completely ignoring the healthy breast tissue It failed miserably at killing the parasites Dr Sakoff said It is definitely a bit of serendipity they were never designed to target cancer We screen a lot of molecules in our research lab and that s when we noticed Today in Australia research has brought the five year survival rate for breast cancer to about 89per cent But on average seven women in the country died each day from the disease according to the Australian Institute of Health Welfare and Cancer Dr Sakoff part of the 
Hunter Medical Research Institute said the drug held new hope for those living with advanced or metastatic where the cancer reappears elsewhere breast cancer This may now actually offer an extra treatment particularly for advanced tumours that have spread Also if someone becomes resistant to treatment options it is that extra tool in the box Because the compounds were so selective in what they destroyed Dr Sakoff said the drug could also have fewer side effects Part of our drug discovery component is that some new molecules have been found to specifically kill breast cancer cell lines but do little to nothing in other tumour types Dr Sakoff said You get this nice selectivity that targets breast cancer cell lines We don t know how they are doing it at the moment it s an observation and we re trying work out how the molecules do it Its selectiveness is a good thing because you are likely to have fewer side effects It also shows that there is something selective and unique about breast cancer Dr Sakoff said 
research was currently up to the early animal testing stage and so far it had been tolerated well It s still in its very early stages of research she said But if everything goes to plan it has the potential to have huge significance Another HMRI researcher has been making huge progress in a study that looks at stopping breast cancer from spreading Dr Nikki Verrills from the University of Newcastle has been looking at how a certain gene is switched off in breast cancer We still don t know whether this the gene switching off is primary or secondary to cancer she said See your ad here The gene s normal role is to stop cells continuously dividing Dr Verrills said they were looking at whether turning the gene back on would be beneficial for those with metastatic breast cancer It seems to stop the movement of cells from one place to the other she said 
69	b	Breast cancer is a tumour in the breast created by an abnormal and uncontrolled growth of cells Quick links About the breasts The basics of breast cancer Risk factors for breast cancer Signs and symptoms of breast cancer Early detection Diagnosing breast cancer Breast cancer treatments Help and support for people with breast cancer Further information Sources About the breasts Breasts are made up of fat connective tissue and gland tissue divided into lobes The lobes produce breast milk which is carried to the nipple by connecting ducts Breast tissue goes into your armpit where there are lymph nodes These are glands that are part of your immune system Diagram of a breast Most women have breasts of different sizes which look and feel different at certain times of the month depending on their menstrual cycle Your breasts also change during the different stages of your life for example after the menopause they become less full or dense Top of page The basics of breast cancer Breast cancer is the most common 
cancer to affect women in Australia excluding non melanoma skin cancer accounting for over 25 of all cancer diagnoses Approximately eight out of 10 women who develop breast cancer are over 50 Men can also develop breast cancer but this is rare Breast cancer usually starts in the cells lining the ends of the lobes and in the ducts themselves It can spread from there into the breast tissue and lymph nodes The two main types of breast cancer are described below Non invasive breast cancers Non invasive breast cancers are cancers that stay within the ducts or lobes of your breast and don t spread to surrounding tissue or to other parts of the body The most common type of non invasive breast cancer is ductal carcinoma in situ DCIS This is a very early type of breast cancer where the cancer cells are only found inside the milk ducts and haven t spread into the breast tissue In some cases DCIS can develop into an invasive form of breast cancer Invasive breast cancers Invasive breast cancers are cancers that have 
spread from the ducts or lobes of your breast into the surrounding tissue Invasive cancer most commonly starts in the ducts and spreads to other parts of the breast Less commonly it starts in the lobes Top of page Risk factors for breast cancer The causes of breast cancer aren t yet fully understood However there are certain factors that make developing breast cancer more likely You re more likely to develop breast cancer if you are female are older while breast cancer can happen earlier in life about three in four cases occur in people aged 50 and over have had breast cancer before or have previously had certain types of benign breast lumps have a close family member mother sister or daughter who has had breast cancer You may also have an increased risk of developing breast cancer if you have been exposed to radiation especially before 20 years of age have three or more standard drinks of alcohol a day have had some types of cancer apart from breast cancer such as skin bowel ovarian or thyroid cancer have a 
family history of ovarian cancer started your periods early before 12 years old went through menopause late after 55 years of age are overweight and have been through menopause There is a small increase in the risk of breast cancer for women who use combined oral contraceptives or hormone replacement therapy HRT Talk to your doctor about the risks and benefits of these medicines to you before starting any treatment or if you are concerned about your current treatment with these medicines There are also factors that appear to decrease your risk of developing breast cancer doing at least two or more hours of moderate intensity physical activity each week giving birth to at least one child having a first pregnancy before the age of 30 However there are inherent risks to pregnancy and birth These are higher than the slight additional risk of breast cancer so it s not recommended you have children just to reduce your risk of breast cancer Maintaining a healthy weight getting regular exercise and not drinking 
excessive amounts of alcohol may help to protect against breast cancer Top of page Signs and symptoms of breast cancer Some women won t notice any changes in their breasts and breast cancer is only found when they undergo breast screening However many women will notice a breast lump or a change in the overlying skin or nipple About nine out of 10 breast lumps are benign non cancerous but if you do find a lump you should see your GP straight away There are also other symptoms to look out for a change in the shape or size of your breasts a different shape to your nipple for example it may turn inwards or become irregular in shape dimpled skin a rash on or around your nipple blood stained discharge from your nipple swelling or a lump in your armpit These symptoms may be caused by problems other than breast cancer If you have any of these symptoms visit your GP for advice Top of page Early detection All women are at risk of developing breast cancer Having certain risk factors may increase your chance of 
developing the condition While you can t change some of these like your age or family health history some of these factors are things in your control There are also things you can do to detect breast cancer early if it does happen and this can mean a greater range of treatment options and may improve your chances of survival Breast cancer screening The BreastScreen Australia program invites all women between the ages of 50 and 69 for a free breast screening every two years Women aged 40 and over can also be screened in this program for free Breast screening means having a mammogram This can help to show very early signs of any cancer Self checks You can also check your breasts regularly yourself and you can do this at any age even if you re having regular mammograms Be aware of how your breasts normally look and feel this can be easily done as part of your everyday such as when showering or dressing so you can spot any changes quickly See your GP straight way if you notice any changes in your breasts Top of 
page Diagnosing breast cancer Your GP will ask about your symptoms and examine you Your GP may refer you to a specialist breast clinic where you re likely to have further tests The most common tests are Ultrasound scan An ultrasound scan uses sound waves to produce an image of the inside of the body or part of the body This is usually done if you re under the age of 35 Mammogram A mammogram is an X ray image of your breasts Biopsy Your doctor will take a small sample of tissue or cells This will be sent to a laboratory for testing to determine the types of cells and if these are benign or cancerous If you re found to have cancer you may need to have other tests to assess if the cancer has spread The process of finding out how big the cancer is and which parts of the body are affected is called staging The tests might include blood tests and a chest X ray Your doctor may also arrange for you to have a scan such as a CT computerised axial tomography or MRI magnetic resonance imaging scan These scans produce 
images of the inside of your body and can help your doctor to see if the cancer has spread You may also have a test to see if the breast cancer cells you have are positive for a certain protein or hormone receptor and will respond to hormone or biological treatment Top of page Breast cancer treatments There are a number of different treatments available for breast cancer and you may have more than one of them The treatment you have will depend on a number of factors such as your age and general health what type of cancer you have whether your cancer has spread what stage it is at whether or not you have been through menopause and whether you ve had breast cancer before Some women may also want to consider the effects of the treatment in their considerations Most cancer treatments work by destroying cancer cells but they can also affect healthy cells which may cause side effects such as nausea and vomiting reduced appetite feeling tired sore mouth hair loss changes in weight loss or gain decreased immunity to 
infections diarrhoea Your doctor will talk to you about the treatments available to you and help you decide on what is most appropriate for you taking into account your personal preferences Surgery The first line treatment for most people with breast cancer is usually surgery Depending on the size and position of your breast lump your surgeon may suggest either a lumpectomy or a mastectomy A lumpectomy is where the lump is removed usually with some healthy tissue around it A mastectomy is where the whole of your affected breast is removed You may be offered surgery to have your breast reconstructed afterwards With both of these procedures some lymph nodes may also be removed from your armpit This is to see whether the cancer has spread After surgery you may have other treatments such as radiotherapy These treatments can help to reduce the chance of the cancer coming back or spreading Radiotherapy Radiotherapy is a treatment to destroy cancer cells with radiation A beam of radiation is targeted on the 
cancerous cells which shrinks the tumour Radiotherapy is often used to treat breast cancer usually after a lumpectomy It may also been given after a mastectomy although the benefit of this treatment after a mastectomy is not so clear cut Your doctor or surgeon can advise you on this You re likely to need a course of radiotherapy that lasts for three to five weeks Radiotherapy is given as out patient treatment and each treatment only takes a few minutes but you will need to attend five days a week Chemotherapy Chemotherapy uses medicines to destroy cancer cells You may have chemotherapy before your surgery to shrink the tumour after surgery to reduce the chance of cancer coming back or spreading as a treatment for breast cancer that has spread or come back Some chemotherapy can be taken as a tablet but generally it is given as an injection into your vein You may need one or more different medicines as part of your treatment You will usually have chemotherapy as a treatment cycle with a few days taking the 
medicine s and then a few weeks of rest afterwards You may have a number of treatment cycles over a period of up to eight months Hormone therapy Hormones such as oestrogen can affect the growth of breast cancer cells Taking medicines that block these hormones can treat some kinds of breast cancer Hormone therapies are usually used after you have had surgery though they can be used before surgery or to treat cancer that has come back Women usually take hormonal treatments for at least five years following initial treatment for breast cancer There are several types of hormone therapy Talk to your doctor about what is the right medicine for you Biological therapies Biological therapy also known as targeted therapy is treatment with monoclonal antibodies that stimulate your body to attack or control the growth of cancer cells Antibodies are proteins produced by your immune system that usually fight against bacteria and viruses Monoclonal antibodies are made in a laboratory and are designed to seek out particular 
cells The most commonly used biological therapy is called trastuzumab Herceptin It works by targeting breast cancer cells that have a particular protein on their surface called HER2 Trastuzumab is given into your vein via a drip If your breast cancer returns you may be given another biological therapy that targets HER2 called lapatinib Tykerb These therapies only work if you have breast cancer that produces the HER2 protein About 20 percent of people have this type of cancer known as HER2 positive breast cancer Another biological therapy that may be used if your breast cancer returns is bevacizumab Avastin This therapy works by blocking another protein that is known to be involved in breast cancer called VEGF vascular endothelial growth factor Bevacizumab is also given into your vein via a drip After treatment After your treatment has finished your doctor will ask you to have regular check ups He or she will ask if you have any symptoms and may also arrange for you to have blood tests and mammograms This is 
to check for signs of the cancer coming back If you re well you will need fewer check ups as time goes on If you have any symptoms in between your appointments or are worried at all speak to your doctor Help and support for people with breast cancer Finding good support if you or someone close to you has cancer is important Being diagnosed with cancer can be distressing Having support to deal with the emotional and physical aspects of living with cancer can make it easier to cope Don t be embarrassed or afraid to reach out to family friends or your health professionals for help and support Specialist cancer doctors and nurses are also able to provide support You may also find it helpful to see a counsellor 
70	b	Together we can make a difference 2010 Together we can make a difference 2010 Over the past four years Mount Franklin has supported breast cancer research donating well over 1 million dollars to the cause We are now pleased to team up with our new friends at the McGrath Foundation to support women diagnosed with breast cancer and their families right across Australia With the McGrath Foundation currently funding 55 McGrath Breast Care Nurses throughout Australia to help provide valuable support to families experiencing breast cancer it s now time to do our part and support Continue Reading Choose Pink 2009 Choose Pink 2009 2009 represents the fourth year of our Corporate Partnership with the National Breast Cancer Foundation To celebrate this milestone we are asking you to show your support by coming together to help create the Mount Franklin Ribbon of Support The Ribbon of Support is an online campaign that harnesses the popularity and strength of Twitter to get people to share their experiences knowledge 
and thoughts on breast cancer It s designed to generate thoughts on the fight against breast cancer The Ribbon is Continue Reading The Power of Positivity 2008 The Power of Positivity 2008 To encourage people to think and act positively Mount Franklin launched the Positive Pink Ribbon Pilates Program during Pink Ribbon month in 2008 The goal was to show people how they could improve their lives by creating a positive outlook for their future Over eight days Mount Franklin took Doreen Puglisis founder of the Program across Australia The response was overwhelming About the program Continuing on the path of spreading positivity and creating awareness for breast cancer research in 2008 Mount Franklin Continue Reading Bringing People Together 2007 Bringing People Together 2007 For Pink Ribbon month in 2007 Mount Franklin focused on bringing people together in the fight against breast cancer We ran a multi tiered campaign that consisted of three main parts Well of Positivity Inspirational Quotes and Every Mouthful 
Helps Each was designed to spread goodwill and help raise awareness for the National Breast Cancer Foundation The results were outstanding About the Project 2007 was all about bringing people together in their fight against breast cancer by encouraging them to share their personal Continue Reading Turning Pink for charity 2006 Turning Pink for charity 2006 In our first campaign to raise awareness for breast cancer research we added a touch of pink to all our water bottles during Pink Ribbon month The lids across the entire range were changed to pink and we released three limited edition designs on the Mount Franklin Lightly Sparkling glass bottles We also ran the Feel Good in Springtime exhibition which included a series of art works designed by high profile Australians 
71	b	mamming meme opens young women up to screening harms Social media can help raise awareness of health issues engaging people in discussion and encouraging them to take action But thoughtless adherence to such trends has the potential to cause harm New ideas or memes can spread quickly on platforms such as Twitter and Facebook Mamming is such a new Author Monika Merkes Honorary Associate Australian Institute for Primary Care Ageing at La Trobe University Disclosure Statement Monika Merkes does not work for consult to own shares in or receive funding from any company or organisation that would benefit from this article and has no relevant affiliations La Trobe University Provides funding as a Member of The Conversation latrobe edu au Director LIMS La Trobe University s success is driven by people who are committed to making a difference They are creative and highly Science Director La Trobe University s success is driven by people who are committed to making a difference They are creative and highly School 
Partnerships Coordinator Regional Manage and lead the partnerships program in Bendigo Full time fixed term appointment until December 2014 81 146 93 More La Trobe University Jobs Where experts find jobs Mamming involves resting breasts on a flat surface and taking a photo Ondra Anderle Social media can help raise awareness of health issues engaging people in discussion and encouraging them to take action But thoughtless adherence to such trends has the potential to cause harm New ideas or memes can spread quickly on platforms such as Twitter and Facebook Mamming is such a new meme Mamming is the brain child of Michelle Lamont and Michele Jaret two young women in the advertising industry It involves resting clothed female or even male breasts on a flat surface Then a photo is taken and sent off to Twitter and Instagram If you find this description not clear enough this video clip will show you how it s done Also the thisismamming website provides further examples Mamming photos are tweeted to raise awareness 
of breast cancer to reduce the awkwardness of mammograms and to encourage women to participate in breast cancer screening Lamont and Jaret propose that Mamming is a chance for all of us to show solidarity with the millions of women getting mammograms this Breast Cancer Awareness Month Because when a woman reaches a certain age doctors recommend that she get a mammogram to screen for the disease and the procedure involves laying her boobs on the machine s flat surface It appears many young women and some young men have taken up the challenge and posted photos of their breasts on Twitter and Instagram Several men on Twitter have greeted the new meme as a welcome distraction while others have expressed their support for breast cancer awareness and screening So far so good But we should be careful about following this trend uncritically Many people may not realise that breast cancer screening involves benefits as well as harms The Cochrane Collaboration did a systematic review of research evidence and found that 
for every 2 000 women invited for screening over ten years one will avoid dying of breast cancer But ten healthy women who would not have been diagnosed if there had not been screened would be treated unnecessarily And more than 200 women will experience psychological distress including anxiety and uncertainty for years because of false positive findings The reason why this is of particular concern is that the women who have so far embraced the mamming meme are for the most part young In Australia breast cancer screening until recently was recommended for women aged between 50 and 69 years But data released today shows that just over half 55 of women in the target age group had a mammogram in 2010 2011 This age range has now been extended to 74 years In the United States the American Cancer Society recommends that women start earlier at 40 and continue to have mammograms every year In the United Kingdom women aged between 50 and 70 years soon to be between 47 and 73 are routinely invited to breast cancer 
screening every three years Lamont and Jaret told Huffington Post that they hope mamming reminds women to get screened and maybe makes them feel a little less awkward about putting their boobs in the mammogram machine And that s what doesn t fit regular breast screening is not recommended for young women yet mamming s purpose is to remind women to get screened The vast majority of women portrayed on the thisismamming website look younger than 35 years of age What s more the women who keep reminding each other by sharing their mamming photos on social media appear to be young women Will the trend make young women feel they should have a mammogram and risk unnecessary psychological distress and interventions such as surgery It s unlikely that they blindly follow every craze but at first sight mamming appears to be in support of a worthwhile cause When Kylie Minogue s diagnosis of breast cancer became public knowledge there was a surge of young women participating in breast cancer screening We can only hope 
this craze dies down as quickly as it emerged In the meantime I ll keep tweeting with the hash tag mamming pointing young women to the Nordic Cochrane Centre s mammography screening leaflet If you re on Twitter why don t you join me in setting the picture straight
72	c	Syrian Electronic Army wrests control of url links in President Obama s tweets emails and facebook messages Obama A Syrian Electronic Army logo is splashed over the control panel of a service used by President Obama s campaign office and inset below a tweet warning we are watching you Source Twitter SEA The logo of the Syrian Electronic Army hacking group which succeeded in diverting links from President Obama s electronic messages to a propaganda video Source Supplied SYRIAN hacktivists have compromised President Barack Obama s Twitter Facebook and Gmail accounts The organisation which calls itself the Syrian Electronic Army SEA have reportedly hijacked a series of the US President s tweets and messages over the weekend with links to propaganda posted on YouTube The attack which was carried out on the President s campaign office and his own accounts was not directed at the major social media service providers Rather it is understood a URL shortener service ShortSwitch was breached allowing the hackers to 
redirect any link inside the President s tweets Facebook message or email Anyone clicking on the links were directed to a video created by the SEA The SEA also claims to have breached a Gmail account related to the President s electoral Organizing for Action campaign team A series of screenshots purporting to show the account s inbox have been circulated online Obama doesn t have any ethical issues with spying on the world so we took it upon ourselves to return the favor SEA SyrianElectronicArmy Official_SEA16 October 28 2013 A Twitter spokesman said in a tweet that The BarackObama Twitter account was not compromised their link shortener was The Syrian group also indicated it took over a Gmail account from Obama s campaign and a page from that website We accessed many Obama campaign emails accounts to assess his terrorism capabilities They are quite high the group tweeted The SEA has attacked a series of high profile organisations in recent months including several media organisations such as the New York 
Times CNN The Guardian and Associated Press The group alleges bias in the coverage of most media outlets over the unrest in Syria 
73	c	Syrian hackers hit Obama linked Twitter Facebook accounts inShare1 Share this Email Print Reprints Related Topics World Tech Syria U S President Barack Obama speaks at the Pathways in Technology Early College High School in Brooklyn October 25 2013 REUTERS Aaron Showalter Pool WASHINGTON Mon Oct 28 2013 4 45pm EDT Reuters The Syrian Electronic Army a hacker group sympathetic to Syrian President Bashar al Assad on Monday seized control of an online tool used by an advocacy organization for U S President Barack Obama to redirect links sent from his Twitter and Facebook accounts The link shortener used by Organizing for Action a group that evolved from Obama s re election campaign was briefly hacked an official from the group said Link shorteners abbreviate Web links so they take up less space in a tweet which is limited to 140 characters Obama s Facebook and Twitter pages carried links that were intended to take readers to a Washington Post story on immigration but as a result of the hack redirected readers to 
a video of the Syrian conflict instead However Obama s Twitter account itself was not hacked Twitter spokesman Jim Prosser said Obama rarely writes his own tweets from the BarackObama Twitter handle which is run by the Organizing for Action staff However when he does they are signed with his initials The Syrian Electronic Army tweeted We accessed many Obama campaign emails sic accounts to assess his terrorism capabilities They are quite high It showed what appeared to be the Google email account of an Organizing for Action staffer The Syrian Electronic Army has undertaken several high profile hacking attempts in the United States In September it appeared to have struck a recruiting website for the U S Marine Corps and the FBI that month also warned that the group might intensify its internet attacks as the United States weighed a military strike against Syria in response to an alleged chemical weapons attack by the Syrian government against its people The group has also targeted the New York Times website 
and Twitter Syria has been locked in a civil war dating to March 2011 which appears to be in a stalemate for now The conflict grew out of an uprising against four decades of Assad family rule pitting mainly Sunni Muslim rebels against a president Bashar al Assad whose Alawite faith is an offshoot of Shi ite Islam 
74	c	Explore this page to learn more about President Obama s response to the Assad regime s use of chemical weapons in Syria and get the latest news from the White House about the situation News and Updates On September 10 President Obama addressed the nation on Syria As part of a briefing on the response to Syria members of Congress were shown video taken in multiple locations near Damascus on August 21st when more than 1 400 Syrians including more than 400 children were killed by a chemical weapons attack perpetrated by the Assad regime It s important for the American people to have access to information about the use of chemical weapons in Syria The U S Senate Select Committee on Intelligence has released these videos which were compiled by the U S Open Source Center You can watch the videos here Warning These videos contain disturbing images of dead bodies including children VIEWER DISCRETION IS ADVISED On September 9 Ambassador Rice laid out the case for the damage that would be done to our national security 
and that of our partners and allies should we fail to respond to enforce the longstanding international norm against the use of chemical weapons On September 7 in his weekly address President Obama makes the case for limited and targeted military action to hold the Assad regime accountable for its violation of international norms prohibiting the use of chemical weapons On September 6 U S Permanent Representative to the United Nations Samantha Power discussed the Assad regime s use of chemical weapons against Syrian civilians and the need for an international response On September 3 following his September 1 announcement President Obama met with Congressional leaders at the White House to discuss his plan for military action On September 1 President Obama laid out the case for a targeted military action against Syrian regime targets as a result of the Assad regime s use of chemical weapons that killed over one thousand people including hundreds of children The President also made clear that this would not be 
an open ended intervention and there will be no American troops on the ground Instead our action would be limited in duration and scope The President has put forward a proposed authorization that is focused on his clearly stated objectives preventing and deterring the use and proliferation of chemical weapons within to or from Syria degrading the Assad regime s capacity to carry out future chemical weapons attacks and deterring this behavior in others including Iran and Hizballah who would otherwise feel emboldened to use such weapons While the President was clear on the need for action he announced he would seek Congressional authorization for the use of force On August 30 Secretary Kerry delivers remarks on the situation in Syria in the Treaty Room at the U S Department of State in Washington D C 
75	c	First Syria now Iran Obama being played for a fool John Bolton The Australian October 01 2013 12 00AM Share expand Share on facebook Your Friends Activity NEW Discover news with your friends Give it a try To get going simply connect with your favourite social network Facebook YESTERDAY ended the worst month of the Obama presidency The Syrian diplomatic and political debacle was bad enough but last week at the UN President Obama embarked on a campaign for progress with Iran that will prove much more dangerous for American interests Just as Vladimir Putin had played him for a fool over Syria Barack Obama was initially snubbed by Iranian President Hassan Rowhani despite frantic White House efforts to produce a handshake On Friday after a brief Obama Rowhani telephone call Obama said that a comprehensive solution between countries is possible And this despite the previous day s meeting of foreign ministers including Secretary of State John Kerry and Iran s Mohammad Javad Zarif that was little more than a photo 
op Obama s yearning fits smoothly into the PR campaign by President Rowhani Iran s new frontman The campaign has included showcasing Iran s only Jewish parliamentarian a staunch opponent of Israel offering dialogue with the West catnip for the gullible and a soothing Washington Post op ed Separating propaganda hype and disinformation from Iran s real objectives is critical Unfortunately too many already believe that Rowhani s election marked a substantive rather than a cosmetic policy shift Instead of blustering about Iran s nuclear program and threatening Israel Rowhani has sounded conciliatory carefully using his first weeks in office to cloud Western memories of his predecessor Mahmoud Ahmadinejad President Rowhani knows what his Western audience wants to hear As Iran s chief nuclear negotiator in 2003 05 he followed the same playbook and it worked By offering what appeared to be concessions Iran acquired precious time and legitimacy to overcome scientific and technical glitches in its nuclear weapons 
program In articles and speeches Rowhani boasted of his successes In 2006 he taunted the West saying by creating a calm environment we were able to complete the work on uranium conversion facility Isfahan Even such open disdain has not triggered enough US or European embarrassment to protect against being suckered again Iran s moderates are now targeting the Obama soft spot in Western opposition to Iran s nuclear program and methodically exploiting it In marked contrast Obama enters negotiations gravely weakened by his Syria failures Yet soothed by his media choir he seems unaware how deeply he has been wounded He confidently believes he is well placed to deal with the ayatollahs despite a series of foreign policy failures Over the past year Obama failed in his stated objective to oust Syria s Assad regime from power failed to impress Assad that his red line against using chemical weapons was serious failed to exact retribution when that red line was crossed failed to rally anything but small minorities in 
either house of congress to support his position and failed to grasp that agreements with the likes of Syria and Russia prolong rather than solve the chemical weapons problem Obama is inverting Dean Acheson s maxim that Washington should only negotiate from strength Even if there were some prospect that Iran could be talked out of its nuclear weapons program which there is not the White House approach is the wrong way to start discussions Given the President s palpable unwillingness to use the military to enforce his Syria red line let alone to answer last year s September 11 Benghazi terrorist attack and his paucity of domestic political support Iran s ayatollahs know that the President s all options on the table incantation regarding their nuclear program carries no weight Iran undoubtedly wants relief from international sanctions which have exacerbated decades of incompetent economic policy But there is no evidence that the sanctions have impaired Iran s nuclear or ballistic missile programs Instead 
Tehran has increased its financial and military assistance to Assad and Hezbollah in Syria Rowhani s strategy is clear lower the rhetorical temperature about the nuclear issue make temporary cosmetic concessions such as allowing inspections by the International Atomic Energy Agency at already declared nuclear sites and gain Western acceptance of its reactor grade uranium enrichment Once that goal is attained Iran s path to nuclear weapons will be unobstructed Iran will demand in return that international sanctions be eased focusing first on obtaining small reductions to signal Western good faith Obama and Europe already seem eager to comply Western diplomats will assert defensively that these concessions are merely a matter of sequencing and that they expect substantive Iranian concessions They will wait a long time Rowhani fully understands that once sanctions start rolling back restoring them will be hard perhaps impossible Rowhani will not supply a major provocation Instead he will continue making on 
again off again gestures seducing the West into protracted negotiations Meanwhile Iran s nuclear weapons and ballistic missile programs will proceed unimpeded in unknown undisclosed locations This was his 2003 05 playbook Extended negotiations will enable Obama to argue that a diplomatic process is under way to resolve the Iranian nuclear threat No phrase is more beloved at the State Department Obama will then use this process on Israel to prevent pre emptive military action against Iran s nuclear program In time even Hamlet came to understand that one may smile and smile and be a villain Maybe one day President Obama will figure it out See more at http www theaustralian com au news world first syria now iran obama being played for a fool story e6frg6ux 1226730266947 sthash prSYDBUa dpuf 
76	c	Obama s Syria dilemma showed that for foreign policy to be successful the management of information is just as important as the substance of ideas Blog Admin Sam Hazelgrove 80x108Last month after a seemingly inevitable march towards U S military invention in Syria a Russian brokered deal was made for the Assad regime to place Syria s chemical weapons under international control While many have critiqued what they term President Obama s improvisational foreign policy Sam Hazelgrove argues that Obama s actions on Syria are a result of a pragmatic approach that should be seen in the wider context of growing questions over America s role in the world He writes that in order to formulate a more coherent foreign policy strategy Obama s administration must move to a more centralised decision making process so that immediate national interests can be reconciled with longer term interests The agreement between the United States and Russia last month to bring Syrian chemical weapons under control was a diplomatic 
achievement that during the past year had seemed a political impossibility A month ago it appeared more than likely that the President would employ military power to deter the Assad regime from using biological weapons in the Syrian civil war As President Obama fluctuated between the options of non interference and military intervention in Syria his critics accused him of reckless improvisation of leading an Administration with no clear objectives and no coherent strategy in the Middle East For example Stephen Walt argued that the President had been guilty of taking an ad hoc approach to policy which left the Administration perennially buffeted by events and vulnerable to a number of domestic and foreign political pressures He mentioned that the Administration had fallen back on the familiar rhetorical gambit of lauding democracy human rights and stability and used this method as a substitute for policy Consequently it appeared to many that the United States stumbled into an international agreement that 
condemned and banned the use of Syrian chemical weapons Credit whitehouse gov Obama addresses the nation on Syria Credit whitehouse gov Yet those sympathetic to the Administration s bind have rebuffed these criticisms and claimed that Obama s consistency can be located in the historically rooted Jeffersonian principle of an aversion to entangled alliances However reflecting on these arguments we find that both are right When taken together these two seemingly diametrically opposed views bring to the surface the intellectual tension that has plagued Obama and disjointed his foreign policy this is a President who is a pragmatist and who also possesses a strong sense of moral obligation to certain international norms His mission regarding Syria might be described as an exercise in trying to reconcile two competing philosophies As the President wrestled with this dilemma he quickly found that there was no good answer to the question of intervention in Syria Since the beginning on the year the Administration put 
forward a series of policy initiatives that oscillated between unilateral military action and multilateral engagement from announcing that the use of chemical weapons crossed a red line to claiming that the Russian proposal for Syria to hand over it s biological weapons as a positive development There is of course nothing strategically wrong with the Administration taking a more pragmatic approach to Syria and changing its mind when circumstances in Syria or in the wider Middle East region alter look at the current negotiations with Iran for example the absence of a doctrine can make Obama s foreign policy supple This should be considered a prudent method of creating policy After all John Kerry even explained to the Senate Foreign Relations Committee upon being nominated as Secretary of State that Never before has the new world order had to be assembled from so many different perceptions or on so global a scale Notwithstanding the absence of a doctrine a strategy is crucial The series of policy initiatives 
have not been a consequence of the President s pragmatism but have resulted from the differing answers that bureaucracies have given to more abstract questions about America s role in the world Obama s existential crisis has permeated foreign policy institutions resulting in a lack of coherent policy regarding Syria His Syria dilemma has resulted from the problem of information management as much as it has from the substance of policy For example as the Syrian crisis developed throughout 2012 Obama s Administration reportedly focused some of their efforts on trying to provide a military solution to the Syrian civil war Following intelligence reports of chemical weapons use in Syria discussions were reportedly held about the feasibility training and arming Syrian rebels Media reports suggest that this proposal was backed by then Secretary of State Hilary Clinton who argued that the US should become more involved This arguably shows the sense of a moral obligation felt by the Obama Administration to act in 
Syria at a time when his National Security Adviser Tom Donilon known as a cautious realist was concerned with shifting America s hard power to Asia The problem of managing information has not just been an inter institutional one it has also been an intra institutional one A perfect example of this was the moment that John Kerry claimed that to stop a seemingly imminent US bombing campaign Assad could turn over his chemical weapons to the international community in the next week turn it over all of it without delay and allow the full and total accounting Almost immediately a spokesperson at the State Department attempted to retract Mr Kerry s remarks by telling reporters that the Secretary was making a rhetorical argument about the impossibility and unlikelihood of Assad turning over the chemical weapons he has denied using Diverse discussions across foreign policy institutions that centre on more abstract questions about the national interest or the use of America power are essential for Obama s team However 
for the purposes of formulating a coherent strategy and conveying a clear message it might be argued that there needs to be a slightly more centralised foreign policy decision making process The conversations that occur between and within the State Department National Security Council and Defense Department need to be channelled in such a way that in the words of Richard Haas immediate interests of the United States can be squared effectively and seamlessly with larger and indeed longer term national security concerns An important lesson from the Syria dilemma is that the management of information can be as crucial as the substance of ideas to the creation of coherent strategy and ultimately the success of policy 
77	c	As it happened Obama addresses the US on Syria Key Points US President Barack Obama has said the US must be prepared to strike Syria to deter other tyrants from using weapons of mass destruction But he said he recognised the US was weary of war and pledged he would not put American boots on the ground Russia s plan for Syria s chemical weapons to be put under international control has set off wrangling at the UN All times BST Obama address on Syria crisis in full Watch Join the discussion Send us an SMS to 44 7624 800 100 Twitter BBC_HaveYourSay Facebook Facebook Comment hereOpen Foldout Required Name Required Your E mail address Required Town Country Your telephone number Required Comments The BBC may edit your comments and not all emails will be published Your comments may be published on any BBC media worldwide Terms and conditions Report Reporters Debbie Siegelbaum and Daniel Nasaw 0100 Hello and welcome to our live coverage of US President Barack Obama s address to the nation from the White House In an 
hour s time Mr Obama is expected to reiterate his call for Congress to authorise the use of force in Syria in order to maintain the threat of a military strike officials say 0101 Mr Obama s speech comes amid disputes in the UN over a Russian proposal that Syria s chemical weapons be put under international control The UK France and the US want a timetable and consequences of failure spelt out while Russia has said any draft putting the blame on Syria is unacceptable 0110 Syrian Foreign Minister Walid al Muallem said on Tuesday his nation was willing to become a party to the Chemical Weapons Convention Russian media reported We are ready to honour our commitments under this convention including providing information about these weapons he said 0117 The BBC s Katty Kay describes Mr Obama s speech as an extremely hard juggling act He must convince the American people that military action is the right course in Syria while at the same time acknowledging the negotiations at the UN over Russia s disarmament 
proposal 0121 NPR s Scott Simon tweets This POTUS is his own best speechwriter this is vital issue he shouldn t leave to young writers trying to imitate him 0127 The BBC s Katty Kay at the White House 10 September 2013 The BBC s Katty Kay reports tonight from the White House in Washington DC 0128 Lyse Doucet BBC Chief International Correspondent tweets Beirut often put in top 10 cities that never sleep Wonder if many people will watch Obama speech at 4 am local time 0132 Meanwhile Asian stock markets are advancing as investors cheer what appears to be an easing of tensions over Syria reports Bloomberg 0134 James London UK emails I hope the current diplomatic moves to avert US air strikes in Syria will go ahead and work As happened recently with Libya forcibly removing the regime in Syria would likely result in something far worse than is the case at present The diplomatic route with the West acting as more honest brokers is a far better option 0134 The BBC s Rajini Vaidyanathan tweets After a day of sleep I 
wake up to find the plates have shifted on syria Wondering what president obama will say to the nation tonight 0136 Mr Obama is expected to explain to Americans why it is in the US national security interest for Syrian President Bashar al Assad s regime to face consequences for chemical weapons use and to stress that any military action in the region will be limited in scope and duration Reuters reports 0136 Join the conversation about tonight s address from the White House and keep up with our producers and correspondents at our live Tweet session BBCNewsUS 0140 Reuters also reports Mr Obama will acknowledge his scepticism of Russia s proposal for Syria s chemical weapons to be put under international control but will pledge to explore the initiative further 0142 The Washington Post s Chris Cillizza tweets On my Verizon channel guide there is 15 minutes marked off for presidential address He has to talk longer than that right 0147 CNN s Piers Morgan tweets Fascinated to see how Pres Obama tries to persuade 
Americans what should be done in Syria when he doesn t seem to have a clue himself CNN 0148 Michelle Fortier Portland United States emails Assad is a moderate dictator not a Saddam Hussein or Osama bin Laden by any means so why are we targeting this man Why are we butting into other countries civil wars 0154 Saleem Brhoum Portland United States emails I am a Syrian American was born in Syria and have been travelling to Syria every summer prior to 2011 I strongly reject any intervention in Syria at this time This is an internal matter and we should and must let the Syrian people decide what happens 0156 Here is where the US House of Representatives stands on a resolution authorising a military strike on Syria according to the Washington Post 149 against 102 leaning no 156 undecided 26 for strikes 0158 Mr Obama is expected to begin his speech from the White House s East Room in two minutes 0158 Arms control experts warn of the complex and risky operation facing any UN teams that would be dispatched to Syria to 
secure chemical weapons stores the Washington Post reports They predict that locating safe guarding and destroying weapons caches in the midst of a civil war would be dangerous and costly and could take years to accomplish 0202 Mr Obama is walking up to the podium 0204 Mr Obama says he has resisted calls for military action because we cannot resolve someone else s civil war through force 0206 Mr Obama attributes the death of more than 1 000 in Syria including children to a chemical attack perpetrated by Mr Assad s regime He describes the images of the dead as sickening 0207 Mr Obama says if fighting spills beyond Syria s borders chemical weapons could threaten US allies such as Turkey and Israel 0210 Mr Obama I will not put American boots on the ground in Syria 0212 Mr Obama says he doesn t think the US should remove another dictator by force but says a targeted strike could make one think twice before wielding weapons of mass destruction 0213 Larry Sabato of the University of Virginia tweets Obama doesn t 
bother to combat view that new boss in Syria may be same as old boss evil 0213 Mr Obama I have a deeply held preference for peaceful solutions 0214 Mr Obama says of the proposed Russian chemical weapons disarmament plan It s too early to tell if this offer will succeed 0217 Mr Obama says he hopes to rally support from international allies who agree on the need for action and says he has ordered the US military to remain ready for action in order to keep pressure on Mr Assad s regime 0218 Mr Obama says It is beyond our means to right every wrong but when with modest effort and risk we can stop children from being gassed to death I believe we should act That s what makes America different 0221 Cassidy Lynn Nashville tweets Beautiful speech Obama This is a matter of human rights Something must be done but if we can prevent more death that s preferable Syria 0222 Mr Obama says the US is not the world s policeman but he says America s national security will be put at risk if the use of chemical weapons is 
unpunished 0225 Mr Obama says any military action in Syria would involve a targeted strike and would be followed by redoubled efforts to find a diplomatic solution to the crisis He pledges to work with Russia on its proposal that Syria turn over control of its chemical weapons but says the US will maintain the threat of force should diplomacy prove unsuccessful 0227 The president said Syria distributed gas masks to its troops before the 21 August chemical weapons attack 0227 Mr Obama has concluded his speech and walked away from the podium in the White House s East Room 0231 Nicholas Kristof of the New York Times tweets Does it bother anyone else that the basic Obama message to dictators is When you slaughter your people don t use gas 0231 The BBC s Katty Kay tweets Obama says he will have conversations with Putin and send Kerry to meet Lavrov but he is ordering US military to stay ready 0234 The BBC s Katty Kay tweets I m not sure that speech will have changed any minds but it was clear and invoking WW1 and 
WW2 was powerful 0236 Reince Priebus chairman of the Republican National Committee describes the White House s response to Syria as haphazard and rudderless diplomacy which has embarrassed America on the world stage For a president who campaigned on building American credibility abroad the lack of leadership coming from the Oval Office is astounding he says 0241 BBC Washington Bureau Chief Paul Danahar tweets Obama s speech on Syria can be summed up as I ve no idea what we re going to do give me a couple of weeks and I ll get back to you 0242 Mr Obama said he had asked Congressional leaders to postpone a vote authorising the use of force against Syria while the US and United Nations pursue diplomatic negotiations But it was far from certain the measure would pass analysts say 0243 The BBC s Katty Kay tweets I suspect most Americans asking if there isn t going to be a vote and a strike isn t imminent why did the President give this speech now 0249 Nancy Pelosi Democratic Leader US House of Representatives 
tweets Pres Obama s leadership brought diplomatic solutions back to the table shows his willingness to exhaust every remedy before use of force 0254 Mr Obama asked members of Congress and Americans at home to watch online videos of the aftermath of the last month s chemical weapons attack But as the BBC s Jane O Brien reports Americans seem uninterested in the gripping if often horrific videos emerging from the conflict 0254 David Kay former chief weapons inspector in Iraq tells CNN that Syria has an improved version of sarin nerve gas at its disposal 0255 Mark Mardell BBC North America editor says Mr Obama s speech was almost entirely lacking in passion and devoid of new arguments and leaves more questions than answers regarding Syria 0308 Republican Senators John McCain and Lindsey Graham say they appreciate Mr Obama addressing the American people directly on the Syrian conflict but regret he did not speak more forcefully about the need to increase our military assistance to moderate opposition forces in 
Syria and lay out a clearer path regarding diplomatic negotiations The proposed resolution would have to threaten serious consequences if the Assad regime does not comply and it would have to be presented to the Security Council for an up or down vote We would expect Russia and China to support such a Resolution without delay they say 0313 This concludes our live coverage of US President Barack Obama s address to the nation on Syria For more coverage of the Syria conflict the diplomatic effort and the possible military strike please continue to visit our website and follow us on Twitter 
78	c	Shift on Syria angers Saudi Arabia muddying Obama s Mideast plans By Julian Pecquet 10 27 13 12 00 PM ET Saudi Arabia s ire at the United States risks complicating President Obama s second term agenda across the Middle East Saudi officials over the past few days have decried U S policy in the region as dithering and refused to take a United Nations Security Council seat in protest The backlash risks setting the two peculiar allies on a collision course on a range of issues that involve Egypt Syria and Iran The Saudis change of strategy was precipitated by Obama s decision last month to call off military strikes against Syria and instead throw in his lot behind a Russia backed effort to have Syrian President Bashar Assad turn his chemical weapons over to the international community The Saudis want Assad deposed in large part because he is allied with their regional rival Iran In response to Obama s move Saudi Arabia took the highly unusual step of turning down a two year stint on the U N Security Council a 
decision the former director of Saudi intelligence said was based on the ineffectual experience of that body The current charade of international control over Bashar s chemical arsenal would be funny if it were not so blatantly perfidious Prince Turki al Faisal a member of the Saudi royal family said during a speech in Washington last week and designed not only to give Mr Obama an opportunity to back down but also to help Assad to butcher his people The administration has sought to play down the rift I am convinced we are on the same page as we are proceeding forward and look forward to working very closely with our Saudi friends and allies Secretary of State John Kerry said last week following reports that the Saudi intelligence chief Prince Bandar bin Sultan had warned Euro pean diplomats of a major shift away from the United States I have great confidence the United States and Saudi Arabia will continue to be close friends and allies as we have been The heated comments come as Saudi Arabia is also 
increasingly disturbed by Obama s desire to reach a deal with Iran on its nuclear program The Saudis fear that closer U S Iran ties could leave them out in the cold They are also upset that the administration could be neglecting the issue of the Islamic Republic s support of militants across the region in order to focus on the nuclear question If these elements are not part of a nuclear deal there s going to be a lot more reassurances that the Saudis need which at the core of it requires time and effort let alone concessions to the Saudis said David Weinberg a Saudi Arabia expert at the Foundation for Defense of Democracies Despite the recent spats Saudi watchers don t believe the relationship with the United States is doomed They point out that relations between the two countries have hit bigger bumps before notably during the OPEC oil embargo 40 years ago this month and after the Sept 11 2001 terror attacks in which 15 of the 19 hijackers were Saudis They also point out that Saudi Arabia needs U S support 
to counter both Iran and Islamist militants who want to overthrow the monarchy Still not everyone is so sanguine Some believe that the Saudis could considerably complicate U S policy in the region It s certainly a headache in terms of what the administration is trying to achieve in the region said Weinberg The question is will it force administration officials to reevaluate any of their fundamental strategies for countries like Syria or Iran Weinberg said Will it undermine their ability to walk and chew gum in terms of being able to pursue these diplomatic regional priorities without having to consume too much time reassuring American allies Tensions that had been bubbling for months broke into the open in July following the Egyptian military s ouster of President Mohammed Morsi As the White House debated freezing U S aid Saudi Arabia a bitter foe of Morsi s Muslim Brotherhood stepped in with a promise to provide 5 billion to the new military government Concerning those who announced stopping their 
assistance to Egypt or are threatening to stop it al Faisal the foreign minister said in August the Arab and Islamic nation is rich with its people and capabilities and will provide a helping hand to Egypt The Saudi support has made it more difficult for the Obama administration to push for its preferred goal of having a democratic Egypt emerge from the current chaos In Syria as well the Saudis are now erecting de facto barriers to U S goals Late last month the country played a behind the scenes role in uniting Islamist rebels opposed to al Qaeda under the banner of a new group called the Army of Islam Saudi tribal figures have been making calls on behalf of Saudi intelligence the commander of an Islamist rebel unit in the Damascus area told Reuters Their strategy is to offer financial backing in return for loyalty and staying away from al Qaeda The ramped up Saudi support could spell trouble for a U S supported peace conference that Kerry hopes to host next month in Geneva The Obama administration has 
consistently advocated for a political not a military solution for the civil war that has been raging since March 2011 One way the U S Saudi rift is manifesting itself on the Syrian portfolio is the Saudis are throwing their lot in with hard line Islamists Weinberg said There s been some talk over the past year that the Saudis were really admirable in their approach to Syria and much better than the Qataris for instance in that they preferred the least Islamist components of the rebellion And in the last month or so in particular we re seeing this go out the window Read more http thehill com blogs global affairs middle east north africa 330801 shift on syria angers saudi arabia muddying obamas mideast plans ixzz2j6ww0JbH Follow us thehill on Twitter TheHill on Facebook 
79	c	Obama and Syria Stumbling Toward Damascus The President s uneven Syria response has damaged his office and weakened the nation It s time for one more pivot By Joe Klein JoeKleinTIMESept 11 2013749 Comments inShare25 Read Later President Barack Obama walks along the colonnade of the White House from the residence to the Oval Office to start his day on September 10 2013 in Washington Kristoffer Tripplaar CNP AdMedia Sipa USA President Barack Obama walks along the colonnade of the White House from the residence to the Oval Office to start his day on September 10 2013 in Washington Email Print Share Comment Follow TIMEPolitics On the eve of the 12th anniversary of the 9 11 terrorist attacks Barack Obama made the strongest possible case for the use of force against Bashar Assad s Syrian regime But it wasn t a very strong case Indeed it was built on a false premise We can stop children from being gassed to death he said after he summoned grisly images of kids writhing and foaming at the mouth and then dying on 
hospital floors Does he really think we can do that with a limited military strike or the rather tenuous course of diplomacy now being pursued We might not be able to do it even if we sent in 250 000 troops and got rid of Assad The gas could be transferred to terrorists most likely Hizballah before we would find all or even most of it And that is the essence of the policy problem Obama has been wrestling with on Syria when you explore the possibilities for intervention any vaguely plausible action quickly reaches a dead end The President knows this which makes his words and gestures during the weeks leading up to his Syria speech all the more perplexing He willingly jumped into a bear trap of his own creation In the process he has damaged his presidency and weakened the nation s standing in the world It has been one of the more stunning and inexplicable displays of presidential incompetence that I ve ever witnessed The failure cuts straight to the heart of a perpetual criticism of the Obama White House that 
the President thinks he can do foreign policy all by his lonesome This has been the most closely held American foreign policy making process since Nixon and Kissinger only there s no Kissinger There is no éminence grise think of someone like Brent Scowcroft who can say to Obama with real power and credibility Mr President you re doing the wrong thing here Let s consider the consequences if you call the use of chemical weapons a red line Or Mr President how can you talk about this being the world s red line if the world isn t willing to take action Perhaps those questions and many others fell through the cracks as his first term national security staff departed and a new team came in But Obama has shown a desire to have national security advisers who were honest brokers people who relayed information to him rather than global strategists In this case his new staff apparently raised the important questions about going to Congress for a vote Do you really want to do this for a limited strike What if they say no 
But the President ignored them which probably means that the staff isn t strong enough MORE In Prime Time Obama Struggles to Reason With Nation Over Syria The public presentation of his policies has been left to the likes of Secretary of State John Kerry whose statements had to be refuted twice by the President in the Syria speech Kerry had said there might be a need for boots on the ground in Syria Obama No boots Kerry had said the military strikes would be unbelievably small Obama We don t do pinpricks Worst of all Kerry bumbled into prematurely mentioning a not very convincing Russian plan to get rid of the Syrian chemical weapons This had been under private discussion for months apparently the sort of dither that bad guys Saddam the Iranians Assad always use as a delaying tactic Kerry in bellicose mode seemed to be making fun of the idea and the Russians called him on it Kerry s staff tried to walk back this megagaffe calling it a rhetorical exercise As it stands no one will be surprised if the offer is 
a ruse but the Administration is now trapped into seeing it through and gambling that it will be easier to get a congressional vote if it fails Which gets close to the Obama Administration s problem there have been too many rhetorical exercises too many loose pronouncements of American intent without having game planned the consequences This persistent problem remember the President s needless and dangerous assertion that his policy wasn t the containment of the Iranian nuclear program has metastasized into a flurry of malarkey about Syria It s been two years since he said Assad must step aside He announced the red line and the world s red line And now We can stop children from being gassed The Chinese believe that the strongest person in the room says the least The President is the strongest person militarily in the world He does not have to broadcast his intentions He should convey them privately wait for a response then take action or not He should do what the Israelis did when they took out the Syrian 
nuclear reactor they did it without advance bluster and didn t even claim credit for it afterward The wolf doesn t have to cry wolf nor should the American eagle We must stand for restrained moral power power that is absolutely lethal and purposeful when it is unleashed but never unleashed wantonly without a precise plan or purpose Creating a precise plan in the Middle East is utterly impossible which is something the American people have clearly come to realize The region is at a hinge of history those straight line borders drawn by the Europeans nearly 100 years ago seem to have passed their sell by date The next decades may see the formation of new countries like Kurdistan along ethnic and sectarian lines and the process will undoubtedly be bloody Some version of Syria will probably emerge there s always been a Syria but perhaps not within the current borders The West will have to stand aside as this is worked out We have slashed our way into these places under the neocolonial assumption that they are 
somehow in need of our wisdom and power and left too much chaos and too many dead bodies in our wake to have any moral credibility left in the region except perhaps in Israel And you have to wonder if after the past few weeks the Israelis would trust us to provide the security for the peace that Kerry is trying to negotiate with the Palestinians Once again the President understands all this The subtext of his presidency has been that it is no longer possible for the U S to go it alone even if he continues to do so himself unless we face a direct and immediate threat to our national security and that we must build multilateral coalitions to enforce the world s red lines And so the question must be asked Why has he persisted in pursuing a limited military option in Syria These things almost never work Often they make the situation worse Ryan Crocker the retired American diplomat with the most experience in the region has speculated that Assad s diabolical response to an American military strike might be to 
launch another chemical attack just as a stick in our eye And then our next move Could the President let another gas attack stand MORE How to Destroy Syria s Chemical Weapons The President isn t crass or stupid enough to say it but I would guess that he is persisting in his public threats of military action because American credibility and more precisely his credibility really is at stake But playing the American credibility card is a foolish and extremely dangerous game In my lifetime more lives including American lives have been lost in the pursuit of American credibility than by any legitimate military factor It was what led Lyndon Johnson to double down in Vietnam It was what helped propel George W Bush into pulling the trigger in Iraq even after it was clear that most of the world and quietly the American military thought it would be a disastrous exercise It was what led Obama deeper into Afghanistan Make no mistake Obama has already lost credibility in the world given his performance of the past few 
weeks But American credibility is easily resurrected given our overwhelming strength by prudent action the next time a crisis erupts a clear strategic vision and a rock steady hand on the wheel It was resurrected by Ronald Reagan in the 1980s The sad thing is that Obama had been rebuilding our international stature after George W Bush s unilateral thrashing about He has now damaged his ability to get his way with the Chinese the Iranians and even the Israelis That may never come back and there were real opportunities to make some progress especially with Iran where the ascension of a nonprovocative President Hassan Rouhani and a reform minded Foreign Minister in Mohammed Javad Zarif had opened the possibility of real progress in the nuclear talks and maybe even in other areas like Afghanistan The question now is whether Obama s inability to make his military threat in Syria real and the American people s clear distaste for more military action will empower the hard liners in the Revolutionary Guards Corps to 
give no quarter in the negotiations The Chinese who have been covetous of the South China Sea oil fields may not be as restrained as they have been in the past The Japanese may feel the need to revive their military or even go nuclear now that the promise of American protection seems less reliable The consequences of Obama s amateur display ripple out across the world There are domestic consequences as well This was supposed to be the month when the nation s serious fiscal and budgetary problems were hashed out or not with the Republicans There was a chance that a coalition could be built to back a compromise to solve the debt ceiling problem and the quiet horrors caused by sequestration and to finally achieve a long term budget compromise But any deal would have required intense single minded negotiation including political protection or sweeteners for those Republicans who crossed the line Precious time has been wasted And after Syria it will be difficult for any member of Congress to believe that this 
President will stick to his guns or provide protection There are those who say Obama has destroyed his presidency It may be true but I doubt it All sorts of things could happen to turn the tide back in his favor The snap polls after the Syria speech indicate that he still has the ability to sell an argument however briefly He has been lucky in his opponents the Republicans will doubtless continue to take positions that most Americans find foolish or extreme Obamacare may prove a success He may make crisp decisions in the next overseas crisis one would hope he s learned something from this one But he has done himself and the nation great and unnecessary harm The road back to credibility and respect will be extremely difficult Read more http swampland time com 2013 09 11 obama and syria stumbling toward damascus ixzz2j6xFj6AH 
80	c	FULL TRANSCRIPT President Obama s Sept 10 speech on Syria By Washington Post Staff September 10 2013 President Obama delivered the following remarks making the case for a military strike against the Syrian government on Sept 10 2013 at the White House PRESIDENT OBAMA My fellow Americans tonight I want to talk to you about Syria why it matters and where we go from here Over the past two years what began as a series of peaceful protests against the repressive regime of Bashar al Assad has turned into a brutal civil war Over 100 000 people have been killed Millions have fled the country In that time America has worked with allies to provide humanitarian support to help the moderate opposition and to shape a political settlement But I have resisted calls for military action because we cannot resolve someone else s civil war through force particularly after a decade of war in Iraq and Afghanistan The situation profoundly changed though on August 21st when Assad s government gassed to death over a thousand people 
including hundreds of children The images from this massacre are sickening Men women children lying in rows killed by poison gas Others foaming at the mouth gasping for breath A father clutching his dead children imploring them to get up and walk On that terrible night the world saw in gruesome detail the terrible nature of chemical weapons and why the overwhelming majority of humanity has declared them off limits a crime against humanity and a violation of the laws of war This was not always the case In World War I American GIs were among the many thousands killed by deadly gas in the trenches of Europe In World War II the Nazis used gas to inflict the horror of the Holocaust Because these weapons can kill on a mass scale with no distinction between soldier and infant the civilized world has spent a century working to ban them And in 1997 the United States Senate overwhelmingly approved an international agreement prohibiting the use of chemical weapons now joined by 189 governments that represent 98 percent 
of humanity On August 21st these basic rules were violated along with our sense of common humanity No one disputes that chemical weapons were used in Syria The world saw thousands of videos cell phone pictures and social media accounts from the attack and humanitarian organizations told stories of hospitals packed with people who had symptoms of poison gas Moreover we know the Assad regime was responsible In the days leading up to August 21st we know that Assad s chemical weapons personnel prepared for an attack near an area where they mix sarin gas They distributed gasmasks to their troops Then they fired rockets from a regime controlled area into 11 neighborhoods that the regime has been trying to wipe clear of opposition forces Shortly after those rockets landed the gas spread and hospitals filled with the dying and the wounded We know senior figures in Assad s military machine reviewed the results of the attack and the regime increased their shelling of the same neighborhoods in the days that followed We 
ve also studied samples of blood and hair from people at the site that tested positive for sarin When dictators commit atrocities they depend upon the world to look the other way until those horrifying pictures fade from memory But these things happened The facts cannot be denied The question now is what the United States of America and the international community is prepared to do about it Because what happened to those people to those children is not only a violation of international law it s also a danger to our security Let me explain why If we fail to act the Assad regime will see no reason to stop using chemical weapons As the ban against these weapons erodes other tyrants will have no reason to think twice about acquiring poison gas and using them Over time our troops would again face the prospect of chemical warfare on the battlefield And it could be easier for terrorist organizations to obtain these weapons and to use them to attack civilians If fighting spills beyond Syria s borders these weapons 
could threaten allies like Turkey Jordan and Israel And a failure to stand against the use of chemical weapons would weaken prohibitions against other weapons of mass destruction and embolden Assad s ally Iran which must decide whether to ignore international law by building a nuclear weapon or to take a more peaceful path This is not a world we should accept This is what s at stake And that is why after careful deliberation I determined that it is in the national security interests of the United States to respond to the Assad regime s use of chemical weapons through a targeted military strike The purpose of this strike would be to deter Assad from using chemical weapons to degrade his regime s ability to use them and to make clear to the world that we will not tolerate their use That s my judgment as Commander in Chief But I m also the President of the world s oldest constitutional democracy So even though I possess the authority to order military strikes I believed it was right in the absence of a direct 
or imminent threat to our security to take this debate to Congress I believe our democracy is stronger when the President acts with the support of Congress And I believe that America acts more effectively abroad when we stand together This is especially true after a decade that put more and more war making power in the hands of the President and more and more burdens on the shoulders of our troops while sidelining the people s representatives from the critical decisions about when we use force Now I know that after the terrible toll of Iraq and Afghanistan the idea of any military action no matter how limited is not going to be popular After all I ve spent four and a half years working to end wars not to start them Our troops are out of Iraq Our troops are coming home from Afghanistan And I know Americans want all of us in Washington especially me to concentrate on the task of building our nation here at home putting people back to work educating our kids growing our middle class It s no wonder then that you 
re asking hard questions So let me answer some of the most important questions that I ve heard from members of Congress and that I ve read in letters that you ve sent to me First many of you have asked won t this put us on a slippery slope to another war One man wrote to me that we are still recovering from our involvement in Iraq A veteran put it more bluntly This nation is sick and tired of war My answer is simple I will not put American boots on the ground in Syria I will not pursue an open ended action like Iraq or Afghanistan I will not pursue a prolonged air campaign like Libya or Kosovo This would be a targeted strike to achieve a clear objective deterring the use of chemical weapons and degrading Assad s capabilities Others have asked whether it s worth acting if we don t take out Assad As some members of Congress have said there s no point in simply doing a pinprick strike in Syria Let me make something clear The United States military doesn t do pinpricks Even a limited strike will send a message 
to Assad that no other nation can deliver I don t think we should remove another dictator with force we learned from Iraq that doing so makes us responsible for all that comes next But a targeted strike can make Assad or any other dictator think twice before using chemical weapons Other questions involve the dangers of retaliation We don t dismiss any threats but the Assad regime does not have the ability to seriously threaten our military Any other retaliation they might seek is in line with threats that we face every day Neither Assad nor his allies have any interest in escalation that would lead to his demise And our ally Israel can defend itself with overwhelming force as well as the unshakeable support of the United States of America Many of you have asked a broader question Why should we get involved at all in a place that s so complicated and where as one person wrote to me those who come after Assad may be enemies of human rights It s true that some of Assad s opponents are extremists But al Qaeda 
will only draw strength in a more chaotic Syria if people there see the world doing nothing to prevent innocent civilians from being gassed to death The majority of the Syrian people and the Syrian opposition we work with just want to live in peace with dignity and freedom And the day after any military action we would redouble our efforts to achieve a political solution that strengthens those who reject the forces of tyranny and extremism Finally many of you have asked Why not leave this to other countries or seek solutions short of force As several people wrote to me We should not be the world s policeman I agree and I have a deeply held preference for peaceful solutions Over the last two years my administration has tried diplomacy and sanctions warning and negotiations but chemical weapons were still used by the Assad regime However over the last few days we ve seen some encouraging signs In part because of the credible threat of U S military action as well as constructive talks that I had with President 
Putin the Russian government has indicated a willingness to join with the international community in pushing Assad to give up his chemical weapons The Assad regime has now admitted that it has these weapons and even said they d join the Chemical Weapons Convention which prohibits their use It s too early to tell whether this offer will succeed and any agreement must verify that the Assad regime keeps its commitments But this initiative has the potential to remove the threat of chemical weapons without the use of force particularly because Russia is one of Assad s strongest allies I have therefore asked the leaders of Congress to postpone a vote to authorize the use of force while we pursue this diplomatic path I m sending Secretary of State John Kerry to meet his Russian counterpart on Thursday and I will continue my own discussions with President Putin I ve spoken to the leaders of two of our closest allies France and the United Kingdom and we will work together in consultation with Russia and China to put 
forward a resolution at the U N Security Council requiring Assad to give up his chemical weapons and to ultimately destroy them under international control We ll also give U N inspectors the opportunity to report their findings about what happened on August 21st And we will continue to rally support from allies from Europe to the Americas from Asia to the Middle East who agree on the need for action Meanwhile I ve ordered our military to maintain their current posture to keep the pressure on Assad and to be in a position to respond if diplomacy fails And tonight I give thanks again to our military and their families for their incredible strength and sacrifices My fellow Americans for nearly seven decades the United States has been the anchor of global security This has meant doing more than forging international agreements it has meant enforcing them The burdens of leadership are often heavy but the world is a better place because we have borne them And so to my friends on the right I ask you to reconcile 
your commitment to America s military might with a failure to act when a cause is so plainly just To my friends on the left I ask you to reconcile your belief in freedom and dignity for all people with those images of children writhing in pain and going still on a cold hospital floor For sometimes resolutions and statements of condemnation are simply not enough Indeed I d ask every member of Congress and those of you watching at home tonight to view those videos of the attack and then ask What kind of world will we live in if the United States of America sees a dictator brazenly violate international law with poison gas and we choose to look the other way Franklin Roosevelt once said Our national determination to keep free of foreign wars and foreign entanglements cannot prevent us from feeling deep concern when ideals and principles that we have cherished are challenged Our ideals and principles as well as our national security are at stake in Syria along with our leadership of a world where we seek to 
ensure that the worst weapons will never be used America is not the world s policeman Terrible things happen across the globe and it is beyond our means to right every wrong But when with modest effort and risk we can stop children from being gassed to death and thereby make our own children safer over the long run I believe we should act That s what makes America different That s what makes us exceptional With humility but with resolve let us never lose sight of that essential truth Thank you God bless you And God bless the United States of America 
81	c	Here s real reason Obama threatened Syria Mideast expert President bringing charm offensive to major terror sponsor Published 10 02 2013 at 9 02 PM Printer Friendly Text smaller Text bigger 317 Longtime Middle East expert Dr Mike Evans says Iran is not directing a charm offensive at President Obama but the U S is deliberately making nice with Iran in a move that will only reassure the mullahs there that the U S has no intention of doing anything to prevent them from acquiring nuclear weapons New Iranian President Hassan Rouhani uses much less incendiary language than former President Mahmoud Ahmadinejad The change in tone even prompted the Obama administration to seek a public handshake at the opening of the United Nations General Assembly last week The Iranians refused but Obama subsequently spoke by phone with Rouhani Most experts see all this as an effort by the Iranians to convince the U S and other critics of its nuclear program that it has no intention of developing nuclear weapons and get the Western 
powers to lift crippling economic sanctions Evans told WND that conventional wisdom is wrong I don t really see this as a charm offensive by Iran I see this is a charm offensive by President Obama said Evans a longtime personal friend of Israeli Prime Minister Benjamin Netanyahu He believes Obama never had any intention of attacking Syria over allegedly using chemical weapons Showdown with Nuclear Iran by Mike Evans is a terrifying examination of how Iran s leaders believe they re on a divine mission to usher in the apocalypse and thereby herald the second coming of a Shia Muslim messiah Obama did this to try to send a signal to Iran that he was serious and also to send a signal to Netanyahu to get Netanyahu to back down from attacking Iran He knew Netanyahu was ready He knew the window was open and he didn t want him to do it So this was more saber rattling for Iran and Netanyahu s benefit than for the Syrians he said President Obama s not going to do a thing against Iran Evans said He s going to let Iran 
go nuclear The reason he s going to let them is he wants a quid pro quo He wants to be an anti war president He wants to get out of the Middle East In order to do it he needs the world s largest terrorist organization to give him a free pass to quit attacking in Iraq to quit attacking in Afghanistan and back off a little bit That s what he s going for Nothing else He could care less if Iran goes nuclear or not Evans met with Iranian diplomats last week while they were at the United Nations Those diplomats reportedly divulged a two headed diplomatic strategy of the Iranian regime They said they propose to the president that they be the intermediary between the U S and Syria I m thinking What did you just say You want to be the mediator to solve the Syrian crisis That s the fox solving the crisis with the chickens That s No 1 Evans said No 2 they told me that they anticipate to begin negotiations over the nuclear program in three to six months with Obama Well why three to six months In three to six months Iran 
will pass the threshold if they continue with enough enriched uranium to build a bomb he said Their plan is to stall and delay this thing and try to checkmate Netanyahu so he can t do anything Listen it worked Evans said the softer diplomatic tone from Iran succeeded in attracting all the coverage at the U N and Netanyahu s address was ignored by the media as a result of the Iran angle and coverage of the partial government shutdown in the U S He also said Israeli officials are very concerned about America s apparent unwillingness to confront Iran and intent to prevent an Israeli strike They re absolutely horrified he said I just came from Israel They know what s going on They know the game they re playing They re not going to have a partner with Obama Obama s got his eyes on being a historical anti war president He s never going to confront Iran It s just never going to happen Period Evans added Israel is alone right now They re alone and they know it Netanyahu knows it too I don t believe Netanyahu s 
message that he gave at the U N was to the Iranians to the world or to the American people I believe his message was to the Israelis He was speaking to his base supporting and strengthening his base for what he s staring squarely in the face and knows he has to do Much attention was given to last week s offer from the Obama administration for a public handshake between Obama and Rouhani at the U N Iran rejected the offer and Obama later called Rouhani for a brief phone conversation Evans said those gestures were very telling and emboldened Iranian leaders What they saw is the president bowing down By the president calling Iran s new president the former head of security that was over the Khobar Towers bombing and the Buenos Aires Jewish Community Center bombing by calling this mad mullah and congratulating him on his fraudulent election and all the other stuff he said was basically showing the weakness of the American president and empowering them and letting them know they have another Jimmy Carter in 
office on steroids Evans said When Jimmy Carter was in office he wire transferred 7 9 billion from the Federal Reserve with 20 cooperating banks to the Bank of England to buy back the hostages So why should Iran do anything except be rewarded for misbehavior That s what happened the last time he said Read more at http www wnd com 2013 10 heres real reason obama threatened syria QuBLjuwgbeKlfsRm 99 
82	c	Former Defense Secretaries Criticize Obama on Syria By THOM SHANKER and LAUREN D AVOLIO Published September 18 2013 180 Comments Facebook Twitter Google Save E mail Share Print Reprints WASHINGTON President Obama s first two defense secretaries publicly questioned the administration s handling of the Syrian crisis on Tuesday night and expressed skepticism about whether Russia can broker a deal to remove Syria s chemical weapons Enlarge This Image Alex Brandon Associated Press Robert M Gates resigned in 2011 Multimedia Think Back America and Isolationism Interactive Feature Syria News Quiz Related Russia Calls U N Chemical Report on Syria Biased September 19 2013 Extremists Take Syrian Town Near Turkey Border September 19 2013 The Lede Assad s Office Promotes Fox News Interview With Kucinich September 18 2013 World Twitter Logo Connect With Us on Twitter Follow nytimesworld for international breaking news and headlines Twitter List Reporters and Editors Enlarge This Image Mandel Ngan Agence France Presse 
Getty Images Leon E Panetta left this year Readers Comments Readers shared their thoughts on this article Read All Comments 180 In a joint appearance in Dallas both former Pentagon chiefs Robert M Gates and Leon E Panetta were critical of Mr Obama for asking Congress to authorize the use of force against Syria in retaliation over its use of chemical weapons But they disagreed on whether military action would be an effective response Mr Gates said Mr Obama s proposed military strike was a mistake while Mr Panetta said it was a mistake not to carry out an attack My bottom line is that I believe that to blow a bunch of stuff up over a couple days to underscore or validate a point or a principle is not a strategy Mr Gates said during a forum at Southern Methodist University If we launch a military attack in the eyes of a lot of people we become the villain instead of Assad he added referring to President Bashar al Assad of Syria Mr Gates the only cabinet member from the administration of George W Bush whom Mr 
Obama asked to stay said missile strikes on Syria would be throwing gasoline on a very complex fire in the Middle East Haven t Iraq Afghanistan and Libya taught us something about the unintended consequences of military action once it s launched Mr Gates said Mr Panetta also speaking at the forum said the president should have kept his word after he had pledged action if Syria used chemical weapons When the president of the United States draws a red line the credibility of this country is dependent on him backing up his word Mr Panetta said Once the president came to that conclusion then he should have directed limited action going after Assad to make very clear to the world that when we draw a line and we give our word then we back it up Mr Panetta said Mr Gates and Mr Panetta made their most extensive comments on current national security policy and certainly their most critical statements on policies of the administration they both served since leaving public service Both former secretaries have announced 
plans to publish memoirs expected to shine more light on the internal policy debates of their tenures Neither expressed a specific reason for breaking their silence on the Obama administration s decisions Tuesday night Asked about the comments at a news conference on Wednesday the current defense secretary Chuck Hagel said he had the greatest respect for his two predecessors but added Obviously I don t agree with their perspectives Another former high ranking Obama administration official Michael J Morell who recently retired as the deputy director of the C I A also expressed skepticism about the negotiations brokered by Russia I think this is the Syrians playing for time Mr Morell told Foreign Policy magazine in an interview published Tuesday on its Web site I do not believe that they would seriously consider giving up their chemical weapons Mr Gates said he doubted whether President Vladimir V Putin of Russia was sincere in his efforts to broker a deal and said he was skeptical that the Syrian government 
would disarm He said it was absurd that Syria needed days or weeks to identify the location and size of its chemical weapons arsenal and he suggested that the timetable should be an ultimatum of 48 hours When asked whether the West should trust Mr Putin Mr Gates said Are you kidding me He advocated identifying credible partners within the Syrian opposition and increasing support including weapons but not surface to air missiles which could be seized by militants for terrorist acts against civilian aviation He also supported a strategy of sanctions that labeled members of the Assad government as war criminals with the threat of arrest if they left Syria and suggested sanctions on Assad family members living or studying overseas including on their financial holdings Such pressure might prompt some in the inner circle to negotiate an end to the civil war Mr Gates said Although Mr Gates said that any unilateral military action against Syria would be a mistake he also said it was unwise for the president to have 
sought Congressional authorization to use force because of the risk to presidential prestige if he was rebuffed If Congress voted no it would weaken him Mr Gates said It would weaken our country It would weaken us in the eyes of our allies as well as our adversaries around the world Under questioning from the moderator David Gergen who advised four presidents and is now on the faculty at the Kennedy School of Government at Harvard both former secretaries said that American credibility on Syria was essential to enduring efforts to prevent Iran from building nuclear weapons Iran is paying very close attention to what we re doing Mr Panetta said There s no question in my mind they re looking at the situation and what they are seeing right now is an element of weakness Mr Panetta said that the president has to retain the responsibility and the authority on this issue and that it was wrong to subcontract the decision to Congress Mr President this Congress has a hard time agreeing as to what the time of day is he 
said 
83	c	President Obama told the nation Tuesday he is exploring a Russian diplomatic plan to end a chemical weapons dispute in Syria but reserves the right to take military action Obama spoke to the country about why Syria matters and where the nation goes from here Among the key questions he attempted to answer 1 What s going on with Russia The Russian government has indicated a willingness to join with the international community in pushing Assad to give up his chemical weapons The Assad regime has now admitted that it has these weapons and even said they d join the chemical weapons convention which prohibits their use 2 Are we going to strike Syria I determined that it is in the national security interests of the United States to respond to the Assad regime s use of chemical weapons through a targeted military strike The purpose of this strike would be to deter Assad from using chemical weapons to degrade his regime s ability to use them and to make clear to the world that we will not tolerate their use That s my 
judgment as commander in chief Meanwhile I ve ordered our military to maintain their current posture to keep the pressure on Assad and to be in a position to respond if diplomacy fails ANALYSIS Did Obama set foreign policy via gaffes CRISIS IN SYRIA Strikes averted for now 3 Is there proof chemical weapons were used in Syria No one disputes that chemical weapons were used in Syria The world saw thousands of videos cellphone pictures and social media accounts from the attack And humanitarian organizations told stories of hospitals packed with people who had symptoms of poison gas 4 Will we put American boots on the ground in Syria I will not put American boots on the ground in Syria I will not pursue an open ended action like Iraq or Afghanistan I will not pursue a prolonged air campaign like Libya or Kosovo This would be a targeted strike to achieve a clear objective deterring the use of chemical weapons and degrading Assad s capabilities 5 Is the strike worth it if we don t take out Assad Others have asked 
whether it s worth acting if we don t take out Assad As some members of Congress have said there s no point in simply doing a pinprick strike in Syria Let me make something clear The United States military doesn t do pinpricks 6 Why are we getting involved in a civil war at all It s true that some of Assad s opponents are extremists But al Qaida will only draw strength in a more chaotic Syria if people there see the world doing nothing to prevent innocent civilians from being gassed to death The majority of the Syrian people and the Syrian opposition we work with just want to live in peace with dignity and freedom And the day after any military action we would redouble our efforts to achieve a political solution that strengthens those who reject the forces of tyranny and extremism 7 Why do we have to be the world s policeman America is not the world s policeman Terrible things happen across the globe and it is beyond our means to right every wrong but when with modest effort and risk we can stop children 
from being gassed to death and thereby make our own children safer over the long run I believe we should act INSIDE STORY How the Syria solution developed 8 What about our allies I ve spoken to the leaders of two of our closest allies France and the United Kingdom and we will work together in consultation with Russia and China to put forward a resolution at the U N Security Council requiring Assad to give up his chemical weapons and to ultimately destroy them under international control 9 How is the U N going to be involved We ll also give U N inspectors the opportunity to report their findings about what happened on August 21st and we will continue to rally support from allies from Europe to the Americas from Asia to the Middle East who agree on the need for action 10 So what about Congress I d ask every member of Congress and those of you watching at home tonight to view those videos of the attack and then ask what kind of world will we live in if the United States of America sees a dictator brazenly 
violate international law with poison gas and we choose to look the other way 
84	c	Syria crisis Barack Obama to give Russia s chemical weapons plan a chance states reasons for military action Updated Fri 4 Oct 2013 1 28pm AEST Video Barack Obama addresses the US on the Syria crisis ABC News Photo Barack Obama says a Congress vote will be delayed while a diplomatic solution is sought AFP Evan Vucci Related Story Russia plan on Syria was no accident US official Related Story Obama says Russia s proposal on Syria s chemical weapons potentially positive Map Syrian Arab Republic United States president Barack Obama has vowed to pursue a diplomatic initiative from Russia over Syria s chemical weapons use but voiced scepticism about it and urged Americans to support his threat to use military force Mr Obama used a nationally televised address to Americans to denounce last month s gas attack which the US says killed 1 400 Syrians describing it as sickening and a danger to national security He stated his case for military action saying president Bashar al Assad must be brought to account or he will 
use chemical weapons again When dictators commit atrocities they depend on the world to look the other way until those horrifying pictures fade from memory Mr Obama said The question now is what the United States of America and the international community is prepared to do about it Key points Barack Obama asks Congress to delay vote on military action in Syria Says plan to have Syria hand over its chemical weapons should be explored Says US military remains on standby as inaction is not an option Because what happened to those people to those children is not only a violation of international law it s also a danger to our security However Mr Obama said he had asked Congress to delay a vote on military action while a Russian initiative was explored further Analysis from The Drum We are seeing the diminished power of the US after Iraq Russia yesterday seized on an off the cuff comment from US secretary of state John Kerry that Syria place its chemical weapons stockpile under international control Russia said it 
would work with Syria to make this happen and Syria has already expressed a willingness to comply with the plan Mr Obama said he would discuss the initiative further with Russian president Vladimir Putin but expressed scepticism about whether it would succeed He said he would work with France Britain China and Russia on a United Nations resolution requiring Mr Assad to give up his chemical weapons Read what ABC News readers thought of Barack Obama s address advocating military action on Syria Any agreement must verify that the Assad regime keeps its commitments Mr Obama said But this initiative has the potential to remove the threat of chemical weapons without the use of force particularly because Russia is one of Assad s strongest allies How would Syria hand over its chemical weapons How might a plan to take Syria s chemical weapons out of president Bashar al Assad s hands actually work Europe correspondent Mary Gearin asked Hamish de Bretton Gordon a former commanding officer at the UK s Joint Chemical 
Biological Radiological and Nuclear Regiment The draft resolution gives Syria 15 days to make a complete declaration of its entire chemical arms program and demands UN weapons inspectors be given immediate access to all sites named in the declaration It would also demand inspectors be granted access to all chemical arms personnel records and equipment The resolution threatens Syria with further necessary measures in the event of non compliance In what amounted to the most explicit high level admission by Syria that it has chemical weapons foreign minister Walid al Moualem said earlier in a statement shown on Russian state television that Damascus was committed to the Russian initiative We want to join the convention on the prohibition of chemical weapons he said We are ready to observe our obligations in accordance with that convention including providing all information about these weapons We are ready to declare the location of the chemical weapons stop production of the chemical weapons and show these 
production facilities to representatives of Russia and other United Nations member states Obama says US inaction is not an option Mr Obama used much of his speech to lay out the case against Syria saying there was plenty of evidence showing the Syrian government was behind the deadly attack The US military doesn t do pinpricks Even a limited strike will send a message to Assad that no other nation can deliver US president Barack Obama He argued that Syria should face consequences for using such weapons because much of the world has long since adopted a ban on chemical weapons He said if the civilised world did nothing to respond it would only embolden US adversaries Mr Obama said he understood Americans were weary of costly conflicts abroad following the wars in Afghanistan and Iraq but he was of the belief that America could not sit back while chemical weapons were being used in flagrant violation of international law And amid confusion about the extent of any US military strike he said that Mr Assad would 
pay a heavy price if military action was used The US military doesn t do pinpricks he said Chemical weapons in Syria Syria is understood to have the third largest stockpile of chemical weapons in the world including sarin and other nerve gases Amid accusations by Syrian activists that forces loyal to president Bashar al Assad have used nerve gas to kill more than 200 people we look back over similar allegations made during the conflict Even a limited strike will send a message to Assad that no other nation can deliver Mr Obama said the Assad regime did not have the capability to seriously threaten the US military He repeated his earlier pledge that any military action would be limited in both its scope and duration and would not involve American boots on the ground He said he had ordered the US military to be ready to respond if diplomacy fails Mr Kerry and defence secretary Chuck Hagel told Congress earlier that the threat of military action was critical to forcing Mr Assad to bend on his chemical weapons 
For this diplomatic option to have a chance of succeeding the threat of a US military action the credible real threat of US military action must continue Mr Hagel told the House of Representatives Armed Services Committee Mr Obama said Mr Kerry would meet Russian foreign minister Sergei Lavrov in Geneva on Thursday for further talks  
